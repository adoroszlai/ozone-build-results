Attaching to ozonesecure-ha_om2_1, ozonesecure-ha_datanode1_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_scm3.org_1, ozonesecure-ha_scm1.org_1, ozonesecure-ha_om3_1, ozonesecure-ha_kms_1, ozonesecure-ha_kdc_1, ozonesecure-ha_om1_1, ozonesecure-ha_httpfs_1, ozonesecure-ha_recon_1, ozonesecure-ha_datanode2_1, ozonesecure-ha_s3g_1, ozonesecure-ha_datanode3_1
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2023-06-27 12:24:21,821 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = 5cbb6cac8588/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
datanode1_1  | STARTUP_MSG:   java = 11.0.19
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2023-06-27 12:24:21,157 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = 1b92348e430f/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
datanode2_1  | STARTUP_MSG:   java = 11.0.19
datanode2_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode2_1  | ************************************************************/
datanode2_1  | 2023-06-27 12:24:21,269 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-06-27 12:24:21,628 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2023-06-27 12:24:22,766 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2023-06-27 12:24:24,416 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2023-06-27 12:24:24,416 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2023-06-27 12:24:25,528 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1b92348e430f ip:172.25.0.103
datanode2_1  | 2023-06-27 12:24:32,023 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2023-06-27 12:24:33,497 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2023-06-27 12:24:33,500 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2023-06-27 12:24:35,923 [main] INFO reflections.Reflections: Reflections took 1572 ms to scan 2 urls, producing 106 keys and 230 values 
datanode2_1  | 2023-06-27 12:24:40,675 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2023-06-27 12:24:43,275 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode2_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode2_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode2_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode2_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode2_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode2_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode2_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode1_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode1_1  | ************************************************************/
datanode1_1  | 2023-06-27 12:24:21,928 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2023-06-27 12:24:22,407 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2023-06-27 12:24:23,560 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2023-06-27 12:24:25,347 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2023-06-27 12:24:25,348 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2023-06-27 12:24:27,328 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5cbb6cac8588 ip:172.25.0.102
datanode1_1  | 2023-06-27 12:24:32,414 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2023-06-27 12:24:33,972 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2023-06-27 12:24:33,975 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2023-06-27 12:24:35,696 [main] INFO reflections.Reflections: Reflections took 1172 ms to scan 2 urls, producing 106 keys and 230 values 
datanode1_1  | 2023-06-27 12:24:40,256 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2023-06-27 12:24:42,627 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode1_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode1_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode1_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode1_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode1_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode1_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode1_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode1_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode1_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode1_1  | 2023-06-27 12:24:42,643 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2023-06-27 12:24:42,657 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2023-06-27 12:24:42,660 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2023-06-27 12:24:54,524 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2023-06-27 12:24:54,789 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:5cbb6cac8588
datanode1_1  | 2023-06-27 12:24:54,790 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2023-06-27 12:24:54,848 [main] ERROR utils.CertificateSignRequest: Invalid domain 5cbb6cac8588
datanode1_1  | 2023-06-27 12:24:54,853 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@5cbb6cac8588
datanode1_1  | 2023-06-27 12:24:59,116 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 399146862779
datanode1_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode1_1  |            Start Date: Tue Jun 27 12:24:57 UTC 2023
datanode1_1  |            Final Date: Wed Jun 26 12:24:57 UTC 2024
datanode1_1  |             SubjectDN: CN=dn@5cbb6cac8588,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode1_1  |            Public Key: RSA Public Key [21:4e:e5:d9:3b:e2:5c:dd:17:0f:70:ad:1a:cf:92:96:f3:48:f6:b1],[56:66:d1:a4]
datanode1_1  |         modulus: a42517e55e13a2baf553cf1eb68ff242a732a10afb5d378329e74d95947260fb977b8790e18a2597f5a5575c924e01eeb9ca74fd95cce25bbacc577ff1a19c3b534ae1b0620da42fcc91062b8e0ff58d856cb1301abef7ec22802e600aaf7a4ac955fad4167c133e9298fe148db33a04741e6ce69594bbb79d0565f5042207461dcc2f4eefce5ebc11da9c85529d89153a4e80adb651baa4c20aec40c04399d163f3fd64a64eb5918d0457d7628fc20e45e677c334fbfbbe9752024f400633d0e0b1c70e209861baffa45e50ac2790804a5e01172fe3db2113d6a39d2a5c9f7401b9edb213e6d7b244415f3b11a21a502cdc72cb58ee7ac7fb259b92c91ba441
datanode1_1  | public exponent: 10001
kdc_1        | Jun 27 12:22:15 kdc krb5kdc[7](info): Loaded
kdc_1        | Jun 27 12:22:15 kdc krb5kdc[7](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jun 27 12:22:15 kdc krb5kdc[7](info): setting up network...
kdc_1        | Jun 27 12:22:15 kdc krb5kdc[7](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jun 27 12:22:15 kdc krb5kdc[7](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jun 27 12:22:15 kdc krb5kdc[7](info): set up 4 sockets
kdc_1        | Jun 27 12:22:15 kdc krb5kdc[7](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jun 27 12:22:18 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687868538, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:22:24 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.119: ISSUE: authtime 1687868544, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, httpfs/httpfs@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:22:24 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.114: ISSUE: authtime 1687868544, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, s3g/s3g@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:22:32 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.115: ISSUE: authtime 1687868552, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:22:48 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1687868568, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:22:54 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1687868574, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:23:01 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1687868568, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:23:02 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687868538, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:23:02 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.115: ISSUE: authtime 1687868552, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:23:15 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687868595, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:23:17 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1687868597, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:23:31 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687868595, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:23:33 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1687868597, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:23:36 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687868616, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:23:41 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1687868621, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:23:42 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1687868621, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:23:50 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687868616, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:23:52 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1687868632, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:23:55 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687868635, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:05 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1687868632, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:24:17 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687868635, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:24:27 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687868667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:33 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1687868673, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:33 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1687868673, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:34 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1687868674, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:38 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1687868678, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:41 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1687868681, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:42 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1687868682, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:24:44 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1687868678, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:24:46 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1687868681, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:24:49 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1687868682, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:24:55 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1687868674, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:24:56 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1687868673, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:24:59 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1687868673, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:25:36 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687868667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:25:39 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1687868673, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 27 12:25:40 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1687868673, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 27 12:25:41 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1687868674, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 27 12:25:42 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1687868742, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:25:42 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1687868742, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:25:48 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687868748, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:25:48 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1687868748, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 12:25:50 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1687868742, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:25:50 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1687868742, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:25:53 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1687868748, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:26:33 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687868748, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 12:26:38 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.115: ISSUE: authtime 1687868552, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for om/om@EXAMPLE.COM
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2023-06-27 12:24:21,630 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = 038f15546f77/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
datanode3_1  | STARTUP_MSG:   java = 11.0.19
datanode3_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode3_1  | ************************************************************/
datanode3_1  | 2023-06-27 12:24:21,749 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2023-06-27 12:24:22,191 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2023-06-27 12:24:23,389 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2023-06-27 12:24:25,019 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2023-06-27 12:24:25,020 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2023-06-27 12:24:26,750 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:038f15546f77 ip:172.25.0.104
datanode3_1  | 2023-06-27 12:24:32,702 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2023-06-27 12:24:34,374 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2023-06-27 12:24:34,374 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2023-06-27 12:24:36,120 [main] INFO reflections.Reflections: Reflections took 1245 ms to scan 2 urls, producing 106 keys and 230 values 
datanode3_1  | 2023-06-27 12:24:40,830 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode3_1  | 2023-06-27 12:24:43,762 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode3_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode3_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode3_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode3_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode3_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode3_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode3_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode3_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode3_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode3_1  | 2023-06-27 12:24:43,828 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2023-06-27 12:24:43,843 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2023-06-27 12:24:43,849 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2023-06-27 12:24:52,504 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2023-06-27 12:24:52,893 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:038f15546f77
datanode3_1  | 2023-06-27 12:24:52,895 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2023-06-27 12:24:52,947 [main] ERROR utils.CertificateSignRequest: Invalid domain 038f15546f77
datanode3_1  | 2023-06-27 12:24:52,956 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@038f15546f77
datanode3_1  | 2023-06-27 12:24:57,298 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 266035175110
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode3_1  |            Start Date: Tue Jun 27 12:22:44 UTC 2023
datanode3_1  |            Final Date: Fri Aug 04 12:22:44 UTC 2028
datanode3_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode3_1  |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
datanode3_1  |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
datanode3_1  |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
datanode3_1  |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
datanode3_1  |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
datanode3_1  |                        6575bb91718be2b62fe8285671670d4afc71639e
datanode3_1  |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
datanode3_1  |                        c63f624dd59419fb4e3d7f57549fb866400baea0
datanode3_1  |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
datanode3_1  |                        acbb5f8d8e5f75c3208e116728c66494080d0391
datanode3_1  |                        4883b896992d697c14fca81b51cc41815ceb8fc5
datanode3_1  |                        9e72ce2f778759033901bef92ec92fabd21c5a89
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 521235a18d68bba055ad12c9c90b2dcaf2173dee
datanode1_1  |                        6d66ef513f0efc78a5222751848136c05d6d4eb4
datanode1_1  |                        b2dccafe9127a9477872b710f515be8ac41539a9
datanode1_1  |                        b2836e960ed5e740a79be92f33fb9f37223e8aab
datanode1_1  |                        94f374f0a8d6dd5f4958ae291a6a8bc77d05e43d
datanode1_1  |                        6d88bf6a5540af3e1a65f4efcdfb73bef932078e
datanode1_1  |                        4e2c65de9f65fb677e831d66fce5621b6317074d
datanode1_1  |                        0399495530cd29be5271fcbbd86217f14674ae44
datanode1_1  |                        5edf57eecbb1a0cac7579f2b2d34335cc38197be
datanode1_1  |                        e3c46f2c4a0b60808cc4faa4ae65e8af71a17bd7
datanode1_1  |                        5ed66a6b2a9f2ac1f291ad3d8875e57c0fffd41b
datanode1_1  |                        5876e63c026f8a8fa169721d8fba3c8bd506c75a
datanode1_1  |                        cb8bca30dcc3a0cf24ab76438ba5cfae
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  | 
datanode1_1  |                        critical(true) KeyUsage: 0xb8
datanode1_1  |  from file: /data/metadata/dn/certs/399146862779.crt.
datanode1_1  | 2023-06-27 12:24:59,172 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 266035175110
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode1_1  |            Start Date: Tue Jun 27 12:22:44 UTC 2023
datanode1_1  |            Final Date: Fri Aug 04 12:22:44 UTC 2028
datanode1_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode1_1  |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
datanode1_1  |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
datanode1_1  |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
datanode1_1  |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
datanode1_1  |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
datanode1_1  |                        6575bb91718be2b62fe8285671670d4afc71639e
datanode1_1  |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
datanode1_1  |                        c63f624dd59419fb4e3d7f57549fb866400baea0
datanode1_1  |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
httpfs_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
httpfs_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
httpfs_1     | 2023-06-27 12:22:20,394 [main] INFO server.HttpFSServerWebServer: STARTUP_MSG: 
httpfs_1     | /************************************************************
httpfs_1     | STARTUP_MSG: Starting HttpFSServerWebServer
httpfs_1     | STARTUP_MSG:   host = httpfs/172.25.0.119
httpfs_1     | STARTUP_MSG:   args = []
httpfs_1     | STARTUP_MSG:   version = 3.3.5
httpfs_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/curator-client-4.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/json-simple-1.1.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/curator-framework-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.5.6.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/zookeeper-jute-3.5.6.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar
httpfs_1     | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 706d88266abcee09ed78fbaa0ad5f74d818ab0e9; compiled by 'stevel' on 2023-03-15T15:56Z
httpfs_1     | STARTUP_MSG:   java = 11.0.19
httpfs_1     | ************************************************************/
httpfs_1     | 2023-06-27 12:22:20,479 [main] INFO server.HttpFSServerWebServer: registered UNIX signal handlers for [TERM, HUP, INT]
httpfs_1     | 2023-06-27 12:22:21,716 [main] INFO util.log: Logging initialized @6398ms to org.eclipse.jetty.util.log.Slf4jLog
httpfs_1     | 2023-06-27 12:22:22,533 [main] INFO http.HttpRequestLog: Http request log for http.requests.webhdfs is not defined
httpfs_1     | 2023-06-27 12:22:22,572 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
httpfs_1     | 2023-06-27 12:22:22,608 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context webhdfs
httpfs_1     | 2023-06-27 12:22:22,608 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
httpfs_1     | 2023-06-27 12:22:22,608 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
httpfs_1     | 2023-06-27 12:22:22,873 [main] INFO http.HttpServer2: Jetty bound to port 14000
httpfs_1     | 2023-06-27 12:22:22,886 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
httpfs_1     | 2023-06-27 12:22:23,098 [main] INFO server.session: DefaultSessionIdManager workerName=node0
httpfs_1     | 2023-06-27 12:22:23,099 [main] INFO server.session: No SessionScavenger set, using defaults
httpfs_1     | 2023-06-27 12:22:23,100 [main] INFO server.session: node0 Scavenging every 600000ms
httpfs_1     | 2023-06-27 12:22:23,207 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2ea41516{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
httpfs_1     | 2023-06-27 12:22:23,209 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@ffaa6af{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
httpfs_1     | log4j:WARN No appenders could be found for logger (org.eclipse.jetty.webapp.WebAppClassLoader).
httpfs_1     | log4j:WARN Please initialize the log4j system properly.
httpfs_1     | log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
httpfs_1     | 12:22:23,798  WARN Server:474 - Log4j [/etc/hadoop/httpfs-log4j.properties] configuration file not found, using default configuration from classpath
httpfs_1     | 12:22:23,805  INFO Server:389 - ++++++++++++++++++++++++++++++++++++++++++++++++++++++
httpfs_1     | 12:22:23,805  INFO Server:390 - Server [httpfs] starting
httpfs_1     | 12:22:23,805  INFO Server:391 -   Built information:
httpfs_1     | 12:22:23,805  INFO Server:392 -     Version           : 1.4.0-SNAPSHOT
httpfs_1     | 12:22:23,806  INFO Server:394 -     Source Repository : REPO NOT AVAIL
httpfs_1     | 12:22:23,806  INFO Server:396 -     Source Revision   : REVISION NOT AVAIL
httpfs_1     | 12:22:23,809  INFO Server:398 -     Built by          : runner
httpfs_1     | 12:22:23,809  INFO Server:400 -     Built timestamp   : 2023-06-27T11:59:31+0000
httpfs_1     | 12:22:23,809  INFO Server:402 -   Runtime information:
httpfs_1     | 12:22:23,809  INFO Server:403 -     Home   dir: /opt/hadoop
httpfs_1     | 12:22:23,810  INFO Server:404 -     Config dir: /etc/hadoop
httpfs_1     | 12:22:23,810  INFO Server:405 -     Log    dir: /opt/hadoop/log
httpfs_1     | 12:22:23,810  INFO Server:406 -     Temp   dir: /opt/hadoop/temp
httpfs_1     | 12:22:24,007  INFO Server:544 - System property sets  httpfs.log.dir: /opt/hadoop/log
httpfs_1     | 12:22:24,007  INFO Server:544 - System property sets  httpfs.temp.dir: /opt/hadoop/temp
httpfs_1     | 12:22:24,007  INFO Server:544 - System property sets  httpfs.config.dir: /etc/hadoop
httpfs_1     | 12:22:24,008  INFO Server:544 - System property sets  httpfs.home.dir: /opt/hadoop
httpfs_1     | 12:22:24,070  INFO FileSystemAccessService:162 - Using FileSystemAccess JARs version [3.3.5]
httpfs_1     | 12:22:25,014  INFO UserGroupInformation:1132 - Login successful for user httpfs/httpfs@EXAMPLE.COM using keytab file httpfs.keytab. Keytab auto renewal enabled : false
httpfs_1     | 12:22:25,015  INFO FileSystemAccessService:191 - Using FileSystemAccess Kerberos authentication, principal [httpfs/httpfs@EXAMPLE.COM] keytab [/etc/security/keytabs/httpfs.keytab]
httpfs_1     | 12:22:25,141  INFO Server:413 - Services initialized
httpfs_1     | 12:22:25,141  INFO Server:423 - Server [httpfs] started!, status [NORMAL]
httpfs_1     | 12:22:25,142  INFO HttpFSServerWebApp:107 - Connects to Namenode [ofs://omservice]
httpfs_1     | 12:22:25,170  INFO HttpFSServerWebApp:126 - Initializing HttpFSServerMetrics
httpfs_1     | 12:22:25,411  INFO JvmPauseMonitor:188 - Starting JVM pause monitor
httpfs_1     | 12:22:26,001  INFO MetricsConfig:120 - Loaded properties from hadoop-metrics2.properties
httpfs_1     | 12:22:26,641  INFO MetricsSystemImpl:378 - Scheduled Metric snapshot period at 10 second(s).
httpfs_1     | 12:22:26,641  INFO MetricsSystemImpl:191 - HttpFSServer metrics system started
httpfs_1     | 12:22:26,826  INFO KerberosAuthenticationHandler:175 - Using keytab /etc/security/keytabs/httpfs.keytab, for principal HTTP/httpfs@EXAMPLE.COM
httpfs_1     | 12:22:26,860  INFO AbstractDelegationTokenSecretManager:411 - Updating the current master key for generating delegation tokens
httpfs_1     | 12:22:26,889  INFO AbstractDelegationTokenSecretManager:763 - Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
httpfs_1     | 12:22:26,890  INFO AbstractDelegationTokenSecretManager:411 - Updating the current master key for generating delegation tokens
httpfs_1     | Jun 27, 2023 12:22:27 PM com.sun.jersey.api.core.PackagesResourceConfig init
httpfs_1     | INFO: Scanning for root resource and provider classes in the packages:
httpfs_1     |   org.apache.ozone.fs.http.server
httpfs_1     |   org.apache.ozone.lib.wsrs
httpfs_1     | Jun 27, 2023 12:22:27 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode2_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode2_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode2_1  | 2023-06-27 12:24:43,331 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2023-06-27 12:24:43,343 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2023-06-27 12:24:43,347 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2023-06-27 12:24:56,475 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2023-06-27 12:24:56,812 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:1b92348e430f
datanode2_1  | 2023-06-27 12:24:56,817 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2023-06-27 12:24:56,863 [main] ERROR utils.CertificateSignRequest: Invalid domain 1b92348e430f
datanode2_1  | 2023-06-27 12:24:56,870 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@1b92348e430f
datanode2_1  | 2023-06-27 12:25:01,283 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 401704538238
datanode2_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  |            Start Date: Tue Jun 27 12:24:59 UTC 2023
datanode2_1  |            Final Date: Wed Jun 26 12:24:59 UTC 2024
datanode2_1  |             SubjectDN: CN=dn@1b92348e430f,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  |            Public Key: RSA Public Key [7d:15:43:86:a4:d0:7b:89:20:60:ed:00:69:47:9d:3d:2d:b1:cc:d7],[56:66:d1:a4]
datanode2_1  |         modulus: b1e9b33bcbb778ec1c4f6522d32d96a945ffddc32e958ae4330f20f07345d3c82d1f8ae5a8dd8546fe06d10981cb164c031f9decabfc28a85b55e1512a44f6e3729076bd09c3738207f42e939327da6a81081fbd3c2ec6631eeeaa4340eddf450ef973557e1b94e97c233dbc2bad6466bb7f79588e265ccd180cf0a6f4370c3619149680d31fb499343fc3b6b6c9f727db2ee9a05050f741655f34286fecdecd225666237efcf4bc2bd46046c941dc0d366b3493f3a870a66a10790d1d2cd9d81204177bd510efd4d5559f62848f16a1c0059ffb892e32b2839349586372fec46ebdff0e95e434d412d179a869e32f109e70b1871464201ca6ef1db784860eb9
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 19edb1e0d17abbd9b78adcc2b7ce4744750f4372
datanode2_1  |                        99ec0e21496c8a92cd8f5377e9299df2f552b7be
datanode2_1  |                        0df8bf5e6b2e807dae951600b9120de6c0283799
datanode2_1  |                        09e2dd7b891a44a507cee121e450492374fedb75
datanode2_1  |                        b7744af8399bea1e85e03b52c3aa1047cd1d83d3
datanode2_1  |                        f633ea8e20da521a98e6ccc102d73281c328666c
datanode2_1  |                        f300da4a21aaa8106eb2c6b48d32944655724f3c
datanode2_1  |                        1fdcd19b07ca89b35122e4e03795111c69e5a6d8
datanode2_1  |                        7b944eba8417a07de2c64a447b3bda4c243b4826
datanode2_1  |                        dce1f5f028fe16a2ee049b2335abeea67a51e82a
datanode2_1  |                        dc2c8b7d1d7180f70bae8b1baaf69ffca5c3c6f3
datanode2_1  |                        0609d7ccdca453774b3567e0589ba1f8af9d8158
datanode1_1  |                        acbb5f8d8e5f75c3208e116728c66494080d0391
datanode1_1  |                        4883b896992d697c14fca81b51cc41815ceb8fc5
datanode1_1  |                        9e72ce2f778759033901bef92ec92fabd21c5a89
datanode1_1  |                        c7396a443e2624b28df5d21502e09909898c3be0
datanode1_1  |                        4466369f70a26034a8bbf4c275f76029
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0xbe
datanode1_1  |  from file: /data/metadata/dn/certs/CA-266035175110.crt.
datanode1_1  | 2023-06-27 12:24:59,202 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 1
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode1_1  |            Start Date: Tue Jun 27 12:22:43 UTC 2023
datanode1_1  |            Final Date: Fri Aug 04 12:22:43 UTC 2028
datanode1_1  |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode1_1  |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
datanode1_1  |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
datanode1_1  |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
datanode1_1  |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
datanode1_1  |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
datanode1_1  |                        eece43d2072924486b8a2e674f6b5465d4190ac6
datanode1_1  |                        f5030570b2b63224eab18c88675f9c9c24008980
datanode1_1  |                        86a902cf862c5da5b230958d06c3dba90324e417
datanode1_1  |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
datanode1_1  |                        1aa70751ced732e39cbf893a239b9956aef292ef
datanode1_1  |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
datanode1_1  |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
datanode1_1  |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
datanode1_1  |                        86212367c9e9316e2a4bb892f0cda036
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0x6
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode1_1  | 2023-06-27 12:24:59,266 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797782 ms and interval 86400000 ms.
datanode1_1  | 2023-06-27 12:24:59,876 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2023-06-27 12:25:00,239 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode1_1  | 2023-06-27 12:25:00,964 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z).
datanode1_1  | 2023-06-27 12:25:00,970 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M4.230992S and interval of PT1M
datanode1_1  | 2023-06-27 12:25:01,111 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode1_1  | 2023-06-27 12:25:01,447 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2023-06-27 12:25:03,273 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2023-06-27 12:25:03,483 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode1_1  | 2023-06-27 12:25:03,534 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2023-06-27 12:25:03,547 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2023-06-27 12:25:03,812 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2023-06-27 12:25:03,860 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
datanode3_1  |                        c7396a443e2624b28df5d21502e09909898c3be0
datanode3_1  |                        4466369f70a26034a8bbf4c275f76029
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
datanode3_1  | 
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
datanode3_1  |                        critical(true) KeyUsage: 0xbe
datanode3_1  |  from file: /data/metadata/dn/certs/CA-266035175110.crt.
datanode3_1  | 2023-06-27 12:24:57,332 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 1
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  |                        4fee68f55708edc0de46cf3bc5c4a2d9
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  | 
datanode2_1  |                        critical(true) KeyUsage: 0xb8
datanode2_1  |  from file: /data/metadata/dn/certs/401704538238.crt.
datanode2_1  | 2023-06-27 12:25:01,324 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 266035175110
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  |            Start Date: Tue Jun 27 12:22:44 UTC 2023
datanode2_1  |            Final Date: Fri Aug 04 12:22:44 UTC 2028
datanode2_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
datanode2_1  |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
datanode2_1  |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
datanode2_1  |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
datanode2_1  |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
datanode2_1  |                        6575bb91718be2b62fe8285671670d4afc71639e
datanode2_1  |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
datanode2_1  |                        c63f624dd59419fb4e3d7f57549fb866400baea0
datanode2_1  |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
datanode3_1  |            Start Date: Tue Jun 27 12:22:43 UTC 2023
datanode3_1  |            Final Date: Fri Aug 04 12:22:43 UTC 2028
datanode3_1  |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode3_1  |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
datanode3_1  |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
datanode3_1  |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
datanode3_1  |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
datanode3_1  |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
datanode3_1  |                        eece43d2072924486b8a2e674f6b5465d4190ac6
datanode3_1  |                        f5030570b2b63224eab18c88675f9c9c24008980
datanode3_1  |                        86a902cf862c5da5b230958d06c3dba90324e417
datanode3_1  |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
datanode3_1  |                        1aa70751ced732e39cbf893a239b9956aef292ef
datanode3_1  |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
datanode3_1  |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
datanode3_1  |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
datanode3_1  |                        86212367c9e9316e2a4bb892f0cda036
datanode3_1  |        Extensions: 
httpfs_1     | INFO: Root resource classes found:
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSServer
datanode1_1  | 2023-06-27 12:25:03,875 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode1_1  | 2023-06-27 12:25:03,882 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
httpfs_1     | Jun 27, 2023 12:22:27 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
httpfs_1     | INFO: Provider classes found:
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSParametersProvider
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSExceptionProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONMapProvider
httpfs_1     | Jun 27, 2023 12:22:28 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
httpfs_1     | INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
httpfs_1     | Jun 27, 2023 12:22:30 PM com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl <init>
httpfs_1     | SEVERE: Implementation of JAXB-API has not been found on module path or classpath.
httpfs_1     | javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.
httpfs_1     |  - with linked exception:
httpfs_1     | [java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory]
httpfs_1     | 	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:177)
httpfs_1     | 	at javax.xml.bind.ContextFinder.find(ContextFinder.java:364)
httpfs_1     | 	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:508)
httpfs_1     | 	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:465)
httpfs_1     | 	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:366)
httpfs_1     | 	at com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl.<init>(WadlApplicationContextImpl.java:107)
httpfs_1     | 	at com.sun.jersey.server.impl.wadl.WadlFactory.init(WadlFactory.java:100)
httpfs_1     | 	at com.sun.jersey.server.impl.application.RootResourceUriRules.initWadl(RootResourceUriRules.java:169)
httpfs_1     | 	at com.sun.jersey.server.impl.application.RootResourceUriRules.<init>(RootResourceUriRules.java:106)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1359)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795)
httpfs_1     | 	at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:795)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:790)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer.initiate(ServletContainer.java:509)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:339)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:605)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:207)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:394)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:577)
httpfs_1     | 	at javax.servlet.GenericServlet.init(GenericServlet.java:244)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHolder$Wrapper.init(ServletHolder.java:1345)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:415)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:750)
httpfs_1     | 	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
httpfs_1     | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
httpfs_1     | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
httpfs_1     | 	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:312)
httpfs_1     | 	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
httpfs_1     | 	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414)
httpfs_1     | 	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524)
httpfs_1     | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
httpfs_1     | 	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
httpfs_1     | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
httpfs_1     | 	at org.eclipse.jetty.server.Server.start(Server.java:423)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)
httpfs_1     | 	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
httpfs_1     | 	at org.eclipse.jetty.server.Server.doStart(Server.java:387)
httpfs_1     | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
httpfs_1     | 	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1301)
httpfs_1     | 	at org.apache.ozone.fs.http.server.HttpFSServerWebServer.start(HttpFSServerWebServer.java:153)
httpfs_1     | 	at org.apache.ozone.fs.http.server.HttpFSServerWebServer.main(HttpFSServerWebServer.java:185)
httpfs_1     | Caused by: java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory
datanode2_1  |                        acbb5f8d8e5f75c3208e116728c66494080d0391
datanode2_1  |                        4883b896992d697c14fca81b51cc41815ceb8fc5
datanode2_1  |                        9e72ce2f778759033901bef92ec92fabd21c5a89
datanode2_1  |                        c7396a443e2624b28df5d21502e09909898c3be0
datanode2_1  |                        4466369f70a26034a8bbf4c275f76029
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0xbe
datanode2_1  |  from file: /data/metadata/dn/certs/CA-266035175110.crt.
datanode2_1  | 2023-06-27 12:25:01,415 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 1
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  |            Start Date: Tue Jun 27 12:22:43 UTC 2023
datanode2_1  |            Final Date: Fri Aug 04 12:22:43 UTC 2028
datanode2_1  |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
datanode2_1  |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
datanode2_1  |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
datanode2_1  |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
datanode2_1  |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
datanode2_1  |                        eece43d2072924486b8a2e674f6b5465d4190ac6
datanode2_1  |                        f5030570b2b63224eab18c88675f9c9c24008980
datanode2_1  |                        86a902cf862c5da5b230958d06c3dba90324e417
datanode2_1  |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
datanode2_1  |                        1aa70751ced732e39cbf893a239b9956aef292ef
datanode2_1  |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
datanode2_1  |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
datanode2_1  |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
datanode2_1  |                        86212367c9e9316e2a4bb892f0cda036
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0x6
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2023-06-27 12:25:01,477 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797563 ms and interval 86400000 ms.
datanode2_1  | 2023-06-27 12:25:02,231 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2023-06-27 12:25:02,507 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode2_1  | 2023-06-27 12:25:03,227 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z).
datanode2_1  | 2023-06-27 12:25:03,235 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M1.965727S and interval of PT1M
datanode1_1  | 2023-06-27 12:25:03,883 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2023-06-27 12:25:03,885 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
om1_1        | Waiting for the service scm3.org:9894
datanode1_1  | 2023-06-27 12:25:04,096 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2023-06-27 12:25:04,101 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2023-06-27 12:25:12,983 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode1_1  | 2023-06-27 12:25:14,128 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
datanode3_1  |                        critical(true) KeyUsage: 0x6
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-27 12:24:22,921 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
om1_1        | STARTUP_MSG:   java = 11.0.19
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-27 12:24:23,034 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-06-27 12:24:35,581 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-27 12:24:39,920 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om1_1        | 2023-06-27 12:24:40,548 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-27 12:24:40,555 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-27 12:24:40,577 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1        | 2023-06-27 12:24:43,277 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-27 12:24:43,279 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-27 12:24:43,399 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 12:24:45,112 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-5bf65396-4407-4e81-983e-589b8c5be712;layoutVersion=6
om1_1        | 2023-06-27 12:24:50,366 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om1_1        | 2023-06-27 12:24:50,367 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2023-06-27 12:24:51,408 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-27 12:24:57,659 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
datanode1_1  | 2023-06-27 12:25:14,747 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode1_1  | 2023-06-27 12:25:15,619 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-06-27 12:25:16,537 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2023-06-27 12:25:17,760 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-27 12:25:17,783 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2023-06-27 12:25:17,794 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-27 12:25:17,800 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2023-06-27 12:25:17,802 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode1_1  | 2023-06-27 12:25:17,804 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2023-06-27 12:25:17,822 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2023-06-27 12:25:17,842 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 
datanode3_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2023-06-27 12:24:57,386 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 397752053077
datanode3_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode3_1  |            Start Date: Tue Jun 27 12:24:55 UTC 2023
datanode3_1  |            Final Date: Wed Jun 26 12:24:55 UTC 2024
datanode3_1  |             SubjectDN: CN=dn@038f15546f77,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode3_1  |            Public Key: RSA Public Key [8c:45:05:e0:bc:7f:4d:32:f2:8f:9f:48:c5:df:1c:9e:b3:c3:b3:56],[56:66:d1:a4]
datanode3_1  |         modulus: d54359590210da9db3afe188b2e6190b6eae916d860f76764d43275255579e9d1f1fabc465769372ca3bcffe24e8da33a87e8ac5dc99dedd1f0a3faa2793e1c0723807d17d9f894fcbd5e00a09e6d1c94e01e07b73e37234a9707a804e09eb8381d522c1b1bdfae95c3dd3e49ee585885f608b7c2e9d8fe0f2f68e6f7ff162e268dd35d2597690d7f45253139dcadd99aa0b88ea49e8d241279522162a6973bc464b243321d90c0018652c18b5868b6aca1bc064df49a5e6fb3d16f775330786f46e1f763e4f02e9d38f1b50f8aeefa6f78957dc581910390767b4094d726fca33fab115b080982c481f393bd9ea94562c8fd2a5fbab257fdfee51b95e4476f9
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 7f47c0e8f773a5e5f16f909ffbce932225758d9b
datanode3_1  |                        cdde0834f4de8180e82b17f72d8c1e5866ee7c52
datanode3_1  |                        caead0a77e77717ed539f4225988acf2814133ea
datanode3_1  |                        4b72b134998800f8d825606c629f6eeb43a61e59
datanode3_1  |                        b8630e45b7ed7dc50e547e7fc882745ab9e6eb4d
datanode3_1  |                        ccd8977c41fe3db38c1256ae0f75c013c27ff923
datanode3_1  |                        4c9e8cc68d799eff4f80ca02885791a86652bd27
datanode3_1  |                        8bb8d2b94be196fec7f00e87a9d297662d965b88
datanode3_1  |                        d92f81168bba00355e721a8a7b84cf2d74a9e0e1
datanode3_1  |                        17c32b44b09ba869a8a308f8653c858b4eeb2446
datanode3_1  |                        d00eeb98e7eade6b6efa0e753dcdedf6e6a7c270
datanode3_1  |                        dc1ff4f630493d9eb7504ae4583dfed2df0da7af
datanode3_1  |                        40b0af50394123d5d965dc98d0ab6a4b
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  | 
datanode3_1  |                        critical(true) KeyUsage: 0xb8
datanode3_1  |  from file: /data/metadata/dn/certs/397752053077.crt.
datanode3_1  | 2023-06-27 12:24:57,441 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797608 ms and interval 86400000 ms.
datanode3_1  | 2023-06-27 12:24:58,137 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2023-06-27 12:24:58,630 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode3_1  | 2023-06-27 12:25:00,862 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z).
datanode3_1  | 2023-06-27 12:25:00,877 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M4.323579S and interval of PT1M
datanode3_1  | 2023-06-27 12:25:01,003 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode3_1  | 2023-06-27 12:25:01,306 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode3_1  | 2023-06-27 12:25:02,927 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2023-06-27 12:25:03,160 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode3_1  | 2023-06-27 12:25:03,204 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2023-06-27 12:25:03,218 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2023-06-27 12:25:03,696 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode3_1  | 2023-06-27 12:25:03,776 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-27 12:25:03,798 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2023-06-27 12:25:03,809 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2023-06-27 12:25:03,809 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2023-06-27 12:25:03,810 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2023-06-27 12:25:04,082 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2023-06-27 12:25:04,092 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2023-06-27 12:25:12,200 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode3_1  | 2023-06-27 12:25:13,768 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode3_1  | 2023-06-27 12:25:13,855 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode3_1  | 2023-06-27 12:25:14,438 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-27 12:25:14,962 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2023-06-27 12:25:16,775 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-27 12:25:16,789 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2023-06-27 12:25:16,791 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-27 12:25:16,808 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2023-06-27 12:25:16,810 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode3_1  | 2023-06-27 12:25:16,811 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2023-06-27 12:25:16,816 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  | 2023-06-27 12:25:16,861 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:25:16,866 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2023-06-27 12:25:16,920 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 12:25:17,176 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-06-27 12:25:17,297 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode3_1  | 2023-06-27 12:25:17,314 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-06-27 12:25:25,213 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode3_1  | 2023-06-27 12:25:25,486 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode3_1  | 2023-06-27 12:25:25,499 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode3_1  | 2023-06-27 12:25:25,506 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode3_1  | 2023-06-27 12:25:25,512 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode3_1  | 2023-06-27 12:25:25,545 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode3_1  | 2023-06-27 12:25:25,552 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode3_1  | 2023-06-27 12:25:25,634 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode3_1  | 2023-06-27 12:25:25,665 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode3_1  | 2023-06-27 12:25:25,683 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode3_1  | 2023-06-27 12:25:25,690 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode3_1  | 2023-06-27 12:25:26,194 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode2_1  | 2023-06-27 12:25:03,568 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode2_1  | 2023-06-27 12:25:03,899 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode2_1  | 2023-06-27 12:25:05,710 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2023-06-27 12:25:05,901 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode2_1  | 2023-06-27 12:25:05,964 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2023-06-27 12:25:05,983 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2023-06-27 12:25:06,437 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  | 2023-06-27 12:25:06,495 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-27 12:25:06,512 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2023-06-27 12:25:06,518 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2023-06-27 12:25:06,518 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode2_1  | 2023-06-27 12:25:06,521 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2023-06-27 12:25:06,798 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2023-06-27 12:25:06,818 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2023-06-27 12:25:14,543 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode2_1  | 2023-06-27 12:25:15,681 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode2_1  | 2023-06-27 12:25:15,774 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode2_1  | 2023-06-27 12:25:16,407 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-27 12:25:17,303 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2023-06-27 12:25:18,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-27 12:25:18,841 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2023-06-27 12:25:18,846 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-27 12:25:18,850 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2023-06-27 12:25:18,859 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode2_1  | 2023-06-27 12:25:18,860 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2023-06-27 12:25:18,867 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode2_1  | 2023-06-27 12:25:18,874 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:25:18,885 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2023-06-27 12:25:18,892 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-27 12:25:19,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode2_1  | 2023-06-27 12:25:19,173 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode2_1  | 2023-06-27 12:25:19,189 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-06-27 12:25:25,921 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode2_1  | 2023-06-27 12:25:26,205 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode2_1  | 2023-06-27 12:25:26,211 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode2_1  | 2023-06-27 12:25:26,217 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode2_1  | 2023-06-27 12:25:26,233 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode2_1  | 2023-06-27 12:25:26,265 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode2_1  | 2023-06-27 12:25:26,271 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode2_1  | 2023-06-27 12:25:26,387 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode2_1  | 2023-06-27 12:25:26,420 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode2_1  | 2023-06-27 12:25:26,449 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode2_1  | 2023-06-27 12:25:26,455 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode2_1  | 2023-06-27 12:25:26,949 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode2_1  | 2023-06-27 12:25:26,950 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode2_1  | 2023-06-27 12:25:26,995 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-27 12:25:26,995 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-27 12:25:27,065 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-27 12:25:27,195 [cc01b188-4e0b-44d2-a762-70392abfc581-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x5e37b52e] REGISTERED
datanode2_1  | 2023-06-27 12:25:27,215 [cc01b188-4e0b-44d2-a762-70392abfc581-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x5e37b52e] BIND: 0.0.0.0/0.0.0.0:9855
datanode2_1  | 2023-06-27 12:25:27,313 [cc01b188-4e0b-44d2-a762-70392abfc581-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x5e37b52e, L:/0.0.0.0:9855] ACTIVE
datanode2_1  | 2023-06-27 12:25:27,633 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode2_1  | 2023-06-27 12:25:28,759 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode1_1  | 2023-06-27 12:25:17,852 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2023-06-27 12:25:17,864 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 12:25:18,079 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-06-27 12:25:18,180 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode1_1  | 2023-06-27 12:25:18,183 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-06-27 12:25:25,065 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode1_1  | 2023-06-27 12:25:25,356 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode1_1  | 2023-06-27 12:25:25,364 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode1_1  | 2023-06-27 12:25:25,372 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode1_1  | 2023-06-27 12:25:25,380 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode1_1  | 2023-06-27 12:25:25,421 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode1_1  | 2023-06-27 12:25:25,425 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode1_1  | 2023-06-27 12:25:25,537 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode1_1  | 2023-06-27 12:25:25,555 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode1_1  | 2023-06-27 12:25:25,573 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode1_1  | 2023-06-27 12:25:25,573 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode1_1  | 2023-06-27 12:25:26,042 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode1_1  | 2023-06-27 12:25:26,042 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode1_1  | 2023-06-27 12:25:26,048 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 12:25:26,048 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 12:25:26,135 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 12:25:26,185 [1a7440b0-385d-4580-b33d-c9708c19e79c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x8fbf70e7] REGISTERED
datanode1_1  | 2023-06-27 12:25:26,221 [1a7440b0-385d-4580-b33d-c9708c19e79c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x8fbf70e7] BIND: 0.0.0.0/0.0.0.0:9855
datanode1_1  | 2023-06-27 12:25:26,264 [1a7440b0-385d-4580-b33d-c9708c19e79c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x8fbf70e7, L:/0.0.0.0:9855] ACTIVE
datanode1_1  | 2023-06-27 12:25:26,605 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode1_1  | 2023-06-27 12:25:27,777 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode1_1  | 2023-06-27 12:25:29,899 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode1_1  | 2023-06-27 12:25:29,902 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2023-06-27 12:25:29,903 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2023-06-27 12:25:30,133 [main] INFO util.log: Logging initialized @84150ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2023-06-27 12:25:31,322 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2023-06-27 12:25:31,410 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2023-06-27 12:25:31,422 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2023-06-27 12:25:31,423 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode1_1  | 2023-06-27 12:25:31,426 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2023-06-27 12:25:31,466 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2023-06-27 12:25:31,882 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode1_1  | 2023-06-27 12:25:31,893 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2023-06-27 12:25:31,904 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode1_1  | 2023-06-27 12:25:32,281 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2023-06-27 12:25:32,282 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2023-06-27 12:25:32,296 [main] INFO server.session: node0 Scavenging every 660000ms
datanode1_1  | 2023-06-27 12:25:32,608 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-27 12:25:32,635 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e0dec0c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2023-06-27 12:25:32,643 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@292d1705{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2023-06-27 12:25:33,860 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-27 12:25:33,997 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2162e4a{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16958774644304874764/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2023-06-27 12:25:34,094 [main] INFO server.AbstractConnector: Started ServerConnector@799bb4ed{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2023-06-27 12:25:34,094 [main] INFO server.Server: Started @88111ms
datanode1_1  | 2023-06-27 12:25:34,122 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2023-06-27 12:25:34,122 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2023-06-27 12:25:34,134 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2023-06-27 12:25:34,329 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode1_1  | 2023-06-27 12:25:34,382 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
om1_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om1_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om1_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om1_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om1_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om1_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1374)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1353)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om1_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om1_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om1_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om1_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om1_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om1_1        | 2023-06-27 12:24:57,705 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2023-06-27 12:24:57,727 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om1_1        | 2023-06-27 12:24:57,739 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2023-06-27 12:25:04,906 [main] INFO om.OzoneManager: Init response: GETCERT
datanode2_1  | 2023-06-27 12:25:30,607 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
om1_1        | 2023-06-27 12:25:05,313 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
httpfs_1     | 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
httpfs_1     | 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
httpfs_1     | 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538)
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-27 12:24:23,173 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
httpfs_1     | 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
httpfs_1     | 	at javax.xml.bind.ServiceLoaderUtil.nullSafeLoadClass(ServiceLoaderUtil.java:122)
om1_1        | 2023-06-27 12:25:05,325 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2023-06-27 12:25:05,406 [main] ERROR utils.CertificateSignRequest: Invalid domain om1
om1_1        | 2023-06-27 12:25:05,412 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:f163470c-d3e0-4891-94bf-0bc988a6aa12,clusterId:CID-5bf65396-4407-4e81-983e-589b8c5be712,subject:om1
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
om2_1        | STARTUP_MSG:   java = 11.0.19
httpfs_1     | 	at javax.xml.bind.ServiceLoaderUtil.safeLoadClass(ServiceLoaderUtil.java:155)
httpfs_1     | 	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:174)
httpfs_1     | 	... 53 more
httpfs_1     | 
httpfs_1     | 12:22:30,972  INFO ContextHandler:921 - Started o.e.j.w.WebAppContext@2f61f937{webhdfs,/,file:///tmp/hadoop-hadoop/httpfs/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/webhdfs}
httpfs_1     | 12:22:31,041  INFO AbstractConnector:333 - Started ServerConnector@76a4ebf2{HTTP/1.1, (http/1.1)}{0.0.0.0:14000}
httpfs_1     | 12:22:31,041  INFO Server:415 - Started @15725ms
om1_1        | 2023-06-27 12:25:07,834 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 408250958148
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Start Date: Tue Jun 27 12:25:06 UTC 2023
om1_1        |            Final Date: Wed Jun 26 12:25:06 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Public Key: RSA Public Key [f2:32:8e:59:c2:c7:d2:c8:94:a1:08:3a:ac:fb:73:0c:0e:46:5c:68],[56:66:d1:a4]
om1_1        |         modulus: b80294424ea2b645ec43bf652f217c3c9f6abc9c2f6621d1fda1818f1157c6777268efa305ff9c175e6f0ac8283e8d5e8be3d2f64ac25f4c2444ab268aec11bd2df325b2007e65ffb31e884a494eabe2cc21d1fdc9f4610501d8990ac93f4899ad0d8fddb80c624c32e58b0a9d00b301be33a6b0d244eadfed0ff1fbef9670f5facb82df5c55a3fd03c5cb50c59d4e9d167e259d3e657758c08c6bf39aa71532a4f934f82e3023c554ff5a021fb98693cbd3edaebf354b8286061a5c9ef2dec7f658382c32d0bb5a94f21d5afa911b9b0595cd1f533708379ca9500d2459b70dee135e05e2740f17bb343efd466f283e04bc0f8ca1d8fcdf36e4b1dfdfff4f8f
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 8744e62ca0715d2e115c9f2d28323744028a6b65
om1_1        |                        2bc022734665988b04509065cc84780324ac6a29
om1_1        |                        b46991022a484ee5b7ef076a9e1b85c61ed87c9a
om1_1        |                        f4d3a01d709216b97a10217463561b52a98db61a
om1_1        |                        05c7d2bae76dde651bf7a5ec38a8d64c81059ea3
om1_1        |                        a2465528f56140bb9916e6c6a55b52ffd8813b5b
om1_1        |                        fc9b67e3fda9483e80d6ea5290ca2c25ad2ecb10
om1_1        |                        1073387322c91074dff3da198541ba5834b5f6ab
om1_1        |                        6b802651072a4711c4ba4f13bda717f350fff421
om1_1        |                        91d6639eebc44fa9f780dc36b825b4ea31d8e2e7
om1_1        |                        efe2687e6f57873c89b124696db336a61cae3f92
om1_1        |                        1431e6c9b27932f595c663c141d34d776b994ce1
om1_1        |                        8085d542d35a56ac4e257848166974c7
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/408250958148.crt.
om1_1        | 2023-06-27 12:25:07,897 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 266035175110
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Start Date: Tue Jun 27 12:22:44 UTC 2023
om1_1        |            Final Date: Fri Aug 04 12:22:44 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
om1_1        |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-27 12:24:23,257 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-06-27 12:24:34,784 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-27 12:24:39,138 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-27 12:24:39,771 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om3_1        | Waiting for the service scm3.org:9894
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-27 12:24:39,776 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-27 12:24:39,776 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-27 12:24:41,881 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-27 12:24:41,881 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-27 12:24:41,960 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 12:24:43,454 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-5bf65396-4407-4e81-983e-589b8c5be712;layoutVersion=6
om3_1        | 2023-06-27 12:24:21,902 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode1_1  | 2023-06-27 12:25:36,602 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode1_1  | 2023-06-27 12:25:36,615 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
om2_1        | 2023-06-27 12:24:48,019 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om2_1        | 2023-06-27 12:24:48,020 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2023-06-27 12:24:48,984 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-27 12:24:22,005 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2023-06-27 12:25:36,624 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode1_1  | 2023-06-27 12:25:36,655 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode1_1  | 2023-06-27 12:25:36,735 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode1_1  | 2023-06-27 12:25:36,781 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 2023-06-27 12:25:37,736 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
om2_1        | 2023-06-27 12:24:55,383 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
datanode3_1  | 2023-06-27 12:25:26,233 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode3_1  | 2023-06-27 12:25:26,233 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 12:25:26,234 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 12:25:26,292 [a650530b-22bc-4ab5-9901-0a035f1b699c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xff6ef2d4] REGISTERED
om2_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om3_1        | 2023-06-27 12:24:32,745 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-27 12:24:36,166 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-27 12:24:36,898 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-27 12:24:36,930 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
datanode1_1  | 2023-06-27 12:25:37,885 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode1_1  | 2023-06-27 12:25:41,121 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om2_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om2_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om2_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om2_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode2_1  | 2023-06-27 12:25:30,611 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2023-06-27 12:25:30,611 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2023-06-27 12:25:26,296 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 12:25:26,316 [a650530b-22bc-4ab5-9901-0a035f1b699c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xff6ef2d4] BIND: 0.0.0.0/0.0.0.0:9855
datanode3_1  | 2023-06-27 12:25:26,354 [a650530b-22bc-4ab5-9901-0a035f1b699c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xff6ef2d4, L:/0.0.0.0:9855] ACTIVE
datanode1_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
om3_1        | 2023-06-27 12:24:36,950 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-27 12:24:39,188 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-27 12:24:39,189 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-27 12:24:39,225 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 12:24:40,528 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-5bf65396-4407-4e81-983e-589b8c5be712;layoutVersion=6
om3_1        | 2023-06-27 12:24:45,261 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om3_1        | 2023-06-27 12:24:45,262 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2023-06-27 12:24:46,048 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-27 12:24:52,412 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om3_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om3_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om3_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om3_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om3_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om3_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1374)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1353)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om3_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om3_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om3_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om3_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om3_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om3_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om3_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om3_1        | 2023-06-27 12:24:52,445 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2023-06-27 12:24:52,453 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om3_1        | 2023-06-27 12:24:52,459 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2023-06-27 12:24:58,585 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2023-06-27 12:24:58,916 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2023-06-27 12:24:58,916 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2023-06-27 12:24:58,985 [main] ERROR utils.CertificateSignRequest: Invalid domain om3
om3_1        | 2023-06-27 12:24:58,985 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:f163470c-d3e0-4891-94bf-0bc988a6aa12,clusterId:CID-5bf65396-4407-4e81-983e-589b8c5be712,subject:om3
om3_1        | 2023-06-27 12:25:02,614 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 402605330599
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Start Date: Tue Jun 27 12:25:00 UTC 2023
om3_1        |            Final Date: Wed Jun 26 12:25:00 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Public Key: RSA Public Key [f9:6a:20:8d:ae:35:0c:b8:ad:52:d7:82:a6:c1:3b:f8:f4:ac:8e:41],[56:66:d1:a4]
om3_1        |         modulus: e96036f1f1f3b906e53eb0030d42921a48856614f1bfbf8d028e8ba36097ab26ccbca088610b997a3431ac5cb0a524b51e084d7dc10d37951a41e83437ebb1cd7baafa1b8c5ba8601eb5aba8e3155d53c32ff558daf900e01962ae0556a20a9a74d41e85d5d7e742f29549fcb74c03de7ab3f4f1552cfa01ce243d59159de04d26ef00e1e342a6aa4bf08358a97de149d01854ab7186d6f29270bb2b068c290efb4f10cf1e876cd68c5813e4a28cf608165ef849b4992d59f7c6ea5e6ac329070f2dac8c224428a5b227a75ef4b04d4d515b1f6b7cea752c0eef27367fe38af6fa07c01ddd291a6ff2d1fd2b1f71306211b55b5683bb383b2e28e56e24d38e35
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: b537d62767a9addd77be653cf93fbc9b40e4bf00
om3_1        |                        6dad44ce4c1a215afe0ef214e2f0c41cf39b8b23
om3_1        |                        405e6885920e8da7b829b9b6897b32f44e826660
om3_1        |                        d15618169a9b2458e47e5b3fc05e54a9a8c9aad1
om3_1        |                        293a47f84dcc72def80e2971e6ff0f698085d2f4
om3_1        |                        f0e22a88e98fc990327a35a544e54a61d8c8ac8b
om3_1        |                        b4cd0d4a46aeceb07111a1632f2dc7d81f3e32fc
om3_1        |                        1f7dbc59aa8b9d342fa2a7fae3ce90b43dbecb19
om3_1        |                        49a5e6880240415a23a2851cc102ff034120ee73
om3_1        |                        99052858115c78132db505a9ac9c7e1b811b4976
datanode2_1  | 2023-06-27 12:25:31,045 [main] INFO util.log: Logging initialized @84944ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2023-06-27 12:25:32,214 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2023-06-27 12:25:32,304 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2023-06-27 12:25:32,317 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2023-06-27 12:25:32,321 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2023-06-27 12:25:26,696 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode3_1  | 2023-06-27 12:25:27,669 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode3_1  | 2023-06-27 12:25:29,906 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2023-06-27 12:25:29,906 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2023-06-27 12:25:29,906 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2023-06-27 12:25:30,227 [main] INFO util.log: Logging initialized @84336ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2023-06-27 12:25:31,588 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2023-06-27 12:25:31,707 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2023-06-27 12:25:31,726 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2023-06-27 12:25:31,727 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
om1_1        |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
om1_1        |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
om1_1        |                        6575bb91718be2b62fe8285671670d4afc71639e
om1_1        |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
om1_1        |                        c63f624dd59419fb4e3d7f57549fb866400baea0
om1_1        |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
om1_1        |                        acbb5f8d8e5f75c3208e116728c66494080d0391
om1_1        |                        4883b896992d697c14fca81b51cc41815ceb8fc5
om1_1        |                        9e72ce2f778759033901bef92ec92fabd21c5a89
om1_1        |                        c7396a443e2624b28df5d21502e09909898c3be0
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om2_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1374)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1353)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om2_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om2_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om2_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om2_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om2_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om2_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om2_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om2_1        | 2023-06-27 12:24:55,413 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2023-06-27 12:24:55,425 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om2_1        | 2023-06-27 12:24:55,470 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2023-06-27 12:25:00,576 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2023-06-27 12:25:00,914 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
om2_1        | 2023-06-27 12:25:00,929 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2023-06-27 12:25:00,985 [main] ERROR utils.CertificateSignRequest: Invalid domain om2
om2_1        | 2023-06-27 12:25:00,992 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:f163470c-d3e0-4891-94bf-0bc988a6aa12,clusterId:CID-5bf65396-4407-4e81-983e-589b8c5be712,subject:om2
om2_1        | 2023-06-27 12:25:03,792 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 266035175110
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Start Date: Tue Jun 27 12:22:44 UTC 2023
om2_1        |            Final Date: Fri Aug 04 12:22:44 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
om2_1        |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
datanode2_1  | 2023-06-27 12:25:32,324 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode2_1  | 2023-06-27 12:25:32,353 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2023-06-27 12:25:32,930 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode2_1  | 2023-06-27 12:25:32,936 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2023-06-27 12:25:32,950 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode2_1  | 2023-06-27 12:25:33,254 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2023-06-27 12:25:33,263 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2023-06-27 12:25:33,304 [main] INFO server.session: node0 Scavenging every 660000ms
datanode2_1  | 2023-06-27 12:25:33,500 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-06-27 12:25:33,527 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3dd4f91d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2023-06-27 12:25:33,539 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@70796b21{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2023-06-27 12:25:34,604 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-06-27 12:25:34,753 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@595626b8{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-15353688653811614625/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2023-06-27 12:25:34,878 [main] INFO server.AbstractConnector: Started ServerConnector@5269aa68{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2023-06-27 12:25:34,881 [main] INFO server.Server: Started @88780ms
datanode2_1  | 2023-06-27 12:25:34,913 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2023-06-27 12:25:34,913 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2023-06-27 12:25:34,921 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2023-06-27 12:25:35,159 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode2_1  | 2023-06-27 12:25:35,256 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode2_1  | 2023-06-27 12:25:37,914 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode2_1  | 2023-06-27 12:25:37,936 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode2_1  | 2023-06-27 12:25:37,936 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode2_1  | 2023-06-27 12:25:37,959 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode2_1  | 2023-06-27 12:25:37,959 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode2_1  | 2023-06-27 12:25:38,000 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode1_1  | Caused by: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 2023-06-27 12:25:31,728 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2023-06-27 12:25:31,762 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2023-06-27 12:25:32,262 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode3_1  | 2023-06-27 12:25:32,274 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2023-06-27 12:25:32,289 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode3_1  | 2023-06-27 12:25:32,619 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2023-06-27 12:25:32,620 [main] INFO server.session: No SessionScavenger set, using defaults
datanode3_1  | 2023-06-27 12:25:32,634 [main] INFO server.session: node0 Scavenging every 660000ms
datanode3_1  | 2023-06-27 12:25:32,871 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-27 12:25:32,911 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@30a0a440{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2023-06-27 12:25:32,931 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c9a0166{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2023-06-27 12:25:34,275 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-27 12:25:34,451 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@707b0b9b{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6541750764529628901/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2023-06-27 12:25:34,595 [main] INFO server.AbstractConnector: Started ServerConnector@6a7c0932{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2023-06-27 12:25:34,595 [main] INFO server.Server: Started @88709ms
datanode3_1  | 2023-06-27 12:25:34,649 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2023-06-27 12:25:34,649 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2023-06-27 12:25:34,654 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2023-06-27 12:25:34,879 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode3_1  | 2023-06-27 12:25:34,970 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode3_1  | 2023-06-27 12:25:38,034 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode3_1  | 2023-06-27 12:25:38,045 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode3_1  | 2023-06-27 12:25:38,046 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode3_1  | 2023-06-27 12:25:38,068 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode3_1  | 2023-06-27 12:25:38,125 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode3_1  | 2023-06-27 12:25:38,167 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode3_1  | 2023-06-27 12:25:39,087 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2023-06-27 12:25:39,408 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode3_1  | 2023-06-27 12:25:42,634 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode3_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
om1_1        |                        4466369f70a26034a8bbf4c275f76029
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
datanode1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	... 1 more
datanode1_1  | 2023-06-27 12:25:43,125 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode1_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0xbe
om1_1        |  from file: /data/metadata/om/certs/CA-266035175110.crt.
om1_1        | 2023-06-27 12:25:07,922 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 1
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Start Date: Tue Jun 27 12:22:43 UTC 2023
om1_1        |            Final Date: Fri Aug 04 12:22:43 UTC 2028
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2023-06-27 12:22:24,719 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
recon_1      | STARTUP_MSG:   java = 11.0.19
recon_1      | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1      | ************************************************************/
recon_1      | 2023-06-27 12:22:24,743 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-06-27 12:22:28,211 [main] INFO reflections.Reflections: Reflections took 436 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1      | 2023-06-27 12:22:31,839 [main] INFO reflections.Reflections: Reflections took 319 ms to scan 3 urls, producing 131 keys and 286 values 
recon_1      | 2023-06-27 12:22:32,064 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2023-06-27 12:22:32,265 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2023-06-27 12:22:33,013 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file recon.keytab. Keytab auto renewal enabled : false
recon_1      | 2023-06-27 12:22:33,013 [main] INFO recon.ReconServer: Recon login successful.
recon_1      | 2023-06-27 12:22:33,052 [main] INFO recon.ReconServer: ReconStorageConfig initialized.Initializing certificate.
recon_1      | 2023-06-27 12:22:33,053 [main] INFO recon.ReconServer: Initializing secure Recon.
recon_1      | 2023-06-27 12:22:35,487 [main] WARN security.ReconCertificateClient: Certificates could not be loaded.
recon_1      | java.nio.file.NoSuchFileException: /data/metadata/recon/certs
recon_1      | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
recon_1      | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
recon_1      | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
recon_1      | 	at java.base/java.nio.file.Files.list(Files.java:3699)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
recon_1      | 	at org.apache.hadoop.ozone.recon.security.ReconCertificateClient.<init>(ReconCertificateClient.java:62)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.initializeCertificateClient(ReconServer.java:184)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:126)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:71)
recon_1      | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
recon_1      | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
recon_1      | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode1_1  | Caused by: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	... 1 more
datanode2_1  | 2023-06-27 12:25:39,085 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2023-06-27 12:25:39,153 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode2_1  | 2023-06-27 12:25:43,515 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-5bf65396-4407-4e81-983e-589b8c5be712/DS-aebc34df-b81c-47c8-abdc-31bcbe9d5a5c/container.db to cache
datanode2_1  | 2023-06-27 12:25:43,516 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-5bf65396-4407-4e81-983e-589b8c5be712/DS-aebc34df-b81c-47c8-abdc-31bcbe9d5a5c/container.db for volume DS-aebc34df-b81c-47c8-abdc-31bcbe9d5a5c
datanode2_1  | 2023-06-27 12:25:43,524 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2023-06-27 12:25:43,575 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode2_1  | 2023-06-27 12:25:43,786 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode2_1  | 2023-06-27 12:25:43,788 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis cc01b188-4e0b-44d2-a762-70392abfc581
datanode2_1  | 2023-06-27 12:25:44,186 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: cc01b188-4e0b-44d2-a762-70392abfc581: start RPC server
datanode2_1  | 2023-06-27 12:25:44,213 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: cc01b188-4e0b-44d2-a762-70392abfc581: GrpcService started, listening on 9858
datanode2_1  | 2023-06-27 12:25:44,245 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: cc01b188-4e0b-44d2-a762-70392abfc581: GrpcService started, listening on 9856
datanode2_1  | 2023-06-27 12:25:44,253 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: cc01b188-4e0b-44d2-a762-70392abfc581: GrpcService started, listening on 9857
datanode2_1  | 2023-06-27 12:25:44,314 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cc01b188-4e0b-44d2-a762-70392abfc581 is started using port 9858 for RATIS
datanode2_1  | 2023-06-27 12:25:44,314 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cc01b188-4e0b-44d2-a762-70392abfc581 is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2023-06-27 12:25:44,315 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cc01b188-4e0b-44d2-a762-70392abfc581 is started using port 9856 for RATIS_SERVER
datanode2_1  | 2023-06-27 12:25:44,315 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cc01b188-4e0b-44d2-a762-70392abfc581 is started using port 9855 for RATIS_DATASTREAM
datanode2_1  | 2023-06-27 12:25:44,315 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-cc01b188-4e0b-44d2-a762-70392abfc581: Started
datanode2_1  | 2023-06-27 12:25:44,439 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-27 12:25:44,439 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-27 12:25:44,946 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode2_1  | 2023-06-27 12:26:17,739 [PipelineCommandHandlerThread-0] INFO server.RaftServer: cc01b188-4e0b-44d2-a762-70392abfc581: addNew group-B1F9E205C4E3:[cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-B1F9E205C4E3:java.util.concurrent.CompletableFuture@1a31cfee[Not completed]
datanode2_1  | 2023-06-27 12:26:17,858 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581: new RaftServerImpl for group-B1F9E205C4E3:[cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-27 12:26:17,866 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-27 12:26:17,867 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-27 12:26:17,868 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-27 12:26:17,869 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-27 12:26:17,879 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-27 12:26:17,879 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-27 12:26:17,997 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3: ConfigurationManager, init=-1: peers:[cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-27 12:26:17,997 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-27 12:26:18,044 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-27 12:26:18,067 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-27 12:26:18,214 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-27 12:26:18,268 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
om1_1        |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
om1_1        |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
om1_1        |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
om1_1        |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
om1_1        |                        eece43d2072924486b8a2e674f6b5465d4190ac6
om1_1        |                        f5030570b2b63224eab18c88675f9c9c24008980
om1_1        |                        86a902cf862c5da5b230958d06c3dba90324e417
om1_1        |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
om1_1        |                        1aa70751ced732e39cbf893a239b9956aef292ef
om1_1        |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
om1_1        |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
om1_1        |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
om1_1        |                        86212367c9e9316e2a4bb892f0cda036
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-27 12:25:08,001 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116798064 ms and interval 86400000 ms.
om1_1        | 2023-06-27 12:25:08,003 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | 2023-06-27 12:25:08,123 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-27 12:25:26,631 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
om1_1        | STARTUP_MSG:   java = 11.0.19
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-27 12:25:26,719 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
om2_1        |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
om2_1        |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
om2_1        |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
om2_1        |                        6575bb91718be2b62fe8285671670d4afc71639e
om1_1        | 2023-06-27 12:25:37,943 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-27 12:25:41,783 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
om3_1        |                        ab703a1569e02535681127643be1a759c885cd4e
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
recon_1      | 	at picocli.CommandLine.execute(CommandLine.java:2078)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om3_1        |                        ec37eff25c45d0a66e63319e6d156eb324796b82
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2023-06-27 12:22:25,033 [main] INFO security.UserGroupInformation: Login successful for user s3g/s3g@EXAMPLE.COM using keytab file s3g.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2023-06-27 12:25:43,596 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-5bf65396-4407-4e81-983e-589b8c5be712/DS-129bf1e6-dd33-448e-9770-1e49fcbe9088/container.db to cache
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode3_1  | Caused by: java.util.concurrent.TimeoutException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	... 1 more
datanode3_1  | 2023-06-27 12:25:42,685 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode3_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode3_1  | Caused by: java.util.concurrent.TimeoutException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	... 1 more
datanode3_1  | 2023-06-27 12:25:43,734 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-5bf65396-4407-4e81-983e-589b8c5be712/DS-bb005935-5474-46ee-8119-0612bc35cfa5/container.db to cache
datanode3_1  | 2023-06-27 12:25:43,734 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-5bf65396-4407-4e81-983e-589b8c5be712/DS-bb005935-5474-46ee-8119-0612bc35cfa5/container.db for volume DS-bb005935-5474-46ee-8119-0612bc35cfa5
datanode3_1  | 2023-06-27 12:25:43,763 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2023-06-27 12:25:43,838 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode3_1  | 2023-06-27 12:25:44,282 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode3_1  | 2023-06-27 12:25:44,285 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a650530b-22bc-4ab5-9901-0a035f1b699c
datanode3_1  | 2023-06-27 12:25:44,639 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: a650530b-22bc-4ab5-9901-0a035f1b699c: start RPC server
datanode3_1  | 2023-06-27 12:25:44,693 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: a650530b-22bc-4ab5-9901-0a035f1b699c: GrpcService started, listening on 9858
datanode3_1  | 2023-06-27 12:25:44,699 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: a650530b-22bc-4ab5-9901-0a035f1b699c: GrpcService started, listening on 9856
datanode3_1  | 2023-06-27 12:25:44,700 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: a650530b-22bc-4ab5-9901-0a035f1b699c: GrpcService started, listening on 9857
datanode3_1  | 2023-06-27 12:25:44,741 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a650530b-22bc-4ab5-9901-0a035f1b699c is started using port 9858 for RATIS
datanode3_1  | 2023-06-27 12:25:44,746 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a650530b-22bc-4ab5-9901-0a035f1b699c is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2023-06-27 12:25:44,746 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a650530b-22bc-4ab5-9901-0a035f1b699c is started using port 9856 for RATIS_SERVER
datanode3_1  | 2023-06-27 12:25:44,747 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a650530b-22bc-4ab5-9901-0a035f1b699c is started using port 9855 for RATIS_DATASTREAM
om3_1        |                        5468cc5919bccfc6f4079b504f1fdd44
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/402605330599.crt.
om3_1        | 2023-06-27 12:25:02,675 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 266035175110
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Start Date: Tue Jun 27 12:22:44 UTC 2023
om3_1        |            Final Date: Fri Aug 04 12:22:44 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
om3_1        |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
om3_1        |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
om3_1        |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
om3_1        |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
om3_1        |                        6575bb91718be2b62fe8285671670d4afc71639e
om3_1        |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
om3_1        |                        c63f624dd59419fb4e3d7f57549fb866400baea0
om3_1        |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
om3_1        |                        acbb5f8d8e5f75c3208e116728c66494080d0391
om3_1        |                        4883b896992d697c14fca81b51cc41815ceb8fc5
om3_1        |                        9e72ce2f778759033901bef92ec92fabd21c5a89
om3_1        |                        c7396a443e2624b28df5d21502e09909898c3be0
om3_1        |                        4466369f70a26034a8bbf4c275f76029
om3_1        |        Extensions: 
s3g_1        | 2023-06-27 12:22:25,034 [main] INFO s3.Gateway: S3Gateway login successful.
s3g_1        | 2023-06-27 12:22:26,038 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2023-06-27 12:22:26,039 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2023-06-27 12:22:26,039 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2023-06-27 12:22:26,493 [main] INFO util.log: Logging initialized @11127ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2023-06-27 12:22:27,035 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2023-06-27 12:22:27,043 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2023-06-27 12:22:27,044 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2023-06-27 12:22:27,045 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2023-06-27 12:22:27,045 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2023-06-27 12:22:27,048 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2023-06-27 12:22:27,201 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir14833680421117767797
s3g_1        | 2023-06-27 12:22:27,811 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
s3g_1        | STARTUP_MSG:   java = 11.0.19
s3g_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir14833680421117767797, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1        | ************************************************************/
s3g_1        | 2023-06-27 12:22:27,847 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2023-06-27 12:22:27,943 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2023-06-27 12:22:28,435 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:94)
recon_1      | 2023-06-27 12:22:35,501 [main] ERROR security.ReconCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
recon_1      | 2023-06-27 12:22:35,504 [main] INFO security.ReconCertificateClient: Certificate client init case: 0
recon_1      | 2023-06-27 12:22:35,511 [main] INFO security.ReconCertificateClient: Creating keypair for client as keypair and certificate not found.
recon_1      | 2023-06-27 12:22:37,232 [main] INFO recon.ReconServer: Init response: GETCERT
recon_1      | 2023-06-27 12:22:37,254 [main] INFO security.ReconCertificateClient: Creating CSR for Recon.
recon_1      | 2023-06-27 12:22:37,370 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.115,host:recon
recon_1      | 2023-06-27 12:22:37,371 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
recon_1      | 2023-06-27 12:22:37,384 [main] ERROR utils.CertificateSignRequest: Invalid domain recon
recon_1      | 2023-06-27 12:22:43,931 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1      | 2023-06-27 12:22:45,933 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1      | 2023-06-27 12:22:47,936 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-27 12:22:49,939 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
recon_1      | 2023-06-27 12:22:51,941 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
recon_1      | 2023-06-27 12:22:53,944 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-27 12:22:55,947 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
recon_1      | 2023-06-27 12:22:57,951 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
recon_1      | 2023-06-27 12:22:59,953 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-27 12:23:02,346 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f163470c-d3e0-4891-94bf-0bc988a6aa12 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1      | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
recon_1      | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1      | , while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
recon_1      | 2023-06-27 12:23:04,349 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
recon_1      | 2023-06-27 12:23:06,351 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
recon_1      | 2023-06-27 12:23:08,652 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 290147637819
recon_1      |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
recon_1      |            Start Date: Tue Jun 27 12:23:08 UTC 2023
recon_1      |            Final Date: Wed Jun 26 12:23:08 UTC 2024
recon_1      |             SubjectDN: CN=recon@recon,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
recon_1      |            Public Key: RSA Public Key [62:61:72:d5:70:54:d1:af:76:5e:4c:7f:ff:7a:73:7b:81:5f:e4:0e],[56:66:d1:a4]
recon_1      |         modulus: c1f55eeeb118dc473071ab0bc7ee62f5f86264e7c3a630d7ca98c2c6d14506266e8317f4b8907ad2157e668088113a8015c93f5c267c8976f92168590922cafcfa3f358a41fc5f6eab536c392fa32dea35f161100b71eec82aa860e33e0d9652e32a76fd24380db41304e53c59ab4aae8cef061ba1bef27beb0121fbad7c792dbb9fb0d5613888f76478fc837d784866246d7ac34247df52f3d82eb8f75aeabec10e3c8a101cedd4a8da5dee2f948728bb4a14f00c5393e04122c2efddf3181ab842ea33221fdf917b62b0745a9aacdb6af4f8649ca0d0955c4ad3beeb822740f9f2cdfa20305e06aceeb7529858372fc5ac2f808659280a1545238fb37459b5
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: bb5a104ac1308d044a8a5849a16f483cc5661fc4
recon_1      |                        0d978e5fb99e4be8561f8f2294949ff0dfcbd2a7
recon_1      |                        d2af6d0359a740e29374ea372f8de6628b3df734
recon_1      |                        8ff9bd42d48e376f9f9bf8d82c617a186e3718f7
recon_1      |                        af3fa86b8f448b5fa8d6806a173afd3a8b0c35c3
recon_1      |                        67c395f86d4667c7d6b8aef8e88e26ca528381fb
recon_1      |                        d00b69ab3034feedd55594e2d86da7e95fecbffd
recon_1      |                        8e0ac9df07668d3696e8f3c042f753d8f7c7518c
recon_1      |                        3a59a58e2e93e1f5e909310f981ec4a92caec7e5
recon_1      |                        cb473f82e554fa646e6c7f20321ba8d563291ab5
recon_1      |                        6492cbd846ffc230e410be4173f90c5aeefd559b
recon_1      |                        72ab93f45711b9e0d83091fa249883b49c561eeb
recon_1      |                        e291d27c84d8359002bf6ff9051327d2
om2_1        |                        c63f624dd59419fb4e3d7f57549fb866400baea0
om2_1        |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
om2_1        |                        acbb5f8d8e5f75c3208e116728c66494080d0391
om2_1        |                        4883b896992d697c14fca81b51cc41815ceb8fc5
om2_1        |                        9e72ce2f778759033901bef92ec92fabd21c5a89
om2_1        |                        c7396a443e2624b28df5d21502e09909898c3be0
om2_1        |                        4466369f70a26034a8bbf4c275f76029
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-266035175110.crt.
om2_1        | 2023-06-27 12:25:03,831 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Start Date: Tue Jun 27 12:22:43 UTC 2023
om2_1        |            Final Date: Fri Aug 04 12:22:43 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
om2_1        |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
om2_1        |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
om2_1        |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
om2_1        |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
om2_1        |                        eece43d2072924486b8a2e674f6b5465d4190ac6
om2_1        |                        f5030570b2b63224eab18c88675f9c9c24008980
om2_1        |                        86a902cf862c5da5b230958d06c3dba90324e417
om2_1        |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
om2_1        |                        1aa70751ced732e39cbf893a239b9956aef292ef
om2_1        |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
om2_1        |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
om2_1        |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
om2_1        |                        86212367c9e9316e2a4bb892f0cda036
om1_1        | 2023-06-27 12:25:42,734 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-27 12:25:42,742 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-27 12:25:42,781 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
datanode2_1  | 2023-06-27 12:26:18,330 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-27 12:26:18,332 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-27 12:26:18,665 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-27 12:26:18,838 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-27 12:26:18,901 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-27 12:26:18,910 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-27 12:26:18,913 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-27 12:26:18,914 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-27 12:26:18,929 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-27 12:26:18,930 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6c75f348-15d4-409f-ae39-b1f9e205c4e3 does not exist. Creating ...
datanode2_1  | 2023-06-27 12:26:19,002 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6c75f348-15d4-409f-ae39-b1f9e205c4e3/in_use.lock acquired by nodename 7@1b92348e430f
datanode2_1  | 2023-06-27 12:26:19,112 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6c75f348-15d4-409f-ae39-b1f9e205c4e3 has been successfully formatted.
datanode2_1  | 2023-06-27 12:26:19,251 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO ratis.ContainerStateMachine: group-B1F9E205C4E3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-27 12:26:19,271 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-27 12:26:19,391 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-27 12:26:19,394 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:19,402 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-27 12:26:19,411 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-27 12:26:19,475 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-27 12:26:19,750 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-27 12:26:19,751 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2023-06-27 12:25:42,940 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 12:25:43,920 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om1_1        | 2023-06-27 12:25:47,094 [main] INFO reflections.Reflections: Reflections took 2494 ms to scan 1 urls, producing 138 keys and 396 values [using 2 cores]
om1_1        | 2023-06-27 12:25:47,613 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om1_1        | 2023-06-27 12:25:49,333 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-27 12:25:49,338 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-27 12:25:49,339 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 12:25:52,658 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-27 12:25:53,182 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-27 12:25:58,111 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-27 12:25:59,883 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 408250958148
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Start Date: Tue Jun 27 12:25:06 UTC 2023
om1_1        |            Final Date: Wed Jun 26 12:25:06 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Public Key: RSA Public Key [f2:32:8e:59:c2:c7:d2:c8:94:a1:08:3a:ac:fb:73:0c:0e:46:5c:68],[56:66:d1:a4]
om1_1        |         modulus: b80294424ea2b645ec43bf652f217c3c9f6abc9c2f6621d1fda1818f1157c6777268efa305ff9c175e6f0ac8283e8d5e8be3d2f64ac25f4c2444ab268aec11bd2df325b2007e65ffb31e884a494eabe2cc21d1fdc9f4610501d8990ac93f4899ad0d8fddb80c624c32e58b0a9d00b301be33a6b0d244eadfed0ff1fbef9670f5facb82df5c55a3fd03c5cb50c59d4e9d167e259d3e657758c08c6bf39aa71532a4f934f82e3023c554ff5a021fb98693cbd3edaebf354b8286061a5c9ef2dec7f658382c32d0bb5a94f21d5afa911b9b0595cd1f533708379ca9500d2459b70dee135e05e2740f17bb343efd466f283e04bc0f8ca1d8fcdf36e4b1dfdfff4f8f
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 8744e62ca0715d2e115c9f2d28323744028a6b65
om1_1        |                        2bc022734665988b04509065cc84780324ac6a29
om1_1        |                        b46991022a484ee5b7ef076a9e1b85c61ed87c9a
om1_1        |                        f4d3a01d709216b97a10217463561b52a98db61a
om1_1        |                        05c7d2bae76dde651bf7a5ec38a8d64c81059ea3
om1_1        |                        a2465528f56140bb9916e6c6a55b52ffd8813b5b
om1_1        |                        fc9b67e3fda9483e80d6ea5290ca2c25ad2ecb10
om1_1        |                        1073387322c91074dff3da198541ba5834b5f6ab
om1_1        |                        6b802651072a4711c4ba4f13bda717f350fff421
om1_1        |                        91d6639eebc44fa9f780dc36b825b4ea31d8e2e7
om1_1        |                        efe2687e6f57873c89b124696db336a61cae3f92
om1_1        |                        1431e6c9b27932f595c663c141d34d776b994ce1
om1_1        |                        8085d542d35a56ac4e257848166974c7
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/408250958148.crt.
om1_1        | 2023-06-27 12:25:59,906 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 266035175110
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Start Date: Tue Jun 27 12:22:44 UTC 2023
om1_1        |            Final Date: Fri Aug 04 12:22:44 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
om1_1        |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
om1_1        |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
om1_1        |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
om1_1        |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
om1_1        |                        6575bb91718be2b62fe8285671670d4afc71639e
om1_1        |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
om1_1        |                        c63f624dd59419fb4e3d7f57549fb866400baea0
om1_1        |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
om1_1        |                        acbb5f8d8e5f75c3208e116728c66494080d0391
om1_1        |                        4883b896992d697c14fca81b51cc41815ceb8fc5
om1_1        |                        9e72ce2f778759033901bef92ec92fabd21c5a89
om1_1        |                        c7396a443e2624b28df5d21502e09909898c3be0
om1_1        |                        4466369f70a26034a8bbf4c275f76029
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0xbe
om1_1        |  from file: /data/metadata/om/certs/CA-266035175110.crt.
om1_1        | 2023-06-27 12:25:59,920 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-266035175110.crt.
om3_1        | 2023-06-27 12:25:02,707 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Start Date: Tue Jun 27 12:22:43 UTC 2023
om3_1        |            Final Date: Fri Aug 04 12:22:43 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
om3_1        |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
om3_1        |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
om3_1        |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
om3_1        |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
s3g_1        | 2023-06-27 12:22:29,298 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1        | 2023-06-27 12:22:29,299 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1        | 2023-06-27 12:22:29,511 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1        | 2023-06-27 12:22:29,537 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2023-06-27 12:22:29,543 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1        | 2023-06-27 12:22:29,667 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2023-06-27 12:22:29,669 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2023-06-27 12:22:29,671 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1        | 2023-06-27 12:22:29,750 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-27 12:22:29,781 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@aa5455e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2023-06-27 12:22:29,793 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1ad777f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | 2023-06-27 12:22:37,894 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-27 12:22:40,684 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5e1d1524{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir14833680421117767797/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-7828742206761252389/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2023-06-27 12:22:40,707 [main] INFO server.AbstractConnector: Started ServerConnector@181d7f28{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2023-06-27 12:22:40,707 [main] INFO server.Server: Started @25342ms
s3g_1        | 2023-06-27 12:22:40,709 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1        | 2023-06-27 12:22:40,709 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1        | 2023-06-27 12:22:40,715 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
datanode3_1  | 2023-06-27 12:25:44,753 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a650530b-22bc-4ab5-9901-0a035f1b699c: Started
datanode3_1  | 2023-06-27 12:25:44,839 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-27 12:25:44,840 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-27 12:25:44,876 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode3_1  | 2023-06-27 12:26:20,172 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a650530b-22bc-4ab5-9901-0a035f1b699c: addNew group-B8CCF1E980EA:[a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] returns group-B8CCF1E980EA:java.util.concurrent.CompletableFuture@38a632ee[Not completed]
datanode3_1  | 2023-06-27 12:26:20,524 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c: new RaftServerImpl for group-B8CCF1E980EA:[a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-27 12:26:20,537 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-27 12:26:20,544 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-27 12:26:20,547 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-27 12:26:20,549 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 12:26:20,550 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 12:25:43,596 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-5bf65396-4407-4e81-983e-589b8c5be712/DS-129bf1e6-dd33-448e-9770-1e49fcbe9088/container.db for volume DS-129bf1e6-dd33-448e-9770-1e49fcbe9088
datanode1_1  | 2023-06-27 12:25:43,642 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2023-06-27 12:25:43,796 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode1_1  | 2023-06-27 12:25:44,266 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode1_1  | 2023-06-27 12:25:44,266 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1a7440b0-385d-4580-b33d-c9708c19e79c
datanode1_1  | 2023-06-27 12:25:44,489 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: 1a7440b0-385d-4580-b33d-c9708c19e79c: start RPC server
datanode1_1  | 2023-06-27 12:25:44,517 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 1a7440b0-385d-4580-b33d-c9708c19e79c: GrpcService started, listening on 9858
datanode1_1  | 2023-06-27 12:25:44,586 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 1a7440b0-385d-4580-b33d-c9708c19e79c: GrpcService started, listening on 9856
datanode1_1  | 2023-06-27 12:25:44,593 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 1a7440b0-385d-4580-b33d-c9708c19e79c: GrpcService started, listening on 9857
datanode1_1  | 2023-06-27 12:25:44,664 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1a7440b0-385d-4580-b33d-c9708c19e79c is started using port 9858 for RATIS
datanode2_1  | 2023-06-27 12:26:19,751 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:19,851 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: new cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6c75f348-15d4-409f-ae39-b1f9e205c4e3
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
datanode1_1  | 2023-06-27 12:25:44,666 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1a7440b0-385d-4580-b33d-c9708c19e79c is started using port 9857 for RATIS_ADMIN
om2_1        |         DER Octet String[4] 
datanode3_1  | 2023-06-27 12:26:20,552 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-27 12:26:19,866 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-27 12:25:44,666 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1a7440b0-385d-4580-b33d-c9708c19e79c is started using port 9856 for RATIS_SERVER
om2_1        |     Tagged [2] IMPLICIT 
datanode3_1  | 2023-06-27 12:26:20,689 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA: ConfigurationManager, init=-1: peers:[a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-27 12:26:19,866 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-27 12:26:19,884 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om2_1        |         DER Octet String[8] 
datanode1_1  | 2023-06-27 12:25:44,667 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1a7440b0-385d-4580-b33d-c9708c19e79c is started using port 9855 for RATIS_DATASTREAM
datanode1_1  | 2023-06-27 12:25:44,671 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1a7440b0-385d-4580-b33d-c9708c19e79c: Started
datanode1_1  | 2023-06-27 12:25:44,784 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-27 12:25:44,784 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-27 12:25:44,865 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode1_1  | 2023-06-27 12:26:18,582 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1a7440b0-385d-4580-b33d-c9708c19e79c: addNew group-B13F88867B8F:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] returns group-B13F88867B8F:java.util.concurrent.CompletableFuture@197626f6[Not completed]
datanode1_1  | 2023-06-27 12:26:18,886 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c: new RaftServerImpl for group-B13F88867B8F:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-27 12:26:18,930 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-27 12:26:18,931 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-27 12:26:18,939 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-27 12:26:18,940 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 12:26:18,978 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 12:26:18,982 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-27 12:26:19,207 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F: ConfigurationManager, init=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-27 12:26:19,208 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 12:26:19,282 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-27 12:26:19,295 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-27 12:26:19,440 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-27 12:26:19,489 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-27 12:26:20,695 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 12:26:20,775 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-27 12:26:20,786 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-27 12:26:20,925 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-27 12:26:20,965 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-27 12:26:21,031 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-27 12:26:21,038 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-27 12:26:21,350 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-27 12:26:21,446 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 12:26:21,466 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 12:26:21,468 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-27 12:26:21,474 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-27 12:26:21,474 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-27 12:26:21,477 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-27 12:26:21,480 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea does not exist. Creating ...
datanode3_1  | 2023-06-27 12:26:21,511 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea/in_use.lock acquired by nodename 6@038f15546f77
datanode3_1  | 2023-06-27 12:26:21,555 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea has been successfully formatted.
datanode3_1  | 2023-06-27 12:26:21,641 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO ratis.ContainerStateMachine: group-B8CCF1E980EA: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-27 12:26:21,652 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-27 12:26:21,745 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-27 12:26:21,745 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:21,767 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-27 12:26:21,774 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-27 12:26:21,811 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:21,902 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-27 12:26:21,912 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1      |        Extensions: 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-06-27 12:25:03,884 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 404278112254
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Start Date: Tue Jun 27 12:25:02 UTC 2023
om2_1        |            Final Date: Wed Jun 26 12:25:02 UTC 2024
om2_1        |             SubjectDN: CN=om2,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Public Key: RSA Public Key [1c:d7:ea:ec:77:7e:e1:f5:b1:36:f9:1e:91:ed:32:d5:76:4d:33:1a],[56:66:d1:a4]
om2_1        |         modulus: ee41970aabf433a256ef824e8e6a7cb5ee9b237626bbb37cf98b94219de341b630e0b2865fa6ab38081a5561218e5436a39f19f252e7998f44353e78b6a6741d69153c97757a431b2931372e657e7df66c02c794a0527ba07e594c487ded9e06846001021c2b9789b5048390e73a5ae6c95444b9fa551477c563c1fec2c2fa1ffcaeee9c70b898566ec3c9045ebf6735e576b4a930411b2c6c306590ca61525cadef190f4860f23a763b1885fd5fc459eb12720b3af84bf3caed5ba6b8707514df1a29a3387a89cd5366e0824a711465c26dae97e8c6581399a5e3084d3b7edc7f0844e7645cf31cc8f603b93cb02da8fc9060c11fbc60aae43ab19360fc4e01
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: bb24d9d9402a1a2a3e454b94079dca370be2fc7e
om2_1        |                        282d05bd2bbc3e3ed68ae2af32e42e74281abca9
om2_1        |                        186860c93bf43fba0a9b028902684072eb05bfec
om2_1        |                        4f9ff085715f6b2cee9f27a164fdf9e2b746e3cc
om2_1        |                        d537cb04bd14590f1e5f766b85d364337347ba06
om2_1        |                        3f4bd3abb7a251e981c1425a1b2555d520be4149
om2_1        |                        621f450be1fad1dbdc88b2f86904236a702220f1
om2_1        |                        4b9c40a6dd86ffedf6afe4685aa6ba84b7d5dfef
om2_1        |                        509c47aacfe7b50dc581d123f58095df93053cbd
om2_1        |                        8ef5e77badcf2fa14b4f5686ed1fc8643d23b4f4
om2_1        |                        b436fd6f5983b68497118cf0f3b6510cb1f97aaa
om2_1        |                        4589b51e8faa35bebc4df1b7565a4e3e7f11cf49
om2_1        |                        1e9585eef02919dafb03470571544114
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
om2_1        |  from file: /data/metadata/om/certs/404278112254.crt.
om2_1        | 2023-06-27 12:25:03,934 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116798108 ms and interval 86400000 ms.
om2_1        | 2023-06-27 12:25:03,934 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | 2023-06-27 12:25:04,040 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-27 12:25:21,727 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
om2_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-27 12:25:21,844 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-06-27 12:26:19,903 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-06-27 12:26:19,904 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-27 12:26:19,920 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-27 12:26:19,929 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-27 12:26:19,930 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-27 12:26:20,036 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-06-27 12:26:20,044 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:20,278 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-27 12:26:20,282 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-06-27 12:26:20,283 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-27 12:26:20,331 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 12:26:20,332 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 12:26:20,347 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3: start as a follower, conf=-1: peers:[cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:20,347 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-06-27 12:26:20,355 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState
datanode2_1  | 2023-06-27 12:26:20,393 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B1F9E205C4E3,id=cc01b188-4e0b-44d2-a762-70392abfc581
datanode2_1  | 2023-06-27 12:26:20,407 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-27 12:26:20,407 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-27 12:26:20,409 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        |                        eece43d2072924486b8a2e674f6b5465d4190ac6
om3_1        |                        f5030570b2b63224eab18c88675f9c9c24008980
om3_1        |                        86a902cf862c5da5b230958d06c3dba90324e417
om3_1        |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
om3_1        |                        1aa70751ced732e39cbf893a239b9956aef292ef
om3_1        |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
om3_1        |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
om3_1        |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
om3_1        |                        86212367c9e9316e2a4bb892f0cda036
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-27 12:25:02,825 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797287 ms and interval 86400000 ms.
om3_1        | 2023-06-27 12:25:02,827 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      | 
recon_1      |                        critical(true) KeyUsage: 0xb8
recon_1      |  from file: /data/metadata/recon/certs/290147637819.crt.
recon_1      | 2023-06-27 12:23:08,671 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 266035175110
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
recon_1      |            Start Date: Tue Jun 27 12:22:44 UTC 2023
recon_1      |            Final Date: Fri Aug 04 12:22:44 UTC 2028
recon_1      |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode1_1  | 2023-06-27 12:26:19,546 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-27 12:26:19,568 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-27 12:26:19,766 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-27 12:26:19,963 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 12:26:19,990 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-27 12:26:20,011 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-27 12:26:20,012 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-27 12:26:20,017 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-27 12:26:20,024 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-27 12:26:20,026 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6edac7e5-f324-41f8-8d35-b13f88867b8f does not exist. Creating ...
datanode1_1  | 2023-06-27 12:26:20,082 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6edac7e5-f324-41f8-8d35-b13f88867b8f/in_use.lock acquired by nodename 7@5cbb6cac8588
datanode1_1  | 2023-06-27 12:26:20,165 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6edac7e5-f324-41f8-8d35-b13f88867b8f has been successfully formatted.
datanode1_1  | 2023-06-27 12:26:20,256 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO ratis.ContainerStateMachine: group-B13F88867B8F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-27 12:26:20,297 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-27 12:26:20,414 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-27 12:26:20,418 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:20,420 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-27 12:26:20,439 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-27 12:26:20,504 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 12:26:20,656 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-27 12:26:20,664 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-27 12:26:20,665 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:20,812 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6edac7e5-f324-41f8-8d35-b13f88867b8f
datanode1_1  | 2023-06-27 12:26:20,815 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-27 12:26:20,823 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 12:26:20,835 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 12:26:20,852 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-27 12:26:20,854 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-27 12:26:20,877 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-27 12:26:20,889 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-27 12:26:20,890 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-27 12:26:21,042 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-27 12:26:21,050 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:21,425 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-27 12:26:21,444 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-27 12:26:21,454 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-27 12:26:21,619 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 12:26:21,619 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 12:26:21,654 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F: start as a follower, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:21,668 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-27 12:26:21,693 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: start 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState
datanode1_1  | 2023-06-27 12:26:21,714 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 12:26:21,714 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 12:26:21,756 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B13F88867B8F,id=1a7440b0-385d-4580-b33d-c9708c19e79c
datanode1_1  | 2023-06-27 12:26:21,782 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-27 12:26:21,782 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-27 12:26:21,787 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-27 12:26:21,788 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-27 12:26:22,177 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=6edac7e5-f324-41f8-8d35-b13f88867b8f
datanode1_1  | 2023-06-27 12:26:22,208 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6edac7e5-f324-41f8-8d35-b13f88867b8f.
datanode1_1  | 2023-06-27 12:26:22,215 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1a7440b0-385d-4580-b33d-c9708c19e79c: addNew group-3CB73F243315:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] returns group-3CB73F243315:java.util.concurrent.CompletableFuture@78791383[Not completed]
datanode1_1  | 2023-06-27 12:26:22,398 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c: new RaftServerImpl for group-3CB73F243315:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-27 12:26:22,407 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-27 12:26:22,414 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-27 12:26:22,420 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1      |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
recon_1      |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
recon_1      |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
recon_1      |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
recon_1      |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
recon_1      |                        6575bb91718be2b62fe8285671670d4afc71639e
recon_1      |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
recon_1      |                        c63f624dd59419fb4e3d7f57549fb866400baea0
recon_1      |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
recon_1      |                        acbb5f8d8e5f75c3208e116728c66494080d0391
recon_1      |                        4883b896992d697c14fca81b51cc41815ceb8fc5
datanode2_1  | 2023-06-27 12:26:20,415 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | 2023-06-27 12:25:02,919 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
datanode2_1  | 2023-06-27 12:26:20,415 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-27 12:22:30,148 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-27 12:22:30,266 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-06-27 12:22:31,211 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 12:22:33,103 [main] INFO reflections.Reflections: Reflections took 1366 ms to scan 3 urls, producing 131 keys and 286 values 
datanode2_1  | 2023-06-27 12:26:20,419 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1        |          SerialNumber: 1
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Start Date: Tue Jun 27 12:22:43 UTC 2023
om1_1        |            Final Date: Fri Aug 04 12:22:43 UTC 2028
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om1_1        |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
om1_1        |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
om1_1        |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
om1_1        |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
om1_1        |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
om1_1        |                        eece43d2072924486b8a2e674f6b5465d4190ac6
om1_1        |                        f5030570b2b63224eab18c88675f9c9c24008980
om1_1        |                        86a902cf862c5da5b230958d06c3dba90324e417
om1_1        |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
om1_1        |                        1aa70751ced732e39cbf893a239b9956aef292ef
om1_1        |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
om1_1        |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
om1_1        |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
om1_1        |                        86212367c9e9316e2a4bb892f0cda036
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-27 12:25:59,943 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116746076 ms and interval 86400000 ms.
om1_1        | 2023-06-27 12:26:00,252 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-06-27 12:25:20,893 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T12:00Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-27 12:25:21,005 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-06-27 12:25:32,393 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-27 12:25:36,451 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-27 12:25:37,363 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-27 12:25:37,376 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
om3_1        | 2023-06-27 12:25:37,396 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-27 12:25:37,514 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 12:25:38,191 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om3_1        | 2023-06-27 12:25:40,708 [main] INFO reflections.Reflections: Reflections took 1951 ms to scan 1 urls, producing 138 keys and 396 values [using 2 cores]
om3_1        | 2023-06-27 12:25:40,912 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om3_1        | 2023-06-27 12:25:42,911 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-27 12:25:42,925 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-27 12:25:42,926 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 12:25:48,015 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-27 12:25:48,722 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-27 12:25:54,123 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-27 12:25:56,110 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 402605330599
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Start Date: Tue Jun 27 12:25:00 UTC 2023
om3_1        |            Final Date: Wed Jun 26 12:25:00 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Public Key: RSA Public Key [f9:6a:20:8d:ae:35:0c:b8:ad:52:d7:82:a6:c1:3b:f8:f4:ac:8e:41],[56:66:d1:a4]
om3_1        |         modulus: e96036f1f1f3b906e53eb0030d42921a48856614f1bfbf8d028e8ba36097ab26ccbca088610b997a3431ac5cb0a524b51e084d7dc10d37951a41e83437ebb1cd7baafa1b8c5ba8601eb5aba8e3155d53c32ff558daf900e01962ae0556a20a9a74d41e85d5d7e742f29549fcb74c03de7ab3f4f1552cfa01ce243d59159de04d26ef00e1e342a6aa4bf08358a97de149d01854ab7186d6f29270bb2b068c290efb4f10cf1e876cd68c5813e4a28cf608165ef849b4992d59f7c6ea5e6ac329070f2dac8c224428a5b227a75ef4b04d4d515b1f6b7cea752c0eef27367fe38af6fa07c01ddd291a6ff2d1fd2b1f71306211b55b5683bb383b2e28e56e24d38e35
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
datanode3_1  | 2023-06-27 12:26:21,912 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:22,059 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea
datanode3_1  | 2023-06-27 12:26:22,065 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-27 12:26:22,080 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 12:26:22,104 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:22,122 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-27 12:26:22,133 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-27 12:26:22,154 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-27 12:26:22,167 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-27 12:26:22,170 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-27 12:26:22,346 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:22,354 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:22,646 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-27 12:26:22,662 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-27 12:26:22,667 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-27 12:26:22,774 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 12:26:22,775 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 12:26:22,809 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA: start as a follower, conf=-1: peers:[a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:22,810 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-27 12:26:22,825 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState
datanode3_1  | 2023-06-27 12:26:22,874 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 12:26:22,874 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 12:26:22,882 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B8CCF1E980EA,id=a650530b-22bc-4ab5-9901-0a035f1b699c
datanode3_1  | 2023-06-27 12:26:22,896 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-27 12:26:22,898 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-27 12:26:22,899 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-27 12:26:22,904 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-27 12:26:23,060 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea
datanode3_1  | 2023-06-27 12:26:23,068 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea.
datanode3_1  | 2023-06-27 12:26:23,068 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a650530b-22bc-4ab5-9901-0a035f1b699c: addNew group-3CB73F243315:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] returns group-3CB73F243315:java.util.concurrent.CompletableFuture@6bccc7b9[Not completed]
datanode3_1  | 2023-06-27 12:26:23,112 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c: new RaftServerImpl for group-3CB73F243315:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-27 12:26:23,122 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-27 12:26:23,122 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-27 12:26:23,122 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-27 12:26:23,122 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 12:26:23,122 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 12:26:23,122 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-27 12:26:23,123 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: ConfigurationManager, init=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-27 12:26:23,123 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 12:26:23,133 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-27 12:26:23,133 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-27 12:26:23,133 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-27 12:26:23,134 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-27 12:26:23,134 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-27 12:26:23,139 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-27 12:26:23,140 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-27 12:26:23,149 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 12:26:23,157 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 12:26:23,157 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-27 12:26:23,157 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-27 12:26:23,157 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-27 12:26:23,157 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-27 12:26:23,157 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315 does not exist. Creating ...
datanode3_1  | 2023-06-27 12:26:23,170 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315/in_use.lock acquired by nodename 6@038f15546f77
datanode3_1  | 2023-06-27 12:26:23,189 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315 has been successfully formatted.
datanode3_1  | 2023-06-27 12:26:23,190 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO ratis.ContainerStateMachine: group-3CB73F243315: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-27 12:26:23,226 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-27 12:26:23,227 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-27 12:26:23,228 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:23,228 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-27 12:26:20,556 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=6c75f348-15d4-409f-ae39-b1f9e205c4e3
datanode2_1  | 2023-06-27 12:26:20,564 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6c75f348-15d4-409f-ae39-b1f9e205c4e3.
datanode2_1  | 2023-06-27 12:26:20,566 [PipelineCommandHandlerThread-0] INFO server.RaftServer: cc01b188-4e0b-44d2-a762-70392abfc581: addNew group-3CB73F243315:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] returns group-3CB73F243315:java.util.concurrent.CompletableFuture@4ceedf43[Not completed]
datanode2_1  | 2023-06-27 12:26:20,640 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581: new RaftServerImpl for group-3CB73F243315:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-27 12:26:20,646 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-27 12:26:20,648 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-27 12:26:20,654 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-27 12:26:20,654 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-27 12:26:20,655 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-27 12:26:20,669 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-27 12:26:20,675 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: ConfigurationManager, init=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-27 12:26:20,677 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-27 12:26:20,677 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-27 12:26:20,679 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-27 12:26:20,680 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-27 12:26:20,690 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-27 12:26:20,705 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-27 12:26:20,705 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-27 12:26:20,706 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-27 12:26:20,708 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-27 12:26:20,712 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-27 12:26:20,713 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-27 12:26:20,713 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-27 12:26:20,714 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-27 12:26:20,714 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-27 12:26:20,714 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315 does not exist. Creating ...
recon_1      |                        9e72ce2f778759033901bef92ec92fabd21c5a89
recon_1      |                        c7396a443e2624b28df5d21502e09909898c3be0
scm1.org_1   | 2023-06-27 12:22:33,995 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-27 12:22:34,031 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-27 12:22:34,197 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-27 12:22:34,206 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-27 12:22:34,319 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2023-06-27 12:22:39,718 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
recon_1      |                        4466369f70a26034a8bbf4c275f76029
recon_1      |        Extensions: 
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
datanode2_1  | 2023-06-27 12:26:20,726 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315/in_use.lock acquired by nodename 7@1b92348e430f
datanode2_1  | 2023-06-27 12:26:20,730 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315 has been successfully formatted.
datanode2_1  | 2023-06-27 12:26:20,737 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO ratis.ContainerStateMachine: group-3CB73F243315: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-27 12:26:20,738 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-27 12:26:20,738 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-27 12:26:20,778 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:20,780 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-27 12:26:20,784 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-27 12:26:20,785 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-27 12:26:20,787 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-27 12:26:20,791 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-06-27 12:26:20,791 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:20,791 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: new cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315
datanode2_1  | 2023-06-27 12:26:20,792 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-06-27 12:26:20,792 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-27 12:26:20,792 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
om2_1        | 2023-06-27 12:25:32,833 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-27 12:25:36,081 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-27 12:25:37,067 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-06-27 12:25:37,075 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-27 12:25:37,115 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-27 12:25:37,228 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 12:25:38,298 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om2_1        | 2023-06-27 12:25:40,410 [main] INFO reflections.Reflections: Reflections took 1609 ms to scan 1 urls, producing 138 keys and 396 values [using 2 cores]
om2_1        | 2023-06-27 12:25:40,574 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om2_1        | 2023-06-27 12:25:42,719 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-27 12:25:42,719 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-27 12:25:42,726 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 12:25:47,857 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-27 12:25:48,498 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-27 12:25:54,501 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-06-27 12:25:56,271 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 266035175110
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Start Date: Tue Jun 27 12:22:44 UTC 2023
om2_1        |            Final Date: Fri Aug 04 12:22:44 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
om2_1        |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
om2_1        |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
om2_1        |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
om2_1        |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
om2_1        |                        6575bb91718be2b62fe8285671670d4afc71639e
om2_1        |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
om2_1        |                        c63f624dd59419fb4e3d7f57549fb866400baea0
om2_1        |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
om2_1        |                        acbb5f8d8e5f75c3208e116728c66494080d0391
om2_1        |                        4883b896992d697c14fca81b51cc41815ceb8fc5
om2_1        |                        9e72ce2f778759033901bef92ec92fabd21c5a89
om2_1        |                        c7396a443e2624b28df5d21502e09909898c3be0
om2_1        |                        4466369f70a26034a8bbf4c275f76029
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0xbe
recon_1      |  from file: /data/metadata/recon/certs/CA-266035175110.crt.
recon_1      | 2023-06-27 12:23:08,689 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 1
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
recon_1      |            Start Date: Tue Jun 27 12:22:43 UTC 2023
recon_1      |            Final Date: Fri Aug 04 12:22:43 UTC 2028
recon_1      |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
recon_1      |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
recon_1      |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
recon_1      |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
recon_1      |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
recon_1      |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
recon_1      |                        eece43d2072924486b8a2e674f6b5465d4190ac6
recon_1      |                        f5030570b2b63224eab18c88675f9c9c24008980
recon_1      |                        86a902cf862c5da5b230958d06c3dba90324e417
recon_1      |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
recon_1      |                        1aa70751ced732e39cbf893a239b9956aef292ef
recon_1      |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
recon_1      |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
recon_1      |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
recon_1      |                        86212367c9e9316e2a4bb892f0cda036
recon_1      |        Extensions: 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0x6
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
recon_1      |  from file: /data/metadata/recon/certs/ROOTCA-1.crt.
recon_1      | 2023-06-27 12:23:08,709 [main] INFO security.ReconCertificateClient: CertificateLifetimeMonitor for recon is started with first delay 29116799304 ms and interval 86400000 ms.
recon_1      | 2023-06-27 12:23:08,713 [main] INFO recon.ReconServer: Successfully stored SCM signed certificate, case:GETCERT.
recon_1      | 2023-06-27 12:23:09,503 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-27 12:23:12,027 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2023-06-27 12:23:12,808 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1      | 2023-06-27 12:23:12,809 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
recon_1      | 2023-06-27 12:23:12,887 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-27 12:23:12,917 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2023-06-27 12:23:12,918 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2023-06-27 12:23:15,502 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2023-06-27 12:23:15,521 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2023-06-27 12:23:15,521 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-27 12:22:39,730 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-27 12:22:48,150 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-27 12:22:48,162 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-27 12:22:48,293 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 12:22:48,500 [main] INFO reflections.Reflections: Reflections took 148 ms to scan 3 urls, producing 131 keys and 286 values 
scm2.org_1   | 2023-06-27 12:22:48,675 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-27 12:22:48,675 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-27 12:22:48,697 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-27 12:22:48,698 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-27 12:22:48,866 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-27 12:22:48,866 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-27 12:22:48,991 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-27 12:22:53,243 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
scm2.org_1   | 2023-06-27 12:22:55,245 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
scm2.org_1   | 2023-06-27 12:22:57,247 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm2.org_1   | 2023-06-27 12:22:59,258 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
scm2.org_1   | 2023-06-27 12:23:01,941 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f163470c-d3e0-4891-94bf-0bc988a6aa12 is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
datanode1_1  | 2023-06-27 12:26:22,422 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 12:26:22,424 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 12:26:22,424 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-27 12:26:22,427 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: ConfigurationManager, init=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-27 12:26:22,437 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 12:26:22,446 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-27 12:26:22,461 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-27 12:26:22,472 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-27 12:26:22,476 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-27 12:26:22,483 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-27 12:26:22,489 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-27 12:26:22,490 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-27 12:26:22,538 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 12:26:22,549 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-27 12:26:22,550 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-27 12:26:22,550 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-27 12:26:22,550 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-27 12:26:22,559 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-27 12:26:22,569 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315 does not exist. Creating ...
datanode1_1  | 2023-06-27 12:26:22,596 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315/in_use.lock acquired by nodename 7@5cbb6cac8588
datanode1_1  | 2023-06-27 12:26:22,660 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315 has been successfully formatted.
datanode1_1  | 2023-06-27 12:26:22,709 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO ratis.ContainerStateMachine: group-3CB73F243315: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-27 12:26:22,709 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-27 12:23:39,554 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
om3_1        |             Signature: b537d62767a9addd77be653cf93fbc9b40e4bf00
om3_1        |                        6dad44ce4c1a215afe0ef214e2f0c41cf39b8b23
om3_1        |                        405e6885920e8da7b829b9b6897b32f44e826660
om3_1        |                        d15618169a9b2458e47e5b3fc05e54a9a8c9aad1
om3_1        |                        293a47f84dcc72def80e2971e6ff0f698085d2f4
om3_1        |                        f0e22a88e98fc990327a35a544e54a61d8c8ac8b
om3_1        |                        b4cd0d4a46aeceb07111a1632f2dc7d81f3e32fc
om3_1        |                        1f7dbc59aa8b9d342fa2a7fae3ce90b43dbecb19
om3_1        |                        49a5e6880240415a23a2851cc102ff034120ee73
om3_1        |                        99052858115c78132db505a9ac9c7e1b811b4976
om3_1        |                        ab703a1569e02535681127643be1a759c885cd4e
om3_1        |                        ec37eff25c45d0a66e63319e6d156eb324796b82
om3_1        |                        5468cc5919bccfc6f4079b504f1fdd44
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/402605330599.crt.
om3_1        | 2023-06-27 12:25:56,138 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 266035175110
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
datanode2_1  | 2023-06-27 12:26:20,793 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-06-27 12:26:20,793 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-27 12:26:20,794 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-27 12:26:20,797 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-27 12:26:20,797 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-27 12:26:20,799 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-06-27 12:26:20,803 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:22,278 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-cc01b188-4e0b-44d2-a762-70392abfc581: Detected pause in JVM or host machine approximately 1.213s with 1.467s GC time.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=70ms
datanode2_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1397ms
datanode2_1  | 2023-06-27 12:26:22,367 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-27 12:26:22,370 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-06-27 12:26:22,370 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-27 12:26:22,376 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 12:26:22,381 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 12:26:22,393 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: start as a follower, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:22,393 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-06-27 12:26:22,395 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState
datanode2_1  | 2023-06-27 12:26:22,416 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3CB73F243315,id=cc01b188-4e0b-44d2-a762-70392abfc581
datanode2_1  | 2023-06-27 12:26:22,417 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-06-27 12:26:22,418 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-06-27 12:26:22,419 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om1_1        | 2023-06-27 12:26:00,521 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 12:26:01,714 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1        | 2023-06-27 12:26:05,444 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1        | 2023-06-27 12:26:05,534 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2023-06-27 12:26:05,535 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2023-06-27 12:26:05,904 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om1_1        | 2023-06-27 12:26:06,010 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 12:26:06,297 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om1_1        | 2023-06-27 12:26:06,303 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om1_1        | 2023-06-27 12:26:08,193 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om1_1        | 2023-06-27 12:26:09,486 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-27 12:26:09,491 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2023-06-27 12:26:09,641 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om1_1        | 2023-06-27 12:26:09,646 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2023-06-27 12:26:10,451 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1        | 2023-06-27 12:26:10,522 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-27 12:26:10,784 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1        | 2023-06-27 12:26:10,951 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2023-06-27 12:26:11,479 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-27 12:26:11,491 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-27 12:26:11,612 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2023-06-27 12:26:11,677 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-27 12:26:11,689 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-27 12:26:11,690 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-27 12:26:11,692 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-27 12:26:11,694 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1        | 2023-06-27 12:26:11,695 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2023-06-27 12:26:11,697 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode1_1  | 2023-06-27 12:26:22,710 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-27 12:26:22,710 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        |            Start Date: Tue Jun 27 12:22:44 UTC 2023
om3_1        |            Final Date: Fri Aug 04 12:22:44 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
om3_1        |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-27 12:23:39,602 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-06-27 12:23:39,990 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 12:23:40,621 [main] INFO reflections.Reflections: Reflections took 466 ms to scan 3 urls, producing 131 keys and 286 values 
scm3.org_1   | 2023-06-27 12:23:41,003 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode1_1  | 2023-06-27 12:26:22,710 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-27 12:26:22,710 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-27 12:26:22,718 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-27 12:26:23,228 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-27 12:26:23,238 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-27 12:26:23,243 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-27 12:26:23,255 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-27 12:26:23,255 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-27 12:26:23,256 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-27 12:26:23,262 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:23,278 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:24,603 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a650530b-22bc-4ab5-9901-0a035f1b699c: Detected pause in JVM or host machine approximately 1.203s with 1.308s GC time.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=85ms
datanode3_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1223ms
datanode3_1  | 2023-06-27 12:26:24,665 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-27 12:26:24,681 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-27 12:26:24,681 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-27 12:26:24,682 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 12:26:24,684 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 12:26:24,688 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: start as a follower, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:24,716 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-27 12:26:24,720 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState
datanode3_1  | 2023-06-27 12:26:24,725 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3CB73F243315,id=a650530b-22bc-4ab5-9901-0a035f1b699c
datanode3_1  | 2023-06-27 12:26:24,725 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-27 12:26:24,725 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-27 12:26:24,725 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-27 12:26:24,725 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-27 12:26:24,729 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 12:26:24,749 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=3108f661-f070-45b5-be9d-3cb73f243315
datanode3_1  | 2023-06-27 12:26:24,783 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 12:26:24,845 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-27 12:26:28,086 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState] INFO impl.FollowerState: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5268096768ns, electionTimeout:5197ms
datanode3_1  | 2023-06-27 12:26:28,091 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: shutdown a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState
datanode3_1  | 2023-06-27 12:26:28,103 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-06-27 12:26:28,143 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode3_1  | 2023-06-27 12:26:28,143 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-FollowerState] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1
datanode3_1  | 2023-06-27 12:26:28,175 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO impl.LeaderElection: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:28,187 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO impl.LeaderElection: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode3_1  | 2023-06-27 12:26:28,232 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO impl.LeaderElection: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:28,232 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO impl.LeaderElection: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode3_1  | 2023-06-27 12:26:28,232 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: shutdown a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1
datanode3_1  | 2023-06-27 12:26:28,239 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2023-06-27 12:26:28,240 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B8CCF1E980EA with new leaderId: a650530b-22bc-4ab5-9901-0a035f1b699c
datanode3_1  | 2023-06-27 12:26:28,326 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA: change Leader from null to a650530b-22bc-4ab5-9901-0a035f1b699c at term 1 for becomeLeader, leader elected after 7323ms
datanode3_1  | 2023-06-27 12:26:28,454 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2023-06-27 12:26:28,572 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 12:26:28,578 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2023-06-27 12:26:28,647 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2023-06-27 12:26:28,647 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2023-06-27 12:26:28,657 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2023-06-27 12:26:28,763 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 12:26:28,778 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2023-06-27 12:26:28,796 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderStateImpl
datanode3_1  | 2023-06-27 12:26:29,658 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a650530b-22bc-4ab5-9901-0a035f1b699c: Detected pause in JVM or host machine approximately 0.533s with 0.803s GC time.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=803ms
scm3.org_1   | 2023-06-27 12:23:41,003 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-27 12:23:41,148 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-27 12:23:41,149 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-27 12:23:41,803 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-27 12:23:41,805 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-27 12:23:42,013 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-27 12:23:43,120 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2023-06-27 12:23:44,820 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-27 12:23:44,825 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-266035175110.crt.
om2_1        | 2023-06-27 12:25:56,318 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Start Date: Tue Jun 27 12:22:43 UTC 2023
om2_1        |            Final Date: Fri Aug 04 12:22:43 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
om2_1        |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
om2_1        |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
om2_1        |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
om2_1        |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
om2_1        |                        eece43d2072924486b8a2e674f6b5465d4190ac6
om2_1        |                        f5030570b2b63224eab18c88675f9c9c24008980
om2_1        |                        86a902cf862c5da5b230958d06c3dba90324e417
om2_1        |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
om2_1        |                        1aa70751ced732e39cbf893a239b9956aef292ef
om2_1        |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
om2_1        |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
om2_1        |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
om2_1        |                        86212367c9e9316e2a4bb892f0cda036
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
datanode2_1  | 2023-06-27 12:26:22,420 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-06-27 12:26:22,431 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
om3_1        |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
om3_1        |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
om3_1        |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
om3_1        |                        6575bb91718be2b62fe8285671670d4afc71639e
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-27 12:22:39,732 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2023-06-27 12:22:39,752 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2023-06-27 12:22:42,250 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2023-06-27 12:22:43,923 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-27 12:22:43,927 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-06-27 12:22:44,081 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712 to CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712, valid from Tue Jun 27 12:22:43 UTC 2023 to Fri Aug 04 12:22:43 UTC 2028
scm1.org_1   | 2023-06-27 12:22:44,137 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-27 12:22:44,143 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-06-27 12:22:44,144 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:f163470c-d3e0-4891-94bf-0bc988a6aa12,clusterId:CID-5bf65396-4407-4e81-983e-589b8c5be712,subject:scm-sub@scm1.org
scm1.org_1   | 2023-06-27 12:22:44,331 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2023-06-27 12:22:44,535 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-27 12:22:44,622 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 12:22:44,624 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-27 12:22:44,624 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 12:22:44,626 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-27 12:22:44,626 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-06-27 12:22:44,626 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-27 12:22:44,628 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-06-27 12:22:44,630 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:44,631 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-06-27 12:22:44,632 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 12:22:44,644 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 12:22:44,648 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:44,649 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-27 12:22:44,885 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-27 12:22:44,889 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:44,889 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-27 12:22:44,890 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-27 12:22:44,890 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 12:22:44,893 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 12:22:44,901 [main] INFO server.RaftServer: f163470c-d3e0-4891-94bf-0bc988a6aa12: addNew group-589B8C5BE712:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] returns group-589B8C5BE712:java.util.concurrent.CompletableFuture@22aee519[Not completed]
scm1.org_1   | 2023-06-27 12:22:44,936 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12: new RaftServerImpl for group-589B8C5BE712:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-27 12:22:44,938 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-27 12:22:44,939 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-27 12:22:44,939 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-27 12:22:44,940 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-27 12:22:44,940 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 12:22:44,940 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-27 12:22:44,948 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: ConfigurationManager, init=-1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-27 12:22:44,948 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 12:22:44,953 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-06-27 12:22:44,955 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-27 12:22:44,982 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-27 12:22:44,986 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-27 12:22:44,992 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-27 12:22:44,992 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-27 12:22:45,014 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-27 12:22:45,031 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-27 12:22:45,186 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 12:22:45,189 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 12:22:45,189 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:45,190 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-27 12:22:45,191 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:45,191 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-27 12:26:29,981 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO impl.FollowerState: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5264699852ns, electionTimeout:5198ms
datanode3_1  | 2023-06-27 12:26:30,024 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: shutdown a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState
datanode3_1  | 2023-06-27 12:26:30,024 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-06-27 12:26:30,025 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode3_1  | 2023-06-27 12:26:30,025 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2
datanode3_1  | 2023-06-27 12:26:30,085 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:30,144 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-06-27 12:26:30,346 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 12:26:30,379 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 12:26:30,377 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 1a7440b0-385d-4580-b33d-c9708c19e79c
datanode3_1  | 2023-06-27 12:26:30,393 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for cc01b188-4e0b-44d2-a762-70392abfc581
datanode3_1  | 2023-06-27 12:26:31,458 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-LeaderElection1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA: set configuration 0: peers:[a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:32,620 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-B8CCF1E980EA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea/current/log_inprogress_0
datanode3_1  | 2023-06-27 12:26:34,209 [grpc-default-executor-1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: receive requestVote(PRE_VOTE, cc01b188-4e0b-44d2-a762-70392abfc581, group-3CB73F243315, 0, (t:0, i:0))
datanode3_1  | 2023-06-27 12:26:34,223 [grpc-default-executor-1] INFO impl.VoteContext: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-CANDIDATE: accept PRE_VOTE from cc01b188-4e0b-44d2-a762-70392abfc581: our priority 0 <= candidate's priority 0
datanode3_1  | 2023-06-27 12:26:34,241 [grpc-default-executor-1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315 replies to PRE_VOTE vote request: cc01b188-4e0b-44d2-a762-70392abfc581<-a650530b-22bc-4ab5-9901-0a035f1b699c#0:OK-t0. Peer's state: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315:t0, leader=null, voted=, raftlog=Memoized:a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:34,862 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode3_1  | 2023-06-27 12:26:34,867 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection:   Response 0: a650530b-22bc-4ab5-9901-0a035f1b699c<-1a7440b0-385d-4580-b33d-c9708c19e79c#0:FAIL-t0
datanode3_1  | 2023-06-27 12:26:34,867 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection:   Response 1: a650530b-22bc-4ab5-9901-0a035f1b699c<-cc01b188-4e0b-44d2-a762-70392abfc581#0:OK-t0
datanode3_1  | 2023-06-27 12:26:34,868 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2 PRE_VOTE round 0: result REJECTED
datanode3_1  | 2023-06-27 12:26:34,897 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode3_1  | 2023-06-27 12:26:34,897 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: shutdown a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2
datanode3_1  | 2023-06-27 12:26:34,898 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-LeaderElection2] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState
datanode3_1  | 2023-06-27 12:26:35,783 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-27 12:26:36,480 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315.
datanode3_1  | 2023-06-27 12:26:36,485 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a650530b-22bc-4ab5-9901-0a035f1b699c: addNew group-561639362392:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-561639362392:java.util.concurrent.CompletableFuture@6d7c4034[Not completed]
datanode2_1  | 2023-06-27 12:26:22,443 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-27 12:26:22,438 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=3108f661-f070-45b5-be9d-3cb73f243315
datanode2_1  | 2023-06-27 12:26:22,531 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-27 12:26:25,555 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState] INFO impl.FollowerState: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5203357981ns, electionTimeout:5141ms
datanode2_1  | 2023-06-27 12:26:25,559 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: shutdown cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState
datanode2_1  | 2023-06-27 12:26:25,563 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-06-27 12:26:25,587 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode2_1  | 2023-06-27 12:26:25,588 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-FollowerState] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1
datanode2_1  | 2023-06-27 12:26:25,622 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO impl.LeaderElection: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:25,634 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO impl.LeaderElection: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode2_1  | 2023-06-27 12:26:25,654 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO impl.LeaderElection: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:25,658 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO impl.LeaderElection: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode2_1  | 2023-06-27 12:26:25,658 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: shutdown cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1
datanode2_1  | 2023-06-27 12:26:25,669 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode2_1  | 2023-06-27 12:26:25,674 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B1F9E205C4E3 with new leaderId: cc01b188-4e0b-44d2-a762-70392abfc581
datanode2_1  | 2023-06-27 12:26:25,751 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3: change Leader from null to cc01b188-4e0b-44d2-a762-70392abfc581 at term 1 for becomeLeader, leader elected after 7468ms
datanode2_1  | 2023-06-27 12:26:25,914 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2023-06-27 12:26:26,027 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2023-06-27 12:26:26,044 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode2_1  | 2023-06-27 12:26:26,147 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2023-06-27 12:26:26,149 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2023-06-27 12:26:26,157 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2023-06-27 12:26:26,293 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2023-06-27 12:26:26,331 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode2_1  | 2023-06-27 12:26:26,361 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderStateImpl
datanode2_1  | 2023-06-27 12:26:26,592 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-06-27 12:26:27,807 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-cc01b188-4e0b-44d2-a762-70392abfc581: Detected pause in JVM or host machine approximately 0.519s with 1.004s GC time.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=1004ms
datanode2_1  | 2023-06-27 12:26:27,806 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] WARN impl.FollowerState: Unexpected long sleep: sleep 5056ms but took extra 306829587ns (> threshold = 300ms)
datanode2_1  | 2023-06-27 12:26:27,854 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-27 12:26:27,855 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-27 12:26:28,417 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-LeaderElection1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3: set configuration 0: peers:[cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:29,543 [cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-B1F9E205C4E3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6c75f348-15d4-409f-ae39-b1f9e205c4e3/current/log_inprogress_0
datanode2_1  | 2023-06-27 12:26:32,996 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO impl.FollowerState: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState: change to CANDIDATE, lastRpcElapsedTime:10600640521ns, electionTimeout:5139ms
datanode2_1  | 2023-06-27 12:26:32,999 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: shutdown cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState
datanode2_1  | 2023-06-27 12:26:33,000 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-06-27 12:26:33,001 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode2_1  | 2023-06-27 12:26:33,006 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2
datanode2_1  | 2023-06-27 12:26:33,031 [grpc-default-executor-1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: receive requestVote(PRE_VOTE, a650530b-22bc-4ab5-9901-0a035f1b699c, group-3CB73F243315, 0, (t:0, i:0))
datanode2_1  | 2023-06-27 12:26:33,068 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:33,108 [grpc-default-executor-1] INFO impl.VoteContext: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-CANDIDATE: accept PRE_VOTE from a650530b-22bc-4ab5-9901-0a035f1b699c: our priority 0 <= candidate's priority 0
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om3_1        |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
datanode1_1  | 2023-06-27 12:26:22,742 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-27 12:26:22,747 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-27 12:26:22,747 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-27 12:26:22,747 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-27 12:26:22,749 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-27 12:26:22,756 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-27 12:26:22,807 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:25,385 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1a7440b0-385d-4580-b33d-c9708c19e79c: Detected pause in JVM or host machine approximately 2.382s with 2.545s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=94ms
datanode1_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2451ms
datanode1_1  | 2023-06-27 12:26:25,493 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-27 12:26:25,510 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-27 12:26:25,513 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-27 12:26:25,516 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 12:26:25,521 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 12:26:25,547 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: start as a follower, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:25,549 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-27 12:26:25,554 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: start 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState
datanode1_1  | 2023-06-27 12:26:25,560 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3CB73F243315,id=1a7440b0-385d-4580-b33d-c9708c19e79c
datanode1_1  | 2023-06-27 12:26:25,562 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-27 12:26:25,563 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-27 12:26:25,569 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-27 12:26:25,574 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-27 12:26:25,572 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 12:26:25,591 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 12:26:25,594 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=3108f661-f070-45b5-be9d-3cb73f243315
datanode1_1  | 2023-06-27 12:26:25,881 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-27 12:26:26,880 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState] INFO impl.FollowerState: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5199195362ns, electionTimeout:5163ms
datanode1_1  | 2023-06-27 12:26:26,884 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: shutdown 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState
datanode1_1  | 2023-06-27 12:26:26,894 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-27 12:26:26,922 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-27 12:26:26,922 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-FollowerState] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: start 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1
datanode1_1  | 2023-06-27 12:26:26,966 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:26,975 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode1_1  | 2023-06-27 12:26:27,018 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:27,018 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2023-06-27 12:26:27,020 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: shutdown 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1
datanode1_1  | 2023-06-27 12:26:27,025 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2023-06-27 12:26:27,031 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B13F88867B8F with new leaderId: 1a7440b0-385d-4580-b33d-c9708c19e79c
datanode1_1  | 2023-06-27 12:26:27,102 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F: change Leader from null to 1a7440b0-385d-4580-b33d-c9708c19e79c at term 1 for becomeLeader, leader elected after 7597ms
datanode1_1  | 2023-06-27 12:26:27,262 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-06-27 12:26:27,384 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 12:26:27,402 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-06-27 12:26:27,498 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-06-27 12:26:27,503 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-06-27 12:26:27,517 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-06-27 12:26:27,636 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om3_1        |                        c63f624dd59419fb4e3d7f57549fb866400baea0
om3_1        |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
om3_1        |                        acbb5f8d8e5f75c3208e116728c66494080d0391
om3_1        |                        4883b896992d697c14fca81b51cc41815ceb8fc5
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
scm2.org_1   | 2023-06-27 12:23:03,945 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
scm2.org_1   | 2023-06-27 12:23:06,082 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
recon_1      | 2023-06-27 12:23:15,549 [main] INFO util.log: Logging initialized @59586ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2023-06-27 12:23:15,802 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1      | 2023-06-27 12:23:15,816 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1      | 2023-06-27 12:23:15,867 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2023-06-27 12:23:15,868 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 2023-06-27 12:23:15,868 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2023-06-27 12:23:15,877 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2023-06-27 12:23:16,017 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1      | 2023-06-27 12:23:16,031 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1      | 2023-06-27 12:23:16,753 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2023-06-27 12:23:16,783 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1      | 2023-06-27 12:23:16,798 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1      | 2023-06-27 12:23:16,847 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1      | 2023-06-27 12:23:18,919 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 12:23:19,645 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 12:23:19,979 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2023-06-27 12:23:19,990 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2023-06-27 12:23:20,404 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 12:23:20,963 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1      | 2023-06-27 12:23:21,053 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2023-06-27 12:23:21,136 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2023-06-27 12:23:21,165 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2023-06-27 12:23:21,197 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1      | 2023-06-27 12:23:22,396 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1      | 2023-06-27 12:23:22,491 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2023-06-27 12:23:22,526 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2023-06-27 12:23:22,654 [Listener at 0.0.0.0/9891] INFO hdds.HddsUtils: Restoring thread name: main
recon_1      | 2023-06-27 12:23:22,690 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1      | 2023-06-27 12:23:23,061 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1      | 2023-06-27 12:23:23,093 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2023-06-27 12:23:23,094 [main] INFO recon.ReconServer: Starting Recon server
recon_1      | 2023-06-27 12:23:23,415 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2023-06-27 12:23:23,445 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2023-06-27 12:23:23,445 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2023-06-27 12:23:24,760 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2023-06-27 12:23:24,774 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1      | 2023-06-27 12:23:24,957 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2023-06-27 12:23:24,958 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2023-06-27 12:23:24,970 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1      | 2023-06-27 12:23:25,134 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-27 12:23:25,162 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@10395bc4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1      | 2023-06-27 12:23:25,163 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@670e6fa3{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1      | 2023-06-27 12:23:26,848 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-27 12:23:26,862 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-27 12:23:33,283 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@13391309{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-7994654434592817034/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1      | 2023-06-27 12:23:33,315 [main] INFO server.AbstractConnector: Started ServerConnector@6049b4c9{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1      | 2023-06-27 12:23:33,317 [main] INFO server.Server: Started @77362ms
recon_1      | 2023-06-27 12:23:33,320 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1      | 2023-06-27 12:23:33,320 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1      | 2023-06-27 12:23:33,324 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1      | 2023-06-27 12:23:33,324 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1      | 2023-06-27 12:23:33,363 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1      | 2023-06-27 12:23:33,398 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1      | 2023-06-27 12:23:33,398 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1      | 2023-06-27 12:23:33,398 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 12:23:33,401 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1      | 2023-06-27 12:23:33,404 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1      | 2023-06-27 12:23:34,539 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1      | 2023-06-27 12:23:34,539 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2023-06-27 12:23:34,539 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1      | 2023-06-27 12:23:34,540 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1      | 2023-06-27 12:23:34,565 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1      | 2023-06-27 12:23:34,715 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1      | 2023-06-27 12:23:53,402 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
om3_1        |                        9e72ce2f778759033901bef92ec92fabd21c5a89
om3_1        |                        c7396a443e2624b28df5d21502e09909898c3be0
om3_1        |                        4466369f70a26034a8bbf4c275f76029
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-266035175110.crt.
om3_1        | 2023-06-27 12:25:56,164 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Start Date: Tue Jun 27 12:22:43 UTC 2023
om3_1        |            Final Date: Fri Aug 04 12:22:43 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om3_1        |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
om3_1        |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
om3_1        |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
om3_1        |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
om3_1        |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
om3_1        |                        eece43d2072924486b8a2e674f6b5465d4190ac6
om3_1        |                        f5030570b2b63224eab18c88675f9c9c24008980
om3_1        |                        86a902cf862c5da5b230958d06c3dba90324e417
om3_1        |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
om3_1        |                        1aa70751ced732e39cbf893a239b9956aef292ef
om3_1        |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
om3_1        |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
om3_1        |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
om3_1        |                        86212367c9e9316e2a4bb892f0cda036
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-27 12:25:56,204 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116743830 ms and interval 86400000 ms.
om3_1        | 2023-06-27 12:25:56,379 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
om1_1        | 2023-06-27 12:26:11,711 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 12:26:11,712 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2023-06-27 12:26:11,719 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-27 12:26:11,786 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-06-27 12:26:11,825 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1        | 2023-06-27 12:26:11,829 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-06-27 12:26:13,955 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1        | 2023-06-27 12:26:13,980 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1        | 2023-06-27 12:26:13,983 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2.org_1   | 2023-06-27 12:23:06,894 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om1_1        | 2023-06-27 12:26:13,983 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-06-27 12:26:13,983 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-06-27 12:26:14,007 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-27 12:26:14,091 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@261d8a9c[Not completed]
om1_1        | 2023-06-27 12:26:14,092 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1        | 2023-06-27 12:26:14,157 [main] INFO om.OzoneManager: Creating RPC Server
om1_1        | 2023-06-27 12:26:14,225 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1        | 2023-06-27 12:26:14,253 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2023-06-27 12:26:14,264 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2023-06-27 12:26:14,265 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2023-06-27 12:26:14,265 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-27 12:25:56,383 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 404278112254
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Start Date: Tue Jun 27 12:25:02 UTC 2023
om2_1        |            Final Date: Wed Jun 26 12:25:02 UTC 2024
om2_1        |             SubjectDN: CN=om2,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
om2_1        |            Public Key: RSA Public Key [1c:d7:ea:ec:77:7e:e1:f5:b1:36:f9:1e:91:ed:32:d5:76:4d:33:1a],[56:66:d1:a4]
om2_1        |         modulus: ee41970aabf433a256ef824e8e6a7cb5ee9b237626bbb37cf98b94219de341b630e0b2865fa6ab38081a5561218e5436a39f19f252e7998f44353e78b6a6741d69153c97757a431b2931372e657e7df66c02c794a0527ba07e594c487ded9e06846001021c2b9789b5048390e73a5ae6c95444b9fa551477c563c1fec2c2fa1ffcaeee9c70b898566ec3c9045ebf6735e576b4a930411b2c6c306590ca61525cadef190f4860f23a763b1885fd5fc459eb12720b3af84bf3caed5ba6b8707514df1a29a3387a89cd5366e0824a711465c26dae97e8c6581399a5e3084d3b7edc7f0844e7645cf31cc8f603b93cb02da8fc9060c11fbc60aae43ab19360fc4e01
om1_1        | 2023-06-27 12:26:14,267 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-06-27 12:26:14,268 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2023-06-27 12:26:14,345 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1        | 2023-06-27 12:26:14,346 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-27 12:26:14,405 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2023-06-27 12:26:14,409 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 2023-06-27 12:26:14,690 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-27 12:23:06,905 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: bb24d9d9402a1a2a3e454b94079dca370be2fc7e
om2_1        |                        282d05bd2bbc3e3ed68ae2af32e42e74281abca9
om2_1        |                        186860c93bf43fba0a9b028902684072eb05bfec
om2_1        |                        4f9ff085715f6b2cee9f27a164fdf9e2b746e3cc
datanode2_1  | 2023-06-27 12:26:33,215 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om1_1        | 2023-06-27 12:26:14,756 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1        | 2023-06-27 12:26:14,782 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2023-06-27 12:26:14,783 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2023-06-27 12:26:15,179 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om1_1        | 2023-06-27 12:26:15,633 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-27 12:26:15,677 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-06-27 12:26:15,688 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1        | 2023-06-27 12:26:15,688 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1        | 2023-06-27 12:26:15,697 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1        | 2023-06-27 12:26:15,697 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1        | 2023-06-27 12:26:19,215 [main] INFO reflections.Reflections: Reflections took 4295 ms to scan 8 urls, producing 24 keys and 639 values [using 2 cores]
om1_1        | 2023-06-27 12:26:23,242 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2023-06-27 12:26:23,357 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2023-06-27 12:26:23,774 [Listener at om1/9862] INFO hdds.HddsUtils: Restoring thread name: main
om1_1        | 2023-06-27 12:26:35,276 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2023-06-27 12:26:35,557 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2023-06-27 12:26:35,559 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2023-06-27 12:26:36,361 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2023-06-27 12:26:36,364 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2023-06-27 12:26:36,398 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1        | 2023-06-27 12:26:36,441 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om1
om1_1        | 2023-06-27 12:26:36,560 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1        | 2023-06-27 12:26:36,618 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2023-06-27 12:26:36,718 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2023-06-27 12:26:36,722 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 12:26:36,736 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1        | 2023-06-27 12:26:36,753 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1        | 2023-06-27 12:26:36,787 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-06-27 12:26:36,845 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2023-06-27 12:26:36,849 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2023-06-27 12:26:36,852 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 12:26:36,912 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1        | 2023-06-27 12:26:36,914 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2023-06-27 12:26:36,917 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2023-06-27 12:26:36,935 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        |                        d537cb04bd14590f1e5f766b85d364337347ba06
om2_1        |                        3f4bd3abb7a251e981c1425a1b2555d520be4149
om2_1        |                        621f450be1fad1dbdc88b2f86904236a702220f1
om2_1        |                        4b9c40a6dd86ffedf6afe4685aa6ba84b7d5dfef
om2_1        |                        509c47aacfe7b50dc581d123f58095df93053cbd
om2_1        |                        8ef5e77badcf2fa14b4f5686ed1fc8643d23b4f4
om2_1        |                        b436fd6f5983b68497118cf0f3b6510cb1f97aaa
om2_1        |                        4589b51e8faa35bebc4df1b7565a4e3e7f11cf49
om2_1        |                        1e9585eef02919dafb03470571544114
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
om2_1        |  from file: /data/metadata/om/certs/404278112254.crt.
om2_1        | 2023-06-27 12:25:56,407 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116745609 ms and interval 86400000 ms.
datanode2_1  | 2023-06-27 12:26:33,290 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-27 12:26:33,306 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 1a7440b0-385d-4580-b33d-c9708c19e79c
datanode2_1  | 2023-06-27 12:26:33,308 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for a650530b-22bc-4ab5-9901-0a035f1b699c
datanode2_1  | 2023-06-27 12:26:33,327 [grpc-default-executor-1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315 replies to PRE_VOTE vote request: a650530b-22bc-4ab5-9901-0a035f1b699c<-cc01b188-4e0b-44d2-a762-70392abfc581#0:OK-t0. Peer's state: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315:t0, leader=null, voted=, raftlog=Memoized:cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:35,156 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-cc01b188-4e0b-44d2-a762-70392abfc581: Detected pause in JVM or host machine approximately 0.257s without any GCs.
datanode2_1  | 2023-06-27 12:26:35,426 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode2_1  | 2023-06-27 12:26:35,429 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection:   Response 0: cc01b188-4e0b-44d2-a762-70392abfc581<-1a7440b0-385d-4580-b33d-c9708c19e79c#0:FAIL-t0
datanode2_1  | 2023-06-27 12:26:35,429 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection:   Response 1: cc01b188-4e0b-44d2-a762-70392abfc581<-a650530b-22bc-4ab5-9901-0a035f1b699c#0:OK-t0
datanode2_1  | 2023-06-27 12:26:35,431 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2 PRE_VOTE round 0: result REJECTED
datanode2_1  | 2023-06-27 12:26:35,443 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode2_1  | 2023-06-27 12:26:35,455 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: shutdown cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2
datanode2_1  | 2023-06-27 12:26:35,465 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-LeaderElection2] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState
datanode2_1  | 2023-06-27 12:26:35,460 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-27 12:26:36,373 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315.
datanode2_1  | 2023-06-27 12:26:36,373 [PipelineCommandHandlerThread-0] INFO server.RaftServer: cc01b188-4e0b-44d2-a762-70392abfc581: addNew group-561639362392:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-561639362392:java.util.concurrent.CompletableFuture@512b3d76[Not completed]
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-27 12:23:06,911 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2023-06-27 12:23:06,914 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2023-06-27 12:23:07,938 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-27 12:23:44,829 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm3.org_1   | 2023-06-27 12:23:44,832 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm3.org_1   | 2023-06-27 12:23:46,765 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2023-06-27 12:23:46,877 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2023-06-27 12:23:46,878 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2023-06-27 12:23:46,884 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:2cf3494c-faba-4bd8-bf38-71b782f966a2,clusterId:CID-5bf65396-4407-4e81-983e-589b8c5be712,subject:scm-sub@scm3.org
scm3.org_1   | 2023-06-27 12:23:48,024 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2023-06-27 12:23:48,086 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-5bf65396-4407-4e81-983e-589b8c5be712, SCMID 2cf3494c-faba-4bd8-bf38-71b782f966a2
scm3.org_1   | 2023-06-27 12:23:48,087 [main] INFO server.StorageContainerManager: Primary SCM Node ID f163470c-d3e0-4891-94bf-0bc988a6aa12
scm3.org_1   | 2023-06-27 12:23:48,131 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-27 12:23:51,951 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
datanode1_1  | 2023-06-27 12:26:27,662 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2023-06-27 12:26:27,709 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: start 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderStateImpl
datanode1_1  | 2023-06-27 12:26:28,169 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-06-27 12:26:28,949 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-LeaderElection1] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F: set configuration 0: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:29,906 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-B13F88867B8F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6edac7e5-f324-41f8-8d35-b13f88867b8f/current/log_inprogress_0
datanode1_1  | 2023-06-27 12:26:31,141 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1a7440b0-385d-4580-b33d-c9708c19e79c: Detected pause in JVM or host machine approximately 0.710s with 0.768s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=768ms
datanode1_1  | 2023-06-27 12:26:31,130 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] WARN impl.FollowerState: Unexpected long sleep: sleep 5048ms but took extra 490340666ns (> threshold = 300ms)
datanode1_1  | 2023-06-27 12:26:31,171 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 12:26:31,171 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 12:26:34,397 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: receive requestVote(PRE_VOTE, a650530b-22bc-4ab5-9901-0a035f1b699c, group-3CB73F243315, 0, (t:0, i:0))
datanode1_1  | 2023-06-27 12:26:34,510 [grpc-default-executor-1] INFO impl.VoteContext: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FOLLOWER: reject PRE_VOTE from a650530b-22bc-4ab5-9901-0a035f1b699c: our priority 1 > candidate's priority 0
datanode1_1  | 2023-06-27 12:26:34,679 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315 replies to PRE_VOTE vote request: a650530b-22bc-4ab5-9901-0a035f1b699c<-1a7440b0-385d-4580-b33d-c9708c19e79c#0:FAIL-t0. Peer's state: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315:t0, leader=null, voted=, raftlog=Memoized:1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:35,351 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: receive requestVote(PRE_VOTE, cc01b188-4e0b-44d2-a762-70392abfc581, group-3CB73F243315, 0, (t:0, i:0))
datanode1_1  | 2023-06-27 12:26:35,353 [grpc-default-executor-1] INFO impl.VoteContext: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FOLLOWER: reject PRE_VOTE from cc01b188-4e0b-44d2-a762-70392abfc581: our priority 1 > candidate's priority 0
datanode1_1  | 2023-06-27 12:26:35,354 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315 replies to PRE_VOTE vote request: cc01b188-4e0b-44d2-a762-70392abfc581<-1a7440b0-385d-4580-b33d-c9708c19e79c#0:FAIL-t0. Peer's state: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315:t0, leader=null, voted=, raftlog=Memoized:1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:36,368 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-27 12:26:36,390 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO impl.FollowerState: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState: change to CANDIDATE, lastRpcElapsedTime:10835889470ns, electionTimeout:5153ms
datanode1_1  | 2023-06-27 12:26:36,440 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: shutdown 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState
datanode1_1  | 2023-06-27 12:26:36,441 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-27 12:26:36,442 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-27 12:26:36,442 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-FollowerState] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: start 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2
datanode1_1  | 2023-06-27 12:26:36,540 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:36,749 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 12:26:36,750 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 12:26:36,790 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for a650530b-22bc-4ab5-9901-0a035f1b699c
datanode1_1  | 2023-06-27 12:26:36,852 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for cc01b188-4e0b-44d2-a762-70392abfc581
datanode1_1  | 2023-06-27 12:26:37,622 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315.
datanode1_1  | 2023-06-27 12:26:37,626 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1a7440b0-385d-4580-b33d-c9708c19e79c: addNew group-561639362392:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-561639362392:java.util.concurrent.CompletableFuture@30dac3e[Not completed]
datanode1_1  | 2023-06-27 12:26:37,635 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c: new RaftServerImpl for group-561639362392:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-27 12:26:37,635 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-27 12:26:37,635 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-27 12:26:37,635 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-27 12:26:37,635 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 12:26:37,635 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 12:26:37,635 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-27 12:26:37,636 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392: ConfigurationManager, init=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-27 12:26:37,641 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 12:26:37,641 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-27 12:26:37,642 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-27 12:26:37,642 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-27 12:26:37,642 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-27 12:26:37,642 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-27 12:26:37,642 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-27 12:26:37,642 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-27 12:26:37,650 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
om3_1        | 2023-06-27 12:25:56,647 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 12:25:57,698 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1        | 2023-06-27 12:26:01,607 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1        | 2023-06-27 12:26:01,789 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2023-06-27 12:26:01,790 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2023-06-27 12:26:02,026 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om3_1        | 2023-06-27 12:26:02,114 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 12:26:02,357 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1        | 2023-06-27 12:26:02,369 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om3_1        | 2023-06-27 12:26:04,111 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om3_1        | 2023-06-27 12:26:04,899 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-27 12:26:04,903 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2023-06-27 12:25:56,781 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
scm1.org_1   | 2023-06-27 12:22:45,192 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712 does not exist. Creating ...
datanode3_1  | 2023-06-27 12:26:36,500 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c: new RaftServerImpl for group-561639362392:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-27 12:26:36,502 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-27 12:26:36,503 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-27 12:26:36,503 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-27 12:26:36,514 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 12:26:36,514 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 12:26:36,515 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-27 12:26:36,515 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392: ConfigurationManager, init=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-27 12:26:36,519 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 12:26:36,521 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-27 12:26:36,521 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-27 12:26:36,521 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-27 12:26:36,521 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-27 12:26:36,522 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-27 12:26:36,522 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-27 12:26:36,523 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-27 12:26:36,524 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 12:26:36,531 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 12:26:36,531 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-27 12:26:36,532 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-27 12:26:36,532 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-27 12:26:36,532 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-27 12:26:36,534 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392 does not exist. Creating ...
datanode2_1  | 2023-06-27 12:26:36,386 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581: new RaftServerImpl for group-561639362392:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-27 12:26:36,386 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-27 12:26:36,387 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-27 12:26:36,388 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-27 12:26:36,389 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-27 12:26:36,389 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-27 12:26:36,389 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-27 12:26:36,394 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392: ConfigurationManager, init=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-27 12:26:36,395 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-27 12:26:36,400 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2023-06-27 12:26:05,013 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
recon_1      | 2023-06-27 12:23:53,405 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
scm1.org_1   | 2023-06-27 12:22:45,198 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/in_use.lock acquired by nodename 13@scm1.org
scm1.org_1   | 2023-06-27 12:22:45,204 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712 has been successfully formatted.
scm1.org_1   | 2023-06-27 12:22:45,210 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-27 12:22:45,219 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-27 12:22:45,220 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:45,221 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-27 12:22:45,223 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-27 12:22:45,225 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 12:22:45,230 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-27 12:22:45,231 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-27 12:22:45,231 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:45,236 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   | 2023-06-27 12:22:45,236 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 12:22:45,237 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 12:22:45,238 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 12:22:45,239 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-27 12:22:45,240 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-27 12:22:45,240 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-27 12:22:45,241 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-27 12:22:45,241 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-27 12:22:45,265 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-27 12:22:45,265 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:45,286 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 12:22:45,287 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 12:22:45,287 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-06-27 12:22:45,293 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-27 12:22:45,294 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-27 12:22:45,296 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: start as a follower, conf=-1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:22:45,299 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2023-06-27 12:22:45,301 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: start f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState
scm1.org_1   | 2023-06-27 12:22:45,309 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-27 12:22:45,309 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-27 12:22:45,312 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-589B8C5BE712,id=f163470c-d3e0-4891-94bf-0bc988a6aa12
scm1.org_1   | 2023-06-27 12:22:45,315 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-27 12:22:45,319 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-27 12:22:45,320 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-27 12:22:45,320 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-27 12:22:45,324 [main] INFO server.RaftServer: f163470c-d3e0-4891-94bf-0bc988a6aa12: start RPC server
scm1.org_1   | 2023-06-27 12:22:45,380 [main] INFO server.GrpcService: f163470c-d3e0-4891-94bf-0bc988a6aa12: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-27 12:22:45,386 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f163470c-d3e0-4891-94bf-0bc988a6aa12: Started
scm1.org_1   | 2023-06-27 12:22:50,458 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO impl.FollowerState: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5157229101ns, electionTimeout:5147ms
scm1.org_1   | 2023-06-27 12:22:50,460 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: shutdown f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState
om2_1        | 2023-06-27 12:25:57,073 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 12:25:58,195 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1        | 2023-06-27 12:26:01,344 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1        | 2023-06-27 12:26:01,518 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2023-06-27 12:26:01,522 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2023-06-27 12:26:01,750 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om2_1        | 2023-06-27 12:26:01,883 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 12:26:02,171 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om2_1        | 2023-06-27 12:26:02,178 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om2_1        | 2023-06-27 12:26:03,708 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om2_1        | 2023-06-27 12:26:04,348 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-27 12:26:04,352 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2023-06-27 12:26:04,496 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om2_1        | 2023-06-27 12:26:04,498 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2023-06-27 12:26:05,185 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2023-06-27 12:26:05,228 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-27 12:26:05,440 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1        | 2023-06-27 12:26:05,495 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2023-06-27 12:26:06,010 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-27 12:26:06,019 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-27 12:26:06,148 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2023-06-27 12:26:06,189 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-27 12:26:06,195 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-27 12:26:06,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-27 12:26:06,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-27 12:26:06,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1        | 2023-06-27 12:26:06,198 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2023-06-27 12:26:06,199 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2023-06-27 12:26:06,207 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 12:26:06,207 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2023-06-27 12:26:06,212 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-06-27 12:26:06,265 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1        | 2023-06-27 12:26:06,284 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1        | 2023-06-27 12:26:06,289 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-06-27 12:26:08,474 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1        | 2023-06-27 12:26:08,504 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1        | 2023-06-27 12:26:08,509 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1        | 2023-06-27 12:26:08,509 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-27 12:26:08,509 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-27 12:26:08,533 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-27 12:26:08,635 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@e43189c[Not completed]
om2_1        | 2023-06-27 12:26:08,635 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1        | 2023-06-27 12:26:08,689 [main] INFO om.OzoneManager: Creating RPC Server
om2_1        | 2023-06-27 12:26:08,865 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
recon_1      | 2023-06-27 12:23:55,559 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately. Current retry count: 3.
recon_1      | 2023-06-27 12:23:55,560 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately. Current retry count: 4.
recon_1      | 2023-06-27 12:23:55,562 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-27 12:23:57,565 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately. Current retry count: 6.
recon_1      | 2023-06-27 12:23:57,566 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately. Current retry count: 7.
recon_1      | 2023-06-27 12:23:57,567 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-27 12:23:59,569 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately. Current retry count: 9.
recon_1      | 2023-06-27 12:23:59,571 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately. Current retry count: 10.
recon_1      | 2023-06-27 12:23:59,572 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-27 12:24:01,585 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately. Current retry count: 12.
recon_1      | 2023-06-27 12:24:01,590 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately. Current retry count: 13.
recon_1      | 2023-06-27 12:24:01,592 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
recon_1      | 2023-06-27 12:24:03,597 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately. Current retry count: 15.
recon_1      | 2023-06-27 12:24:03,605 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately. Current retry count: 16.
recon_1      | 2023-06-27 12:24:03,610 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
recon_1      | 2023-06-27 12:24:05,612 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately. Current retry count: 18.
recon_1      | 2023-06-27 12:24:05,633 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately. Current retry count: 19.
recon_1      | 2023-06-27 12:24:05,634 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 20.
recon_1      | 2023-06-27 12:24:07,637 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately. Current retry count: 21.
recon_1      | 2023-06-27 12:24:07,639 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately. Current retry count: 22.
recon_1      | 2023-06-27 12:24:07,646 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 23.
recon_1      | 2023-06-27 12:24:09,648 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately. Current retry count: 24.
recon_1      | 2023-06-27 12:24:09,649 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately. Current retry count: 25.
recon_1      | 2023-06-27 12:24:09,651 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 26.
recon_1      | 2023-06-27 12:24:11,652 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately. Current retry count: 27.
recon_1      | 2023-06-27 12:24:11,655 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately. Current retry count: 28.
recon_1      | 2023-06-27 12:24:11,656 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 29.
recon_1      | 2023-06-27 12:24:13,666 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately. Current retry count: 30.
recon_1      | 2023-06-27 12:24:13,671 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately. Current retry count: 31.
recon_1      | 2023-06-27 12:24:13,677 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 32.
recon_1      | 2023-06-27 12:24:15,679 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately. Current retry count: 33.
recon_1      | 2023-06-27 12:24:15,685 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately. Current retry count: 34.
recon_1      | 2023-06-27 12:24:15,690 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 35.
recon_1      | 2023-06-27 12:24:17,691 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately. Current retry count: 36.
recon_1      | 2023-06-27 12:24:17,694 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately. Current retry count: 37.
recon_1      | 2023-06-27 12:24:17,696 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 38.
recon_1      | 2023-06-27 12:24:19,698 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately. Current retry count: 39.
recon_1      | 2023-06-27 12:24:19,699 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately. Current retry count: 40.
recon_1      | 2023-06-27 12:24:19,700 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 41.
recon_1      | 2023-06-27 12:24:21,702 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately. Current retry count: 42.
recon_1      | 2023-06-27 12:24:21,706 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately. Current retry count: 43.
recon_1      | 2023-06-27 12:24:21,707 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 44.
recon_1      | 2023-06-27 12:24:23,709 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately. Current retry count: 45.
recon_1      | 2023-06-27 12:24:23,710 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately. Current retry count: 46.
recon_1      | 2023-06-27 12:24:23,720 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 47.
recon_1      | 2023-06-27 12:24:25,722 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover immediately. Current retry count: 48.
recon_1      | 2023-06-27 12:24:25,723 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 49 failover attempts. Trying to failover immediately. Current retry count: 49.
recon_1      | 2023-06-27 12:24:25,724 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 50 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 50.
recon_1      | 2023-06-27 12:24:27,726 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 51 failover attempts. Trying to failover immediately. Current retry count: 51.
recon_1      | 2023-06-27 12:24:27,728 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 52 failover attempts. Trying to failover immediately. Current retry count: 52.
om3_1        | 2023-06-27 12:26:05,018 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2023-06-27 12:26:05,898 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2023-06-27 12:26:05,947 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-27 12:26:06,195 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1        | 2023-06-27 12:26:06,387 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2023-06-27 12:26:06,888 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-27 12:26:06,904 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-27 12:26:07,040 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2023-06-27 12:26:07,143 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-27 12:26:07,159 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-27 12:26:07,163 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-27 12:26:07,164 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-27 12:26:07,169 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1        | 2023-06-27 12:26:07,170 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2023-06-27 12:26:07,173 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2023-06-27 12:26:07,183 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 12:26:07,189 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2023-06-27 12:26:07,190 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-27 12:26:07,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-06-27 12:26:07,316 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1        | 2023-06-27 12:26:07,320 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-06-27 12:26:09,340 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1        | 2023-06-27 12:26:09,383 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1        | 2023-06-27 12:26:09,392 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1        | 2023-06-27 12:26:09,392 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-27 12:26:09,392 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-27 12:26:09,454 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-06-27 12:26:09,577 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@261d8a9c[Not completed]
om3_1        | 2023-06-27 12:26:09,579 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2023-06-27 12:26:09,626 [main] INFO om.OzoneManager: Creating RPC Server
om3_1        | 2023-06-27 12:26:09,708 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1        | 2023-06-27 12:26:09,750 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2023-06-27 12:26:09,756 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2023-06-27 12:26:09,756 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2023-06-27 12:26:09,758 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-27 12:26:09,758 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-27 12:26:09,759 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2023-06-27 12:26:09,836 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1        | 2023-06-27 12:26:09,845 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-06-27 12:26:09,899 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2023-06-27 12:26:09,900 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1        | 2023-06-27 12:26:10,230 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm2.org_1   | 2023-06-27 12:23:07,966 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2023-06-27 12:23:07,966 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2023-06-27 12:23:07,970 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:7480df95-74c6-4260-b2a9-e3614d893449,clusterId:CID-5bf65396-4407-4e81-983e-589b8c5be712,subject:scm-sub@scm2.org
scm2.org_1   | 2023-06-27 12:23:10,413 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2023-06-27 12:23:10,450 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-5bf65396-4407-4e81-983e-589b8c5be712, SCMID 7480df95-74c6-4260-b2a9-e3614d893449
scm2.org_1   | 2023-06-27 12:23:10,450 [main] INFO server.StorageContainerManager: Primary SCM Node ID f163470c-d3e0-4891-94bf-0bc988a6aa12
scm2.org_1   | 2023-06-27 12:23:10,542 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
datanode2_1  | 2023-06-27 12:26:36,401 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-27 12:26:36,402 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-27 12:26:36,405 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-27 12:26:36,409 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-27 12:26:36,409 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-27 12:26:36,409 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-27 12:26:36,420 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-27 12:26:36,423 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-27 12:26:36,424 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-27 12:26:36,424 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-27 12:26:36,424 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-27 12:26:36,426 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-27 12:26:36,426 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392 does not exist. Creating ...
datanode2_1  | 2023-06-27 12:26:36,443 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392/in_use.lock acquired by nodename 7@1b92348e430f
datanode2_1  | 2023-06-27 12:26:36,466 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392 has been successfully formatted.
datanode2_1  | 2023-06-27 12:26:36,665 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO ratis.ContainerStateMachine: group-561639362392: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-27 12:26:36,669 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-27 12:26:36,670 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-27 12:26:36,671 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:36,762 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-27 12:26:36,764 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-27 12:26:36,765 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-27 12:26:36,776 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-27 12:26:36,802 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-06-27 12:26:36,803 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:36,805 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: new cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392
datanode2_1  | 2023-06-27 12:26:36,808 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-06-27 12:26:36,808 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-27 12:26:36,808 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-27 12:26:36,874 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om1_1        | 2023-06-27 12:26:36,938 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2023-06-27 12:26:36,940 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1        | 2023-06-27 12:26:36,946 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1        | 2023-06-27 12:26:36,948 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1        | 2023-06-27 12:26:36,954 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2023-06-27 12:26:37,124 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2023-06-27 12:26:37,133 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 12:26:37,267 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1        | 2023-06-27 12:26:37,287 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1        | 2023-06-27 12:26:37,290 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1        | 2023-06-27 12:26:37,355 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-27 12:23:15,962 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
datanode3_1  | 2023-06-27 12:26:36,544 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392/in_use.lock acquired by nodename 6@038f15546f77
datanode3_1  | 2023-06-27 12:26:36,552 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392 has been successfully formatted.
datanode3_1  | 2023-06-27 12:26:36,636 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO ratis.ContainerStateMachine: group-561639362392: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-27 12:26:36,648 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-27 12:26:36,649 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-27 12:26:36,649 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:36,650 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-27 12:26:36,650 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-27 12:26:36,650 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-27 12:23:51,963 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-06-27 12:23:52,034 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 12:23:52,239 [main] INFO reflections.Reflections: Reflections took 147 ms to scan 3 urls, producing 131 keys and 286 values 
scm3.org_1   | 2023-06-27 12:23:52,358 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-06-27 12:23:52,369 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-27 12:23:52,409 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-27 12:23:52,410 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-27 12:23:52,641 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-27 12:23:52,641 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-27 12:23:54,735 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 328974195072
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   |            Start Date: Tue Jun 27 12:23:47 UTC 2023
scm3.org_1   |            Final Date: Fri Aug 04 12:23:47 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=2cf3494c-faba-4bd8-bf38-71b782f966a2,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   |            Public Key: RSA Public Key [33:e2:26:0a:6a:7e:fe:f5:f8:79:c8:dc:1c:2f:65:e7:9e:f7:7b:8b],[56:66:d1:a4]
scm3.org_1   |         modulus: acfff83a54fcec51ddc59b32fa1e9922d70fa2659a08b68b7d2a80132fa17853ffc4b26ff44cd05bd4e79f2428c762702c6bda736852877c3f12dcfc9dc46022c29e4b1ae8fd13c4a49140961f14363842d4d7661f21cfd882d8f63d6e6a8416040f0f7f87ddedb6057740ca21529436bd39f90a0b13956f2da45b01e88bbb5802e00d3229303ffa4a65c8a909745159c074632101c155188e112d544161abfc8a0871a43cfa3fd8d66a07e0b9227e53704f40e05d75df740275df4af07517cd869344f787e241281b761fc759abd3863bcf2fb3f578ab8181b6a440e8a9090e80059a853ac8db43648500607b51993480bc0dd776434da92fefe73fdddd6281
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: 480b0a3f73148035e03a54000ca857ceadf67ecd
scm3.org_1   |                        7d660017f0c6678cc981570a85eddf650ead6d0c
scm3.org_1   |                        96ddd2cbb6d683cc90069adbb603cde53801c243
scm3.org_1   |                        b5744a3d99e625a8e97f7308fba8dde8fb23851a
scm3.org_1   |                        10e866039ac015673740f5bc6d730880218d7e52
scm3.org_1   |                        7eb54cd7ed262e22d6132bbf414a22d4378fdd62
scm3.org_1   |                        4415c1e0fc93a970b48b640ccb00ea390cb1e3c8
scm3.org_1   |                        73bde84cf052f54262adc9049b4c3655ead7ecde
scm3.org_1   |                        f14b6889f98ead3787df02d888cfb07590408dcc
scm3.org_1   |                        b05900811b55cf729ad52a3c56ba9ce7249c5a5d
scm3.org_1   |                        9459d6fa86f54f2bb7a67f38bbaa6489318d36b6
scm3.org_1   |                        593a03155b713abfd60f68fb5c34b76a562d19a9
scm3.org_1   |                        4ad734411dec29ebb5a1d554a7ce5019
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/328974195072.crt.
scm3.org_1   | 2023-06-27 12:23:54,759 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 1
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   |            Start Date: Tue Jun 27 12:22:43 UTC 2023
om1_1        | 2023-06-27 12:26:37,363 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-27 12:26:37,424 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-27 12:26:37,437 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2023-06-27 12:26:37,464 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-27 12:26:37,523 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-27 12:26:37,540 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-27 12:23:15,994 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-27 12:23:16,209 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 12:23:16,882 [main] INFO reflections.Reflections: Reflections took 522 ms to scan 3 urls, producing 131 keys and 286 values 
scm2.org_1   | 2023-06-27 12:23:17,234 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-27 12:23:17,275 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-27 12:23:17,352 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-27 12:23:17,353 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-27 12:23:17,819 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-27 12:23:17,819 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-27 12:23:20,453 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 1
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm2.org_1   |            Start Date: Tue Jun 27 12:22:43 UTC 2023
scm2.org_1   |            Final Date: Fri Aug 04 12:22:43 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm2.org_1   |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
scm2.org_1   |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
scm2.org_1   |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
scm2.org_1   |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
scm2.org_1   |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
scm2.org_1   |                        eece43d2072924486b8a2e674f6b5465d4190ac6
scm2.org_1   |                        f5030570b2b63224eab18c88675f9c9c24008980
scm2.org_1   |                        86a902cf862c5da5b230958d06c3dba90324e417
scm2.org_1   |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
scm2.org_1   |                        1aa70751ced732e39cbf893a239b9956aef292ef
scm2.org_1   |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
scm2.org_1   |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
scm2.org_1   |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
scm2.org_1   |                        86212367c9e9316e2a4bb892f0cda036
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0x6
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
datanode1_1  | 2023-06-27 12:26:37,654 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-27 12:26:37,654 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-27 12:26:37,654 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-27 12:26:37,654 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-27 12:26:37,654 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-27 12:26:37,654 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392 does not exist. Creating ...
datanode1_1  | 2023-06-27 12:26:37,662 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392/in_use.lock acquired by nodename 7@5cbb6cac8588
datanode1_1  | 2023-06-27 12:26:37,713 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392 has been successfully formatted.
datanode1_1  | 2023-06-27 12:26:37,714 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO ratis.ContainerStateMachine: group-561639362392: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-27 12:26:37,714 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-27 12:26:37,769 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-27 12:26:37,769 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:37,770 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-27 12:26:37,770 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-27 12:26:37,770 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1        | 2023-06-27 12:26:37,554 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1        | 2023-06-27 12:26:37,563 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2023-06-27 12:26:37,568 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2023-06-27 12:26:37,570 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2023-06-27 12:26:37,574 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2023-06-27 12:26:37,606 [main] INFO server.RaftServer: om1: start RPC server
om1_1        | 2023-06-27 12:26:38,998 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om2_1        | 2023-06-27 12:26:08,935 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2023-06-27 12:26:08,943 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2023-06-27 12:26:08,943 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2023-06-27 12:26:08,944 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-27 12:26:08,944 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-27 12:26:08,944 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2023-06-27 12:26:09,048 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1        | 2023-06-27 12:26:09,055 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-27 12:26:09,158 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2023-06-27 12:26:09,163 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1        | 2023-06-27 12:26:09,510 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
datanode2_1  | 2023-06-27 12:26:36,875 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-27 12:26:36,875 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-27 12:26:36,875 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-27 12:26:36,875 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-27 12:26:36,876 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:36,666 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-27 12:26:36,678 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-27 12:26:36,746 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 12:26:36,795 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-27 12:26:36,795 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-27 12:26:36,815 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-27 12:26:36,817 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 12:26:36,817 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 12:26:36,863 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392: start as a follower, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:36,863 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-27 12:26:36,863 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392-FollowerState
datanode3_1  | 2023-06-27 12:26:36,886 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-561639362392,id=a650530b-22bc-4ab5-9901-0a035f1b699c
datanode3_1  | 2023-06-27 12:26:36,887 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-27 12:26:36,887 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-27 12:26:36,887 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-27 12:26:36,894 [a650530b-22bc-4ab5-9901-0a035f1b699c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-27 12:26:36,897 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=aeeab16c-188d-4045-bf49-561639362392
datanode3_1  | 2023-06-27 12:26:36,927 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-27 12:26:36,958 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-27 12:22:50,460 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2023-06-27 12:22:50,464 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-27 12:22:50,464 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: start f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1
scm1.org_1   | 2023-06-27 12:22:50,470 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:22:50,470 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm1.org_1   | 2023-06-27 12:22:50,473 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:22:50,473 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-27 12:22:50,473 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: shutdown f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1
scm1.org_1   | 2023-06-27 12:22:50,474 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1.org_1   | 2023-06-27 12:22:50,474 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: change Leader from null to f163470c-d3e0-4891-94bf-0bc988a6aa12 at term 1 for becomeLeader, leader elected after 5492ms
scm1.org_1   | 2023-06-27 12:22:50,481 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-27 12:22:50,485 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 12:22:50,486 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 12:22:50,490 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-27 12:22:50,490 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-27 12:22:50,491 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-27 12:22:50,498 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 12:22:50,499 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-27 12:22:50,501 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: start f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl
scm1.org_1   | 2023-06-27 12:22:50,524 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2023-06-27 12:22:50,554 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:22:50,626 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_0
scm1.org_1   | 2023-06-27 12:22:51,387 [main] INFO server.RaftServer: f163470c-d3e0-4891-94bf-0bc988a6aa12: close
scm1.org_1   | 2023-06-27 12:22:51,388 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: shutdown
scm1.org_1   | 2023-06-27 12:22:51,389 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-589B8C5BE712,id=f163470c-d3e0-4891-94bf-0bc988a6aa12
scm1.org_1   | 2023-06-27 12:22:51,389 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: shutdown f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl
scm1.org_1   | 2023-06-27 12:22:51,393 [main] INFO server.GrpcService: f163470c-d3e0-4891-94bf-0bc988a6aa12: shutdown server GrpcServerProtocolService now
scm1.org_1   | 2023-06-27 12:22:51,398 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO impl.PendingRequests: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2023-06-27 12:22:51,408 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO impl.StateMachineUpdater: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2023-06-27 12:22:51,408 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO impl.StateMachineUpdater: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2023-06-27 12:22:51,411 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO impl.StateMachineUpdater: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2023-06-27 12:22:51,415 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: closes. applyIndex: 0
scm1.org_1   | 2023-06-27 12:22:51,417 [main] INFO server.GrpcService: f163470c-d3e0-4891-94bf-0bc988a6aa12: shutdown server GrpcServerProtocolService successfully
scm1.org_1   | 2023-06-27 12:22:51,633 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker close()
scm1.org_1   | 2023-06-27 12:22:51,635 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f163470c-d3e0-4891-94bf-0bc988a6aa12: Stopped
scm1.org_1   | 2023-06-27 12:22:51,635 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 12:22:51,639 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-5bf65396-4407-4e81-983e-589b8c5be712; layoutVersion=7; scmId=f163470c-d3e0-4891-94bf-0bc988a6aa12
scm1.org_1   | 2023-06-27 12:22:51,665 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-27 12:22:53,868 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
om1_1        | 2023-06-27 12:26:39,078 [main] INFO om.OzoneManager: Starting secret key client.
om1_1        | 2023-06-27 12:26:39,167 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2023-06-27 12:26:39,975 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z).
om1_1        | 2023-06-27 12:26:39,990 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M25.211004S and interval of PT1M
om1_1        | 2023-06-27 12:26:40,012 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2023-06-27 12:26:40,018 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 408250958148
om1_1        | 2023-06-27 12:26:40,113 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2023-06-27 12:26:40,117 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2023-06-27 12:26:41,902 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om1: Detected pause in JVM or host machine approximately 0.119s with 0.368s GC time.
om1_1        | GC pool 'ParNew' had collection(s): count=1 time=368ms
om3_1        | 2023-06-27 12:26:10,280 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1        | 2023-06-27 12:26:10,325 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2023-06-27 12:26:10,326 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2023-06-27 12:26:10,852 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1        | 2023-06-27 12:26:11,419 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-27 12:26:11,462 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-06-27 12:26:11,467 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1        | 2023-06-27 12:26:11,469 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1        | 2023-06-27 12:26:11,483 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1        | 2023-06-27 12:26:11,496 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1        | 2023-06-27 12:26:13,834 [main] INFO reflections.Reflections: Reflections took 3529 ms to scan 8 urls, producing 24 keys and 639 values [using 2 cores]
om3_1        | 2023-06-27 12:26:15,234 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2023-06-27 12:26:15,307 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2023-06-27 12:26:15,454 [Listener at om3/9862] INFO hdds.HddsUtils: Restoring thread name: main
om3_1        | 2023-06-27 12:26:25,677 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2023-06-27 12:26:25,823 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2023-06-27 12:26:25,823 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2023-06-27 12:26:26,797 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2023-06-27 12:26:26,806 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2023-06-27 12:26:26,835 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1        | 2023-06-27 12:26:26,880 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om3
om3_1        | 2023-06-27 12:26:26,946 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om3_1        | 2023-06-27 12:26:26,984 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2023-06-27 12:26:27,132 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2023-06-27 12:26:27,139 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 12:26:27,170 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1        | 2023-06-27 12:26:27,180 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1        | 2023-06-27 12:26:27,223 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-06-27 12:26:27,307 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2023-06-27 12:26:27,320 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2023-06-27 12:26:27,325 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 12:26:27,418 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1        | 2023-06-27 12:26:27,422 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1        | 2023-06-27 12:26:27,427 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2023-06-27 12:26:27,450 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
datanode2_1  | 2023-06-27 12:26:36,924 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 12:26:37,029 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-27 12:26:37,074 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-06-27 12:26:37,077 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-27 12:26:37,086 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 12:26:37,086 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 12:26:37,102 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392: start as a follower, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:37,104 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-06-27 12:26:37,107 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392-FollowerState
datanode2_1  | 2023-06-27 12:26:37,117 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-561639362392,id=cc01b188-4e0b-44d2-a762-70392abfc581
datanode2_1  | 2023-06-27 12:26:37,196 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-06-27 12:26:37,197 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-06-27 12:26:37,197 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-06-27 12:26:37,197 [cc01b188-4e0b-44d2-a762-70392abfc581-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-06-27 12:26:37,219 [cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-27 12:26:37,255 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=aeeab16c-188d-4045-bf49-561639362392
datanode2_1  | 2023-06-27 12:26:37,274 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-27 12:26:37,291 [cc01b188-4e0b-44d2-a762-70392abfc581@group-561639362392-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-27 12:26:37,595 [grpc-default-executor-1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: receive requestVote(PRE_VOTE, 1a7440b0-385d-4580-b33d-c9708c19e79c, group-3CB73F243315, 0, (t:0, i:0))
datanode2_1  | 2023-06-27 12:26:37,597 [grpc-default-executor-1] INFO impl.VoteContext: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FOLLOWER: accept PRE_VOTE from 1a7440b0-385d-4580-b33d-c9708c19e79c: our priority 0 <= candidate's priority 1
datanode2_1  | 2023-06-27 12:26:37,657 [grpc-default-executor-1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315 replies to PRE_VOTE vote request: 1a7440b0-385d-4580-b33d-c9708c19e79c<-cc01b188-4e0b-44d2-a762-70392abfc581#0:OK-t0. Peer's state: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315:t0, leader=null, voted=, raftlog=Memoized:cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:38,282 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-27 12:26:38,650 [grpc-default-executor-1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: receive requestVote(ELECTION, 1a7440b0-385d-4580-b33d-c9708c19e79c, group-3CB73F243315, 1, (t:0, i:0))
datanode2_1  | 2023-06-27 12:26:38,654 [grpc-default-executor-1] INFO impl.VoteContext: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FOLLOWER: accept ELECTION from 1a7440b0-385d-4580-b33d-c9708c19e79c: our priority 0 <= candidate's priority 1
datanode2_1  | 2023-06-27 12:26:38,659 [grpc-default-executor-1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1a7440b0-385d-4580-b33d-c9708c19e79c
datanode2_1  | 2023-06-27 12:26:38,660 [grpc-default-executor-1] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: shutdown cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState
datanode2_1  | 2023-06-27 12:26:38,661 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState] INFO impl.FollowerState: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState was interrupted
datanode2_1  | 2023-06-27 12:26:38,662 [grpc-default-executor-1] INFO impl.RoleInfo: cc01b188-4e0b-44d2-a762-70392abfc581: start cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-FollowerState
recon_1      | 2023-06-27 12:24:27,728 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 53 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 53.
recon_1      | 2023-06-27 12:24:29,730 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 54 failover attempts. Trying to failover immediately. Current retry count: 54.
recon_1      | 2023-06-27 12:24:29,731 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 55 failover attempts. Trying to failover immediately. Current retry count: 55.
recon_1      | 2023-06-27 12:24:29,734 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 56 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 56.
recon_1      | 2023-06-27 12:24:31,739 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 57 failover attempts. Trying to failover immediately. Current retry count: 57.
recon_1      | 2023-06-27 12:24:31,741 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 58 failover attempts. Trying to failover immediately. Current retry count: 58.
recon_1      | 2023-06-27 12:24:31,743 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 59 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 59.
recon_1      | 2023-06-27 12:24:33,746 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 60 failover attempts. Trying to failover immediately. Current retry count: 60.
recon_1      | 2023-06-27 12:24:33,758 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 61 failover attempts. Trying to failover immediately. Current retry count: 61.
recon_1      | 2023-06-27 12:24:33,774 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 62 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 62.
recon_1      | 2023-06-27 12:24:35,786 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 63 failover attempts. Trying to failover immediately. Current retry count: 63.
recon_1      | 2023-06-27 12:24:35,792 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 64 failover attempts. Trying to failover immediately. Current retry count: 64.
recon_1      | 2023-06-27 12:24:35,793 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 65 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 65.
recon_1      | 2023-06-27 12:24:37,797 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 66 failover attempts. Trying to failover immediately. Current retry count: 66.
recon_1      | 2023-06-27 12:24:37,799 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 67 failover attempts. Trying to failover immediately. Current retry count: 67.
recon_1      | 2023-06-27 12:24:37,800 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 68 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 68.
recon_1      | 2023-06-27 12:24:39,801 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 69 failover attempts. Trying to failover immediately. Current retry count: 69.
recon_1      | 2023-06-27 12:24:39,802 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 70 failover attempts. Trying to failover immediately. Current retry count: 70.
recon_1      | 2023-06-27 12:24:39,804 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 71 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 71.
recon_1      | 2023-06-27 12:24:41,806 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 72 failover attempts. Trying to failover immediately. Current retry count: 72.
recon_1      | 2023-06-27 12:24:41,808 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 73 failover attempts. Trying to failover immediately. Current retry count: 73.
recon_1      | 2023-06-27 12:24:41,809 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 74 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 74.
recon_1      | 2023-06-27 12:24:43,819 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 75 failover attempts. Trying to failover immediately. Current retry count: 75.
datanode1_1  | 2023-06-27 12:26:37,770 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-27 12:26:37,770 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-27 12:26:37,770 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:37,789 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/aeeab16c-188d-4045-bf49-561639362392
datanode1_1  | 2023-06-27 12:26:37,789 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-27 12:26:37,789 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 12:26:37,789 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 12:26:37,789 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-27 12:26:37,791 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-27 12:26:37,791 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-27 12:26:37,791 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-27 12:26:37,791 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-27 12:26:37,800 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-27 12:26:37,821 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-06-27 12:26:37,859 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection:   Response 0: 1a7440b0-385d-4580-b33d-c9708c19e79c<-cc01b188-4e0b-44d2-a762-70392abfc581#0:OK-t0
datanode1_1  | 2023-06-27 12:26:37,860 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode1_1  | 2023-06-27 12:26:37,824 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:37,869 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:37,991 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-27 12:26:37,991 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-27 12:26:37,991 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-27 12:26:37,991 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 12:26:37,991 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 12:26:38,029 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 12:26:38,098 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-27 12:26:09,570 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1        | 2023-06-27 12:26:09,599 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2023-06-27 12:26:09,600 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2023-06-27 12:26:10,064 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om2_1        | 2023-06-27 12:26:10,574 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-06-27 12:26:10,626 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2023-06-27 12:26:10,626 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1        | 2023-06-27 12:26:10,637 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1        | 2023-06-27 12:26:10,643 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1        | 2023-06-27 12:26:10,658 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        | 2023-06-27 12:26:12,610 [main] INFO reflections.Reflections: Reflections took 2965 ms to scan 8 urls, producing 24 keys and 639 values [using 2 cores]
om2_1        | 2023-06-27 12:26:14,548 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1        | 2023-06-27 12:26:14,648 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2023-06-27 12:26:14,850 [Listener at om2/9862] INFO hdds.HddsUtils: Restoring thread name: main
om2_1        | 2023-06-27 12:26:22,746 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2023-06-27 12:26:22,975 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2023-06-27 12:26:22,976 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2023-06-27 12:26:23,718 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2023-06-27 12:26:23,718 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2023-06-27 12:26:23,767 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om2_1        | 2023-06-27 12:26:23,824 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om2
om2_1        | 2023-06-27 12:26:23,947 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1        | 2023-06-27 12:26:23,967 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2023-06-27 12:26:24,097 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2023-06-27 12:26:24,097 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 12:26:24,120 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1        | 2023-06-27 12:26:24,132 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1        | 2023-06-27 12:26:24,167 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-27 12:26:24,221 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2023-06-27 12:26:24,232 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2023-06-27 12:26:24,233 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 12:26:24,300 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1        | 2023-06-27 12:26:24,301 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2023-06-27 12:26:24,310 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2023-06-27 12:26:24,325 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-27 12:26:24,334 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2023-06-27 12:26:24,340 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1        | 2023-06-27 12:26:24,354 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2023-06-27 12:26:24,361 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2023-06-27 12:26:24,372 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2023-06-27 12:26:24,570 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2023-06-27 12:26:24,574 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 12:26:24,772 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1        | 2023-06-27 12:26:24,810 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1        | 2023-06-27 12:26:24,812 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2023-06-27 12:26:24,878 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-27 12:26:24,883 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-27 12:26:24,898 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-27 12:26:24,902 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2023-06-27 12:26:24,914 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-27 12:26:24,962 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1        | 2023-06-27 12:26:24,970 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 12:26:36,974 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-561639362392-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 12:26:37,353 [grpc-default-executor-1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: receive requestVote(PRE_VOTE, 1a7440b0-385d-4580-b33d-c9708c19e79c, group-3CB73F243315, 0, (t:0, i:0))
datanode3_1  | 2023-06-27 12:26:37,354 [grpc-default-executor-1] INFO impl.VoteContext: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FOLLOWER: accept PRE_VOTE from 1a7440b0-385d-4580-b33d-c9708c19e79c: our priority 0 <= candidate's priority 1
datanode3_1  | 2023-06-27 12:26:37,413 [grpc-default-executor-1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315 replies to PRE_VOTE vote request: 1a7440b0-385d-4580-b33d-c9708c19e79c<-a650530b-22bc-4ab5-9901-0a035f1b699c#0:OK-t0. Peer's state: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315:t0, leader=null, voted=, raftlog=Memoized:a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:38,022 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a650530b-22bc-4ab5-9901-0a035f1b699c: Detected pause in JVM or host machine approximately 0.188s without any GCs.
datanode3_1  | 2023-06-27 12:26:38,190 [grpc-default-executor-1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: receive requestVote(ELECTION, 1a7440b0-385d-4580-b33d-c9708c19e79c, group-3CB73F243315, 1, (t:0, i:0))
datanode3_1  | 2023-06-27 12:26:38,195 [grpc-default-executor-1] INFO impl.VoteContext: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FOLLOWER: accept ELECTION from 1a7440b0-385d-4580-b33d-c9708c19e79c: our priority 0 <= candidate's priority 1
datanode3_1  | 2023-06-27 12:26:38,195 [grpc-default-executor-1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1a7440b0-385d-4580-b33d-c9708c19e79c
datanode3_1  | 2023-06-27 12:26:38,196 [grpc-default-executor-1] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: shutdown a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState
datanode3_1  | 2023-06-27 12:26:38,197 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState] INFO impl.FollowerState: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState was interrupted
datanode3_1  | 2023-06-27 12:26:38,197 [grpc-default-executor-1] INFO impl.RoleInfo: a650530b-22bc-4ab5-9901-0a035f1b699c: start a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-FollowerState
datanode3_1  | 2023-06-27 12:26:38,310 [grpc-default-executor-1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315 replies to ELECTION vote request: 1a7440b0-385d-4580-b33d-c9708c19e79c<-a650530b-22bc-4ab5-9901-0a035f1b699c#0:OK-t1. Peer's state: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315:t1, leader=null, voted=1a7440b0-385d-4580-b33d-c9708c19e79c, raftlog=Memoized:a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 12:26:38,491 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-27 12:26:39,951 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=aeeab16c-188d-4045-bf49-561639362392.
datanode3_1  | 2023-06-27 12:26:40,875 [a650530b-22bc-4ab5-9901-0a035f1b699c-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3CB73F243315 with new leaderId: 1a7440b0-385d-4580-b33d-c9708c19e79c
datanode3_1  | 2023-06-27 12:26:40,946 [a650530b-22bc-4ab5-9901-0a035f1b699c-server-thread1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: change Leader from null to 1a7440b0-385d-4580-b33d-c9708c19e79c at term 1 for appendEntries, leader elected after 17726ms
datanode3_1  | 2023-06-27 12:26:40,987 [a650530b-22bc-4ab5-9901-0a035f1b699c-server-thread1] INFO server.RaftServer$Division: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315: set configuration 0: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   |            Final Date: Fri Aug 04 12:22:43 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
scm3.org_1   |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
scm3.org_1   |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
scm3.org_1   |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
scm3.org_1   |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
scm3.org_1   |                        eece43d2072924486b8a2e674f6b5465d4190ac6
scm3.org_1   |                        f5030570b2b63224eab18c88675f9c9c24008980
scm3.org_1   |                        86a902cf862c5da5b230958d06c3dba90324e417
scm3.org_1   |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
scm3.org_1   |                        1aa70751ced732e39cbf893a239b9956aef292ef
scm3.org_1   |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
scm3.org_1   |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
scm3.org_1   |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
scm3.org_1   |                        86212367c9e9316e2a4bb892f0cda036
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0x6
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2023-06-27 12:23:54,787 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 328974195072
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   |            Start Date: Tue Jun 27 12:23:47 UTC 2023
scm3.org_1   |            Final Date: Fri Aug 04 12:23:47 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=2cf3494c-faba-4bd8-bf38-71b782f966a2,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   |            Public Key: RSA Public Key [33:e2:26:0a:6a:7e:fe:f5:f8:79:c8:dc:1c:2f:65:e7:9e:f7:7b:8b],[56:66:d1:a4]
scm3.org_1   |         modulus: acfff83a54fcec51ddc59b32fa1e9922d70fa2659a08b68b7d2a80132fa17853ffc4b26ff44cd05bd4e79f2428c762702c6bda736852877c3f12dcfc9dc46022c29e4b1ae8fd13c4a49140961f14363842d4d7661f21cfd882d8f63d6e6a8416040f0f7f87ddedb6057740ca21529436bd39f90a0b13956f2da45b01e88bbb5802e00d3229303ffa4a65c8a909745159c074632101c155188e112d544161abfc8a0871a43cfa3fd8d66a07e0b9227e53704f40e05d75df740275df4af07517cd869344f787e241281b761fc759abd3863bcf2fb3f578ab8181b6a440e8a9090e80059a853ac8db43648500607b51993480bc0dd776434da92fefe73fdddd6281
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2023-06-27 12:23:20,474 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 290021601257
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm2.org_1   |            Start Date: Tue Jun 27 12:23:08 UTC 2023
scm2.org_1   |            Final Date: Fri Aug 04 12:23:08 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=7480df95-74c6-4260-b2a9-e3614d893449,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm2.org_1   |            Public Key: RSA Public Key [ba:e2:0c:de:10:43:47:5c:cc:b0:0f:5e:d8:01:62:63:a2:e0:6a:60],[56:66:d1:a4]
scm2.org_1   |         modulus: e5255c43f586351db13286806412f62e184c1f3db4c1b51f875192238b7a49d723bdda92aabc6d05757f9ac4e4aee7a1dbe99657105566aa278128fa5fecb405dda86ee3b50d656304ee39c794a53bc8554da01ed8e8bf8f2dec8b068f707502abbfa4fc6f4411f29832c564327954942da63b3299d8aafc92ca7e257dd15eaf152f2f55dfdc3eb2c40a771f0e3d7237f922ac5a7e18b88e35898fdd4986c286b4262261404fb4e519dbc64a37da513532c8bdb2602656cb64f6fd071a73d15a72a61ebc96c5bbfe1b2b4b30e2947266777aa28cff5a3972cd9abecc0a13b3e2ed7d472c08ecc4c74586fd7357f5122f1cfdd98a901923679bea0a5ea6ca1163
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 21d8a50ec484a392ccbc308bb247c2f51e7437bc
scm2.org_1   |                        561160e406d4d2579c27e1bad1ce9dc25fed8d84
scm2.org_1   |                        197e69e6706bc70a31cfbfc1f00a36bb60baf1b0
scm2.org_1   |                        54b9911c37eac56db5f7e85ec2d9bfe8b355fe59
scm2.org_1   |                        15a2646d56ffe9e4b515b5cf60f8ef26dc083a7f
scm2.org_1   |                        a64104d73388d7b4bed500e0cf07ee0c467966b1
scm2.org_1   |                        cad256699876c4604fc38c188391434fe3d9b14b
scm2.org_1   |                        28acd396e79f33471c78fdca6db1ccb0015875c1
scm2.org_1   |                        b1c543cc9bb2b6b0eeb5781467882709f0e938b8
scm2.org_1   |                        b10eb5777f79c54a81ae96c7ef02fe9cb7dba10c
scm2.org_1   |                        315db7a4d434f8aed48a73834ea42b60b7f2117e
scm2.org_1   |                        b19cff296be9ed50482ed2263054c7f16e3b72d6
scm2.org_1   |                        cb250ca13eda39b6659398b24ddb12e5
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/290021601257.crt.
scm2.org_1   | 2023-06-27 12:23:20,481 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 290021601257
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm2.org_1   |            Start Date: Tue Jun 27 12:23:08 UTC 2023
scm2.org_1   |            Final Date: Fri Aug 04 12:23:08 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=7480df95-74c6-4260-b2a9-e3614d893449,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm2.org_1   |            Public Key: RSA Public Key [ba:e2:0c:de:10:43:47:5c:cc:b0:0f:5e:d8:01:62:63:a2:e0:6a:60],[56:66:d1:a4]
scm2.org_1   |         modulus: e5255c43f586351db13286806412f62e184c1f3db4c1b51f875192238b7a49d723bdda92aabc6d05757f9ac4e4aee7a1dbe99657105566aa278128fa5fecb405dda86ee3b50d656304ee39c794a53bc8554da01ed8e8bf8f2dec8b068f707502abbfa4fc6f4411f29832c564327954942da63b3299d8aafc92ca7e257dd15eaf152f2f55dfdc3eb2c40a771f0e3d7237f922ac5a7e18b88e35898fdd4986c286b4262261404fb4e519dbc64a37da513532c8bdb2602656cb64f6fd071a73d15a72a61ebc96c5bbfe1b2b4b30e2947266777aa28cff5a3972cd9abecc0a13b3e2ed7d472c08ecc4c74586fd7357f5122f1cfdd98a901923679bea0a5ea6ca1163
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
datanode3_1  | 2023-06-27 12:26:41,010 [a650530b-22bc-4ab5-9901-0a035f1b699c-server-thread1] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-06-27 12:26:41,047 [a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a650530b-22bc-4ab5-9901-0a035f1b699c@group-3CB73F243315-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315/current/log_inprogress_0
recon_1      | 2023-06-27 12:24:43,833 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 76 failover attempts. Trying to failover immediately. Current retry count: 76.
om3_1        | 2023-06-27 12:26:27,459 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2023-06-27 12:26:27,462 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1        | 2023-06-27 12:26:27,480 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2023-06-27 12:26:27,483 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2023-06-27 12:26:27,491 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2023-06-27 12:26:27,624 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2023-06-27 12:26:27,624 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 12:26:27,743 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1        | 2023-06-27 12:26:27,745 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1        | 2023-06-27 12:26:27,748 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2023-06-27 12:26:27,819 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-27 12:26:27,822 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-27 12:26:27,851 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:38,120 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392: start as a follower, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:38,121 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-27 12:26:38,606 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: start 1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392-FollowerState
datanode1_1  | 2023-06-27 12:26:38,606 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1a7440b0-385d-4580-b33d-c9708c19e79c: Detected pause in JVM or host machine approximately 0.227s without any GCs.
datanode1_1  | 2023-06-27 12:26:38,666 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 12:26:38,717 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-561639362392-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 12:26:38,665 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-561639362392,id=1a7440b0-385d-4580-b33d-c9708c19e79c
datanode1_1  | 2023-06-27 12:26:38,719 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-27 12:26:38,720 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-27 12:26:38,720 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-27 12:26:38,721 [1a7440b0-385d-4580-b33d-c9708c19e79c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-27 12:26:38,832 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=aeeab16c-188d-4045-bf49-561639362392
datanode1_1  | 2023-06-27 12:26:38,845 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-27 12:26:39,139 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-06-27 12:26:39,139 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection:   Response 0: 1a7440b0-385d-4580-b33d-c9708c19e79c<-a650530b-22bc-4ab5-9901-0a035f1b699c#0:OK-t1
datanode1_1  | 2023-06-27 12:26:39,141 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.LeaderElection: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2 ELECTION round 0: result PASSED
datanode1_1  | 2023-06-27 12:26:39,142 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: shutdown 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2
datanode1_1  | 2023-06-27 12:26:39,142 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2023-06-27 12:26:39,147 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3CB73F243315 with new leaderId: 1a7440b0-385d-4580-b33d-c9708c19e79c
datanode1_1  | 2023-06-27 12:26:39,148 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: change Leader from null to 1a7440b0-385d-4580-b33d-c9708c19e79c at term 1 for becomeLeader, leader elected after 16677ms
datanode1_1  | 2023-06-27 12:26:39,187 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-06-27 12:26:39,192 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 12:26:39,199 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-06-27 12:26:39,202 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-06-27 12:26:39,203 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-06-27 12:26:39,232 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-06-27 12:26:39,234 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 12:26:39,244 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2023-06-27 12:26:39,607 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode1_1  | 2023-06-27 12:26:39,611 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:39,612 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode1_1  | 2023-06-27 12:26:39,726 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode1_1  | 2023-06-27 12:26:39,740 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode1_1  | 2023-06-27 12:26:39,740 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 12:26:39,745 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode1_1  | 2023-06-27 12:26:39,788 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode1_1  | 2023-06-27 12:26:39,795 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/4559ef4956cc24eb3a049cf9664322b140e6853a ; compiled by 'runner' on 2023-06-27T11:59Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-27 12:22:53,887 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-06-27 12:26:38,689 [grpc-default-executor-1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315 replies to ELECTION vote request: 1a7440b0-385d-4580-b33d-c9708c19e79c<-cc01b188-4e0b-44d2-a762-70392abfc581#0:OK-t1. Peer's state: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315:t1, leader=null, voted=1a7440b0-385d-4580-b33d-c9708c19e79c, raftlog=Memoized:cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:39,841 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=aeeab16c-188d-4045-bf49-561639362392.
datanode2_1  | 2023-06-27 12:26:41,277 [cc01b188-4e0b-44d2-a762-70392abfc581-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3CB73F243315 with new leaderId: 1a7440b0-385d-4580-b33d-c9708c19e79c
datanode2_1  | 2023-06-27 12:26:41,329 [cc01b188-4e0b-44d2-a762-70392abfc581-server-thread1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: change Leader from null to 1a7440b0-385d-4580-b33d-c9708c19e79c at term 1 for appendEntries, leader elected after 20597ms
datanode2_1  | 2023-06-27 12:26:41,373 [cc01b188-4e0b-44d2-a762-70392abfc581-server-thread1] INFO server.RaftServer$Division: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315: set configuration 0: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 12:26:41,398 [cc01b188-4e0b-44d2-a762-70392abfc581-server-thread1] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-06-27 12:26:41,422 [cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cc01b188-4e0b-44d2-a762-70392abfc581@group-3CB73F243315-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315/current/log_inprogress_0
recon_1      | 2023-06-27 12:24:43,836 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 77 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 77.
recon_1      | 2023-06-27 12:24:45,841 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 78 failover attempts. Trying to failover immediately. Current retry count: 78.
recon_1      | 2023-06-27 12:24:45,843 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 79 failover attempts. Trying to failover immediately. Current retry count: 79.
recon_1      | 2023-06-27 12:24:45,844 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 80 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 80.
recon_1      | 2023-06-27 12:24:47,846 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 81 failover attempts. Trying to failover immediately. Current retry count: 81.
recon_1      | 2023-06-27 12:24:47,847 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 82 failover attempts. Trying to failover immediately. Current retry count: 82.
recon_1      | 2023-06-27 12:24:47,848 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 83 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 83.
recon_1      | 2023-06-27 12:24:49,860 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 84 failover attempts. Trying to failover immediately. Current retry count: 84.
recon_1      | 2023-06-27 12:24:49,864 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 85 failover attempts. Trying to failover immediately. Current retry count: 85.
recon_1      | 2023-06-27 12:24:49,897 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 86 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 86.
recon_1      | 2023-06-27 12:24:51,898 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 87 failover attempts. Trying to failover immediately. Current retry count: 87.
recon_1      | 2023-06-27 12:24:51,901 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 88 failover attempts. Trying to failover immediately. Current retry count: 88.
recon_1      | 2023-06-27 12:24:51,902 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 89 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 89.
recon_1      | 2023-06-27 12:24:53,904 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 90 failover attempts. Trying to failover immediately. Current retry count: 90.
recon_1      | 2023-06-27 12:24:53,906 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 91 failover attempts. Trying to failover immediately. Current retry count: 91.
recon_1      | 2023-06-27 12:24:53,907 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 92 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 92.
recon_1      | 2023-06-27 12:24:55,908 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 93 failover attempts. Trying to failover immediately. Current retry count: 93.
recon_1      | 2023-06-27 12:24:55,911 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 94 failover attempts. Trying to failover immediately. Current retry count: 94.
recon_1      | 2023-06-27 12:24:55,913 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 95 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 95.
recon_1      | 2023-06-27 12:24:57,914 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 96 failover attempts. Trying to failover immediately. Current retry count: 96.
recon_1      | 2023-06-27 12:24:57,916 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 97 failover attempts. Trying to failover immediately. Current retry count: 97.
recon_1      | 2023-06-27 12:24:57,923 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 98 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 98.
recon_1      | 2023-06-27 12:24:59,927 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 99 failover attempts. Trying to failover immediately. Current retry count: 99.
recon_1      | 2023-06-27 12:24:59,929 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 100 failover attempts. Trying to failover immediately. Current retry count: 100.
recon_1      | 2023-06-27 12:24:59,931 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 101 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 101.
recon_1      | 2023-06-27 12:25:01,936 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 102 failover attempts. Trying to failover immediately. Current retry count: 102.
recon_1      | 2023-06-27 12:25:01,938 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 103 failover attempts. Trying to failover immediately. Current retry count: 103.
recon_1      | 2023-06-27 12:25:01,943 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 104 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 104.
recon_1      | 2023-06-27 12:25:03,944 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 105 failover attempts. Trying to failover immediately. Current retry count: 105.
recon_1      | 2023-06-27 12:25:03,946 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 106 failover attempts. Trying to failover immediately. Current retry count: 106.
om3_1        | 2023-06-27 12:26:27,856 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2023-06-27 12:26:27,876 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-27 12:26:24,984 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-27 12:22:53,982 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 12:22:54,187 [main] INFO reflections.Reflections: Reflections took 156 ms to scan 3 urls, producing 131 keys and 286 values 
scm1.org_1   | 2023-06-27 12:22:54,320 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-27 12:22:54,335 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-27 12:22:54,365 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-27 12:22:54,366 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-27 12:22:54,590 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2023-06-27 12:22:54,590 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2023-06-27 12:22:55,543 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 266035175110
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   |            Start Date: Tue Jun 27 12:22:44 UTC 2023
scm1.org_1   |            Final Date: Fri Aug 04 12:22:44 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
scm1.org_1   |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
om2_1        | 2023-06-27 12:26:25,036 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2023-06-27 12:26:25,045 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2023-06-27 12:26:25,046 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2023-06-27 12:26:25,052 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1        | 2023-06-27 12:26:25,125 [main] INFO server.RaftServer: om2: start RPC server
om2_1        | 2023-06-27 12:26:25,805 [main] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2023-06-27 12:26:25,829 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2023-06-27 12:26:25,834 [main] INFO om.OzoneManager: Starting secret key client.
om2_1        | 2023-06-27 12:26:26,925 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z).
om2_1        | 2023-06-27 12:26:26,972 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M38.234338S and interval of PT1M
om2_1        | 2023-06-27 12:26:27,010 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2023-06-27 12:26:27,012 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 404278112254
om2_1        | 2023-06-27 12:26:27,133 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2023-06-27 12:26:27,139 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om2_1        | 2023-06-27 12:26:28,976 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1        | 2023-06-27 12:26:28,979 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om2_1        | 2023-06-27 12:26:28,979 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om2_1        | 2023-06-27 12:26:29,508 [main] INFO util.log: Logging initialized @83566ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1        | 2023-06-27 12:26:30,072 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5152574955ns, electionTimeout:5074ms
om2_1        | 2023-06-27 12:26:30,088 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-27 12:26:30,092 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1        | 2023-06-27 12:26:30,130 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om2_1        | 2023-06-27 12:26:30,131 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1        | 2023-06-27 12:26:30,230 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-27 12:26:30,951 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-27 12:26:30,951 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-27 12:26:30,983 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1        | 2023-06-27 12:26:30,985 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1        | 2023-06-27 12:26:32,703 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1        | 2023-06-27 12:26:32,929 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2023-06-27 12:26:32,970 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1        | 2023-06-27 12:26:32,974 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1        | 2023-06-27 12:26:32,975 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        | 2023-06-27 12:26:33,029 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om2_1        | 2023-06-27 12:26:34,243 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om2_1        | 2023-06-27 12:26:34,346 [main] INFO http.HttpServer2: Jetty bound to port 9874
om2_1        | 2023-06-27 12:26:34,396 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om2_1        | 2023-06-27 12:26:35,126 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1        | 2023-06-27 12:26:35,131 [main] INFO server.session: No SessionScavenger set, using defaults
om2_1        | 2023-06-27 12:26:35,170 [main] INFO server.session: node0 Scavenging every 660000ms
om2_1        | 2023-06-27 12:26:35,285 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1        | 2023-06-27 12:26:35,601 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om2_1        | 2023-06-27 12:26:35,642 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22d460fe{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1        | 2023-06-27 12:26:35,644 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@40aabc7b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1        | 2023-06-27 12:26:35,988 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 1 exception(s):
om2_1        | 2023-06-27 12:26:35,992 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1        | 2023-06-27 12:26:35,995 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
om3_1        | 2023-06-27 12:26:27,956 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-06-27 12:26:27,982 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-06-27 12:26:27,988 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1        | 2023-06-27 12:26:28,014 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2023-06-27 12:26:28,026 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2023-06-27 12:26:28,029 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2023-06-27 12:26:28,041 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2023-06-27 12:26:28,128 [main] INFO server.RaftServer: om3: start RPC server
om3_1        | 2023-06-27 12:26:28,969 [main] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2023-06-27 12:26:29,019 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2023-06-27 12:26:29,024 [main] INFO om.OzoneManager: Starting secret key client.
om3_1        | 2023-06-27 12:26:29,946 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z).
om3_1        | 2023-06-27 12:26:29,951 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M35.249448S and interval of PT1M
om3_1        | 2023-06-27 12:26:29,954 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2023-06-27 12:26:29,958 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 402605330599
om3_1        | 2023-06-27 12:26:30,033 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        | 2023-06-27 12:26:30,089 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2023-06-27 12:26:31,933 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1        | 2023-06-27 12:26:31,935 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om3_1        | 2023-06-27 12:26:31,939 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om3_1        | 2023-06-27 12:26:32,505 [main] INFO util.log: Logging initialized @86773ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1        | 2023-06-27 12:26:33,177 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5301521115ns, electionTimeout:5188ms
om3_1        | 2023-06-27 12:26:33,186 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1        | 2023-06-27 12:26:33,202 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1        | 2023-06-27 12:26:33,227 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1        | 2023-06-27 12:26:33,237 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1        | 2023-06-27 12:26:33,296 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-06-27 12:26:34,234 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-06-27 12:26:34,248 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-06-27 12:26:34,299 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om3_1        | 2023-06-27 12:26:34,307 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1        | 2023-06-27 12:26:35,803 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1        | 2023-06-27 12:26:36,371 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1        | 2023-06-27 12:26:36,422 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om3_1        | 2023-06-27 12:26:36,438 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om3_1        | 2023-06-27 12:26:36,449 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om3_1        | 2023-06-27 12:26:36,547 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om3_1        | 2023-06-27 12:26:37,982 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om3_1        | 2023-06-27 12:26:38,304 [main] INFO http.HttpServer2: Jetty bound to port 9874
om3_1        | 2023-06-27 12:26:38,380 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om3_1        | 2023-06-27 12:26:39,000 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1        | 2023-06-27 12:26:39,253 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 1 exception(s):
om3_1        | 2023-06-27 12:26:39,268 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1.org_1   |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
scm1.org_1   |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
scm1.org_1   |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
scm1.org_1   |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
scm1.org_1   |                        6575bb91718be2b62fe8285671670d4afc71639e
scm1.org_1   |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
scm1.org_1   |                        c63f624dd59419fb4e3d7f57549fb866400baea0
scm1.org_1   |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
scm1.org_1   |                        acbb5f8d8e5f75c3208e116728c66494080d0391
scm1.org_1   |                        4883b896992d697c14fca81b51cc41815ceb8fc5
scm1.org_1   |                        9e72ce2f778759033901bef92ec92fabd21c5a89
scm1.org_1   |                        c7396a443e2624b28df5d21502e09909898c3be0
scm1.org_1   |                        4466369f70a26034a8bbf4c275f76029
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/266035175110.crt.
scm1.org_1   | 2023-06-27 12:22:55,552 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 1
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   |            Start Date: Tue Jun 27 12:22:43 UTC 2023
scm1.org_1   |            Final Date: Fri Aug 04 12:22:43 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   |            Public Key: RSA Public Key [a8:60:72:f5:2a:71:43:2e:3d:54:84:dd:e2:31:97:59:d8:4c:b4:1d],[56:66:d1:a4]
scm1.org_1   |         modulus: c566751941d566d358bf5891076b5b513cb88084192ccb4f572bdb5f42aad5b1ea897e8177fa9215dc912f1876299c42ff886443ec2252a669a88365844e60bf8165680bbf7a2585d0b6b32bad2413755fa233211e8d984c8b75d3d7eb5a0f06d616a4daaa017cdc3a0476d4603b3bf2b4fdb592ce827f686ecfbff75dc5200893089db3f2dbbacf6756a4ffdb25bfdf2851d0dc543910e01e7b5721ce918355da280b950c3c1a6a84d186a35a6025e9a46e7b37b38d846071e571c69957027691f8dc0b16f260da4cb90eda80c54d1c11526c1d99a223f4a7047fa3841ce88bc876d87cf7bbdd11cb4182feb414c0ad4001d517fdff055329ced37e8840a4ed
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: 02de6c268bacf89c6cc78a89f361065051058433
scm1.org_1   |                        a2d41a7e15964561ca37fcedf80ec8f6afd990c0
scm1.org_1   |                        35bf6f1bd0e074a927ce6dd289fbad4b606cabcf
scm1.org_1   |                        8693d2981b5c14d0971218b8015471c7b70d7fa8
recon_1      | 2023-06-27 12:25:03,947 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 107 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 107.
recon_1      | 2023-06-27 12:25:05,949 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 108 failover attempts. Trying to failover immediately. Current retry count: 108.
recon_1      | 2023-06-27 12:25:05,950 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 109 failover attempts. Trying to failover immediately. Current retry count: 109.
recon_1      | 2023-06-27 12:25:05,951 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 110 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 110.
recon_1      | 2023-06-27 12:25:07,953 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 111 failover attempts. Trying to failover immediately. Current retry count: 111.
recon_1      | 2023-06-27 12:25:07,954 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 112 failover attempts. Trying to failover immediately. Current retry count: 112.
recon_1      | 2023-06-27 12:25:07,955 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 113 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 113.
recon_1      | 2023-06-27 12:25:09,957 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 114 failover attempts. Trying to failover immediately. Current retry count: 114.
recon_1      | 2023-06-27 12:25:09,958 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 115 failover attempts. Trying to failover immediately. Current retry count: 115.
recon_1      | 2023-06-27 12:25:09,959 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 116 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 116.
recon_1      | 2023-06-27 12:25:11,976 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 117 failover attempts. Trying to failover immediately. Current retry count: 117.
recon_1      | 2023-06-27 12:25:11,978 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 118 failover attempts. Trying to failover immediately. Current retry count: 118.
recon_1      | 2023-06-27 12:25:11,978 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 119 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 119.
recon_1      | 2023-06-27 12:25:14,006 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 120 failover attempts. Trying to failover immediately. Current retry count: 120.
recon_1      | 2023-06-27 12:25:14,054 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 121 failover attempts. Trying to failover immediately. Current retry count: 121.
recon_1      | 2023-06-27 12:25:14,122 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 122 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 122.
recon_1      | 2023-06-27 12:25:16,130 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 123 failover attempts. Trying to failover immediately. Current retry count: 123.
recon_1      | 2023-06-27 12:25:16,131 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 124 failover attempts. Trying to failover immediately. Current retry count: 124.
recon_1      | 2023-06-27 12:25:16,132 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 125 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 125.
recon_1      | 2023-06-27 12:25:18,135 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 126 failover attempts. Trying to failover immediately. Current retry count: 126.
recon_1      | 2023-06-27 12:25:18,151 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 127 failover attempts. Trying to failover immediately. Current retry count: 127.
recon_1      | 2023-06-27 12:25:18,162 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 128 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 128.
recon_1      | 2023-06-27 12:25:20,164 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 129 failover attempts. Trying to failover immediately. Current retry count: 129.
recon_1      | 2023-06-27 12:25:20,167 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 130 failover attempts. Trying to failover immediately. Current retry count: 130.
recon_1      | 2023-06-27 12:25:20,168 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 131 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 131.
recon_1      | 2023-06-27 12:25:22,170 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 132 failover attempts. Trying to failover immediately. Current retry count: 132.
recon_1      | 2023-06-27 12:25:22,170 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 133 failover attempts. Trying to failover immediately. Current retry count: 133.
recon_1      | 2023-06-27 12:25:22,171 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 134 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 134.
recon_1      | 2023-06-27 12:25:24,182 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 135 failover attempts. Trying to failover immediately. Current retry count: 135.
recon_1      | 2023-06-27 12:25:24,196 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 136 failover attempts. Trying to failover immediately. Current retry count: 136.
recon_1      | 2023-06-27 12:25:24,203 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 137 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 137.
recon_1      | 2023-06-27 12:25:26,212 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 138 failover attempts. Trying to failover immediately. Current retry count: 138.
recon_1      | 2023-06-27 12:25:26,214 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 139 failover attempts. Trying to failover immediately. Current retry count: 139.
recon_1      | 2023-06-27 12:25:26,216 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 140 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 140.
recon_1      | 2023-06-27 12:25:28,217 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 141 failover attempts. Trying to failover immediately. Current retry count: 141.
recon_1      | 2023-06-27 12:25:28,219 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 142 failover attempts. Trying to failover immediately. Current retry count: 142.
recon_1      | 2023-06-27 12:25:28,220 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 143 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 143.
recon_1      | 2023-06-27 12:25:30,222 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 144 failover attempts. Trying to failover immediately. Current retry count: 144.
recon_1      | 2023-06-27 12:25:30,226 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 145 failover attempts. Trying to failover immediately. Current retry count: 145.
recon_1      | 2023-06-27 12:25:30,231 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 146 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 146.
recon_1      | 2023-06-27 12:25:32,235 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 147 failover attempts. Trying to failover immediately. Current retry count: 147.
recon_1      | 2023-06-27 12:25:32,238 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 148 failover attempts. Trying to failover immediately. Current retry count: 148.
recon_1      | 2023-06-27 12:25:32,239 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 149 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 149.
recon_1      | 2023-06-27 12:25:34,241 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 150 failover attempts. Trying to failover immediately. Current retry count: 150.
datanode1_1  | 2023-06-27 12:26:39,797 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-06-27 12:26:39,813 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode1_1  | 2023-06-27 12:26:39,814 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 12:26:39,817 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode1_1  | 2023-06-27 12:26:39,820 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode1_1  | 2023-06-27 12:26:39,820 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode1_1  | 2023-06-27 12:26:39,824 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: 480b0a3f73148035e03a54000ca857ceadf67ecd
scm3.org_1   |                        7d660017f0c6678cc981570a85eddf650ead6d0c
scm3.org_1   |                        96ddd2cbb6d683cc90069adbb603cde53801c243
scm3.org_1   |                        b5744a3d99e625a8e97f7308fba8dde8fb23851a
scm3.org_1   |                        10e866039ac015673740f5bc6d730880218d7e52
scm3.org_1   |                        7eb54cd7ed262e22d6132bbf414a22d4378fdd62
scm3.org_1   |                        4415c1e0fc93a970b48b640ccb00ea390cb1e3c8
scm3.org_1   |                        73bde84cf052f54262adc9049b4c3655ead7ecde
scm3.org_1   |                        f14b6889f98ead3787df02d888cfb07590408dcc
scm3.org_1   |                        b05900811b55cf729ad52a3c56ba9ce7249c5a5d
scm3.org_1   |                        9459d6fa86f54f2bb7a67f38bbaa6489318d36b6
scm3.org_1   |                        593a03155b713abfd60f68fb5c34b76a562d19a9
scm3.org_1   |                        4ad734411dec29ebb5a1d554a7ce5019
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
datanode1_1  | 2023-06-27 12:26:39,834 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode1_1  | 2023-06-27 12:26:39,840 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode1_1  | 2023-06-27 12:26:39,842 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-27 12:26:39,843 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-06-27 12:26:39,878 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO impl.RoleInfo: 1a7440b0-385d-4580-b33d-c9708c19e79c: start 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderStateImpl
datanode1_1  | 2023-06-27 12:26:39,884 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-06-27 12:26:39,969 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3108f661-f070-45b5-be9d-3cb73f243315/current/log_inprogress_0
datanode1_1  | 2023-06-27 12:26:39,999 [1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315-LeaderElection2] INFO server.RaftServer$Division: 1a7440b0-385d-4580-b33d-c9708c19e79c@group-3CB73F243315: set configuration 0: peers:[1a7440b0-385d-4580-b33d-c9708c19e79c|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, a650530b-22bc-4ab5-9901-0a035f1b699c|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, cc01b188-4e0b-44d2-a762-70392abfc581|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 12:26:40,044 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-27 12:26:41,618 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=aeeab16c-188d-4045-bf49-561639362392.
scm2.org_1   |             Signature: 21d8a50ec484a392ccbc308bb247c2f51e7437bc
scm2.org_1   |                        561160e406d4d2579c27e1bad1ce9dc25fed8d84
scm2.org_1   |                        197e69e6706bc70a31cfbfc1f00a36bb60baf1b0
scm2.org_1   |                        54b9911c37eac56db5f7e85ec2d9bfe8b355fe59
scm2.org_1   |                        15a2646d56ffe9e4b515b5cf60f8ef26dc083a7f
scm2.org_1   |                        a64104d73388d7b4bed500e0cf07ee0c467966b1
scm2.org_1   |                        cad256699876c4604fc38c188391434fe3d9b14b
scm2.org_1   |                        28acd396e79f33471c78fdca6db1ccb0015875c1
scm2.org_1   |                        b1c543cc9bb2b6b0eeb5781467882709f0e938b8
scm2.org_1   |                        b10eb5777f79c54a81ae96c7ef02fe9cb7dba10c
scm2.org_1   |                        315db7a4d434f8aed48a73834ea42b60b7f2117e
scm2.org_1   |                        b19cff296be9ed50482ed2263054c7f16e3b72d6
scm2.org_1   |                        cb250ca13eda39b6659398b24ddb12e5
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2023-06-27 12:23:20,501 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716787512 ms and interval 86400000 ms.
scm2.org_1   | 2023-06-27 12:23:20,754 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 12:23:21,260 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 12:23:22,268 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2023-06-27 12:23:22,273 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2023-06-27 12:23:22,517 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2023-06-27 12:23:23,170 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:7480df95-74c6-4260-b2a9-e3614d893449
scm2.org_1   | 2023-06-27 12:23:23,291 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-27 12:23:23,296 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-27 12:23:23,451 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2023-06-27 12:23:23,478 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-27 12:23:23,488 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-27 12:23:23,492 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-27 12:23:23,494 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-27 12:26:35,996 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-27 12:26:36,033 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-27 12:26:36,034 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-27 12:26:36,037 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1        | 2023-06-27 12:26:37,041 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om2_1        | 2023-06-27 12:26:37,266 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@31d26296{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-16679869212676379627/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1        | 2023-06-27 12:26:37,375 [main] INFO server.AbstractConnector: Started ServerConnector@14a28980{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1        | 2023-06-27 12:26:37,381 [main] INFO server.Server: Started @91439ms
om2_1        | 2023-06-27 12:26:37,409 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1        | 2023-06-27 12:26:37,409 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1        | 2023-06-27 12:26:39,271 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
scm1.org_1   |                        eece43d2072924486b8a2e674f6b5465d4190ac6
scm1.org_1   |                        f5030570b2b63224eab18c88675f9c9c24008980
scm1.org_1   |                        86a902cf862c5da5b230958d06c3dba90324e417
scm1.org_1   |                        4a3243e42eeb0d65c9a0c4e5a03f9c886b4dbf8e
scm1.org_1   |                        1aa70751ced732e39cbf893a239b9956aef292ef
scm1.org_1   |                        f2a3b8f10e3a874b22815704e12fdcd5c03b31c4
scm1.org_1   |                        d08d83f183f3c5f1239b0bdc87ac5f8a6e885ec3
scm1.org_1   |                        652ea4f6a7de0b242be91e70d65a49c09e7aca76
scm1.org_1   |                        86212367c9e9316e2a4bb892f0cda036
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0x6
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2023-06-27 12:22:55,564 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 266035175110
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   | 2023-06-27 12:23:54,810 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716792210 ms and interval 86400000 ms.
scm3.org_1   | 2023-06-27 12:23:55,164 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 12:23:55,498 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 12:23:56,263 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2023-06-27 12:23:56,267 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2023-06-27 12:23:56,517 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2023-06-27 12:23:56,845 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:2cf3494c-faba-4bd8-bf38-71b782f966a2
scm3.org_1   | 2023-06-27 12:23:56,902 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-27 12:23:56,908 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-27 12:23:56,982 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2023-06-27 12:23:57,001 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-27 12:23:57,005 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-27 12:23:57,005 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-27 12:23:57,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-27 12:23:57,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3.org_1   | 2023-06-27 12:23:57,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2023-06-27 12:23:57,008 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2023-06-27 12:23:57,011 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 12:23:57,012 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2023-06-27 12:23:57,014 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-27 12:23:57,039 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3.org_1   | 2023-06-27 12:23:57,047 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3.org_1   | 2023-06-27 12:23:57,047 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-06-27 12:23:57,873 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3.org_1   | 2023-06-27 12:23:57,876 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3.org_1   | 2023-06-27 12:23:57,876 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3.org_1   | 2023-06-27 12:23:57,877 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-27 12:23:57,877 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-27 12:23:57,881 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-27 12:23:57,899 [main] INFO server.RaftServer: 2cf3494c-faba-4bd8-bf38-71b782f966a2: addNew group-589B8C5BE712:[] returns group-589B8C5BE712:java.util.concurrent.CompletableFuture@477bea57[Not completed]
scm3.org_1   | 2023-06-27 12:23:58,004 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2: new RaftServerImpl for group-589B8C5BE712:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2023-06-27 12:23:58,010 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2023-06-27 12:23:58,012 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2023-06-27 12:23:58,015 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2023-06-27 12:23:58,015 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-27 12:23:58,015 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-27 12:23:58,016 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3.org_1   | 2023-06-27 12:23:58,036 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2023-06-27 12:23:58,037 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-27 12:23:58,057 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2023-06-27 12:23:58,058 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2023-06-27 12:23:58,104 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
om3_1        | 2023-06-27 12:26:39,313 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-06-27 12:26:39,366 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1        | 2023-06-27 12:26:39,369 [main] INFO server.session: No SessionScavenger set, using defaults
om3_1        | 2023-06-27 12:26:39,372 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-27 12:26:37,418 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1        | 2023-06-27 12:26:37,427 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1        | 2023-06-27 12:26:37,450 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1        | 2023-06-27 12:26:37,726 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om2: Detected pause in JVM or host machine approximately 0.138s with 0.207s GC time.
om2_1        | GC pool 'ParNew' had collection(s): count=1 time=207ms
om2_1        | 2023-06-27 12:26:38,965 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:33383
om2_1        | 2023-06-27 12:26:39,038 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om2_1        | 2023-06-27 12:26:39,059 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.981268721s. [buffered_nanos=2985106794, waiting_for_connection]
om2_1        | 2023-06-27 12:26:39,060 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
om2_1        | 2023-06-27 12:26:39,061 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1        | 2023-06-27 12:26:39,062 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.981268721s. [buffered_nanos=2985106794, waiting_for_connection]
om2_1        | 2023-06-27 12:26:39,064 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 1: result REJECTED
recon_1      | 2023-06-27 12:25:34,242 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 151 failover attempts. Trying to failover immediately. Current retry count: 151.
recon_1      | 2023-06-27 12:25:34,243 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 152 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 152.
recon_1      | 2023-06-27 12:25:36,245 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 153 failover attempts. Trying to failover immediately. Current retry count: 153.
recon_1      | 2023-06-27 12:25:36,246 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 154 failover attempts. Trying to failover immediately. Current retry count: 154.
recon_1      | 2023-06-27 12:25:36,247 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 155 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 155.
recon_1      | 2023-06-27 12:25:38,250 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 156 failover attempts. Trying to failover immediately. Current retry count: 156.
recon_1      | 2023-06-27 12:25:38,251 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 157 failover attempts. Trying to failover immediately. Current retry count: 157.
recon_1      | 2023-06-27 12:25:38,252 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 158 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 158.
recon_1      | 2023-06-27 12:25:39,711 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:36560
recon_1      | 2023-06-27 12:25:39,753 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:25:40,274 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 159 failover attempts. Trying to failover immediately. Current retry count: 159.
recon_1      | 2023-06-27 12:25:40,277 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 160 failover attempts. Trying to failover immediately. Current retry count: 160.
recon_1      | 2023-06-27 12:25:40,285 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 161 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 161.
recon_1      | 2023-06-27 12:25:40,964 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:35010
recon_1      | 2023-06-27 12:25:41,164 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:25:41,181 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:50890
recon_1      | 2023-06-27 12:25:41,456 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:25:42,288 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 162 failover attempts. Trying to failover immediately. Current retry count: 162.
recon_1      | 2023-06-27 12:25:42,291 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 163 failover attempts. Trying to failover immediately. Current retry count: 163.
recon_1      | 2023-06-27 12:25:42,332 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 164 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 164.
recon_1      | 2023-06-27 12:25:44,339 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 165 failover attempts. Trying to failover immediately. Current retry count: 165.
recon_1      | 2023-06-27 12:25:44,340 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 166 failover attempts. Trying to failover immediately. Current retry count: 166.
recon_1      | 2023-06-27 12:25:44,343 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 167 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 167.
recon_1      | 2023-06-27 12:25:44,860 [IPC Server handler 5 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a650530b-22bc-4ab5-9901-0a035f1b699c
recon_1      | 2023-06-27 12:25:44,862 [IPC Server handler 2 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1a7440b0-385d-4580-b33d-c9708c19e79c
recon_1      | 2023-06-27 12:25:44,904 [IPC Server handler 2 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 1a7440b0-385d-4580-b33d-c9708c19e79c{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886], networkLocation: /default-rack, certSerialId: 399146862779, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-27 12:25:44,939 [IPC Server handler 5 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a650530b-22bc-4ab5-9901-0a035f1b699c{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886], networkLocation: /default-rack, certSerialId: 397752053077, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-27 12:25:45,522 [IPC Server handler 3 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cc01b188-4e0b-44d2-a762-70392abfc581
recon_1      | 2023-06-27 12:25:45,537 [IPC Server handler 3 on default port 9891] INFO node.SCMNodeManager: Registered Data node : cc01b188-4e0b-44d2-a762-70392abfc581{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 401704538238, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-27 12:25:46,346 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 168 failover attempts. Trying to failover immediately. Current retry count: 168.
recon_1      | 2023-06-27 12:25:46,357 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 169 failover attempts. Trying to failover immediately. Current retry count: 169.
recon_1      | 2023-06-27 12:25:46,367 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 170 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 170.
recon_1      | 2023-06-27 12:25:48,073 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 1a7440b0-385d-4580-b33d-c9708c19e79c to Node DB.
recon_1      | 2023-06-27 12:25:48,112 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a650530b-22bc-4ab5-9901-0a035f1b699c to Node DB.
recon_1      | 2023-06-27 12:25:48,114 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node cc01b188-4e0b-44d2-a762-70392abfc581 to Node DB.
recon_1      | 2023-06-27 12:25:48,382 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 171 failover attempts. Trying to failover immediately. Current retry count: 171.
recon_1      | 2023-06-27 12:25:48,402 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 172 failover attempts. Trying to failover immediately. Current retry count: 172.
recon_1      | 2023-06-27 12:25:48,419 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 173 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 173.
recon_1      | 2023-06-27 12:25:50,421 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 174 failover attempts. Trying to failover immediately. Current retry count: 174.
recon_1      | 2023-06-27 12:25:50,421 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 175 failover attempts. Trying to failover immediately. Current retry count: 175.
recon_1      | 2023-06-27 12:25:50,422 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 176 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 176.
recon_1      | 2023-06-27 12:25:52,423 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 177 failover attempts. Trying to failover immediately. Current retry count: 177.
recon_1      | 2023-06-27 12:25:52,424 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 178 failover attempts. Trying to failover immediately. Current retry count: 178.
om3_1        | 2023-06-27 12:26:39,374 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-06-27 12:26:39,375 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1        | 2023-06-27 12:26:39,395 [main] INFO server.session: node0 Scavenging every 600000ms
om3_1        | 2023-06-27 12:26:39,851 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2023-06-27 12:26:39,928 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@32bd7445{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1        | 2023-06-27 12:26:39,945 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@15aae3b6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1        | 2023-06-27 12:26:39,078 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
om2_1        | 2023-06-27 12:26:39,082 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
scm1.org_1   |            Start Date: Tue Jun 27 12:22:44 UTC 2023
scm1.org_1   |            Final Date: Fri Aug 04 12:22:44 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=f163470c-d3e0-4891-94bf-0bc988a6aa12,O=CID-5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   |            Public Key: RSA Public Key [eb:02:88:98:ca:8f:1a:58:f9:db:15:50:1c:b1:88:0e:c5:ad:1b:b8],[56:66:d1:a4]
scm1.org_1   |         modulus: c499fb168a8d6667a6efc38645b1617321f131779908eb89dcfdc8e424b27f50eb4b90663e6709604089705838bf08b35bbcc73a44b57ae90f88585840053d005d5aedbedff0bf5106131a4b21520c2cf7e2ea46276187144ecfdc6ab48c562ecf426db2b32060ca3af869f4cd000ba6350f3522b12f58fa2918e709f8a97f6012f6653dfb625a1fecd5af54e8e667044503d7e198259184e00f806f9f9454e56c608a0e4e24dd58dc7fc45d6e63fdbc679a41bb603947db744055fe07f0feceb8c0435b9fecc83cd062c6387998e4705e63c2fa8a68cfcdba569aca33470480756fddecb9b4e2ee52b476ef4520c9856d814f2a7fe7750f9fdcf57859054661
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: 7820a57fff0e0f97f57cccef6340d2ccabfa5fcd
scm1.org_1   |                        08de97622c6798bfbd9da0688e0f34c4a3b5604e
scm1.org_1   |                        80f1fe5ffdd8739f29fcd8549f53509d7f0a9e3b
scm1.org_1   |                        3f91b1bbf4b4abf2ffd99a1c73c18fd55ef607f6
scm1.org_1   |                        6575bb91718be2b62fe8285671670d4afc71639e
scm1.org_1   |                        4291c1e80aea22ef41a08b7c7c07bfed3d01fe79
scm1.org_1   |                        c63f624dd59419fb4e3d7f57549fb866400baea0
scm1.org_1   |                        f9d538af9ce464d09013b680d0fdc14d1b7edc86
scm1.org_1   |                        acbb5f8d8e5f75c3208e116728c66494080d0391
scm1.org_1   |                        4883b896992d697c14fca81b51cc41815ceb8fc5
scm1.org_1   |                        9e72ce2f778759033901bef92ec92fabd21c5a89
scm1.org_1   |                        c7396a443e2624b28df5d21502e09909898c3be0
scm1.org_1   |                        4466369f70a26034a8bbf4c275f76029
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2023-06-27 12:22:55,573 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716788431 ms and interval 86400000 ms.
scm1.org_1   | 2023-06-27 12:22:55,662 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 12:22:55,867 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 12:22:56,235 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2023-06-27 12:22:56,237 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2023-06-27 12:22:56,324 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-27 12:22:56,506 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:f163470c-d3e0-4891-94bf-0bc988a6aa12
scm1.org_1   | 2023-06-27 12:22:56,544 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 12:22:56,548 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 12:22:56,593 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-27 12:22:56,602 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 12:22:56,604 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-27 12:22:56,605 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 12:22:56,605 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-27 12:23:23,495 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2.org_1   | 2023-06-27 12:23:23,500 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2023-06-27 12:23:23,502 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2023-06-27 12:23:23,506 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 12:23:23,512 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2023-06-27 12:23:23,513 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-27 12:23:23,540 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2.org_1   | 2023-06-27 12:23:23,559 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2.org_1   | 2023-06-27 12:23:23,568 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-06-27 12:23:24,616 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2.org_1   | 2023-06-27 12:23:24,625 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2.org_1   | 2023-06-27 12:23:24,629 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2.org_1   | 2023-06-27 12:23:24,629 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-27 12:23:24,629 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-27 12:23:24,632 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-27 12:23:24,660 [main] INFO server.RaftServer: 7480df95-74c6-4260-b2a9-e3614d893449: addNew group-589B8C5BE712:[] returns group-589B8C5BE712:java.util.concurrent.CompletableFuture@4e26040f[Not completed]
scm2.org_1   | 2023-06-27 12:23:24,786 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449: new RaftServerImpl for group-589B8C5BE712:[] with SCMStateMachine:uninitialized
scm2.org_1   | 2023-06-27 12:23:24,793 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2.org_1   | 2023-06-27 12:23:24,794 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2.org_1   | 2023-06-27 12:23:24,796 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2.org_1   | 2023-06-27 12:23:24,797 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-27 12:23:24,797 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-27 12:23:24,798 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2023-06-27 12:23:24,834 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2023-06-27 12:23:24,835 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-27 12:23:24,869 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2023-06-27 12:23:24,871 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2.org_1   | 2023-06-27 12:23:24,925 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
om2_1        | 2023-06-27 12:26:39,083 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-27 12:26:39,460 [main] INFO om.TrashPolicyOzone: The configured checkpoint interval is 0 minutes. Using an interval of 1 minutes that is used for deletion instead
om2_1        | 2023-06-27 12:26:39,520 [main] INFO om.TrashPolicyOzone: Ozone Manager trash configuration: Deletion interval = 1 minutes, Emptier interval = 1 minutes.
om2_1        | 2023-06-27 12:26:40,353 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om2: Detected pause in JVM or host machine approximately 0.103s without any GCs.
scm3.org_1   | 2023-06-27 12:23:58,117 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm3.org_1   | 2023-06-27 12:23:58,128 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2023-06-27 12:23:58,129 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2023-06-27 12:23:58,178 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm3.org_1   | 2023-06-27 12:23:58,428 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-27 12:23:58,436 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2023-06-27 12:23:58,438 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3.org_1   | 2023-06-27 12:23:58,439 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3.org_1   | 2023-06-27 12:23:58,440 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3.org_1   | 2023-06-27 12:23:58,443 [2cf3494c-faba-4bd8-bf38-71b782f966a2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3.org_1   | 2023-06-27 12:23:58,448 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2023-06-27 12:23:58,448 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2023-06-27 12:23:58,450 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2023-06-27 12:23:58,581 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm3.org_1   | 2023-06-27 12:23:58,650 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3.org_1   | 2023-06-27 12:23:58,652 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2023-06-27 12:23:58,662 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2023-06-27 12:23:58,668 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2023-06-27 12:23:58,855 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2023-06-27 12:23:58,900 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm3.org_1   | 2023-06-27 12:23:58,912 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-27 12:23:58,960 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2023-06-27 12:23:59,070 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2023-06-27 12:23:59,074 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-27 12:23:59,130 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3.org_1   | 2023-06-27 12:23:59,130 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2023-06-27 12:23:59,135 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3.org_1   | 2023-06-27 12:23:59,136 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3.org_1   | 2023-06-27 12:23:59,154 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3.org_1   | 2023-06-27 12:23:59,155 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3.org_1   | 2023-06-27 12:23:59,255 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-27 12:23:59,256 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-27 12:23:59,354 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2023-06-27 12:23:59,591 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2023-06-27 12:23:59,646 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2023-06-27 12:23:59,687 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm3.org_1   | 2023-06-27 12:23:59,749 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2023-06-27 12:23:59,770 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:23:59,782 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-06-27 12:24:00,421 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm3.org_1   | 2023-06-27 12:24:00,421 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm3.org_1   | 2023-06-27 12:24:00,478 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2023-06-27 12:24:00,553 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 12:24:00,642 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2023-06-27 12:24:00,733 [Listener at 0.0.0.0/9961] INFO hdds.HddsUtils: Restoring thread name: main
scm3.org_1   | 2023-06-27 12:24:00,782 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
recon_1      | 2023-06-27 12:25:52,427 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 179 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 179.
recon_1      | 2023-06-27 12:25:54,429 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 180 failover attempts. Trying to failover immediately. Current retry count: 180.
scm2.org_1   | 2023-06-27 12:23:24,948 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2.org_1   | 2023-06-27 12:23:24,963 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2023-06-27 12:23:24,965 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2023-06-27 12:23:25,006 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2.org_1   | 2023-06-27 12:23:25,306 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-27 12:23:25,311 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2023-06-27 12:23:25,313 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2.org_1   | 2023-06-27 12:23:25,316 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2.org_1   | 2023-06-27 12:23:25,317 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:56,606 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1      | 2023-06-27 12:25:54,430 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 181 failover attempts. Trying to failover immediately. Current retry count: 181.
scm2.org_1   | 2023-06-27 12:23:25,318 [7480df95-74c6-4260-b2a9-e3614d893449-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-06-27 12:22:56,606 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-27 12:22:56,607 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-06-27 12:22:56,609 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:56,609 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1      | 2023-06-27 12:25:54,431 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 182 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 182.
scm2.org_1   | 2023-06-27 12:23:25,320 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2023-06-27 12:23:25,320 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2023-06-27 12:23:25,320 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2023-06-27 12:23:25,574 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm2.org_1   | 2023-06-27 12:23:25,702 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2.org_1   | 2023-06-27 12:23:25,707 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2023-06-27 12:23:25,730 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2023-06-27 12:23:25,735 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2023-06-27 12:23:25,978 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2023-06-27 12:23:26,035 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm2.org_1   | 2023-06-27 12:23:26,041 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-27 12:23:26,106 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2023-06-27 12:23:26,161 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2023-06-27 12:23:26,162 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-27 12:23:26,297 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2023-06-27 12:23:26,298 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2023-06-27 12:23:26,314 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2.org_1   | 2023-06-27 12:23:26,351 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2.org_1   | 2023-06-27 12:23:26,396 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2.org_1   | 2023-06-27 12:23:26,403 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2.org_1   | 2023-06-27 12:23:26,562 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-27 12:23:26,563 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-27 12:23:26,628 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2023-06-27 12:23:26,999 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2023-06-27 12:23:27,075 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2023-06-27 12:23:27,085 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm2.org_1   | 2023-06-27 12:23:27,166 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2023-06-27 12:23:27,244 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:23:27,247 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-27 12:23:27,801 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm2.org_1   | 2023-06-27 12:23:27,802 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm2.org_1   | 2023-06-27 12:23:27,951 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2023-06-27 12:23:28,051 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 12:23:28,179 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2023-06-27 12:23:28,251 [Listener at 0.0.0.0/9961] INFO hdds.HddsUtils: Restoring thread name: main
scm2.org_1   | 2023-06-27 12:23:28,287 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm2.org_1   | 2023-06-27 12:23:28,291 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm2.org_1   | 2023-06-27 12:23:30,449 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-27 12:23:30,480 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 12:23:30,489 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2023-06-27 12:23:30,553 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
scm2.org_1   | 2023-06-27 12:23:30,622 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-27 12:23:30,635 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 12:23:30,636 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2023-06-27 12:23:30,808 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
scm2.org_1   | 2023-06-27 12:23:31,057 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-27 12:23:31,113 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 12:23:31,150 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2023-06-27 12:23:31,171 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
scm2.org_1   | 2023-06-27 12:23:31,785 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2.org_1   | 2023-06-27 12:23:31,786 [main] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        false
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                                                Value
scm2.org_1   | Threshold                                          10
scm2.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2.org_1   | Max Size to Move per Iteration                     500GB
recon_1      | 2023-06-27 12:25:56,432 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 183 failover attempts. Trying to failover immediately. Current retry count: 183.
recon_1      | 2023-06-27 12:25:56,433 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 184 failover attempts. Trying to failover immediately. Current retry count: 184.
recon_1      | 2023-06-27 12:25:56,434 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 185 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 185.
recon_1      | 2023-06-27 12:25:58,436 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 186 failover attempts. Trying to failover immediately. Current retry count: 186.
recon_1      | 2023-06-27 12:25:58,437 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 187 failover attempts. Trying to failover immediately. Current retry count: 187.
recon_1      | 2023-06-27 12:25:58,438 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 188 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 188.
recon_1      | 2023-06-27 12:26:00,440 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 189 failover attempts. Trying to failover immediately. Current retry count: 189.
recon_1      | 2023-06-27 12:26:00,440 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 190 failover attempts. Trying to failover immediately. Current retry count: 190.
recon_1      | 2023-06-27 12:26:00,441 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 191 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 191.
recon_1      | 2023-06-27 12:26:02,442 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 192 failover attempts. Trying to failover immediately. Current retry count: 192.
recon_1      | 2023-06-27 12:26:02,444 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 193 failover attempts. Trying to failover immediately. Current retry count: 193.
recon_1      | 2023-06-27 12:26:02,445 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 194 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 194.
recon_1      | 2023-06-27 12:26:04,446 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 195 failover attempts. Trying to failover immediately. Current retry count: 195.
recon_1      | 2023-06-27 12:26:04,447 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 196 failover attempts. Trying to failover immediately. Current retry count: 196.
recon_1      | 2023-06-27 12:26:04,449 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 197 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 197.
recon_1      | 2023-06-27 12:26:06,451 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 198 failover attempts. Trying to failover immediately. Current retry count: 198.
recon_1      | 2023-06-27 12:26:06,452 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 199 failover attempts. Trying to failover immediately. Current retry count: 199.
scm1.org_1   | 2023-06-27 12:22:56,611 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 12:22:56,623 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 12:22:56,626 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:56,627 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-27 12:22:56,988 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-27 12:22:56,991 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:56,992 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-27 12:22:56,992 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-27 12:22:56,993 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 12:22:56,996 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 12:22:57,001 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer: f163470c-d3e0-4891-94bf-0bc988a6aa12: found a subdirectory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   | 2023-06-27 12:22:57,009 [main] INFO server.RaftServer: f163470c-d3e0-4891-94bf-0bc988a6aa12: addNew group-589B8C5BE712:[] returns group-589B8C5BE712:java.util.concurrent.CompletableFuture@5dc7391e[Not completed]
scm1.org_1   | 2023-06-27 12:22:57,032 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12: new RaftServerImpl for group-589B8C5BE712:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-27 12:22:57,034 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-27 12:22:57,035 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-27 12:22:57,035 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-27 12:22:57,035 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-27 12:24:00,786 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm3.org_1   | 2023-06-27 12:24:02,591 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-27 12:24:02,644 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 12:24:02,650 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2023-06-27 12:24:02,686 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
scm3.org_1   | 2023-06-27 12:24:02,733 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-27 12:24:02,755 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 12:24:02,757 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2023-06-27 12:24:02,824 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
scm3.org_1   | 2023-06-27 12:24:02,955 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-27 12:24:02,971 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 12:24:02,971 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2023-06-27 12:24:02,987 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
scm3.org_1   | 2023-06-27 12:24:03,425 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3.org_1   | 2023-06-27 12:24:03,426 [main] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        false
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                                                Value
scm3.org_1   | Threshold                                          10
scm3.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3.org_1   | Max Size to Move per Iteration                     500GB
scm3.org_1   | Max Size Entering Target per Iteration             26GB
scm3.org_1   | Max Size Leaving Source per Iteration              26GB
scm3.org_1   | 
scm3.org_1   | 2023-06-27 12:24:03,426 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2023-06-27 12:24:03,435 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-27 12:24:03,443 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2023-06-27 12:24:03,460 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712 does not exist. Creating ...
scm3.org_1   | 2023-06-27 12:24:03,491 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/in_use.lock acquired by nodename 7@scm3.org
scm3.org_1   | 2023-06-27 12:24:03,528 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712 has been successfully formatted.
scm3.org_1   | 2023-06-27 12:24:03,548 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2023-06-27 12:24:03,608 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2023-06-27 12:24:03,608 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 12:24:03,622 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3.org_1   | 2023-06-27 12:24:03,628 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3.org_1   | 2023-06-27 12:24:03,650 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-27 12:24:03,664 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2023-06-27 12:24:03,665 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2023-06-27 12:24:03,665 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 12:24:03,677 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712
scm3.org_1   | 2023-06-27 12:24:03,678 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2023-06-27 12:24:03,682 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2023-06-27 12:24:03,692 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-27 12:24:03,699 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3.org_1   | 2023-06-27 12:24:03,700 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2023-06-27 12:24:03,706 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2023-06-27 12:24:03,706 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-27 12:22:57,036 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 12:22:57,036 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-27 12:22:57,044 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-27 12:22:57,045 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 12:22:57,048 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-06-27 12:22:57,049 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-27 12:22:57,061 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-27 12:22:57,065 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-27 12:22:57,100 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-27 12:22:57,100 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-27 12:22:57,134 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-27 12:22:57,306 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 12:22:57,311 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 12:22:57,313 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:57,316 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-27 12:22:57,316 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-27 12:22:57,316 [f163470c-d3e0-4891-94bf-0bc988a6aa12-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-06-27 12:22:57,321 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2023-06-27 12:22:57,321 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2023-06-27 12:22:57,322 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2023-06-27 12:22:57,357 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm1.org_1   | 2023-06-27 12:22:57,412 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm1.org_1   | 2023-06-27 12:22:57,413 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2023-06-27 12:22:57,422 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2023-06-27 12:22:57,424 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2023-06-27 12:22:57,555 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2023-06-27 12:22:57,601 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm1.org_1   | 2023-06-27 12:22:57,604 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-27 12:22:57,615 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1.org_1   | 2023-06-27 12:22:57,638 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2023-06-27 12:22:57,639 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-27 12:22:57,647 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2023-06-27 12:22:57,647 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 12:22:57,650 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1.org_1   | 2023-06-27 12:22:57,651 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1.org_1   | 2023-06-27 12:22:57,659 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1.org_1   | 2023-06-27 12:22:57,660 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1.org_1   | 2023-06-27 12:22:57,687 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-27 12:22:57,687 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-27 12:22:57,712 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2023-06-27 12:22:57,804 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2023-06-27 12:22:57,837 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2023-06-27 12:22:57,844 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1.org_1   | 2023-06-27 12:22:57,857 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2023-06-27 12:22:57,861 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:22:57,865 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-27 12:22:58,122 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm2.org_1   | Max Size Entering Target per Iteration             26GB
scm2.org_1   | Max Size Leaving Source per Iteration              26GB
scm2.org_1   | 
recon_1      | 2023-06-27 12:26:06,453 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 200 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 200.
recon_1      | 2023-06-27 12:26:08,454 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 201 failover attempts. Trying to failover immediately. Current retry count: 201.
recon_1      | 2023-06-27 12:26:08,455 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 202 failover attempts. Trying to failover immediately. Current retry count: 202.
recon_1      | 2023-06-27 12:26:08,456 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 203 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 203.
recon_1      | 2023-06-27 12:26:10,457 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 204 failover attempts. Trying to failover immediately. Current retry count: 204.
recon_1      | 2023-06-27 12:26:10,458 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 205 failover attempts. Trying to failover immediately. Current retry count: 205.
recon_1      | 2023-06-27 12:26:10,459 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 206 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 206.
recon_1      | 2023-06-27 12:26:12,461 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 207 failover attempts. Trying to failover immediately. Current retry count: 207.
recon_1      | 2023-06-27 12:26:12,462 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 208 failover attempts. Trying to failover immediately. Current retry count: 208.
recon_1      | 2023-06-27 12:26:12,463 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 209 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 209.
recon_1      | 2023-06-27 12:26:14,464 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 210 failover attempts. Trying to failover immediately. Current retry count: 210.
recon_1      | 2023-06-27 12:26:14,465 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 211 failover attempts. Trying to failover immediately. Current retry count: 211.
recon_1      | 2023-06-27 12:26:14,465 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 212 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 212.
recon_1      | 2023-06-27 12:26:16,467 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 213 failover attempts. Trying to failover immediately. Current retry count: 213.
recon_1      | 2023-06-27 12:26:16,613 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52322
recon_1      | 2023-06-27 12:26:16,837 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:26:17,258 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44662
recon_1      | 2023-06-27 12:26:17,363 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:26:18,913 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:55156
recon_1      | 2023-06-27 12:26:19,050 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:26:19,497 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=6c75f348-15d4-409f-ae39-b1f9e205c4e3. Trying to get from SCM.
recon_1      | 2023-06-27 12:26:20,132 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 6c75f348-15d4-409f-ae39-b1f9e205c4e3, Nodes: cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:cc01b188-4e0b-44d2-a762-70392abfc581, CreationTimestamp2023-06-27T12:25:46.560Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 12:26:20,801 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=6edac7e5-f324-41f8-8d35-b13f88867b8f. Trying to get from SCM.
recon_1      | 2023-06-27 12:26:20,833 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 6edac7e5-f324-41f8-8d35-b13f88867b8f, Nodes: 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:1a7440b0-385d-4580-b33d-c9708c19e79c, CreationTimestamp2023-06-27T12:25:45.392Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 12:26:20,885 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=3108f661-f070-45b5-be9d-3cb73f243315. Trying to get from SCM.
recon_1      | 2023-06-27 12:26:20,949 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 3108f661-f070-45b5-be9d-3cb73f243315, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T12:25:47.543Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 12:26:21,002 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-27 12:26:22,053 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea. Trying to get from SCM.
recon_1      | 2023-06-27 12:26:22,220 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:a650530b-22bc-4ab5-9901-0a035f1b699c, CreationTimestamp2023-06-27T12:25:47.053Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 12:26:22,745 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 12:26:23,225 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-27 12:26:25,743 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-27 12:26:27,097 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 12:26:28,344 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-27 12:26:36,604 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=aeeab16c-188d-4045-bf49-561639362392. Trying to get from SCM.
recon_1      | 2023-06-27 12:26:36,668 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:38434
recon_1      | 2023-06-27 12:26:36,950 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: aeeab16c-188d-4045-bf49-561639362392, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T12:25:47.806Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 12:26:36,952 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=aeeab16c-188d-4045-bf49-561639362392 reported by a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-27 12:26:36,953 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-27 12:26:37,032 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:26:37,035 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=aeeab16c-188d-4045-bf49-561639362392 reported by cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-27 12:26:37,036 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-27 12:26:38,860 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54298
recon_1      | 2023-06-27 12:26:39,128 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 12:26:39,153 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=aeeab16c-188d-4045-bf49-561639362392 reported by 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
scm3.org_1   | 2023-06-27 12:24:03,713 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2023-06-27 12:24:03,804 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2023-06-27 12:24:03,804 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 12:24:03,905 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3.org_1   | 2023-06-27 12:24:03,911 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3.org_1   | 2023-06-27 12:24:03,913 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3.org_1   | 2023-06-27 12:24:03,947 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-27 12:24:03,947 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-27 12:24:03,962 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm3.org_1   | 2023-06-27 12:24:03,963 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3.org_1   | 2023-06-27 12:24:03,968 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-589B8C5BE712,id=2cf3494c-faba-4bd8-bf38-71b782f966a2
scm3.org_1   | 2023-06-27 12:24:03,976 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2023-06-27 12:24:03,979 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3.org_1   | 2023-06-27 12:24:03,980 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2023-06-27 12:24:03,981 [2cf3494c-faba-4bd8-bf38-71b782f966a2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2023-06-27 12:24:03,999 [main] INFO server.RaftServer: 2cf3494c-faba-4bd8-bf38-71b782f966a2: start RPC server
scm3.org_1   | 2023-06-27 12:24:04,150 [main] INFO server.GrpcService: 2cf3494c-faba-4bd8-bf38-71b782f966a2: GrpcService started, listening on 9894
scm3.org_1   | 2023-06-27 12:24:04,163 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-2cf3494c-faba-4bd8-bf38-71b782f966a2: Started
scm3.org_1   | 2023-06-27 12:24:04,199 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-27 12:24:09,702 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: receive installSnapshot: f163470c-d3e0-4891-94bf-0bc988a6aa12->2cf3494c-faba-4bd8-bf38-71b782f966a2#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-27 12:24:09,875 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2023-06-27 12:24:09,885 [grpc-default-executor-0] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: change Leader from null to f163470c-d3e0-4891-94bf-0bc988a6aa12 at term 2 for installSnapshot, leader elected after 11773ms
scm3.org_1   | 2023-06-27 12:24:09,957 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: Received notification to install snapshot at index 0
scm3.org_1   | 2023-06-27 12:24:09,978 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3.org_1   | 2023-06-27 12:24:11,991 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set new configuration index: 11
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "7480df95-74c6-4260-b2a9-e3614d893449"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "f163470c-d3e0-4891-94bf-0bc988a6aa12"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2023-06-27 12:24:12,023 [grpc-default-executor-0] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set configuration 11: peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 12:24:12,116 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: reply installSnapshot: f163470c-d3e0-4891-94bf-0bc988a6aa12<-2cf3494c-faba-4bd8-bf38-71b782f966a2#0:OK-t0,ALREADY_INSTALLED
scm3.org_1   | 2023-06-27 12:24:12,325 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 2cf3494c-faba-4bd8-bf38-71b782f966a2: Completed INSTALL_SNAPSHOT, lastRequest: f163470c-d3e0-4891-94bf-0bc988a6aa12->2cf3494c-faba-4bd8-bf38-71b782f966a2#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-27 12:24:12,332 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 2cf3494c-faba-4bd8-bf38-71b782f966a2: Completed INSTALL_SNAPSHOT, lastReply: null
scm3.org_1   | 2023-06-27 12:24:12,758 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO impl.RoleInfo: 2cf3494c-faba-4bd8-bf38-71b782f966a2: start 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState
scm3.org_1   | 2023-06-27 12:24:12,806 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-06-27 12:24:12,818 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:12,837 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:12,840 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: inconsistency entries. Reply:f163470c-d3e0-4891-94bf-0bc988a6aa12<-2cf3494c-faba-4bd8-bf38-71b782f966a2#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-27 12:24:12,946 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set configuration 0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 12:24:12,947 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set configuration 1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 12:24:12,952 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set configuration 9: peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 12:24:12,957 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set configuration 11: peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 12:24:13,131 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO segmented.SegmentedRaftLogWorker: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker: Starting segment from index:0
scm3.org_1   | 2023-06-27 12:24:13,486 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO segmented.SegmentedRaftLogWorker: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2023-06-27 12:23:31,786 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2023-06-27 12:23:31,803 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-27 12:23:31,818 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2023-06-27 12:23:31,824 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712 does not exist. Creating ...
scm2.org_1   | 2023-06-27 12:23:31,872 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/in_use.lock acquired by nodename 7@scm2.org
scm2.org_1   | 2023-06-27 12:23:31,899 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712 has been successfully formatted.
scm2.org_1   | 2023-06-27 12:23:31,911 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2023-06-27 12:23:31,940 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2023-06-27 12:23:31,942 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 12:23:31,948 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2.org_1   | 2023-06-27 12:23:31,951 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2.org_1   | 2023-06-27 12:23:31,958 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-27 12:23:31,996 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2023-06-27 12:23:32,017 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2023-06-27 12:23:32,018 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 12:23:32,069 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712
scm2.org_1   | 2023-06-27 12:23:32,071 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2023-06-27 12:23:32,071 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2023-06-27 12:23:32,075 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-27 12:23:32,086 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2.org_1   | 2023-06-27 12:23:32,092 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2023-06-27 12:23:32,097 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2023-06-27 12:23:32,098 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2023-06-27 12:23:32,101 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2023-06-27 12:23:32,178 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2023-06-27 12:23:32,178 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 12:23:32,255 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2.org_1   | 2023-06-27 12:23:32,255 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2.org_1   | 2023-06-27 12:23:32,255 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-06-27 12:23:32,265 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-27 12:23:32,265 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-27 12:23:32,267 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2.org_1   | 2023-06-27 12:23:32,274 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: changes role from      null to FOLLOWER at term 0 for startInitializing
scm2.org_1   | 2023-06-27 12:23:32,282 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-589B8C5BE712,id=7480df95-74c6-4260-b2a9-e3614d893449
scm2.org_1   | 2023-06-27 12:23:32,291 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2.org_1   | 2023-06-27 12:23:32,296 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2023-06-27 12:23:32,296 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2023-06-27 12:23:32,296 [7480df95-74c6-4260-b2a9-e3614d893449-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2023-06-27 12:23:32,325 [main] INFO server.RaftServer: 7480df95-74c6-4260-b2a9-e3614d893449: start RPC server
scm2.org_1   | 2023-06-27 12:23:32,591 [main] INFO server.GrpcService: 7480df95-74c6-4260-b2a9-e3614d893449: GrpcService started, listening on 9894
scm2.org_1   | 2023-06-27 12:23:32,618 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-7480df95-74c6-4260-b2a9-e3614d893449: Started
scm2.org_1   | 2023-06-27 12:23:32,678 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-27 12:23:34,933 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: receive installSnapshot: f163470c-d3e0-4891-94bf-0bc988a6aa12->7480df95-74c6-4260-b2a9-e3614d893449#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-27 12:23:34,969 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2023-06-27 12:23:34,970 [grpc-default-executor-0] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: change Leader from null to f163470c-d3e0-4891-94bf-0bc988a6aa12 at term 2 for installSnapshot, leader elected after 10045ms
scm2.org_1   | 2023-06-27 12:23:34,986 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: Received notification to install snapshot at index 0
scm2.org_1   | 2023-06-27 12:23:34,995 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2.org_1   | 2023-06-27 12:23:35,669 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "f163470c-d3e0-4891-94bf-0bc988a6aa12"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |     startupRole: FOLLOWER
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2023-06-27 12:23:35,678 [grpc-default-executor-0] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set configuration 1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 12:23:35,699 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: reply installSnapshot: f163470c-d3e0-4891-94bf-0bc988a6aa12<-7480df95-74c6-4260-b2a9-e3614d893449#0:OK-t0,ALREADY_INSTALLED
scm2.org_1   | 2023-06-27 12:23:35,766 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 7480df95-74c6-4260-b2a9-e3614d893449: Completed INSTALL_SNAPSHOT, lastRequest: f163470c-d3e0-4891-94bf-0bc988a6aa12->7480df95-74c6-4260-b2a9-e3614d893449#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-27 12:23:35,767 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 7480df95-74c6-4260-b2a9-e3614d893449: Completed INSTALL_SNAPSHOT, lastReply: null
scm2.org_1   | 2023-06-27 12:23:35,942 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread1] INFO impl.RoleInfo: 7480df95-74c6-4260-b2a9-e3614d893449: start 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState
scm2.org_1   | 2023-06-27 12:23:35,946 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread1] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-06-27 12:23:35,953 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:23:35,954 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:23:35,956 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread1] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: inconsistency entries. Reply:f163470c-d3e0-4891-94bf-0bc988a6aa12<-7480df95-74c6-4260-b2a9-e3614d893449#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-06-27 12:23:36,221 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread1] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set configuration 0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 12:23:36,223 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread1] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set configuration 1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 12:23:36,266 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread1] INFO segmented.SegmentedRaftLogWorker: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2023-06-27 12:23:36,467 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread1] INFO segmented.SegmentedRaftLogWorker: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2023-06-27 12:23:36,556 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread3] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set configuration 9: peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-06-27 12:23:36,830 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_0
scm2.org_1   | 2023-06-27 12:23:36,852 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_0 to /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_0-0
scm2.org_1   | 2023-06-27 12:23:36,888 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_1
scm2.org_1   | 2023-06-27 12:23:36,918 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:23:36,920 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2023-06-27 12:23:36,922 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-27 12:23:36,923 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2023-06-27 12:23:36,944 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 12:23:36,946 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2023-06-27 12:23:37,044 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread4] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set configuration 11: peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 12:23:37,387 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-589B8C5BE712:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2.org_1   | 2023-06-27 12:23:37,397 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2023-06-27 12:23:37,421 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm2.org_1   | 2023-06-27 12:23:37,445 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm2.org_1   | 2023-06-27 12:23:37,446 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2.org_1   | 2023-06-27 12:23:37,470 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)]
scm2.org_1   | 2023-06-27 12:23:37,476 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)
scm2.org_1   | 2023-06-27 12:23:37,881 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2023-06-27 12:23:37,909 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2023-06-27 12:23:37,909 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2023-06-27 12:23:38,087 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)] to file /data/metadata/scm/keys/secret_keys.json
scm1.org_1   | 2023-06-27 12:22:58,122 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm1.org_1   | 2023-06-27 12:22:58,151 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-27 12:22:58,160 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-27 12:22:58,162 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 266035175110 on primary SCM
scm1.org_1   | 2023-06-27 12:22:58,177 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2023-06-27 12:22:58,202 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-27 12:22:58,248 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2023-06-27 12:22:58,286 [Listener at 0.0.0.0/9961] INFO hdds.HddsUtils: Restoring thread name: main
scm1.org_1   | 2023-06-27 12:22:58,335 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm1.org_1   | 2023-06-27 12:22:58,336 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm1.org_1   | 2023-06-27 12:22:59,139 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-27 12:22:59,153 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-27 12:22:59,160 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2023-06-27 12:22:59,169 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
scm1.org_1   | 2023-06-27 12:22:59,222 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-27 12:22:59,229 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-27 12:22:59,229 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2023-06-27 12:22:59,245 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
scm1.org_1   | 2023-06-27 12:22:59,321 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-27 12:22:59,350 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-27 12:22:59,351 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2023-06-27 12:22:59,357 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
scm1.org_1   | 2023-06-27 12:22:59,479 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1.org_1   | 2023-06-27 12:22:59,480 [main] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        false
scm1.org_1   | Container Balancer Configuration values:
scm1.org_1   | Key                                                Value
scm1.org_1   | Threshold                                          10
scm1.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1.org_1   | Max Size to Move per Iteration                     500GB
scm1.org_1   | Max Size Entering Target per Iteration             26GB
scm1.org_1   | Max Size Leaving Source per Iteration              26GB
scm1.org_1   | 
scm1.org_1   | 2023-06-27 12:22:59,481 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2023-06-27 12:22:59,484 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-27 12:22:59,487 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2023-06-27 12:22:59,506 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/in_use.lock acquired by nodename 7@scm1.org
scm1.org_1   | 2023-06-27 12:22:59,516 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=f163470c-d3e0-4891-94bf-0bc988a6aa12} from /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/raft-meta
scm1.org_1   | 2023-06-27 12:22:59,616 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:22:59,619 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-27 12:22:59,646 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-27 12:22:59,646 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:59,657 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-27 12:22:59,663 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-27 12:22:59,666 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 12:22:59,685 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-27 12:22:59,685 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-27 12:22:59,686 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:59,705 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712
scm1.org_1   | 2023-06-27 12:22:59,706 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 12:22:59,706 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 12:22:59,707 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 12:22:59,707 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-27 12:22:59,708 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-27 12:22:59,713 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-27 12:22:59,714 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-27 12:22:59,714 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-27 12:22:59,742 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-27 12:22:59,743 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:22:59,765 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 12:22:59,765 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 12:22:59,765 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1      | 2023-06-27 12:26:39,154 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 12:26:39,343 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=aeeab16c-188d-4045-bf49-561639362392 reported by 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 12:26:39,343 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 reported by 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 12:26:39,345 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3108f661-f070-45b5-be9d-3cb73f243315, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1a7440b0-385d-4580-b33d-c9708c19e79c, CreationTimestamp2023-06-27T12:25:47.543Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-06-27 12:23:38,109 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:24:13,870 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread1] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set configuration 15: peers:[2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 12:24:13,936 [2cf3494c-faba-4bd8-bf38-71b782f966a2-server-thread3] INFO server.RaftServer$Division: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712: set configuration 17: peers:[2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 12:24:15,017 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_0
scm3.org_1   | 2023-06-27 12:24:15,172 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_0 to /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_0-0
scm3.org_1   | 2023-06-27 12:24:15,189 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-589B8C5BE712:[2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3.org_1   | 2023-06-27 12:24:15,200 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2023-06-27 12:24:15,234 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm3.org_1   | 2023-06-27 12:24:15,282 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm3.org_1   | 2023-06-27 12:24:15,283 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm3.org_1   | 2023-06-27 12:24:15,807 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_1
scm3.org_1   | 2023-06-27 12:24:16,042 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:24:16,054 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2023-06-27 12:24:16,083 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-06-27 12:24:16,083 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2023-06-27 12:24:17,640 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2023-06-27 12:24:17,808 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2023-06-27 12:24:17,808 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2023-06-27 12:24:18,013 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:18,016 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:18,370 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2023-06-27 12:24:18,388 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 12:24:19,203 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-27 12:24:19,484 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 12:24:19,496 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2023-06-27 12:24:19,958 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-27 12:24:20,009 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-27 12:24:20,021 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 12:24:20,031 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2023-06-27 12:24:20,683 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)]
scm3.org_1   | 2023-06-27 12:24:20,725 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)
scm3.org_1   | 2023-06-27 12:24:21,605 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2023-06-27 12:24:21,647 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 12:24:21,751 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2023-06-27 12:24:21,756 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2023-06-27 12:24:21,878 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)] to file /data/metadata/scm/keys/secret_keys.json
scm3.org_1   | 2023-06-27 12:24:22,246 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:22:59,792 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:22:59,793 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_0
scm1.org_1   | 2023-06-27 12:22:59,805 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-27 12:22:59,879 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: start as a follower, conf=0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:22:59,879 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2023-06-27 12:22:59,880 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: start f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState
scm1.org_1   | 2023-06-27 12:22:59,889 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-589B8C5BE712,id=f163470c-d3e0-4891-94bf-0bc988a6aa12
scm1.org_1   | 2023-06-27 12:22:59,889 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-27 12:22:59,891 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-27 12:22:59,895 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-27 12:22:59,895 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-27 12:22:59,896 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-27 12:22:59,897 [f163470c-d3e0-4891-94bf-0bc988a6aa12-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-27 12:22:59,902 [main] INFO server.RaftServer: f163470c-d3e0-4891-94bf-0bc988a6aa12: start RPC server
scm1.org_1   | 2023-06-27 12:22:59,973 [main] INFO server.GrpcService: f163470c-d3e0-4891-94bf-0bc988a6aa12: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-27 12:22:59,986 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f163470c-d3e0-4891-94bf-0bc988a6aa12: Started
scm1.org_1   | 2023-06-27 12:22:59,988 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-27 12:22:59,988 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2023-06-27 12:22:59,997 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm1.org_1   | 2023-06-27 12:22:59,999 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm1.org_1   | 2023-06-27 12:22:59,999 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm1.org_1   | 2023-06-27 12:23:00,090 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2023-06-27 12:23:00,123 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2023-06-27 12:23:00,123 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2023-06-27 12:23:00,266 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-27 12:23:00,266 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 12:23:00,270 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2023-06-27 12:23:00,362 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-27 12:23:00,362 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-27 12:23:00,367 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2023-06-27 12:23:00,367 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 12:23:00,404 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2023-06-27 12:23:00,417 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 12:23:00,418 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm1.org_1   | 2023-06-27 12:23:00,418 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm1.org_1   | 2023-06-27 12:23:00,504 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm1.org_1   | 2023-06-27 12:23:00,542 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1.org_1   | 2023-06-27 12:23:00,542 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm1.org_1   | 2023-06-27 12:23:00,544 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm1.org_1   | 2023-06-27 12:23:00,576 [main] INFO util.log: Logging initialized @8323ms to org.eclipse.jetty.util.log.Slf4jLog
scm1.org_1   | 2023-06-27 12:23:00,722 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1.org_1   | 2023-06-27 12:23:00,735 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1.org_1   | 2023-06-27 12:23:00,737 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm1.org_1   | 2023-06-27 12:23:00,737 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2023-06-27 12:23:38,112 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-27 12:23:38,119 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2023-06-27 12:23:38,219 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-27 12:23:38,249 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2023-06-27 12:23:38,273 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 12:23:38,466 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-27 12:23:38,479 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-27 12:23:38,486 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 12:23:38,486 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2023-06-27 12:23:38,892 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:23:38,905 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2023-06-27 12:23:38,914 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 12:23:38,914 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2023-06-27 12:23:38,923 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm2.org_1   | 2023-06-27 12:23:38,943 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:23:39,758 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 7480df95-74c6-4260-b2a9-e3614d893449
scm2.org_1   | 2023-06-27 12:23:39,778 [main] INFO server.StorageContainerManager: Persist certificate serialId 266035175110 on Scm Bootstrap Node 7480df95-74c6-4260-b2a9-e3614d893449
scm2.org_1   | 2023-06-27 12:23:39,803 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm2.org_1   | 2023-06-27 12:23:39,870 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2023-06-27 12:23:39,871 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2023-06-27 12:23:39,876 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2023-06-27 12:23:40,053 [main] INFO util.log: Logging initialized @28824ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2023-06-27 12:23:40,741 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2023-06-27 12:23:40,762 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2023-06-27 12:23:40,766 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2023-06-27 12:24:22,248 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-27 12:24:22,248 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2023-06-27 12:24:22,885 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:24:23,074 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:23,079 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:23,147 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:24:23,247 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:24:24,104 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 2cf3494c-faba-4bd8-bf38-71b782f966a2
scm3.org_1   | 2023-06-27 12:24:24,198 [main] INFO server.StorageContainerManager: Persist certificate serialId 266035175110 on Scm Bootstrap Node 2cf3494c-faba-4bd8-bf38-71b782f966a2
scm3.org_1   | 2023-06-27 12:24:24,324 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm3.org_1   | 2023-06-27 12:24:24,549 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2023-06-27 12:24:24,550 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2023-06-27 12:24:24,848 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2023-06-27 12:24:25,181 [main] INFO util.log: Logging initialized @36363ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2023-06-27 12:24:27,716 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2023-06-27 12:24:27,855 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2023-06-27 12:24:27,868 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2023-06-27 12:24:27,879 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2023-06-27 12:24:27,881 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2023-06-27 12:24:27,938 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2023-06-27 12:24:28,123 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:28,124 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:28,479 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm3.org_1   | 2023-06-27 12:24:28,498 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2023-06-27 12:24:28,516 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm3.org_1   | 2023-06-27 12:24:28,985 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2023-06-27 12:24:28,993 [main] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2023-06-27 12:24:29,019 [main] INFO server.session: node0 Scavenging every 600000ms
scm3.org_1   | 2023-06-27 12:24:29,282 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-06-27 12:24:29,337 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@67954442{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2023-06-27 12:24:29,355 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c43d4dd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2023-06-27 12:24:30,873 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-06-27 12:24:31,111 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4668f98c{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-17594848253284854500/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2023-06-27 12:24:31,309 [main] INFO server.AbstractConnector: Started ServerConnector@7e22f3da{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2023-06-27 12:24:31,312 [main] INFO server.Server: Started @42494ms
scm3.org_1   | 2023-06-27 12:24:31,325 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2023-06-27 12:24:31,329 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2023-06-27 12:24:31,335 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2023-06-27 12:24:33,308 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:33,309 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:38,509 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:38,519 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:43,663 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:43,664 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:48,780 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:48,780 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:53,831 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:53,831 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:24:56,408 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:24:57,665 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:24:58,906 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:24:58,906 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:00,328 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:01,341 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:02,875 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:04,051 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:04,053 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:06,708 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:09,237 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:09,237 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:14,280 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:14,280 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:19,303 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:19,304 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:24,481 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:24,481 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:29,553 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:29,553 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-27 12:23:00,738 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm1.org_1   | 2023-06-27 12:23:00,741 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | 2023-06-27 12:23:00,787 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1.org_1   | 2023-06-27 12:23:00,788 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2023-06-27 12:23:00,789 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm2.org_1   | 2023-06-27 12:23:40,766 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2023-06-27 12:23:40,767 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2023-06-27 12:23:40,773 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm2.org_1   | 2023-06-27 12:23:40,871 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm2.org_1   | 2023-06-27 12:23:40,873 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm2.org_1   | 2023-06-27 12:23:40,876 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm2.org_1   | 2023-06-27 12:23:41,020 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm2.org_1   | 2023-06-27 12:23:41,021 [main] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2023-06-27 12:23:41,027 [main] INFO server.session: node0 Scavenging every 660000ms
scm2.org_1   | 2023-06-27 12:23:41,074 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-27 12:23:41,081 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4d682552{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2023-06-27 12:23:41,083 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5710fca8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2.org_1   | 2023-06-27 12:23:41,110 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:23:41,111 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:23:41,537 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-27 12:23:41,554 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5fddd0fc{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-5477317053237910433/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2023-06-27 12:23:41,602 [main] INFO server.AbstractConnector: Started ServerConnector@fd6784a{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2023-06-27 12:23:41,604 [main] INFO server.Server: Started @30382ms
scm2.org_1   | 2023-06-27 12:23:41,609 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2023-06-27 12:23:41,611 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2.org_1   | 2023-06-27 12:23:41,615 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2023-06-27 12:23:46,138 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:23:46,140 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:23:47,786 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:23:51,238 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:23:51,238 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:23:56,308 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:23:56,309 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:01,368 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:01,368 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:06,411 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:06,411 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:11,586 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:11,586 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:13,845 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread4] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set configuration 15: peers:[2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-06-27 12:24:13,964 [7480df95-74c6-4260-b2a9-e3614d893449-server-thread4] INFO server.RaftServer$Division: 7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712: set configuration 17: peers:[2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 12:24:16,658 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:16,660 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:21,848 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:21,848 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:26,914 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:26,914 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:32,030 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:32,030 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:37,188 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:37,188 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:42,329 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:42,330 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:47,360 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:47,361 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-27 12:23:00,822 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2023-06-27 12:23:00,822 [main] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2023-06-27 12:23:00,824 [main] INFO server.session: node0 Scavenging every 600000ms
scm1.org_1   | 2023-06-27 12:23:00,848 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-27 12:23:00,862 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@60df0131{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2023-06-27 12:23:00,863 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6559f04e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2023-06-27 12:23:00,987 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-27 12:23:00,998 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@73e3831e{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-6180816769674113772/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2023-06-27 12:23:01,009 [main] INFO server.AbstractConnector: Started ServerConnector@45a33515{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2023-06-27 12:23:01,009 [main] INFO server.Server: Started @8757ms
scm1.org_1   | 2023-06-27 12:23:01,011 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2023-06-27 12:23:01,012 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2023-06-27 12:23:01,014 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2023-06-27 12:23:01,643 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:54244
scm1.org_1   | 2023-06-27 12:23:01,687 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:23:02,237 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:45537
scm1.org_1   | 2023-06-27 12:23:02,232 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:35890
scm1.org_1   | 2023-06-27 12:23:02,282 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:23:02,295 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#12 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from 172.25.0.115:45537
scm1.org_1   | org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:f163470c-d3e0-4891-94bf-0bc988a6aa12 is not the leader. Could not determine the leader node.
scm1.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1.org_1   | 2023-06-27 12:23:02,297 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:23:05,089 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO impl.FollowerState: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5208709280ns, electionTimeout:5196ms
scm1.org_1   | 2023-06-27 12:23:05,090 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: shutdown f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState
scm1.org_1   | 2023-06-27 12:23:05,091 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2023-06-27 12:23:05,095 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-27 12:23:05,095 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-FollowerState] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: start f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1
scm1.org_1   | 2023-06-27 12:23:05,101 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:23:05,102 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-27 12:23:05,119 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:23:05,119 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.LeaderElection: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1.org_1   | 2023-06-27 12:23:05,119 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: shutdown f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1
scm1.org_1   | 2023-06-27 12:23:05,120 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1.org_1   | 2023-06-27 12:23:05,120 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1.org_1   | 2023-06-27 12:23:05,121 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm3.org_1   | 2023-06-27 12:25:34,657 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:34,657 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:39,699 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:39,700 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:39,783 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:56760
scm3.org_1   | 2023-06-27 12:25:39,820 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:25:40,936 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53274
scm3.org_1   | 2023-06-27 12:25:41,187 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:25:41,227 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-2cf3494c-faba-4bd8-bf38-71b782f966a2: Detected pause in JVM or host machine approximately 0.123s without any GCs.
scm3.org_1   | 2023-06-27 12:25:41,253 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41900
scm3.org_1   | 2023-06-27 12:25:41,498 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:25:44,821 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:52,502 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:52,502 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:56,467 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:24:57,528 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:24:57,528 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:24:57,741 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:00,297 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:01,376 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:02,567 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:02,568 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:02,855 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:06,676 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:07,601 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:07,602 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:12,669 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:12,670 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:17,786 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:17,786 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:22,834 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:22,835 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:27,888 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:27,888 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:32,904 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:32,905 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:37,928 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:37,928 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:39,893 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49114
scm2.org_1   | 2023-06-27 12:25:39,944 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:25:41,025 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59690
scm2.org_1   | 2023-06-27 12:25:41,231 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:25:41,305 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:47776
scm2.org_1   | 2023-06-27 12:25:41,502 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:25:43,031 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:43,032 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:45,508 [IPC Server handler 58 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1a7440b0-385d-4580-b33d-c9708c19e79c
scm2.org_1   | 2023-06-27 12:25:45,519 [IPC Server handler 92 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cc01b188-4e0b-44d2-a762-70392abfc581
scm2.org_1   | 2023-06-27 12:25:45,568 [IPC Server handler 58 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1a7440b0-385d-4580-b33d-c9708c19e79c{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 399146862779, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-27 12:25:45,578 [IPC Server handler 92 on default port 9861] INFO node.SCMNodeManager: Registered Data node : cc01b188-4e0b-44d2-a762-70392abfc581{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 401704538238, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-27 12:25:45,665 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-27 12:25:45,733 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-27 12:25:45,732 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 12:25:45,781 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 12:25:46,962 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a650530b-22bc-4ab5-9901-0a035f1b699c
scm2.org_1   | 2023-06-27 12:25:46,962 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a650530b-22bc-4ab5-9901-0a035f1b699c{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 397752053077, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-27 12:25:47,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-27 12:25:47,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-27 12:25:47,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2.org_1   | 2023-06-27 12:25:47,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2.org_1   | 2023-06-27 12:25:47,455 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 12:25:47,753 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 12:25:47,773 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:47,998 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:48,069 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:48,125 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:48,131 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:48,133 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:44,821 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:45,741 [IPC Server handler 45 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1a7440b0-385d-4580-b33d-c9708c19e79c
scm3.org_1   | 2023-06-27 12:25:45,788 [IPC Server handler 65 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cc01b188-4e0b-44d2-a762-70392abfc581
scm3.org_1   | 2023-06-27 12:25:45,805 [IPC Server handler 65 on default port 9861] INFO node.SCMNodeManager: Registered Data node : cc01b188-4e0b-44d2-a762-70392abfc581{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 401704538238, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-27 12:25:45,829 [IPC Server handler 45 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1a7440b0-385d-4580-b33d-c9708c19e79c{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 399146862779, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-27 12:25:45,924 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-27 12:25:46,003 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-27 12:25:46,001 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 12:25:46,063 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 12:25:46,969 [IPC Server handler 27 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a650530b-22bc-4ab5-9901-0a035f1b699c
scm3.org_1   | 2023-06-27 12:25:47,123 [IPC Server handler 27 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a650530b-22bc-4ab5-9901-0a035f1b699c{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 397752053077, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-27 12:25:47,343 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-27 12:25:47,345 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-27 12:25:47,356 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2023-06-27 12:25:47,357 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2023-06-27 12:25:47,402 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 12:25:47,664 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 12:25:47,664 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:47,690 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:47,775 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:47,859 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:47,994 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:25:49,859 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:49,860 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:25:54,879 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:25:54,879 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:00,061 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:00,062 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:05,225 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:05,226 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:10,329 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:10,330 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:15,515 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:15,516 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:16,726 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56004
scm1.org_1   | 2023-06-27 12:23:05,125 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: change Leader from null to f163470c-d3e0-4891-94bf-0bc988a6aa12 at term 2 for becomeLeader, leader elected after 8059ms
scm1.org_1   | 2023-06-27 12:23:05,137 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-27 12:23:05,142 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 12:23:05,143 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 12:23:05,148 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-27 12:23:05,148 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-27 12:23:05,148 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-27 12:23:05,154 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 12:23:05,156 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-27 12:23:05,157 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO impl.RoleInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12: start f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl
scm1.org_1   | 2023-06-27 12:23:05,162 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1.org_1   | 2023-06-27 12:23:05,167 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_0 to /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_0-0
scm1.org_1   | 2023-06-27 12:23:05,187 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderElection1] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 1: peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:23:05,190 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5bf65396-4407-4e81-983e-589b8c5be712/current/log_inprogress_1
scm1.org_1   | 2023-06-27 12:23:05,198 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2023-06-27 12:23:05,199 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1.org_1   | 2023-06-27 12:23:05,200 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
scm1.org_1   | 2023-06-27 12:23:05,202 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201405Z, expire at: 2023-06-27T13:23:05.201405Z)
scm1.org_1   | 2023-06-27 12:23:05,212 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:23:05,213 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1.org_1   | 2023-06-27 12:23:05,214 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-27 12:23:05,214 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1.org_1   | 2023-06-27 12:23:05,224 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2023-06-27 12:23:05,269 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 12:23:05,400 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)]
scm1.org_1   | 2023-06-27 12:23:05,404 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)
scm1.org_1   | 2023-06-27 12:23:05,467 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 7fdd8910-b7c0-4e6a-91aa-5c90dbbb3274, creation at: 2023-06-27T12:23:05.201Z, expire at: 2023-06-27T13:23:05.201Z)] to file /data/metadata/scm/keys/secret_keys.json
scm1.org_1   | 2023-06-27 12:23:05,471 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:23:05,471 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-27 12:23:05,471 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2023-06-27 12:23:08,086 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:46832
scm1.org_1   | 2023-06-27 12:23:08,097 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:23:08,164 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: 7480df95-74c6-4260-b2a9-e3614d893449
scm1.org_1   | 2023-06-27 12:23:08,308 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 12:23:08,355 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: 02eb813a-3dfc-420b-9919-338f1f8f04eb
scm1.org_1   | 2023-06-27 12:23:08,411 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:23:10,107 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:23:31,412 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47032
scm1.org_1   | 2023-06-27 12:23:31,495 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:23:33,564 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:41757
scm1.org_1   | 2023-06-27 12:23:33,611 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:23:33,945 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:58392
scm1.org_1   | 2023-06-27 12:23:33,983 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:23:33,985 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: f163470c-d3e0-4891-94bf-0bc988a6aa12: Submitting SetConfiguration request to Ratis server with new SCM peers list: [f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-27 12:23:33,993 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: receive setConfiguration SetConfigurationRequest:client-1C252A47527F->f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 12:23:33,993 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-1C252A47527F->f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 12:23:34,028 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-27 12:23:34,032 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:23:34,032 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-27 12:23:34,045 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-27 12:23:34,050 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-27 12:23:34,067 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 12:23:34,067 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-27 12:23:34,067 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1.org_1   | 2023-06-27 12:23:34,068 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 12:23:34,068 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 12:23:34,089 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-06-27 12:23:34,140 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-GrpcLogAppender: send f163470c-d3e0-4891-94bf-0bc988a6aa12->7480df95-74c6-4260-b2a9-e3614d893449#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-27 12:23:34,145 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 7480df95-74c6-4260-b2a9-e3614d893449
scm1.org_1   | 2023-06-27 12:23:35,766 [grpc-default-executor-0] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-InstallSnapshotResponseHandler: received the first reply f163470c-d3e0-4891-94bf-0bc988a6aa12<-7480df95-74c6-4260-b2a9-e3614d893449#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 12:23:35,783 [grpc-default-executor-0] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-27 12:23:35,785 [grpc-default-executor-0] INFO leader.FollowerInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-27 12:23:35,785 [grpc-default-executor-0] INFO leader.FollowerInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-27 12:23:35,785 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449 acknowledged installing snapshot
scm1.org_1   | 2023-06-27 12:23:35,786 [grpc-default-executor-0] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 12:23:36,002 [grpc-default-executor-2] WARN server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 12:23:36,038 [grpc-default-executor-2] INFO leader.FollowerInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->7480df95-74c6-4260-b2a9-e3614d893449: setNextIndex nextIndex: updateUnconditionally 9 -> 0
scm1.org_1   | 2023-06-27 12:23:36,146 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 9: peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-27 12:23:36,956 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 11: peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:23:37,061 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 7480df95-74c6-4260-b2a9-e3614d893449.
scm1.org_1   | 2023-06-27 12:23:39,252 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:60602
scm1.org_1   | 2023-06-27 12:23:39,362 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:23:42,923 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:38124
scm1.org_1   | 2023-06-27 12:23:42,950 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:23:47,146 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:54688
scm1.org_1   | 2023-06-27 12:23:47,182 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:23:47,183 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: 2cf3494c-faba-4bd8-bf38-71b782f966a2
scm1.org_1   | 2023-06-27 12:23:47,350 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 12:23:47,777 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:23:50,615 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43170
scm1.org_1   | 2023-06-27 12:23:50,654 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:24:06,535 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:47106
scm1.org_1   | 2023-06-27 12:24:06,920 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:24:06,924 [IPC Server handler 1 on default port 9863] INFO ha.SCMRatisServerImpl: f163470c-d3e0-4891-94bf-0bc988a6aa12: Submitting SetConfiguration request to Ratis server with new SCM peers list: [7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-27 12:24:06,927 [IPC Server handler 1 on default port 9863] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: receive setConfiguration SetConfigurationRequest:client-1C252A47527F->f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 12:24:06,928 [IPC Server handler 1 on default port 9863] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-1C252A47527F->f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 12:24:06,938 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-27 12:24:06,938 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 12:24:06,939 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-27 12:24:06,948 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-27 12:24:06,950 [IPC Server handler 1 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-27 12:24:06,951 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 12:24:06,951 [IPC Server handler 1 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-27 12:24:06,952 [IPC Server handler 1 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1.org_1   | 2023-06-27 12:24:06,952 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 12:24:06,952 [IPC Server handler 1 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 12:24:06,998 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm3.org_1   | 2023-06-27 12:26:16,841 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:26:17,338 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49004
scm3.org_1   | 2023-06-27 12:26:17,392 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:26:18,971 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40924
scm3.org_1   | 2023-06-27 12:26:19,039 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:26:19,853 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:26:19,855 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:20,652 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:20,652 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:20,770 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:26:20,970 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:22,181 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:26:22,730 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:23,236 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:25,670 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:25,670 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:25,745 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:27,098 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:28,298 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:30,756 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:30,757 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:35,866 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:35,867 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 12:26:36,609 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:36,725 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:40036
scm3.org_1   | 2023-06-27 12:26:37,107 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:26:37,115 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:38,649 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:51542
scm3.org_1   | 2023-06-27 12:26:39,106 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 12:26:39,110 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:39,261 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 12:26:39,287 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 12:26:41,064 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 12:26:41,064 [2cf3494c-faba-4bd8-bf38-71b782f966a2@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:48,222 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:25:53,241 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:53,241 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:25:58,421 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:25:58,422 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:03,438 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:03,439 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:08,439 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:08,440 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:13,483 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:13,487 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:16,752 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52126
scm2.org_1   | 2023-06-27 12:26:16,900 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:26:17,242 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44684
scm2.org_1   | 2023-06-27 12:26:17,367 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:26:18,626 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:18,627 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:18,969 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:60288
scm2.org_1   | 2023-06-27 12:26:19,032 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:26:19,816 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:26:20,756 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:26:20,784 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:22,166 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 12:26:22,728 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:23,220 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:23,702 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:23,702 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:25,691 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:27,057 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:28,323 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:28,761 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:28,761 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:33,800 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:33,801 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:36,692 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:36,737 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:38992
scm2.org_1   | 2023-06-27 12:26:37,106 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:26:37,112 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:38,087 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49518
scm2.org_1   | 2023-06-27 12:26:38,804 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 12:26:38,810 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:38,878 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 12:26:38,879 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 12:26:39,249 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 12:26:39,282 [7480df95-74c6-4260-b2a9-e3614d893449@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:24:07,017 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-GrpcLogAppender: send f163470c-d3e0-4891-94bf-0bc988a6aa12->2cf3494c-faba-4bd8-bf38-71b782f966a2#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-27 12:24:07,025 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 2cf3494c-faba-4bd8-bf38-71b782f966a2
scm1.org_1   | 2023-06-27 12:24:12,348 [grpc-default-executor-2] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-InstallSnapshotResponseHandler: received the first reply f163470c-d3e0-4891-94bf-0bc988a6aa12<-2cf3494c-faba-4bd8-bf38-71b782f966a2#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 12:24:12,385 [grpc-default-executor-2] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-27 12:24:12,388 [grpc-default-executor-2] INFO leader.FollowerInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-27 12:24:12,389 [grpc-default-executor-2] INFO leader.FollowerInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-27 12:24:12,391 [grpc-default-executor-2] INFO leader.FollowerInfo: Follower f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2 acknowledged installing snapshot
scm1.org_1   | 2023-06-27 12:24:12,391 [grpc-default-executor-2] INFO server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 12:24:12,913 [grpc-default-executor-0] WARN server.GrpcLogAppender: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 12:24:12,925 [grpc-default-executor-0] INFO leader.FollowerInfo: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712->2cf3494c-faba-4bd8-bf38-71b782f966a2: setNextIndex nextIndex: updateUnconditionally 15 -> 0
scm1.org_1   | 2023-06-27 12:24:13,827 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 15: peers:[2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-27 12:24:13,906 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-LeaderStateImpl] INFO server.RaftServer$Division: f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712: set configuration 17: peers:[2cf3494c-faba-4bd8-bf38-71b782f966a2|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER, 7480df95-74c6-4260-b2a9-e3614d893449|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, f163470c-d3e0-4891-94bf-0bc988a6aa12|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 12:24:14,118 [IPC Server handler 1 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 2cf3494c-faba-4bd8-bf38-71b782f966a2.
scm1.org_1   | 2023-06-27 12:24:17,880 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43238
scm1.org_1   | 2023-06-27 12:24:18,120 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:24:23,403 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:42238
scm1.org_1   | 2023-06-27 12:24:23,475 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:24:34,717 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:34125
scm1.org_1   | 2023-06-27 12:24:34,761 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:24:44,681 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:59248
scm1.org_1   | 2023-06-27 12:24:44,843 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:24:47,339 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:57912
scm1.org_1   | 2023-06-27 12:24:47,507 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:24:49,490 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:55690
scm1.org_1   | 2023-06-27 12:24:49,727 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:24:55,747 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33040
scm1.org_1   | 2023-06-27 12:24:55,867 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:24:55,871 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 038f15546f77, UUID: a650530b-22bc-4ab5-9901-0a035f1b699c
scm1.org_1   | 2023-06-27 12:24:56,440 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:24:57,190 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40618
scm1.org_1   | 2023-06-27 12:24:57,322 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:24:57,335 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 5cbb6cac8588, UUID: 1a7440b0-385d-4580-b33d-c9708c19e79c
scm1.org_1   | 2023-06-27 12:24:57,677 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:24:59,041 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33052
scm1.org_1   | 2023-06-27 12:24:59,077 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-27 12:24:59,749 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59896
scm1.org_1   | 2023-06-27 12:24:59,864 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:24:59,866 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 1b92348e430f, UUID: cc01b188-4e0b-44d2-a762-70392abfc581
scm1.org_1   | 2023-06-27 12:25:00,161 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:47686
scm1.org_1   | 2023-06-27 12:25:00,195 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:25:00,269 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:00,556 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38110
scm1.org_1   | 2023-06-27 12:25:00,672 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: cf5a7ed5-dd09-427a-9544-48d982d86a42
scm1.org_1   | 2023-06-27 12:25:00,712 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-27 12:25:01,330 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:02,398 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:40946
scm1.org_1   | 2023-06-27 12:25:02,442 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:25:02,443 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: fe9eedc1-8aea-403c-b77d-67484ba486f2
scm1.org_1   | 2023-06-27 12:25:02,857 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:02,935 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58444
scm1.org_1   | 2023-06-27 12:25:02,979 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-27 12:25:06,397 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:38670
scm1.org_1   | 2023-06-27 12:25:06,438 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:25:06,438 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: 091eac35-0c01-47dc-8423-b78fec20b57b
scm1.org_1   | 2023-06-27 12:25:06,643 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:28,456 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36098
scm1.org_1   | 2023-06-27 12:25:28,546 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:51748
scm1.org_1   | 2023-06-27 12:25:28,559 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:25:28,697 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:25:29,226 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:47864
scm1.org_1   | 2023-06-27 12:25:29,285 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:25:36,700 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42746
scm1.org_1   | 2023-06-27 12:25:37,033 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:25:39,807 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49920
scm1.org_1   | 2023-06-27 12:25:39,891 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:25:40,547 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f163470c-d3e0-4891-94bf-0bc988a6aa12: Detected pause in JVM or host machine approximately 0.305s with 0.246s GC time.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=246ms
scm1.org_1   | 2023-06-27 12:25:40,949 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55190
scm1.org_1   | 2023-06-27 12:25:41,151 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:50922
scm1.org_1   | 2023-06-27 12:25:41,161 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:25:41,497 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:25:45,313 [IPC Server handler 53 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1a7440b0-385d-4580-b33d-c9708c19e79c
scm1.org_1   | 2023-06-27 12:25:45,340 [IPC Server handler 53 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1a7440b0-385d-4580-b33d-c9708c19e79c{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 399146862779, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-27 12:25:45,360 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 12:25:45,398 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6edac7e5-f324-41f8-8d35-b13f88867b8f to datanode:1a7440b0-385d-4580-b33d-c9708c19e79c
scm1.org_1   | 2023-06-27 12:25:45,595 [IPC Server handler 24 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cc01b188-4e0b-44d2-a762-70392abfc581
scm1.org_1   | 2023-06-27 12:25:45,633 [IPC Server handler 24 on default port 9861] INFO node.SCMNodeManager: Registered Data node : cc01b188-4e0b-44d2-a762-70392abfc581{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 401704538238, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-27 12:25:45,678 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 12:25:45,678 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-27 12:25:45,690 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-27 12:25:46,412 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:46,417 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: 6edac7e5-f324-41f8-8d35-b13f88867b8f, Nodes: 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T12:25:45.392987Z[UTC]].
scm1.org_1   | 2023-06-27 12:25:46,560 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6c75f348-15d4-409f-ae39-b1f9e205c4e3 to datanode:cc01b188-4e0b-44d2-a762-70392abfc581
scm1.org_1   | 2023-06-27 12:25:46,818 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:46,820 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: 6c75f348-15d4-409f-ae39-b1f9e205c4e3, Nodes: cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T12:25:46.560754Z[UTC]].
scm1.org_1   | 2023-06-27 12:25:46,941 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f163470c-d3e0-4891-94bf-0bc988a6aa12: Detected pause in JVM or host machine approximately 0.240s without any GCs.
scm1.org_1   | 2023-06-27 12:25:46,972 [IPC Server handler 21 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a650530b-22bc-4ab5-9901-0a035f1b699c
scm1.org_1   | 2023-06-27 12:25:46,972 [IPC Server handler 21 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a650530b-22bc-4ab5-9901-0a035f1b699c{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 397752053077, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-27 12:25:47,021 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-27 12:25:47,021 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-27 12:25:47,021 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2023-06-27 12:25:47,021 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2023-06-27 12:25:47,021 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 12:25:47,053 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea to datanode:a650530b-22bc-4ab5-9901-0a035f1b699c
scm1.org_1   | 2023-06-27 12:25:47,173 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 12:25:47,237 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:47,305 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T12:25:47.053404Z[UTC]].
scm1.org_1   | 2023-06-27 12:25:47,543 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 to datanode:a650530b-22bc-4ab5-9901-0a035f1b699c
scm1.org_1   | 2023-06-27 12:25:47,568 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 to datanode:1a7440b0-385d-4580-b33d-c9708c19e79c
scm1.org_1   | 2023-06-27 12:25:47,569 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 to datanode:cc01b188-4e0b-44d2-a762-70392abfc581
scm1.org_1   | 2023-06-27 12:25:47,766 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:47,776 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: 3108f661-f070-45b5-be9d-3cb73f243315, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T12:25:47.543704Z[UTC]].
scm1.org_1   | 2023-06-27 12:25:47,806 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aeeab16c-188d-4045-bf49-561639362392 to datanode:a650530b-22bc-4ab5-9901-0a035f1b699c
scm1.org_1   | 2023-06-27 12:25:47,815 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aeeab16c-188d-4045-bf49-561639362392 to datanode:cc01b188-4e0b-44d2-a762-70392abfc581
scm1.org_1   | 2023-06-27 12:25:47,816 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aeeab16c-188d-4045-bf49-561639362392 to datanode:1a7440b0-385d-4580-b33d-c9708c19e79c
scm1.org_1   | 2023-06-27 12:25:47,934 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:25:47,980 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: aeeab16c-188d-4045-bf49-561639362392, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T12:25:47.806673Z[UTC]].
scm1.org_1   | 2023-06-27 12:25:47,981 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=aeeab16c-188d-4045-bf49-561639362392 contains same datanodes as previous pipelines: PipelineID=3108f661-f070-45b5-be9d-3cb73f243315 nodeIds: a650530b-22bc-4ab5-9901-0a035f1b699c, cc01b188-4e0b-44d2-a762-70392abfc581, 1a7440b0-385d-4580-b33d-c9708c19e79c
scm1.org_1   | 2023-06-27 12:25:47,982 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-27 12:25:47,994 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-27 12:25:50,338 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:56632
scm1.org_1   | 2023-06-27 12:25:50,474 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:25:50,594 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:36162
scm1.org_1   | 2023-06-27 12:25:50,706 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:25:54,055 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:39076
scm1.org_1   | 2023-06-27 12:25:54,135 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 12:26:16,799 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:38060
scm1.org_1   | 2023-06-27 12:26:16,909 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:26:17,250 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50278
scm1.org_1   | 2023-06-27 12:26:17,380 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:26:18,008 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-27 12:26:18,973 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:43874
scm1.org_1   | 2023-06-27 12:26:19,049 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:26:19,718 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:26:19,763 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:34163
scm1.org_1   | 2023-06-27 12:26:19,777 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6c75f348-15d4-409f-ae39-b1f9e205c4e3, Nodes: cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cc01b188-4e0b-44d2-a762-70392abfc581, CreationTimestamp2023-06-27T12:25:46.560Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-06-27 12:26:19,811 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:26:20,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:20,716 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:26:20,733 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6edac7e5-f324-41f8-8d35-b13f88867b8f, Nodes: 1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1a7440b0-385d-4580-b33d-c9708c19e79c, CreationTimestamp2023-06-27T12:25:45.392Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-06-27 12:26:20,741 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:20,796 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:22,206 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:26:22,274 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e2dd7c8c-90c8-46e5-a4e2-b8ccf1e980ea, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a650530b-22bc-4ab5-9901-0a035f1b699c, CreationTimestamp2023-06-27T12:25:47.053Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-06-27 12:26:22,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:22,778 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:23,237 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:25,758 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:26,316 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:58488
scm1.org_1   | 2023-06-27 12:26:26,446 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-27 12:26:27,113 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:27,672 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:58504
scm1.org_1   | 2023-06-27 12:26:27,695 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:26:28,355 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:29,562 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:39964
scm1.org_1   | 2023-06-27 12:26:29,624 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-27 12:26:30,524 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:53498
scm1.org_1   | 2023-06-27 12:26:30,608 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 12:26:33,658 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:56304
scm1.org_1   | 2023-06-27 12:26:33,882 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:26:36,621 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:36,803 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:44448
scm1.org_1   | 2023-06-27 12:26:36,823 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:42921
scm1.org_1   | 2023-06-27 12:26:36,877 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 12:26:37,085 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:26:37,093 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:38,824 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:52238
scm1.org_1   | 2023-06-27 12:26:39,065 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 12:26:39,069 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:39,222 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:39,246 [f163470c-d3e0-4891-94bf-0bc988a6aa12@group-589B8C5BE712-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 12:26:39,250 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3108f661-f070-45b5-be9d-3cb73f243315, Nodes: a650530b-22bc-4ab5-9901-0a035f1b699c(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)1a7440b0-385d-4580-b33d-c9708c19e79c(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)cc01b188-4e0b-44d2-a762-70392abfc581(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1a7440b0-385d-4580-b33d-c9708c19e79c, CreationTimestamp2023-06-27T12:25:47.543Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-06-27 12:26:39,271 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 12:26:39,271 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-27 12:26:39,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1.org_1   | 2023-06-27 12:26:39,277 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1.org_1   | 2023-06-27 12:26:39,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2023-06-27 12:26:39,281 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1.org_1   | 2023-06-27 12:26:39,283 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1.org_1   | 2023-06-27 12:26:39,286 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1.org_1   | 2023-06-27 12:26:39,288 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO security.RootCARotationManager: notifyStatusChanged: enable monitor task
scm1.org_1   | 2023-06-27 12:26:39,415 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:53380
scm1.org_1   | 2023-06-27 12:26:39,450 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1.org_1   | 2023-06-27 12:26:39,469 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm1.org_1   | 2023-06-27 12:26:39,511 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-27 12:26:41,306 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:35216
scm1.org_1   | 2023-06-27 12:26:41,917 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
