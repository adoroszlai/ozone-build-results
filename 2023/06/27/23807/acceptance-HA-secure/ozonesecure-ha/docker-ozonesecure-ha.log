Attaching to ozonesecure-ha_om3_1, ozonesecure-ha_om2_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_om1_1, ozonesecure-ha_datanode3_1, ozonesecure-ha_recon_1, ozonesecure-ha_kdc_1, ozonesecure-ha_s3g_1, ozonesecure-ha_kms_1, ozonesecure-ha_scm3.org_1, ozonesecure-ha_datanode1_1, ozonesecure-ha_datanode2_1, ozonesecure-ha_httpfs_1, ozonesecure-ha_scm1.org_1
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2023-06-27 17:01:29,871 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = 4cab33795ed1/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
datanode1_1  | STARTUP_MSG:   java = 11.0.19
datanode1_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode1_1  | ************************************************************/
datanode1_1  | 2023-06-27 17:01:30,022 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2023-06-27 17:01:30,461 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2023-06-27 17:01:31,831 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2023-06-27 17:01:33,254 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2023-06-27 17:01:33,254 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2023-06-27 17:01:34,779 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:4cab33795ed1 ip:172.25.0.102
datanode1_1  | 2023-06-27 17:01:40,220 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2023-06-27 17:01:41,640 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2023-06-27 17:01:41,640 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2023-06-27 17:01:43,581 [main] INFO reflections.Reflections: Reflections took 1338 ms to scan 2 urls, producing 106 keys and 230 values 
datanode1_1  | 2023-06-27 17:01:48,397 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2023-06-27 17:01:51,011 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode1_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2023-06-27 17:01:30,789 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = c4c5f0c38083/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
datanode3_1  | STARTUP_MSG:   java = 11.0.19
datanode3_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode3_1  | ************************************************************/
datanode3_1  | 2023-06-27 17:01:30,968 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2023-06-27 17:01:31,472 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2023-06-27 17:01:32,650 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2023-06-27 17:01:33,816 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2023-06-27 17:01:33,817 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2023-06-27 17:01:35,219 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c4c5f0c38083 ip:172.25.0.104
datanode3_1  | 2023-06-27 17:01:40,308 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2023-06-27 17:01:41,771 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2023-06-27 17:01:41,771 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2023-06-27 17:01:43,444 [main] INFO reflections.Reflections: Reflections took 1135 ms to scan 2 urls, producing 106 keys and 230 values 
datanode3_1  | 2023-06-27 17:01:48,085 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode3_1  | 2023-06-27 17:01:50,609 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode3_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode3_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode3_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode3_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode3_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode3_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode3_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode3_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode3_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode3_1  | 2023-06-27 17:01:50,650 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2023-06-27 17:01:50,652 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2023-06-27 17:01:50,665 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2023-06-27 17:01:58,621 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2023-06-27 17:01:59,007 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:c4c5f0c38083
datanode3_1  | 2023-06-27 17:01:59,007 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2023-06-27 17:01:59,057 [main] ERROR utils.CertificateSignRequest: Invalid domain c4c5f0c38083
datanode3_1  | 2023-06-27 17:01:59,063 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@c4c5f0c38083
datanode3_1  | 2023-06-27 17:02:03,449 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 253775241875
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode3_1  |            Start Date: Tue Jun 27 16:59:54 UTC 2023
datanode3_1  |            Final Date: Fri Aug 04 16:59:54 UTC 2028
datanode3_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode3_1  |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
datanode3_1  |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
datanode3_1  |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
datanode3_1  |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
datanode3_1  |                        6031d6bbcce2d911b293b01715f60941bca01f22
datanode3_1  |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
datanode3_1  |                        e53925b79b76c1e822589654cf8f339218feaedf
datanode3_1  |                        29211ff344b921aef19d39fe27b1f1791e557b58
datanode3_1  |                        de0b8dc672e394da848c5d954638a49ff0bd8708
datanode3_1  |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
datanode3_1  |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
datanode3_1  |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
datanode3_1  |                        df6e7ecf25455532985737512133afdf952d415e
datanode3_1  |                        23e015a3d5c6b073bba64e6a6a58ef15
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
datanode3_1  | 
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
datanode3_1  |                        critical(true) KeyUsage: 0xbe
datanode3_1  |  from file: /data/metadata/dn/certs/CA-253775241875.crt.
datanode3_1  | 2023-06-27 17:02:03,477 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 380923164129
datanode3_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode3_1  |            Start Date: Tue Jun 27 17:02:01 UTC 2023
datanode3_1  |            Final Date: Wed Jun 26 17:02:01 UTC 2024
datanode3_1  |             SubjectDN: CN=dn@c4c5f0c38083,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode3_1  |            Public Key: RSA Public Key [c1:da:b7:b2:03:cc:cf:02:c7:f6:07:f6:1d:10:28:50:a1:e4:85:92],[56:66:d1:a4]
datanode3_1  |         modulus: 90afa9884910b833a6fa958ec1de9a4f4bc480f78cd2b89efa5e8e68fb760a04cb777cb74188a725142afba09581f8461e647f49224c66921c23b8956f5abad122e69f4f0fe1689c66a980b2cf484fa430e92dacfe48046cec548066507bf51963dcf355b03c88947eba884657f9544fd65dfc33a3c8f3697c42210e380150957d45d8f305d4a4fe95a99f813aceb9462343d9d5db27891f82263e5d30c7368c995ceb7dad56e567ff4a614310286764bcd6a5b28259dd4695f4c80c9a0a946d3530e5f1a2c695640f47fcc71fc23b222bb49d517d59c566c3b347561eb87b8f5569998e2eebc413e24f2be937f9e5cbf59aeb3ed68a0cc9d0f14a74cc74dbb5
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 00895e3a05360d56f2089429a1f49228216f911d
datanode3_1  |                        911de073f5f6512db475af6299ca1c2fd68c4bba
datanode3_1  |                        817a9be4bbe82cd9f6760edebe62e715184adf24
datanode3_1  |                        070a42b04a4cbe14ddb266ffc34430dac1a65e98
datanode3_1  |                        34c38ec0e974a048216815f8e03e1ecfbbf739a3
datanode3_1  |                        d86716295fdeeb5f5fa6447f6137bf8e4a23d803
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
kdc_1        | Jun 27 16:59:25 kdc krb5kdc[8](info): Loaded
kdc_1        | Jun 27 16:59:25 kdc krb5kdc[8](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jun 27 16:59:25 kdc krb5kdc[8](info): setting up network...
kdc_1        | Jun 27 16:59:25 kdc krb5kdc[8](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jun 27 16:59:25 kdc krb5kdc[8](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jun 27 16:59:25 kdc krb5kdc[8](info): set up 4 sockets
kdc_1        | Jun 27 16:59:25 kdc krb5kdc[8](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jun 27 16:59:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687885170, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 16:59:36 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.119: ISSUE: authtime 1687885176, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, httpfs/httpfs@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 16:59:36 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.114: ISSUE: authtime 1687885176, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, s3g/s3g@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 16:59:44 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.115: ISSUE: authtime 1687885184, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 16:59:58 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1687885198, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:00:05 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1687885205, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:00:11 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1687885198, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:00:12 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687885170, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:00:14 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.115: ISSUE: authtime 1687885184, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:00:24 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687885224, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:00:25 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1687885225, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:00:39 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687885224, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:00:41 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1687885225, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:00:45 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687885245, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:00:49 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1687885249, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:00:50 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1687885249, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:00:57 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687885245, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:00:59 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1687885259, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:02 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687885262, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode1_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode1_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode1_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode1_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode1_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode1_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode1_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode1_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode1_1  | 2023-06-27 17:01:51,054 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2023-06-27 17:01:51,066 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2023-06-27 17:01:51,074 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2023-06-27 17:01:57,942 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2023-06-27 17:01:58,253 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:4cab33795ed1
datanode1_1  | 2023-06-27 17:01:58,259 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2023-06-27 17:01:58,311 [main] ERROR utils.CertificateSignRequest: Invalid domain 4cab33795ed1
datanode1_1  | 2023-06-27 17:01:58,322 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@4cab33795ed1
datanode1_1  | 2023-06-27 17:02:03,198 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 253775241875
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode1_1  |            Start Date: Tue Jun 27 16:59:54 UTC 2023
datanode1_1  |            Final Date: Fri Aug 04 16:59:54 UTC 2028
datanode1_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode1_1  |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
datanode1_1  |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
datanode1_1  |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
datanode1_1  |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
datanode1_1  |                        6031d6bbcce2d911b293b01715f60941bca01f22
datanode1_1  |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
datanode1_1  |                        e53925b79b76c1e822589654cf8f339218feaedf
datanode1_1  |                        29211ff344b921aef19d39fe27b1f1791e557b58
datanode1_1  |                        de0b8dc672e394da848c5d954638a49ff0bd8708
datanode1_1  |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
datanode1_1  |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
datanode1_1  |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
datanode1_1  |                        df6e7ecf25455532985737512133afdf952d415e
datanode1_1  |                        23e015a3d5c6b073bba64e6a6a58ef15
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0xbe
datanode1_1  |  from file: /data/metadata/dn/certs/CA-253775241875.crt.
datanode1_1  | 2023-06-27 17:02:03,233 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 1
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode1_1  |            Start Date: Tue Jun 27 16:59:54 UTC 2023
datanode1_1  |            Final Date: Fri Aug 04 16:59:54 UTC 2028
datanode1_1  |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode1_1  |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
datanode1_1  |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
datanode1_1  |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
datanode1_1  |                        dcfc06fd580359247f53abb2878410a9ef225129
datanode1_1  |                        820a87eb87b24799aec84587b5f9d56ce0e93904
datanode1_1  |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
datanode1_1  |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2023-06-27 17:01:30,400 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = afaecf158e34/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
datanode2_1  | STARTUP_MSG:   java = 11.0.19
datanode2_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode2_1  | ************************************************************/
datanode2_1  | 2023-06-27 17:01:30,526 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-06-27 17:01:31,019 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2023-06-27 17:01:31,708 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2023-06-27 17:01:33,077 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2023-06-27 17:01:33,077 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2023-06-27 17:01:34,530 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:afaecf158e34 ip:172.25.0.103
datanode2_1  | 2023-06-27 17:01:39,604 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2023-06-27 17:01:40,665 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2023-06-27 17:01:40,665 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2023-06-27 17:01:42,592 [main] INFO reflections.Reflections: Reflections took 1397 ms to scan 2 urls, producing 106 keys and 230 values 
datanode2_1  | 2023-06-27 17:01:47,229 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2023-06-27 17:01:49,889 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode2_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode2_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode2_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode2_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode2_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode2_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode2_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode2_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode2_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode2_1  | 2023-06-27 17:01:49,895 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2023-06-27 17:01:49,897 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2023-06-27 17:01:49,905 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2023-06-27 17:02:00,488 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2023-06-27 17:02:00,886 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:afaecf158e34
datanode2_1  | 2023-06-27 17:02:00,890 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2023-06-27 17:02:00,958 [main] ERROR utils.CertificateSignRequest: Invalid domain afaecf158e34
datanode2_1  | 2023-06-27 17:02:00,958 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@afaecf158e34
datanode2_1  | 2023-06-27 17:02:05,014 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 253775241875
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode2_1  |            Start Date: Tue Jun 27 16:59:54 UTC 2023
datanode2_1  |            Final Date: Fri Aug 04 16:59:54 UTC 2028
datanode2_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode2_1  |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
datanode2_1  |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
datanode2_1  |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
datanode2_1  |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
datanode2_1  |                        6031d6bbcce2d911b293b01715f60941bca01f22
datanode2_1  |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
datanode2_1  |                        e53925b79b76c1e822589654cf8f339218feaedf
datanode2_1  |                        29211ff344b921aef19d39fe27b1f1791e557b58
datanode2_1  |                        de0b8dc672e394da848c5d954638a49ff0bd8708
datanode2_1  |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
datanode2_1  |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
datanode2_1  |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
datanode2_1  |                        df6e7ecf25455532985737512133afdf952d415e
datanode2_1  |                        23e015a3d5c6b073bba64e6a6a58ef15
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0xbe
datanode2_1  |  from file: /data/metadata/dn/certs/CA-253775241875.crt.
datanode2_1  | 2023-06-27 17:02:05,055 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 382350562679
datanode2_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode2_1  |            Start Date: Tue Jun 27 17:02:03 UTC 2023
datanode2_1  |            Final Date: Wed Jun 26 17:02:03 UTC 2024
datanode2_1  |             SubjectDN: CN=dn@afaecf158e34,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode2_1  |            Public Key: RSA Public Key [b8:fb:87:c8:9a:a7:62:14:18:db:a7:e6:22:25:ec:7c:df:f3:54:d6],[56:66:d1:a4]
datanode2_1  |         modulus: bb1eda2a81315f1eb6b9e60519328008097c497bdad3b61af2df7a2e0becd1bd1a57017bfc67af6f4b2ef04da14b8a49ef5ab3b81e10b3ef685c4031de72be336a7bbbf23f68369e004debeda34b5755ce1f58b82333ac6f6bf9c6a1c888497106090aa7972ab6d2a415da73449c11a8735e20046f0943a23ce5fccdc2cf4893144360ee4f94b4a2de69ce2e0e1d0c204b83f8b06c88ca6d404df53f74e241de08baea9a062fa7479a6ad792cc75c5ae38374a3a40de7c27ccb3f6b0c088d646e0aa10d2a72afec1f9537f91a7f236eb3448ca0f10ab00722cc168c6a96e7d0e55985f4b855703fbf12e0e130c93388759d7470acf5bb67d75954bfbdd6617bb
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 66a20fd489d44c217d6ee543e392e79dd0ad0564
datanode2_1  |                        aa18920c5f0d4149ca697d5910004e1eeb798f70
datanode2_1  |                        68941426e57db04b7b2461d610074d21578dce11
datanode2_1  |                        1e36f3f87f86c2d3a17c6f019b268a687d8c0540
datanode2_1  |                        c855c403bdb8e29b27e03b124b7914169a91fb1d
datanode2_1  |                        67a071bc0559c9b51a6a48051ed2fe3584215daf
kdc_1        | Jun 27 17:01:14 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1687885259, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:01:18 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687885262, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:01:28 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687885288, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:40 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1687885300, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:41 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1687885301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:41 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1687885301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:44 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1687885304, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:44 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1687885304, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:45 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1687885305, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:01:49 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1687885304, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:01:50 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1687885304, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:01:50 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1687885305, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:02:00 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1687885301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:02:01 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1687885301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:02:03 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1687885300, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:02:30 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687885288, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:02:39 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1687885359, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:02:40 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1687885301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 27 17:02:41 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1687885301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 27 17:02:42 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1687885300, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 27 17:02:43 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1687885363, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
httpfs_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
httpfs_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
httpfs_1     | 2023-06-27 16:59:32,168 [main] INFO server.HttpFSServerWebServer: STARTUP_MSG: 
httpfs_1     | /************************************************************
httpfs_1     | STARTUP_MSG: Starting HttpFSServerWebServer
httpfs_1     | STARTUP_MSG:   host = httpfs/172.25.0.119
httpfs_1     | STARTUP_MSG:   args = []
httpfs_1     | STARTUP_MSG:   version = 3.3.6
httpfs_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/curator-client-4.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/json-simple-1.1.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/curator-framework-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.5.6.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/zookeeper-jute-3.5.6.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar
httpfs_1     | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
httpfs_1     | STARTUP_MSG:   java = 11.0.19
httpfs_1     | ************************************************************/
httpfs_1     | 2023-06-27 16:59:32,255 [main] INFO server.HttpFSServerWebServer: registered UNIX signal handlers for [TERM, HUP, INT]
httpfs_1     | 2023-06-27 16:59:33,408 [main] INFO util.log: Logging initialized @6161ms to org.eclipse.jetty.util.log.Slf4jLog
httpfs_1     | 2023-06-27 16:59:34,275 [main] INFO http.HttpRequestLog: Http request log for http.requests.webhdfs is not defined
httpfs_1     | 2023-06-27 16:59:34,314 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
httpfs_1     | 2023-06-27 16:59:34,371 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context webhdfs
httpfs_1     | 2023-06-27 16:59:34,371 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
httpfs_1     | 2023-06-27 16:59:34,371 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
httpfs_1     | 2023-06-27 16:59:34,593 [main] INFO http.HttpServer2: Jetty bound to port 14000
httpfs_1     | 2023-06-27 16:59:34,594 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
httpfs_1     | 2023-06-27 16:59:34,928 [main] INFO server.session: DefaultSessionIdManager workerName=node0
httpfs_1     | 2023-06-27 16:59:34,928 [main] INFO server.session: No SessionScavenger set, using defaults
httpfs_1     | 2023-06-27 16:59:34,930 [main] INFO server.session: node0 Scavenging every 600000ms
httpfs_1     | 2023-06-27 16:59:35,048 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3a44431a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
httpfs_1     | 2023-06-27 16:59:35,057 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@53ce1329{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
httpfs_1     | log4j:WARN No appenders could be found for logger (org.eclipse.jetty.webapp.WebAppClassLoader).
httpfs_1     | log4j:WARN Please initialize the log4j system properly.
httpfs_1     | log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
httpfs_1     | 16:59:35,753  WARN Server:474 - Log4j [/etc/hadoop/httpfs-log4j.properties] configuration file not found, using default configuration from classpath
httpfs_1     | 16:59:35,758  INFO Server:389 - ++++++++++++++++++++++++++++++++++++++++++++++++++++++
httpfs_1     | 16:59:35,759  INFO Server:390 - Server [httpfs] starting
httpfs_1     | 16:59:35,762  INFO Server:391 -   Built information:
httpfs_1     | 16:59:35,765  INFO Server:392 -     Version           : 1.4.0-SNAPSHOT
httpfs_1     | 16:59:35,765  INFO Server:394 -     Source Repository : REPO NOT AVAIL
httpfs_1     | 16:59:35,765  INFO Server:396 -     Source Revision   : REVISION NOT AVAIL
httpfs_1     | 16:59:35,765  INFO Server:398 -     Built by          : runner
httpfs_1     | 16:59:35,766  INFO Server:400 -     Built timestamp   : 2023-06-27T16:34:14+0000
httpfs_1     | 16:59:35,766  INFO Server:402 -   Runtime information:
httpfs_1     | 16:59:35,770  INFO Server:403 -     Home   dir: /opt/hadoop
httpfs_1     | 16:59:35,772  INFO Server:404 -     Config dir: /etc/hadoop
httpfs_1     | 16:59:35,773  INFO Server:405 -     Log    dir: /opt/hadoop/log
httpfs_1     | 16:59:35,773  INFO Server:406 -     Temp   dir: /opt/hadoop/temp
httpfs_1     | 16:59:35,980  INFO Server:544 - System property sets  httpfs.log.dir: /opt/hadoop/log
httpfs_1     | 16:59:35,981  INFO Server:544 - System property sets  httpfs.temp.dir: /opt/hadoop/temp
httpfs_1     | 16:59:35,981  INFO Server:544 - System property sets  httpfs.config.dir: /etc/hadoop
httpfs_1     | 16:59:35,982  INFO Server:544 - System property sets  httpfs.home.dir: /opt/hadoop
httpfs_1     | 16:59:36,021  INFO FileSystemAccessService:162 - Using FileSystemAccess JARs version [3.3.6]
httpfs_1     | 16:59:36,591  INFO UserGroupInformation:1132 - Login successful for user httpfs/httpfs@EXAMPLE.COM using keytab file httpfs.keytab. Keytab auto renewal enabled : false
httpfs_1     | 16:59:36,597  INFO FileSystemAccessService:191 - Using FileSystemAccess Kerberos authentication, principal [httpfs/httpfs@EXAMPLE.COM] keytab [/etc/security/keytabs/httpfs.keytab]
httpfs_1     | 16:59:36,635  INFO Server:413 - Services initialized
httpfs_1     | 16:59:36,639  INFO Server:423 - Server [httpfs] started!, status [NORMAL]
httpfs_1     | 16:59:36,640  INFO HttpFSServerWebApp:107 - Connects to Namenode [ofs://omservice]
httpfs_1     | 16:59:36,642  INFO HttpFSServerWebApp:126 - Initializing HttpFSServerMetrics
httpfs_1     | 16:59:36,821  INFO JvmPauseMonitor:188 - Starting JVM pause monitor
httpfs_1     | 16:59:37,189  INFO MetricsConfig:120 - Loaded properties from hadoop-metrics2.properties
httpfs_1     | 16:59:37,804  INFO MetricsSystemImpl:378 - Scheduled Metric snapshot period at 10 second(s).
httpfs_1     | 16:59:37,804  INFO MetricsSystemImpl:191 - HttpFSServer metrics system started
httpfs_1     | 16:59:37,955  INFO KerberosAuthenticationHandler:175 - Using keytab /etc/security/keytabs/httpfs.keytab, for principal HTTP/httpfs@EXAMPLE.COM
httpfs_1     | 16:59:37,996  INFO AbstractDelegationTokenSecretManager:415 - Updating the current master key for generating delegation tokens
httpfs_1     | 16:59:38,014  INFO AbstractDelegationTokenSecretManager:797 - Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
httpfs_1     | 16:59:38,028  INFO AbstractDelegationTokenSecretManager:415 - Updating the current master key for generating delegation tokens
httpfs_1     | Jun 27, 2023 4:59:38 PM com.sun.jersey.api.core.PackagesResourceConfig init
httpfs_1     | INFO: Scanning for root resource and provider classes in the packages:
httpfs_1     |   org.apache.ozone.fs.http.server
httpfs_1     |   org.apache.ozone.lib.wsrs
httpfs_1     | Jun 27, 2023 4:59:38 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
httpfs_1     | INFO: Root resource classes found:
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSServer
httpfs_1     | Jun 27, 2023 4:59:38 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
httpfs_1     | INFO: Provider classes found:
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSParametersProvider
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSExceptionProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONMapProvider
httpfs_1     | Jun 27, 2023 4:59:39 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
httpfs_1     | INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
httpfs_1     | Jun 27, 2023 4:59:40 PM com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl <init>
httpfs_1     | SEVERE: Implementation of JAXB-API has not been found on module path or classpath.
httpfs_1     | javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.
httpfs_1     |  - with linked exception:
kdc_1        | Jun 27 17:02:48 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1687885368, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:02:49 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1687885369, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 27 17:02:50 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1687885363, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:02:54 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1687885368, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:02:54 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1687885369, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:03:25 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1687885359, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 27 17:03:33 kdc krb5kdc[8](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.115: ISSUE: authtime 1687885184, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for om/om@EXAMPLE.COM
datanode3_1  |                        418f602fa52c624daa8f19c7f0a1529608aaafb1
datanode3_1  |                        e7ac49350a943ce5ff286e1546b2fee9405141c8
datanode3_1  |                        109a1ebab4dfc34df152fa84772f0f5565567dbd
datanode3_1  |                        f5df5646e2e8e8d9259dc61b156e825ed094ed8e
datanode3_1  |                        e252ea657f8e0bae2dd0ed6a3d9fb83458de5080
datanode3_1  |                        27398763185a185b7e430d7a7440f85697ef8fc2
datanode3_1  |                        df939b224e45997dbee728dc81d24c62
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  | 
datanode3_1  |                        critical(true) KeyUsage: 0xb8
datanode3_1  |  from file: /data/metadata/dn/certs/380923164129.crt.
datanode3_1  | 2023-06-27 17:02:03,523 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 1
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode3_1  |            Start Date: Tue Jun 27 16:59:54 UTC 2023
datanode3_1  |            Final Date: Fri Aug 04 16:59:54 UTC 2028
datanode3_1  |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode3_1  |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
datanode3_1  |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
datanode3_1  |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
datanode3_1  |                        dcfc06fd580359247f53abb2878410a9ef225129
datanode3_1  |                        820a87eb87b24799aec84587b5f9d56ce0e93904
datanode3_1  |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
datanode3_1  |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
datanode3_1  |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
datanode3_1  |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
datanode3_1  |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
datanode3_1  |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
datanode3_1  |                        862deee8bb1ba8320f84b774cf369b4523011f63
datanode3_1  |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
datanode3_1  |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
datanode3_1  |                        critical(true) KeyUsage: 0x6
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
datanode3_1  | 
datanode3_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2023-06-27 17:02:03,562 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797475 ms and interval 86400000 ms.
datanode3_1  | 2023-06-27 17:02:04,043 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2023-06-27 17:02:04,480 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode3_1  | 2023-06-27 17:02:05,674 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z).
datanode3_1  | 2023-06-27 17:02:05,679 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M9.958134S and interval of PT1M
datanode3_1  | 2023-06-27 17:02:05,836 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode3_1  | 2023-06-27 17:02:06,112 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode3_1  | 2023-06-27 17:02:07,791 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2023-06-27 17:02:08,009 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode3_1  | 2023-06-27 17:02:08,039 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2023-06-27 17:02:08,056 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2023-06-27 17:02:08,433 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode3_1  | 2023-06-27 17:02:08,513 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-27 17:02:08,535 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2023-06-27 17:02:08,535 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2023-06-27 17:02:08,543 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2023-06-27 17:02:08,544 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2023-06-27 17:02:08,796 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2023-06-27 17:02:08,817 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2023-06-27 17:02:16,980 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode3_1  | 2023-06-27 17:02:18,183 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode3_1  | 2023-06-27 17:02:18,268 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode3_1  | 2023-06-27 17:02:18,686 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-27 17:02:19,604 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2023-06-27 17:02:20,901 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-27 17:02:20,958 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2023-06-27 17:02:20,966 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-27 17:02:20,971 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2023-06-27 17:02:20,987 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode3_1  | 2023-06-27 17:02:20,987 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2023-06-27 17:02:20,995 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  | 2023-06-27 17:02:21,003 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
datanode1_1  |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
datanode1_1  |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
datanode1_1  |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
datanode1_1  |                        862deee8bb1ba8320f84b774cf369b4523011f63
datanode1_1  |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
datanode1_1  |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0x6
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode1_1  | 2023-06-27 17:02:03,314 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 380187848912
datanode1_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode1_1  |            Start Date: Tue Jun 27 17:02:01 UTC 2023
datanode1_1  |            Final Date: Wed Jun 26 17:02:01 UTC 2024
datanode1_1  |             SubjectDN: CN=dn@4cab33795ed1,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode1_1  |            Public Key: RSA Public Key [84:b2:d8:76:24:26:22:43:44:4c:87:1f:8c:61:09:e1:b7:86:a4:e2],[56:66:d1:a4]
datanode1_1  |         modulus: 87e2f8a0de52a31d40d7167f82f02c9b328f1667ea5619abbbf4ff029a19f4eea87f2889847fc26c58362f6a0dc323bed454a028196f35df55438e96c24dc86662b7be67e9096bcf4a6bd0f43a2cc8a018bcbc6e65e32101cd4d85a1fdaf1aa8521e11f11a69416155ab18d243c8abdb12bea2f5de8f793fea274a86a7b9e67a30d1e3bef06f9e81dc299b9beea765d116baee025240436ca1922de0d960fe8b2198bdbe7c1da162aea0c2ef88cf42a30608a168f8babef54b6122ce4c2e7b2522a5cb1292e104b8709db6aa5d284712336451b6f6210b3d9e6559299b1ff1382eb9de133c0b372160c25feac3750a2ba7bd6c8e5cca289a52577d164f9acc33
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 852bbe9b788314b00b55c42238e3c881436a8ab4
datanode1_1  |                        8504472574c232393ea1fc2adf5899161d1df86d
datanode1_1  |                        4f223dec7162c1e9fb397ebb8ce8a2446193512b
datanode1_1  |                        bf3644f1e5de4870145d69fd117aa2bd5257ee3c
datanode1_1  |                        de4e1cb03343283fe49bd6b71ceb557ee1c64ecc
datanode1_1  |                        f5ae3d0d824dcd5c59aa4ca12064f495f3f5f51b
datanode1_1  |                        0a9205e016513c3793381ee78977bc9c8ed65206
datanode1_1  |                        517b020c951391fb2d1f31c27d26a6e377c3099b
datanode1_1  |                        339b64653cce774cb731cc7acacaa25000cdd25c
datanode1_1  |                        0657a0aceb61565f0fd62d226f0371c014c32540
datanode1_1  |                        c9aabb98e27cf1b9fcd5f6bf91ae7e038d22e07b
datanode1_1  |                        73e925b3b7ac015ea37d178a9821c3b7a8a87607
datanode1_1  |                        d8ec371c8c949f028b8c7edf6df89844
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  | 
datanode1_1  |                        critical(true) KeyUsage: 0xb8
datanode1_1  |  from file: /data/metadata/dn/certs/380187848912.crt.
datanode1_1  | 2023-06-27 17:02:03,375 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797679 ms and interval 86400000 ms.
datanode1_1  | 2023-06-27 17:02:03,934 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2023-06-27 17:02:04,184 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode1_1  | 2023-06-27 17:02:05,675 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z).
datanode1_1  | 2023-06-27 17:02:05,677 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M9.95915S and interval of PT1M
datanode1_1  | 2023-06-27 17:02:05,827 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode1_1  | 2023-06-27 17:02:06,094 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2023-06-27 17:02:07,750 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2023-06-27 17:02:07,881 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode1_1  | 2023-06-27 17:02:07,975 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2023-06-27 17:02:07,989 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2023-06-27 17:02:08,371 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2023-06-27 17:02:08,424 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-06-27 17:02:08,444 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode1_1  | 2023-06-27 17:02:08,459 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode1_1  | 2023-06-27 17:02:08,460 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2023-06-27 17:02:08,461 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode1_1  | 2023-06-27 17:02:08,857 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2023-06-27 17:02:08,860 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2023-06-27 17:02:16,600 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode1_1  | 2023-06-27 17:02:17,602 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode1_1  | 2023-06-27 17:02:17,688 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode1_1  | 2023-06-27 17:02:18,062 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-06-27 17:02:18,854 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2023-06-27 17:02:20,372 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-27 17:02:20,436 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2023-06-27 17:02:20,441 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-27 17:02:20,455 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2023-06-27 17:02:20,466 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode1_1  | 2023-06-27 17:02:20,470 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2023-06-27 17:02:20,483 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2023-06-27 17:02:20,491 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | Waiting for the service scm3.org:9894
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-27 17:01:30,148 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
om1_1        | STARTUP_MSG:   java = 11.0.19
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-27 17:01:30,216 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-06-27 17:01:40,529 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-27 17:01:43,225 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om1_1        | 2023-06-27 17:01:44,045 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-27 17:01:44,060 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-27 17:01:44,081 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1        | 2023-06-27 17:01:45,952 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-27 17:01:45,953 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-27 17:01:46,021 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 17:01:47,577 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f;layoutVersion=6
om1_1        | 2023-06-27 17:01:51,619 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om1_1        | 2023-06-27 17:01:51,620 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2023-06-27 17:01:52,220 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-27 17:01:58,098 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om1_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om1_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om1_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om1_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om1_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om1_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1374)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1353)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om1_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om1_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om1_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om1_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om1_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om1_1        | 2023-06-27 17:01:58,153 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2023-06-27 17:01:58,168 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om1_1        | 2023-06-27 17:01:58,175 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2023-06-27 17:02:05,195 [main] INFO om.OzoneManager: Init response: GETCERT
om1_1        | 2023-06-27 17:02:05,632 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om1_1        | 2023-06-27 17:02:05,647 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2023-06-27 17:02:05,710 [main] ERROR utils.CertificateSignRequest: Invalid domain om1
om1_1        | 2023-06-27 17:02:05,715 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:a2237aa5-d04c-471b-bae4-225264529c4c,clusterId:CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f,subject:om1
om1_1        | 2023-06-27 17:02:08,327 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 253775241875
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om1_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
om1_1        |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
om1_1        |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
om1_1        |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
om1_1        |                        6031d6bbcce2d911b293b01715f60941bca01f22
om1_1        |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
om1_1        |                        e53925b79b76c1e822589654cf8f339218feaedf
om1_1        |                        29211ff344b921aef19d39fe27b1f1791e557b58
om1_1        |                        de0b8dc672e394da848c5d954638a49ff0bd8708
om1_1        |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
om1_1        |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
om1_1        |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
om1_1        |                        df6e7ecf25455532985737512133afdf952d415e
om1_1        |                        23e015a3d5c6b073bba64e6a6a58ef15
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0xbe
om1_1        |  from file: /data/metadata/om/certs/CA-253775241875.crt.
om1_1        | 2023-06-27 17:02:08,381 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 1
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om1_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
om1_1        |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
httpfs_1     | [java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory]
httpfs_1     | 	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:177)
httpfs_1     | 	at javax.xml.bind.ContextFinder.find(ContextFinder.java:364)
httpfs_1     | 	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:508)
httpfs_1     | 	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:465)
httpfs_1     | 	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:366)
httpfs_1     | 	at com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl.<init>(WadlApplicationContextImpl.java:107)
httpfs_1     | 	at com.sun.jersey.server.impl.wadl.WadlFactory.init(WadlFactory.java:100)
httpfs_1     | 	at com.sun.jersey.server.impl.application.RootResourceUriRules.initWadl(RootResourceUriRules.java:169)
httpfs_1     | 	at com.sun.jersey.server.impl.application.RootResourceUriRules.<init>(RootResourceUriRules.java:106)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1359)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795)
httpfs_1     | 	at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:795)
httpfs_1     | 	at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:790)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer.initiate(ServletContainer.java:509)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:339)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:605)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:207)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:394)
httpfs_1     | 	at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:577)
httpfs_1     | 	at javax.servlet.GenericServlet.init(GenericServlet.java:244)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHolder$Wrapper.init(ServletHolder.java:1345)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:415)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:750)
httpfs_1     | 	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
httpfs_1     | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
httpfs_1     | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
httpfs_1     | 	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:312)
httpfs_1     | 	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
httpfs_1     | 	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414)
httpfs_1     | 	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
httpfs_1     | 	at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524)
httpfs_1     | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
httpfs_1     | 	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
httpfs_1     | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
httpfs_1     | 	at org.eclipse.jetty.server.Server.start(Server.java:423)
httpfs_1     | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)
httpfs_1     | 	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
httpfs_1     | 	at org.eclipse.jetty.server.Server.doStart(Server.java:387)
httpfs_1     | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
httpfs_1     | 	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1301)
httpfs_1     | 	at org.apache.ozone.fs.http.server.HttpFSServerWebServer.start(HttpFSServerWebServer.java:153)
httpfs_1     | 	at org.apache.ozone.fs.http.server.HttpFSServerWebServer.main(HttpFSServerWebServer.java:185)
httpfs_1     | Caused by: java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory
httpfs_1     | 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
httpfs_1     | 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
httpfs_1     | 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
httpfs_1     | 	at org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538)
httpfs_1     | 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
httpfs_1     | 	at javax.xml.bind.ServiceLoaderUtil.nullSafeLoadClass(ServiceLoaderUtil.java:122)
httpfs_1     | 	at javax.xml.bind.ServiceLoaderUtil.safeLoadClass(ServiceLoaderUtil.java:155)
httpfs_1     | 	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:174)
httpfs_1     | 	... 53 more
httpfs_1     | 
httpfs_1     | 16:59:41,392  INFO ContextHandler:921 - Started o.e.j.w.WebAppContext@369c9bb{webhdfs,/,file:///tmp/hadoop-hadoop/httpfs/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/webhdfs}
httpfs_1     | 16:59:41,448  INFO AbstractConnector:333 - Started ServerConnector@2e8c1c9b{HTTP/1.1, (http/1.1)}{0.0.0.0:14000}
httpfs_1     | 16:59:41,448  INFO Server:415 - Started @14202ms
datanode2_1  |                        797b16c40fb821b3d8cd4d83d860dfa925039513
datanode2_1  |                        e41b24fde08c2ba1286d1d473a9753d534f05b52
datanode2_1  |                        102bbb467c018f784dd572ea66d08bb0495deb72
datanode2_1  |                        215fadb32c03bdaa2e72a4fb2fa77b36e0d64496
datanode2_1  |                        cff7b5e23c44dc7f21b41f7f1b1c5d6090f09b95
datanode2_1  |                        375d1231e6b0d819cedc01b7af6d499e4821138f
datanode2_1  |                        0e083569f9eea258233c352f54850785
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  | 
datanode2_1  |                        critical(true) KeyUsage: 0xb8
datanode2_1  |  from file: /data/metadata/dn/certs/382350562679.crt.
datanode2_1  | 2023-06-27 17:02:05,084 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 1
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode2_1  |            Start Date: Tue Jun 27 16:59:54 UTC 2023
datanode2_1  |            Final Date: Fri Aug 04 16:59:54 UTC 2028
datanode2_1  |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode2_1  |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
datanode2_1  |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
datanode2_1  |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
datanode2_1  |                        dcfc06fd580359247f53abb2878410a9ef225129
datanode2_1  |                        820a87eb87b24799aec84587b5f9d56ce0e93904
datanode2_1  |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
datanode2_1  |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
datanode2_1  |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
datanode2_1  |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
datanode2_1  |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
datanode2_1  |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
datanode2_1  |                        862deee8bb1ba8320f84b774cf369b4523011f63
datanode2_1  |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
datanode2_1  |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0x6
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2023-06-27 17:02:05,134 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797911 ms and interval 86400000 ms.
datanode2_1  | 2023-06-27 17:02:05,887 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2023-06-27 17:02:06,317 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode3_1  | 2023-06-27 17:02:21,009 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2023-06-27 17:02:21,010 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 17:02:21,085 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-06-27 17:02:21,178 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode3_1  | 2023-06-27 17:02:21,187 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-06-27 17:02:27,301 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode3_1  | 2023-06-27 17:02:27,492 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode3_1  | 2023-06-27 17:02:27,500 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode3_1  | 2023-06-27 17:02:27,508 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode3_1  | 2023-06-27 17:02:27,519 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode3_1  | 2023-06-27 17:02:27,548 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode3_1  | 2023-06-27 17:02:27,555 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode3_1  | 2023-06-27 17:02:27,634 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode3_1  | 2023-06-27 17:02:27,645 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode3_1  | 2023-06-27 17:02:27,672 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode3_1  | 2023-06-27 17:02:27,681 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode3_1  | 2023-06-27 17:02:28,092 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode3_1  | 2023-06-27 17:02:28,097 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode3_1  | 2023-06-27 17:02:28,121 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 17:02:28,121 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 17:02:28,172 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 17:02:28,194 [d17c8765-e162-413f-ae01-cbbd58f77ae9-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbd193cdb] REGISTERED
datanode3_1  | 2023-06-27 17:02:28,224 [d17c8765-e162-413f-ae01-cbbd58f77ae9-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbd193cdb] BIND: 0.0.0.0/0.0.0.0:9855
datanode3_1  | 2023-06-27 17:02:28,282 [d17c8765-e162-413f-ae01-cbbd58f77ae9-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbd193cdb, L:/0.0.0.0:9855] ACTIVE
datanode3_1  | 2023-06-27 17:02:28,684 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode3_1  | 2023-06-27 17:02:29,440 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode3_1  | 2023-06-27 17:02:30,704 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2023-06-27 17:02:30,706 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2023-06-27 17:02:30,708 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2023-06-27 17:02:30,974 [main] INFO util.log: Logging initialized @76718ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2023-06-27 17:02:32,287 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2023-06-27 17:02:32,331 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2023-06-27 17:02:32,342 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2023-06-27 17:02:32,342 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2023-06-27 17:02:32,345 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2023-06-27 17:02:32,366 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2023-06-27 17:02:32,811 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode3_1  | 2023-06-27 17:02:32,814 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2023-06-27 17:02:32,826 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode3_1  | 2023-06-27 17:02:33,136 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2023-06-27 17:02:33,136 [main] INFO server.session: No SessionScavenger set, using defaults
datanode3_1  | 2023-06-27 17:02:33,148 [main] INFO server.session: node0 Scavenging every 660000ms
datanode3_1  | 2023-06-27 17:02:33,284 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-27 17:02:33,297 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@633aedeb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2023-06-27 17:02:33,300 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e0dec0c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2023-06-27 17:02:34,121 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-27 17:02:34,300 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@50be5d6f{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-17900143963956860560/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2023-06-27 17:02:34,388 [main] INFO server.AbstractConnector: Started ServerConnector@79378802{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2023-06-27 17:02:34,392 [main] INFO server.Server: Started @80136ms
datanode3_1  | 2023-06-27 17:02:34,442 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2023-06-27 17:02:34,443 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2023-06-27 17:02:34,451 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2023-06-27 17:02:34,746 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode3_1  | 2023-06-27 17:02:34,810 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode3_1  | 2023-06-27 17:02:34,842 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode1_1  | 2023-06-27 17:02:20,495 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2023-06-27 17:02:20,500 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 17:02:20,600 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-06-27 17:02:20,652 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode1_1  | 2023-06-27 17:02:20,660 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-06-27 17:02:28,020 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode1_1  | 2023-06-27 17:02:28,257 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode1_1  | 2023-06-27 17:02:28,265 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode1_1  | 2023-06-27 17:02:28,271 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode1_1  | 2023-06-27 17:02:28,287 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode1_1  | 2023-06-27 17:02:28,315 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode1_1  | 2023-06-27 17:02:28,321 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode1_1  | 2023-06-27 17:02:28,441 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode1_1  | 2023-06-27 17:02:28,467 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode1_1  | 2023-06-27 17:02:28,478 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode1_1  | 2023-06-27 17:02:28,481 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode1_1  | 2023-06-27 17:02:28,871 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode1_1  | 2023-06-27 17:02:28,880 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode1_1  | 2023-06-27 17:02:28,880 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 17:02:28,889 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 17:02:28,920 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 17:02:29,008 [e5cd4d6c-bb68-491f-b474-5dc217379157-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x4974db8d] REGISTERED
datanode1_1  | 2023-06-27 17:02:29,031 [e5cd4d6c-bb68-491f-b474-5dc217379157-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x4974db8d] BIND: 0.0.0.0/0.0.0.0:9855
datanode1_1  | 2023-06-27 17:02:29,088 [e5cd4d6c-bb68-491f-b474-5dc217379157-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x4974db8d, L:/0.0.0.0:9855] ACTIVE
datanode1_1  | 2023-06-27 17:02:29,413 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode1_1  | 2023-06-27 17:02:30,261 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode1_1  | 2023-06-27 17:02:31,692 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode1_1  | 2023-06-27 17:02:31,699 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2023-06-27 17:02:31,699 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2023-06-27 17:02:31,987 [main] INFO util.log: Logging initialized @78091ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2023-06-27 17:02:33,246 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2023-06-27 17:02:33,426 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2023-06-27 17:02:33,461 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2023-06-27 17:02:33,467 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2023-06-27 17:02:33,476 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode1_1  | 2023-06-27 17:02:33,512 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2023-06-27 17:02:33,981 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode1_1  | 2023-06-27 17:02:33,984 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2023-06-27 17:02:34,008 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode1_1  | 2023-06-27 17:02:34,369 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2023-06-27 17:02:34,374 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2023-06-27 17:02:34,387 [main] INFO server.session: node0 Scavenging every 660000ms
datanode1_1  | 2023-06-27 17:02:34,469 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-27 17:02:34,482 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4d2068ea{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2023-06-27 17:02:34,485 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a65ce06{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2023-06-27 17:02:35,351 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-27 17:02:35,512 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@38ad8c75{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-12310693192717054388/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2023-06-27 17:02:35,625 [main] INFO server.AbstractConnector: Started ServerConnector@6682e9b0{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2023-06-27 17:02:35,626 [main] INFO server.Server: Started @81730ms
datanode1_1  | 2023-06-27 17:02:35,652 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2023-06-27 17:02:35,652 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2023-06-27 17:02:35,660 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2023-06-27 17:02:35,912 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode1_1  | 2023-06-27 17:02:35,966 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode1_1  | 2023-06-27 17:02:36,015 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode1_1  | 2023-06-27 17:02:38,669 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode1_1  | 2023-06-27 17:02:38,680 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode1_1  | 2023-06-27 17:02:38,702 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode1_1  | 2023-06-27 17:02:38,706 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode1_1  | 2023-06-27 17:02:38,810 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 2023-06-27 17:02:40,074 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode1_1  | 2023-06-27 17:02:40,185 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode1_1  | 2023-06-27 17:02:43,323 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode1_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode1_1  | Caused by: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	... 1 more
datanode1_1  | 2023-06-27 17:02:44,152 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/DS-d8e5b3ea-fce7-4f91-b7a4-ba582c7e3ee3/container.db to cache
datanode1_1  | 2023-06-27 17:02:44,154 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/DS-d8e5b3ea-fce7-4f91-b7a4-ba582c7e3ee3/container.db for volume DS-d8e5b3ea-fce7-4f91-b7a4-ba582c7e3ee3
datanode1_1  | 2023-06-27 17:02:44,165 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2023-06-27 17:02:44,184 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode1_1  | 2023-06-27 17:02:44,299 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode1_1  | 2023-06-27 17:02:44,307 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis e5cd4d6c-bb68-491f-b474-5dc217379157
datanode1_1  | 2023-06-27 17:02:44,534 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.RaftServer: e5cd4d6c-bb68-491f-b474-5dc217379157: start RPC server
datanode1_1  | 2023-06-27 17:02:44,552 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: e5cd4d6c-bb68-491f-b474-5dc217379157: GrpcService started, listening on 9858
datanode1_1  | 2023-06-27 17:02:44,584 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: e5cd4d6c-bb68-491f-b474-5dc217379157: GrpcService started, listening on 9856
datanode1_1  | 2023-06-27 17:02:44,625 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: e5cd4d6c-bb68-491f-b474-5dc217379157: GrpcService started, listening on 9857
datanode1_1  | 2023-06-27 17:02:44,650 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e5cd4d6c-bb68-491f-b474-5dc217379157 is started using port 9858 for RATIS
datanode1_1  | 2023-06-27 17:02:44,656 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e5cd4d6c-bb68-491f-b474-5dc217379157 is started using port 9857 for RATIS_ADMIN
datanode1_1  | 2023-06-27 17:02:44,657 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e5cd4d6c-bb68-491f-b474-5dc217379157 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2023-06-27 17:02:44,660 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e5cd4d6c-bb68-491f-b474-5dc217379157 is started using port 9855 for RATIS_DATASTREAM
datanode1_1  | 2023-06-27 17:02:44,666 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e5cd4d6c-bb68-491f-b474-5dc217379157: Started
datanode1_1  | 2023-06-27 17:02:44,764 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-27 17:02:44,764 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-27 17:02:44,818 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode1_1  | 2023-06-27 17:03:18,811 [PipelineCommandHandlerThread-0] INFO server.RaftServer: e5cd4d6c-bb68-491f-b474-5dc217379157: addNew group-B9564BF6E306:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] returns group-B9564BF6E306:java.util.concurrent.CompletableFuture@39e7327d[Not completed]
datanode1_1  | 2023-06-27 17:03:19,011 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157: new RaftServerImpl for group-B9564BF6E306:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-27 17:03:19,018 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-27 17:03:19,027 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-27 17:01:29,513 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
om2_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-27 17:01:29,601 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-06-27 17:01:39,895 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-27 17:01:42,652 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-27 17:01:43,439 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-06-27 17:01:43,446 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-27 17:01:43,475 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-27 17:01:45,401 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-27 17:01:45,401 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-27 17:01:45,495 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 17:01:47,084 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f;layoutVersion=6
om2_1        | 2023-06-27 17:01:50,678 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om2_1        | 2023-06-27 17:01:50,684 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2023-06-27 17:01:51,340 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-06-27 17:01:56,456 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om2_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om2_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om2_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om2_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om2_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om2_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1374)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1353)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om2_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om2_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om2_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om2_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om2_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om2_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om2_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om2_1        | 2023-06-27 17:01:56,485 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2023-06-27 17:01:56,498 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om2_1        | 2023-06-27 17:01:56,508 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2023-06-27 17:02:09,605 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2023-06-27 17:02:10,017 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
om2_1        | 2023-06-27 17:02:10,024 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2023-06-27 17:02:10,075 [main] ERROR utils.CertificateSignRequest: Invalid domain om2
om2_1        | 2023-06-27 17:02:10,083 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:a2237aa5-d04c-471b-bae4-225264529c4c,clusterId:CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f,subject:om2
om2_1        | 2023-06-27 17:02:12,506 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 389874921037
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Start Date: Tue Jun 27 17:02:11 UTC 2023
om2_1        |            Final Date: Wed Jun 26 17:02:11 UTC 2024
om2_1        |             SubjectDN: CN=om2,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Public Key: RSA Public Key [b4:3a:b1:88:e8:74:99:de:0e:75:4b:35:8b:07:07:9e:af:99:63:4d],[56:66:d1:a4]
om2_1        |         modulus: d1e2119240c32e134376a176edae1f4e3bafaa6043506988a3dd36d7df4a9f3c2dc16e6913ea1cb59639ea4618924b4df81f32e4b2723e923e95d7b83414e20f7dad9709bd3c458d9686e15ee8f3064574e0b32556061914bb10f3d4d229b6c49d53b31e91fd1addcc0cc49e4e92520317a058553609c346d5d18ec8cc83a2732f0be6d5e1c8028c0c792f61541c14ce450b87bf5e4f8b46443b4268f6495332c4d6e295a2695f5a9fd185fbe5de0a9adc5719599d7c2e4bbff473eb2c1a6ffab1a2cb23b87bb9fd4d216f114ff5f925f1d1c54e0a102d176a0f68a7f054fcae19fe7aa2091d97863d77c121d5c7ba621d8a70c9c1f285ce5e4a785ebf15e463
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 099e7344a853e6ea68260ca86ce14ce052fcb5f4
om2_1        |                        589ffa915fed299edbf388b07f8cfc91287e155d
om2_1        |                        8aad4cbb4ea09e914f147cd3252026344a3304a1
om2_1        |                        bad0d80d663232ae2ca982d5f7b0487a087bebc0
om2_1        |                        090ddbca794a4ee1aa5c3281096504cb87b87958
om2_1        |                        ef85b9b09ecbd2ea0b72b518ab5ce195dc4c5906
om2_1        |                        b09fd2fdc1fb4c96975db8ee79d71a3b4cc27b5c
om2_1        |                        9bd79092e7d2394c88c82f08479380761b5e3da5
om2_1        |                        c0985dec4d4dc6a9d7f853a7b1d1e02cb08db1f5
om2_1        |                        4c457a9d9c8f06ab511ddecdec9e913fcd109755
om2_1        |                        4c2dbde1755f2ac1ac3e6663d9a406774a8daa04
om2_1        |                        1bef4e1beac5eeef3a6badd0e416f50ca4d0a8b7
om2_1        |                        8524e639238271c7f90917a2834f792f
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
om2_1        |  from file: /data/metadata/om/certs/389874921037.crt.
om2_1        | 2023-06-27 17:02:12,568 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 253775241875
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
datanode2_1  | 2023-06-27 17:02:07,007 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z).
datanode2_1  | 2023-06-27 17:02:07,010 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M8.626982S and interval of PT1M
datanode2_1  | 2023-06-27 17:02:07,097 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode2_1  | 2023-06-27 17:02:07,332 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode2_1  | 2023-06-27 17:02:08,883 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2023-06-27 17:02:09,114 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode2_1  | 2023-06-27 17:02:09,173 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2023-06-27 17:02:09,187 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2023-06-27 17:02:09,664 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  | 2023-06-27 17:02:09,710 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-27 17:02:09,725 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2023-06-27 17:02:09,731 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2023-06-27 17:02:09,731 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode2_1  | 2023-06-27 17:02:09,731 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2023-06-27 17:02:09,985 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2023-06-27 17:02:10,000 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2023-06-27 17:02:17,363 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode2_1  | 2023-06-27 17:02:18,068 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode2_1  | 2023-06-27 17:02:18,174 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode2_1  | 2023-06-27 17:02:18,885 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-27 17:02:20,138 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2023-06-27 17:02:20,831 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-27 17:02:20,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2023-06-27 17:02:20,846 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-27 17:02:20,852 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2023-06-27 17:02:20,860 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode2_1  | 2023-06-27 17:02:20,860 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2023-06-27 17:02:20,868 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode2_1  | 2023-06-27 17:02:20,871 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 17:02:20,906 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2023-06-27 17:02:20,915 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-27 17:02:21,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode2_1  | 2023-06-27 17:02:21,140 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode2_1  | 2023-06-27 17:02:21,148 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-06-27 17:02:28,254 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode2_1  | 2023-06-27 17:02:28,485 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode2_1  | 2023-06-27 17:02:28,491 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode2_1  | 2023-06-27 17:02:28,507 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode2_1  | 2023-06-27 17:02:28,517 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode2_1  | 2023-06-27 17:02:28,547 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode2_1  | 2023-06-27 17:02:28,556 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode2_1  | 2023-06-27 17:02:28,634 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode2_1  | 2023-06-27 17:02:28,651 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode2_1  | 2023-06-27 17:02:28,660 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode2_1  | 2023-06-27 17:02:28,665 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode2_1  | 2023-06-27 17:02:29,100 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode2_1  | 2023-06-27 17:02:29,105 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode2_1  | 2023-06-27 17:02:29,108 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-27 17:02:29,108 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-27 17:02:29,197 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-27 17:02:29,248 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xcc1069ef] REGISTERED
datanode2_1  | 2023-06-27 17:02:29,294 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xcc1069ef] BIND: 0.0.0.0/0.0.0.0:9855
datanode2_1  | 2023-06-27 17:02:29,331 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xcc1069ef, L:/0.0.0.0:9855] ACTIVE
datanode2_1  | 2023-06-27 17:02:29,644 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode2_1  | 2023-06-27 17:02:30,376 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode2_1  | 2023-06-27 17:02:32,106 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | 2023-06-27 17:02:32,106 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2023-06-27 17:02:32,107 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode2_1  | 2023-06-27 17:02:32,401 [main] INFO util.log: Logging initialized @78435ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2023-06-27 17:02:33,692 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2023-06-27 17:02:33,753 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2023-06-27 17:02:33,764 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2023-06-27 17:02:33,764 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode2_1  | 2023-06-27 17:02:33,768 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode2_1  | 2023-06-27 17:02:33,787 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2023-06-27 17:02:34,308 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode2_1  | 2023-06-27 17:02:34,317 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2023-06-27 17:02:34,333 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode2_1  | 2023-06-27 17:02:34,798 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2023-06-27 17:02:34,798 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2023-06-27 17:02:34,819 [main] INFO server.session: node0 Scavenging every 600000ms
datanode2_1  | 2023-06-27 17:02:34,968 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-06-27 17:02:34,993 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5145dddc{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2023-06-27 17:02:35,000 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@116c43d4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2023-06-27 17:02:35,696 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-06-27 17:02:35,836 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d67f6f{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8776416876527878999/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2023-06-27 17:02:35,978 [main] INFO server.AbstractConnector: Started ServerConnector@6245303e{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2023-06-27 17:02:35,978 [main] INFO server.Server: Started @82014ms
datanode2_1  | 2023-06-27 17:02:36,052 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2023-06-27 17:02:36,052 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2023-06-27 17:02:36,066 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2023-06-27 17:02:36,525 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode2_1  | 2023-06-27 17:02:36,583 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode2_1  | 2023-06-27 17:02:36,611 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode2_1  | 2023-06-27 17:02:39,973 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode2_1  | 2023-06-27 17:02:39,980 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode2_1  | 2023-06-27 17:02:39,986 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode2_1  | 2023-06-27 17:02:40,026 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode2_1  | 2023-06-27 17:02:40,060 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode2_1  | 2023-06-27 17:02:41,031 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2023-06-27 17:02:41,152 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode2_1  | 2023-06-27 17:02:44,490 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/DS-c5f3b390-f99f-4767-983f-0fa88a7d4b72/container.db to cache
datanode2_1  | 2023-06-27 17:02:44,492 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/DS-c5f3b390-f99f-4767-983f-0fa88a7d4b72/container.db for volume DS-c5f3b390-f99f-4767-983f-0fa88a7d4b72
datanode2_1  | 2023-06-27 17:02:44,515 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2023-06-27 17:02:44,600 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode2_1  | 2023-06-27 17:02:45,221 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode2_1  | 2023-06-27 17:02:45,222 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3b0419d0-b93c-4a38-a8f8-ba0374c67423
datanode2_1  | 2023-06-27 17:02:45,568 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: start RPC server
datanode2_1  | 2023-06-27 17:02:45,582 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: GrpcService started, listening on 9858
datanode2_1  | 2023-06-27 17:02:45,598 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: GrpcService started, listening on 9856
datanode2_1  | 2023-06-27 17:02:45,612 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: GrpcService started, listening on 9857
datanode2_1  | 2023-06-27 17:02:45,630 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3b0419d0-b93c-4a38-a8f8-ba0374c67423 is started using port 9858 for RATIS
datanode2_1  | 2023-06-27 17:02:45,630 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3b0419d0-b93c-4a38-a8f8-ba0374c67423 is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2023-06-27 17:02:45,634 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3b0419d0-b93c-4a38-a8f8-ba0374c67423 is started using port 9856 for RATIS_SERVER
datanode2_1  | 2023-06-27 17:02:45,636 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3b0419d0-b93c-4a38-a8f8-ba0374c67423 is started using port 9855 for RATIS_DATASTREAM
datanode2_1  | 2023-06-27 17:02:45,643 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3b0419d0-b93c-4a38-a8f8-ba0374c67423: Started
datanode2_1  | 2023-06-27 17:02:45,756 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-27 17:02:45,756 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-27 17:02:45,863 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode2_1  | 2023-06-27 17:03:19,819 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: addNew group-CBF4F2545280:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] returns group-CBF4F2545280:java.util.concurrent.CompletableFuture@2b3ac671[Not completed]
datanode2_1  | 2023-06-27 17:03:20,102 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: new RaftServerImpl for group-CBF4F2545280:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-27 17:03:20,109 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-27 17:03:20,121 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-27 17:03:20,122 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-27 17:03:20,122 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-27 17:03:20,125 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 17:02:37,852 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode3_1  | 2023-06-27 17:02:37,854 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode3_1  | 2023-06-27 17:02:37,856 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode3_1  | 2023-06-27 17:02:37,875 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode3_1  | 2023-06-27 17:02:37,914 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode3_1  | 2023-06-27 17:02:39,531 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2023-06-27 17:02:39,661 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode3_1  | 2023-06-27 17:02:44,105 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/DS-d455aace-2a4d-479f-a3c9-9ed9a450cb36/container.db to cache
datanode3_1  | 2023-06-27 17:02:44,105 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/DS-d455aace-2a4d-479f-a3c9-9ed9a450cb36/container.db for volume DS-d455aace-2a4d-479f-a3c9-9ed9a450cb36
datanode3_1  | 2023-06-27 17:02:44,169 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2023-06-27 17:02:44,214 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode3_1  | 2023-06-27 17:02:44,620 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode3_1  | 2023-06-27 17:02:44,629 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode3_1  | 2023-06-27 17:02:45,243 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.RaftServer: d17c8765-e162-413f-ae01-cbbd58f77ae9: start RPC server
datanode3_1  | 2023-06-27 17:02:45,307 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: d17c8765-e162-413f-ae01-cbbd58f77ae9: GrpcService started, listening on 9858
datanode3_1  | 2023-06-27 17:02:45,360 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: d17c8765-e162-413f-ae01-cbbd58f77ae9: GrpcService started, listening on 9856
datanode3_1  | 2023-06-27 17:02:45,374 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: d17c8765-e162-413f-ae01-cbbd58f77ae9: GrpcService started, listening on 9857
datanode3_1  | 2023-06-27 17:02:45,465 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d17c8765-e162-413f-ae01-cbbd58f77ae9 is started using port 9858 for RATIS
datanode3_1  | 2023-06-27 17:02:45,465 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d17c8765-e162-413f-ae01-cbbd58f77ae9 is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2023-06-27 17:02:45,465 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d17c8765-e162-413f-ae01-cbbd58f77ae9 is started using port 9856 for RATIS_SERVER
datanode3_1  | 2023-06-27 17:02:45,476 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d17c8765-e162-413f-ae01-cbbd58f77ae9: Started
datanode3_1  | 2023-06-27 17:02:45,476 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d17c8765-e162-413f-ae01-cbbd58f77ae9 is started using port 9855 for RATIS_DATASTREAM
datanode3_1  | 2023-06-27 17:02:45,646 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-27 17:02:45,652 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-27 17:02:45,760 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode3_1  | 2023-06-27 17:03:19,627 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d17c8765-e162-413f-ae01-cbbd58f77ae9: addNew group-CBF4F2545280:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] returns group-CBF4F2545280:java.util.concurrent.CompletableFuture@2cca7c29[Not completed]
datanode3_1  | 2023-06-27 17:03:19,749 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9: new RaftServerImpl for group-CBF4F2545280:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-27 17:03:19,762 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-27 17:03:19,764 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-27 17:03:19,767 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-27 17:03:19,769 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 17:03:19,770 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 17:03:19,771 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-27 17:03:19,852 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: ConfigurationManager, init=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-27 17:03:19,859 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 17:03:19,911 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-27 17:03:19,925 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-27 17:03:20,046 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-27 17:03:20,086 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-27 17:03:20,179 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-27 17:03:20,201 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-27 17:03:20,412 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-27 17:03:20,539 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 17:03:20,572 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 17:03:20,582 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-27 17:03:20,583 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-27 17:03:20,590 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-27 17:03:20,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-27 17:03:20,600 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280 does not exist. Creating ...
datanode3_1  | 2023-06-27 17:03:20,658 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280/in_use.lock acquired by nodename 7@c4c5f0c38083
datanode3_1  | 2023-06-27 17:03:20,790 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280 has been successfully formatted.
datanode3_1  | 2023-06-27 17:03:20,963 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO ratis.ContainerStateMachine: group-CBF4F2545280: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-27 17:03:21,002 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-27 17:03:21,163 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-27 17:03:21,169 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:21,189 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-27 17:03:21,199 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-27 17:03:21,254 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:21,387 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-27 17:03:21,405 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-27 17:03:21,410 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:21,557 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280
datanode3_1  | 2023-06-27 17:03:21,569 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-27 17:03:21,586 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 17:03:21,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:21,639 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-27 17:03:21,648 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-27 17:03:21,661 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-27 17:03:21,661 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-27 17:03:21,667 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-27 17:03:21,789 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:21,798 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:21,934 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-27 17:03:21,940 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-27 17:03:21,941 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-27 17:03:22,002 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO segmented.SegmentedRaftLogWorker: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 17:03:19,030 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-27 17:03:19,031 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 17:03:19,031 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 17:03:19,031 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-27 17:03:19,082 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306: ConfigurationManager, init=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-27 17:03:19,086 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 17:03:19,159 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-27 17:03:19,163 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-27 17:03:19,269 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-27 17:03:19,348 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-27 17:03:19,402 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-27 17:03:19,413 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-27 17:03:19,642 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-27 17:03:19,825 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 17:03:19,869 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-27 17:03:19,875 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-27 17:03:19,885 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-27 17:03:19,886 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-27 17:03:19,889 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-27 17:03:19,895 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bb6b299a-fe5d-49b8-b1dc-b9564bf6e306 does not exist. Creating ...
datanode1_1  | 2023-06-27 17:03:19,940 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bb6b299a-fe5d-49b8-b1dc-b9564bf6e306/in_use.lock acquired by nodename 7@4cab33795ed1
datanode1_1  | 2023-06-27 17:03:20,014 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bb6b299a-fe5d-49b8-b1dc-b9564bf6e306 has been successfully formatted.
datanode1_1  | 2023-06-27 17:03:20,098 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO ratis.ContainerStateMachine: group-B9564BF6E306: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-27 17:03:20,139 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-27 17:03:20,178 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-27 17:03:20,179 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:20,181 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-27 17:03:20,182 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-27 17:03:20,184 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:20,317 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-27 17:03:20,339 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-27 17:03:20,350 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:20,555 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: new e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bb6b299a-fe5d-49b8-b1dc-b9564bf6e306
datanode1_1  | 2023-06-27 17:03:20,565 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-27 17:03:20,566 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 17:03:20,571 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:20,576 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-27 17:03:20,579 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-27 17:03:20,587 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-27 17:03:20,590 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-27 17:03:20,593 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-27 17:03:20,706 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
om1_1        |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
om1_1        |                        dcfc06fd580359247f53abb2878410a9ef225129
om1_1        |                        820a87eb87b24799aec84587b5f9d56ce0e93904
om1_1        |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
om1_1        |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
om1_1        |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
om1_1        |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
om1_1        |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
om1_1        |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
om1_1        |                        862deee8bb1ba8320f84b774cf369b4523011f63
om1_1        |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
om1_1        |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-27 17:02:08,477 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 385622236209
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Start Date: Tue Jun 27 17:02:06 UTC 2023
om1_1        |            Final Date: Wed Jun 26 17:02:06 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Public Key: RSA Public Key [01:8f:e1:a9:8a:b9:6b:f0:40:94:bc:2f:7c:58:7e:bf:53:b9:08:dc],[56:66:d1:a4]
om1_1        |         modulus: 8d48d35fcd7cb73410ce327656c0008ee5d7ba109c6f59e491736c022b11940452d7e1f6152ad1731dd3b1225b27e579ad97b90890a00f1ab9b200b4e42dd8b0df90ca629f0fbbae09639344346413b1bcb19b3cc930f46266f4e174c77d07a7d458fd5445d6154079046fc9ca4d58d8dfcd790b670e6c4184ebdfb72c3e732b6f4f77ae361dd713b8d65714860721d2718b8f4f8a8f381d07dbe17cf99d9015924f719146ef31efeedf422afe0fad999e459fa3f57db448444bb58c538e203c26dfd46d68c5310490c45e30008786014d92db5b55076abd699dfa854edfb33e3d911634c9201e2f70b1f4eb0811b196cfe64472a35ea87b2ea50637e6c31019
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 5fb339f3ce5ffd1334ccf9f95a0d91fbbd9d09f6
om1_1        |                        71be858aab5a543b23809be21ac834e657ba1fdc
om1_1        |                        5c9835d9d36ca4cc2ab4cd14d9f138a70d0e6d00
om1_1        |                        0ced6b1246101b929d2c2b1ef341eeb304d3f523
om1_1        |                        4a7a3cd0aa85f15a4eea400d012391859bc8eba6
om1_1        |                        f53f91b2c13db0acec6c80fb3dc169b8b5396b7a
om1_1        |                        44f1b4b3a15f06494c61402e39cadb9e453378cf
om1_1        |                        60a9ffaa92ebcb91f014e74403d71fdf10f1fb5f
om1_1        |                        e4621daaca1a62ebfbf00d55e417d047e7214c4c
om1_1        |                        4478105643e01daf96ced92bd203979900fa7341
om1_1        |                        7370b2dde59fe76a48006b90422562c2d844e4da
om1_1        |                        e7379ae66ba00448d69f9be58b91056384d9ca58
om1_1        |                        13649fd050ade95040d4d1d4542217cd
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/385622236209.crt.
om1_1        | 2023-06-27 17:02:08,512 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797520 ms and interval 86400000 ms.
om1_1        | 2023-06-27 17:02:08,518 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | 2023-06-27 17:02:08,618 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-27 17:02:24,717 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
om1_1        | STARTUP_MSG:   java = 11.0.19
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-27 17:02:24,825 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-06-27 17:02:34,059 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-27 17:02:36,645 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om1_1        | 2023-06-27 17:02:37,634 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-27 17:02:37,639 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-27 17:02:37,659 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1        | 2023-06-27 17:02:37,801 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 17:02:39,140 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om1_1        | 2023-06-27 17:02:41,899 [main] INFO reflections.Reflections: Reflections took 2246 ms to scan 1 urls, producing 138 keys and 396 values [using 2 cores]
om1_1        | 2023-06-27 17:02:42,014 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om1_1        | 2023-06-27 17:02:43,459 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-27 17:02:43,465 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-27 17:02:43,466 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 17:02:47,879 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-27 17:02:48,660 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-27 17:02:54,701 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-27 17:02:56,556 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 253775241875
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om1_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
om1_1        |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
om1_1        |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
om1_1        |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
om1_1        |                        6031d6bbcce2d911b293b01715f60941bca01f22
om1_1        |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
om1_1        |                        e53925b79b76c1e822589654cf8f339218feaedf
om1_1        |                        29211ff344b921aef19d39fe27b1f1791e557b58
om1_1        |                        de0b8dc672e394da848c5d954638a49ff0bd8708
om1_1        |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
om1_1        |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
om1_1        |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
om1_1        |                        df6e7ecf25455532985737512133afdf952d415e
om1_1        |                        23e015a3d5c6b073bba64e6a6a58ef15
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0xbe
om1_1        |  from file: /data/metadata/om/certs/CA-253775241875.crt.
om1_1        | 2023-06-27 17:02:56,582 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 1
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om1_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
om1_1        |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
om1_1        |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
om1_1        |                        dcfc06fd580359247f53abb2878410a9ef225129
om1_1        |                        820a87eb87b24799aec84587b5f9d56ce0e93904
om1_1        |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
om1_1        |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
om1_1        |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
om1_1        |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
om1_1        |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
om1_1        |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
om1_1        |                        862deee8bb1ba8320f84b774cf369b4523011f63
om1_1        |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
om1_1        |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-27 17:02:56,613 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 385622236209
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Start Date: Tue Jun 27 17:02:06 UTC 2023
om1_1        |            Final Date: Wed Jun 26 17:02:06 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om1_1        |            Public Key: RSA Public Key [01:8f:e1:a9:8a:b9:6b:f0:40:94:bc:2f:7c:58:7e:bf:53:b9:08:dc],[56:66:d1:a4]
om1_1        |         modulus: 8d48d35fcd7cb73410ce327656c0008ee5d7ba109c6f59e491736c022b11940452d7e1f6152ad1731dd3b1225b27e579ad97b90890a00f1ab9b200b4e42dd8b0df90ca629f0fbbae09639344346413b1bcb19b3cc930f46266f4e174c77d07a7d458fd5445d6154079046fc9ca4d58d8dfcd790b670e6c4184ebdfb72c3e732b6f4f77ae361dd713b8d65714860721d2718b8f4f8a8f381d07dbe17cf99d9015924f719146ef31efeedf422afe0fad999e459fa3f57db448444bb58c538e203c26dfd46d68c5310490c45e30008786014d92db5b55076abd699dfa854edfb33e3d911634c9201e2f70b1f4eb0811b196cfe64472a35ea87b2ea50637e6c31019
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 5fb339f3ce5ffd1334ccf9f95a0d91fbbd9d09f6
om1_1        |                        71be858aab5a543b23809be21ac834e657ba1fdc
om1_1        |                        5c9835d9d36ca4cc2ab4cd14d9f138a70d0e6d00
om1_1        |                        0ced6b1246101b929d2c2b1ef341eeb304d3f523
om1_1        |                        4a7a3cd0aa85f15a4eea400d012391859bc8eba6
om1_1        |                        f53f91b2c13db0acec6c80fb3dc169b8b5396b7a
om1_1        |                        44f1b4b3a15f06494c61402e39cadb9e453378cf
om1_1        |                        60a9ffaa92ebcb91f014e74403d71fdf10f1fb5f
om1_1        |                        e4621daaca1a62ebfbf00d55e417d047e7214c4c
om1_1        |                        4478105643e01daf96ced92bd203979900fa7341
om1_1        |                        7370b2dde59fe76a48006b90422562c2d844e4da
om1_1        |                        e7379ae66ba00448d69f9be58b91056384d9ca58
om1_1        |                        13649fd050ade95040d4d1d4542217cd
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/385622236209.crt.
om1_1        | 2023-06-27 17:02:56,639 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116749383 ms and interval 86400000 ms.
om1_1        | 2023-06-27 17:02:56,915 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
om2_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om2_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
om2_1        |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
om2_1        |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
om2_1        |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
om2_1        |                        6031d6bbcce2d911b293b01715f60941bca01f22
om2_1        |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
om2_1        |                        e53925b79b76c1e822589654cf8f339218feaedf
om2_1        |                        29211ff344b921aef19d39fe27b1f1791e557b58
om2_1        |                        de0b8dc672e394da848c5d954638a49ff0bd8708
om2_1        |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
om2_1        |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
om2_1        |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
om2_1        |                        df6e7ecf25455532985737512133afdf952d415e
om2_1        |                        23e015a3d5c6b073bba64e6a6a58ef15
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
datanode3_1  | 2023-06-27 17:03:22,002 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO segmented.SegmentedRaftLogWorker: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 17:03:22,016 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: start as a follower, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 17:03:22,021 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-27 17:03:22,031 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO impl.RoleInfo: d17c8765-e162-413f-ae01-cbbd58f77ae9: start d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState
datanode3_1  | 2023-06-27 17:03:22,155 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 17:03:22,159 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 17:03:22,198 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBF4F2545280,id=d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode3_1  | 2023-06-27 17:03:22,261 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-27 17:03:22,268 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-27 17:03:22,285 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-27 17:03:22,312 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-27 17:03:22,736 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280
datanode3_1  | 2023-06-27 17:03:22,938 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-27 17:03:23,934 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d17c8765-e162-413f-ae01-cbbd58f77ae9: Detected pause in JVM or host machine approximately 0.250s with 0.074s GC time.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=74ms
datanode3_1  | 2023-06-27 17:03:27,322 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState] INFO impl.FollowerState: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5291572211ns, electionTimeout:5160ms
datanode3_1  | 2023-06-27 17:03:27,323 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState] INFO impl.RoleInfo: d17c8765-e162-413f-ae01-cbbd58f77ae9: shutdown d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState
datanode3_1  | 2023-06-27 17:03:27,325 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-06-27 17:03:27,382 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode3_1  | 2023-06-27 17:03:27,382 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-FollowerState] INFO impl.RoleInfo: d17c8765-e162-413f-ae01-cbbd58f77ae9: start d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1
datanode3_1  | 2023-06-27 17:03:27,430 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 17:03:27,555 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 17:03:27,555 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 17:03:27,577 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for e5cd4d6c-bb68-491f-b474-5dc217379157
datanode3_1  | 2023-06-27 17:03:27,581 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 3b0419d0-b93c-4a38-a8f8-ba0374c67423
datanode3_1  | 2023-06-27 17:03:30,761 [grpc-default-executor-2] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: receive requestVote(PRE_VOTE, 3b0419d0-b93c-4a38-a8f8-ba0374c67423, group-CBF4F2545280, 0, (t:0, i:0))
datanode3_1  | 2023-06-27 17:03:30,795 [grpc-default-executor-2] INFO impl.VoteContext: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-CANDIDATE: reject PRE_VOTE from 3b0419d0-b93c-4a38-a8f8-ba0374c67423: our priority 1 > candidate's priority 0
datanode3_1  | 2023-06-27 17:03:30,827 [grpc-default-executor-2] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280 replies to PRE_VOTE vote request: 3b0419d0-b93c-4a38-a8f8-ba0374c67423<-d17c8765-e162-413f-ae01-cbbd58f77ae9#0:FAIL-t0. Peer's state: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280:t0, leader=null, voted=, raftlog=Memoized:d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 17:03:30,946 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-06-27 17:03:20,712 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:20,759 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-27 17:03:20,760 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-27 17:03:20,760 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-27 17:03:20,782 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 17:03:20,783 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 17:03:20,788 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306: start as a follower, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:20,788 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-27 17:03:20,791 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: start e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState
datanode1_1  | 2023-06-27 17:03:20,816 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B9564BF6E306,id=e5cd4d6c-bb68-491f-b474-5dc217379157
datanode1_1  | 2023-06-27 17:03:20,837 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-27 17:03:20,837 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-27 17:03:20,847 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-27 17:03:20,847 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 17:03:20,857 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-27 17:03:20,859 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 17:03:21,008 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=bb6b299a-fe5d-49b8-b1dc-b9564bf6e306
datanode1_1  | 2023-06-27 17:03:21,009 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=bb6b299a-fe5d-49b8-b1dc-b9564bf6e306.
datanode1_1  | 2023-06-27 17:03:21,014 [PipelineCommandHandlerThread-0] INFO server.RaftServer: e5cd4d6c-bb68-491f-b474-5dc217379157: addNew group-CBF4F2545280:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] returns group-CBF4F2545280:java.util.concurrent.CompletableFuture@221dcb99[Not completed]
datanode1_1  | 2023-06-27 17:03:21,134 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157: new RaftServerImpl for group-CBF4F2545280:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-27 17:03:21,141 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-27 17:03:21,142 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-27 17:03:21,142 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-27 17:03:21,156 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 17:03:21,156 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 17:03:21,178 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-27 17:03:21,179 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: ConfigurationManager, init=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-27 17:03:21,179 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 17:03:21,182 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-27 17:03:21,182 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-27 17:03:21,183 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-27 17:03:20,125 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-27 17:03:20,192 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: ConfigurationManager, init=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-27 17:03:20,209 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-27 17:03:20,267 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-27 17:03:20,273 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-27 17:03:20,411 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-27 17:03:20,429 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-27 17:03:20,478 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-27 17:03:20,481 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-27 17:03:20,830 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-27 17:03:21,033 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-27 17:03:21,058 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-27 17:03:21,062 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-27 17:03:21,065 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-27 17:03:21,068 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-27 17:03:21,072 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-27 17:03:21,075 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280 does not exist. Creating ...
datanode2_1  | 2023-06-27 17:03:21,118 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280/in_use.lock acquired by nodename 7@afaecf158e34
datanode2_1  | 2023-06-27 17:03:21,183 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280 has been successfully formatted.
datanode2_1  | 2023-06-27 17:03:21,374 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO ratis.ContainerStateMachine: group-CBF4F2545280: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-27 17:03:21,420 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-27 17:03:21,580 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-27 17:03:21,580 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 17:03:21,582 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-27 17:03:21,587 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-27 17:03:21,625 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-27 17:03:21,681 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-27 17:03:21,684 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-06-27 17:03:21,687 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 17:03:21,755 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280
datanode2_1  | 2023-06-27 17:03:21,763 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-06-27 17:03:21,771 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-27 17:03:21,781 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-27 17:03:21,791 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-06-27 17:03:21,791 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-27 17:03:21,795 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-27 17:03:21,797 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-27 17:03:21,797 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-27 17:03:21,916 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-06-27 17:03:21,926 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 17:03:22,167 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-27 17:03:22,168 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-06-27 17:03:22,169 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-27 17:03:22,244 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 17:03:22,252 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-27 17:03:22,281 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: start as a follower, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 17:03:22,284 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-06-27 17:03:22,292 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO impl.RoleInfo: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: start 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState
datanode2_1  | 2023-06-27 17:03:22,301 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-27 17:03:22,313 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-27 17:03:22,312 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBF4F2545280,id=3b0419d0-b93c-4a38-a8f8-ba0374c67423
datanode2_1  | 2023-06-27 17:03:22,341 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-06-27 17:03:22,346 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-06-27 17:03:22,350 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-06-27 17:03:22,358 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-06-27 17:03:22,517 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280
datanode2_1  | 2023-06-27 17:03:22,684 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-27 17:03:27,388 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO impl.FollowerState: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5099863141ns, electionTimeout:5054ms
datanode2_1  | 2023-06-27 17:03:27,392 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO impl.RoleInfo: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: shutdown 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState
datanode2_1  | 2023-06-27 17:03:27,411 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-06-27 17:03:27,428 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode2_1  | 2023-06-27 17:03:27,439 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO impl.RoleInfo: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: start 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1
datanode2_1  | 2023-06-27 17:03:27,564 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 17:03:27,795 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-27 17:03:27,795 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-27 17:03:27,812 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for e5cd4d6c-bb68-491f-b474-5dc217379157
datanode2_1  | 2023-06-27 17:03:27,822 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode2_1  | 2023-06-27 17:03:30,688 [grpc-default-executor-1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: receive requestVote(PRE_VOTE, d17c8765-e162-413f-ae01-cbbd58f77ae9, group-CBF4F2545280, 0, (t:0, i:0))
datanode2_1  | 2023-06-27 17:03:30,730 [grpc-default-executor-1] INFO impl.VoteContext: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-CANDIDATE: accept PRE_VOTE from d17c8765-e162-413f-ae01-cbbd58f77ae9: our priority 0 <= candidate's priority 1
datanode2_1  | 2023-06-27 17:03:30,815 [grpc-default-executor-1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280 replies to PRE_VOTE vote request: d17c8765-e162-413f-ae01-cbbd58f77ae9<-3b0419d0-b93c-4a38-a8f8-ba0374c67423#0:OK-t0. Peer's state: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280:t0, leader=null, voted=, raftlog=Memoized:3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 17:03:30,990 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
datanode2_1  | 2023-06-27 17:03:30,991 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3b0419d0-b93c-4a38-a8f8-ba0374c67423<-d17c8765-e162-413f-ae01-cbbd58f77ae9#0:FAIL-t0
datanode2_1  | 2023-06-27 17:03:30,991 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1 PRE_VOTE round 0: result REJECTED
datanode2_1  | 2023-06-27 17:03:31,005 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode2_1  | 2023-06-27 17:03:31,008 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO impl.RoleInfo: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: shutdown 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1
datanode2_1  | 2023-06-27 17:03:31,010 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-LeaderElection1] INFO impl.RoleInfo: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: start 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState
datanode2_1  | 2023-06-27 17:03:31,171 [grpc-default-executor-1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: receive requestVote(ELECTION, d17c8765-e162-413f-ae01-cbbd58f77ae9, group-CBF4F2545280, 1, (t:0, i:0))
datanode2_1  | 2023-06-27 17:03:31,177 [grpc-default-executor-1] INFO impl.VoteContext: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FOLLOWER: accept ELECTION from d17c8765-e162-413f-ae01-cbbd58f77ae9: our priority 0 <= candidate's priority 1
datanode2_1  | 2023-06-27 17:03:31,178 [grpc-default-executor-1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode2_1  | 2023-06-27 17:03:31,179 [grpc-default-executor-1] INFO impl.RoleInfo: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: shutdown 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState
datanode2_1  | 2023-06-27 17:03:31,180 [grpc-default-executor-1] INFO impl.RoleInfo: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: start 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState
datanode2_1  | 2023-06-27 17:03:31,181 [3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState] INFO impl.FollowerState: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FollowerState was interrupted
datanode2_1  | 2023-06-27 17:03:31,223 [grpc-default-executor-1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280 replies to ELECTION vote request: d17c8765-e162-413f-ae01-cbbd58f77ae9<-3b0419d0-b93c-4a38-a8f8-ba0374c67423#0:OK-t1. Peer's state: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280:t1, leader=null, voted=d17c8765-e162-413f-ae01-cbbd58f77ae9, raftlog=Memoized:3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 17:03:31,987 [grpc-default-executor-1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: receive requestVote(PRE_VOTE, e5cd4d6c-bb68-491f-b474-5dc217379157, group-CBF4F2545280, 0, (t:0, i:0))
datanode2_1  | 2023-06-27 17:03:31,992 [grpc-default-executor-1] INFO impl.VoteContext: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-FOLLOWER: accept PRE_VOTE from e5cd4d6c-bb68-491f-b474-5dc217379157: our priority 0 <= candidate's priority 0
datanode2_1  | 2023-06-27 17:03:31,993 [grpc-default-executor-1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280 replies to PRE_VOTE vote request: e5cd4d6c-bb68-491f-b474-5dc217379157<-3b0419d0-b93c-4a38-a8f8-ba0374c67423#0:OK-t1. Peer's state: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280:t1, leader=null, voted=d17c8765-e162-413f-ae01-cbbd58f77ae9, raftlog=Memoized:3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 17:03:32,883 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-27 17:03:33,982 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CBF4F2545280 with new leaderId: d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode2_1  | 2023-06-27 17:03:33,992 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-server-thread1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: change Leader from null to d17c8765-e162-413f-ae01-cbbd58f77ae9 at term 1 for appendEntries, leader elected after 13570ms
om1_1        | 2023-06-27 17:02:57,153 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 17:02:58,224 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1        | 2023-06-27 17:03:00,399 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1        | 2023-06-27 17:03:00,456 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2023-06-27 17:03:00,456 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2023-06-27 17:03:00,616 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om1_1        | 2023-06-27 17:03:00,761 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-27 17:03:01,220 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om1_1        | 2023-06-27 17:03:01,244 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om1_1        | 2023-06-27 17:03:02,891 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om1_1        | 2023-06-27 17:03:03,832 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-27 17:03:03,836 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2023-06-27 17:03:03,950 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om1_1        | 2023-06-27 17:03:03,951 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2023-06-27 17:03:04,627 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1        | 2023-06-27 17:03:04,699 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-27 17:03:04,867 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1        | 2023-06-27 17:03:04,924 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2023-06-27 17:03:05,221 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-27 17:03:05,233 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-27 17:03:05,341 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2023-06-27 17:03:05,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-27 17:03:05,426 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-27 17:03:05,437 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-27 17:03:05,442 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-27 17:03:05,444 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1        | 2023-06-27 17:03:05,448 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2023-06-27 17:03:05,448 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1        | 2023-06-27 17:03:05,465 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 17:03:05,472 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2023-06-27 17:03:05,475 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-27 17:03:05,576 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-06-27 17:03:05,603 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1        | 2023-06-27 17:03:05,609 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-06-27 17:03:07,792 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1        | 2023-06-27 17:03:07,819 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1        | 2023-06-27 17:03:07,824 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1        | 2023-06-27 17:03:07,826 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-06-27 17:03:07,830 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-06-27 17:03:07,870 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-27 17:03:07,938 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@14df8bee[Not completed]
om1_1        | 2023-06-27 17:03:07,941 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1        | 2023-06-27 17:03:07,987 [main] INFO om.OzoneManager: Creating RPC Server
om1_1        | 2023-06-27 17:03:08,189 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1        | 2023-06-27 17:03:08,229 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2023-06-27 17:03:08,235 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2023-06-27 17:03:08,241 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2023-06-27 17:03:08,244 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-06-27 17:03:08,248 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-06-27 17:03:08,253 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2023-06-27 17:03:08,359 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1        | 2023-06-27 17:03:08,360 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-27 17:03:08,476 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2023-06-27 17:03:08,489 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 2023-06-27 17:03:08,695 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-253775241875.crt.
om2_1        | 2023-06-27 17:02:12,614 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om2_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
om2_1        |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
om2_1        |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
om2_1        |                        dcfc06fd580359247f53abb2878410a9ef225129
om2_1        |                        820a87eb87b24799aec84587b5f9d56ce0e93904
om2_1        |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
om2_1        |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
om2_1        |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
om2_1        |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
om2_1        |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
om2_1        |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
om2_1        |                        862deee8bb1ba8320f84b774cf369b4523011f63
om2_1        |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
om2_1        |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-06-27 17:02:12,635 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116798383 ms and interval 86400000 ms.
om2_1        | 2023-06-27 17:02:12,639 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | 2023-06-27 17:02:12,804 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-27 17:02:28,705 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
om2_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-27 17:02:28,821 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-06-27 17:02:39,178 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-27 17:02:43,418 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-27 17:02:44,369 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-06-27 17:02:44,369 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-27 17:02:44,393 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-27 17:02:44,444 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 17:02:45,117 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om2_1        | 2023-06-27 17:02:47,943 [main] INFO reflections.Reflections: Reflections took 2246 ms to scan 1 urls, producing 138 keys and 396 values [using 2 cores]
om2_1        | 2023-06-27 17:02:48,123 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om2_1        | 2023-06-27 17:02:49,420 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-27 17:02:49,420 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-27 17:02:49,421 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 17:02:52,749 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-27 17:02:53,399 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-27 17:02:57,751 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-06-27 17:02:59,151 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 389874921037
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Start Date: Tue Jun 27 17:02:11 UTC 2023
om2_1        |            Final Date: Wed Jun 26 17:02:11 UTC 2024
om2_1        |             SubjectDN: CN=om2,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Public Key: RSA Public Key [b4:3a:b1:88:e8:74:99:de:0e:75:4b:35:8b:07:07:9e:af:99:63:4d],[56:66:d1:a4]
om2_1        |         modulus: d1e2119240c32e134376a176edae1f4e3bafaa6043506988a3dd36d7df4a9f3c2dc16e6913ea1cb59639ea4618924b4df81f32e4b2723e923e95d7b83414e20f7dad9709bd3c458d9686e15ee8f3064574e0b32556061914bb10f3d4d229b6c49d53b31e91fd1addcc0cc49e4e92520317a058553609c346d5d18ec8cc83a2732f0be6d5e1c8028c0c792f61541c14ce450b87bf5e4f8b46443b4268f6495332c4d6e295a2695f5a9fd185fbe5de0a9adc5719599d7c2e4bbff473eb2c1a6ffab1a2cb23b87bb9fd4d216f114ff5f925f1d1c54e0a102d176a0f68a7f054fcae19fe7aa2091d97863d77c121d5c7ba621d8a70c9c1f285ce5e4a785ebf15e463
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 099e7344a853e6ea68260ca86ce14ce052fcb5f4
om2_1        |                        589ffa915fed299edbf388b07f8cfc91287e155d
om2_1        |                        8aad4cbb4ea09e914f147cd3252026344a3304a1
om2_1        |                        bad0d80d663232ae2ca982d5f7b0487a087bebc0
om2_1        |                        090ddbca794a4ee1aa5c3281096504cb87b87958
om2_1        |                        ef85b9b09ecbd2ea0b72b518ab5ce195dc4c5906
om2_1        |                        b09fd2fdc1fb4c96975db8ee79d71a3b4cc27b5c
om2_1        |                        9bd79092e7d2394c88c82f08479380761b5e3da5
om2_1        |                        c0985dec4d4dc6a9d7f853a7b1d1e02cb08db1f5
om2_1        |                        4c457a9d9c8f06ab511ddecdec9e913fcd109755
om2_1        |                        4c2dbde1755f2ac1ac3e6663d9a406774a8daa04
om2_1        |                        1bef4e1beac5eeef3a6badd0e416f50ca4d0a8b7
om2_1        |                        8524e639238271c7f90917a2834f792f
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
om2_1        |  from file: /data/metadata/om/certs/389874921037.crt.
om2_1        | 2023-06-27 17:02:59,188 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 253775241875
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om2_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
om2_1        |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
om2_1        |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
om2_1        |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
om2_1        |                        6031d6bbcce2d911b293b01715f60941bca01f22
om2_1        |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
om2_1        |                        e53925b79b76c1e822589654cf8f339218feaedf
om2_1        |                        29211ff344b921aef19d39fe27b1f1791e557b58
om2_1        |                        de0b8dc672e394da848c5d954638a49ff0bd8708
om2_1        |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
om2_1        |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
om2_1        |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
om2_1        |                        df6e7ecf25455532985737512133afdf952d415e
om2_1        |                        23e015a3d5c6b073bba64e6a6a58ef15
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-253775241875.crt.
om2_1        | 2023-06-27 17:02:59,208 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om2_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
om2_1        |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
om2_1        |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
om2_1        |                        dcfc06fd580359247f53abb2878410a9ef225129
om2_1        |                        820a87eb87b24799aec84587b5f9d56ce0e93904
om2_1        |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
om2_1        |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
om2_1        |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
om2_1        |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
om2_1        |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
om2_1        |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
om2_1        |                        862deee8bb1ba8320f84b774cf369b4523011f63
om2_1        |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
om2_1        |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-06-27 17:02:59,219 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116751789 ms and interval 86400000 ms.
om2_1        | 2023-06-27 17:02:59,484 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
om2_1        | 2023-06-27 17:02:59,709 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode3_1  | 2023-06-27 17:03:30,948 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection:   Response 0: d17c8765-e162-413f-ae01-cbbd58f77ae9<-3b0419d0-b93c-4a38-a8f8-ba0374c67423#0:OK-t0
datanode3_1  | 2023-06-27 17:03:30,952 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode3_1  | 2023-06-27 17:03:30,963 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 17:03:31,015 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 17:03:31,015 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 17:03:31,240 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode3_1  | 2023-06-27 17:03:31,240 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection:   Response 0: d17c8765-e162-413f-ae01-cbbd58f77ae9<-3b0419d0-b93c-4a38-a8f8-ba0374c67423#0:OK-t1
datanode3_1  | 2023-06-27 17:03:31,241 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.LeaderElection: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1 ELECTION round 0: result PASSED
datanode3_1  | 2023-06-27 17:03:31,242 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.RoleInfo: d17c8765-e162-413f-ae01-cbbd58f77ae9: shutdown d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1
datanode3_1  | 2023-06-27 17:03:31,245 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2023-06-27 17:03:31,245 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CBF4F2545280 with new leaderId: d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode3_1  | 2023-06-27 17:03:31,245 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: change Leader from null to d17c8765-e162-413f-ae01-cbbd58f77ae9 at term 1 for becomeLeader, leader elected after 11206ms
datanode3_1  | 2023-06-27 17:03:31,572 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2023-06-27 17:03:31,773 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 17:03:31,786 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2023-06-27 17:03:31,810 [grpc-default-executor-2] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: receive requestVote(PRE_VOTE, e5cd4d6c-bb68-491f-b474-5dc217379157, group-CBF4F2545280, 0, (t:0, i:0))
datanode3_1  | 2023-06-27 17:03:31,927 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2023-06-27 17:03:31,930 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2023-06-27 17:03:31,948 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2023-06-27 17:03:32,038 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 17:03:32,045 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2023-06-27 17:03:32,167 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode3_1  | 2023-06-27 17:03:32,169 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:32,171 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode3_1  | 2023-06-27 17:03:32,217 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode3_1  | 2023-06-27 17:03:32,226 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode3_1  | 2023-06-27 17:03:32,231 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 17:03:32,238 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode3_1  | 2023-06-27 17:03:32,241 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode3_1  | 2023-06-27 17:03:32,241 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 17:03:32,244 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-06-27 17:03:32,267 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode3_1  | 2023-06-27 17:03:32,271 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:32,279 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode2_1  | 2023-06-27 17:03:34,056 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-server-thread1] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280: set configuration 0: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-27 17:03:34,116 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-server-thread1] INFO segmented.SegmentedRaftLogWorker: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-CBF4F2545280-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-06-27 17:03:34,877 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280.
datanode2_1  | 2023-06-27 17:03:34,885 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: addNew group-5F40B2F80ADE:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER] returns group-5F40B2F80ADE:java.util.concurrent.CompletableFuture@299294a3[Not completed]
datanode2_1  | 2023-06-27 17:03:34,961 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423: new RaftServerImpl for group-5F40B2F80ADE:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-27 17:03:34,964 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-27 17:03:34,975 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-27 17:03:34,978 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-27 17:03:34,979 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-27 17:03:34,981 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-27 17:03:34,982 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-27 17:03:34,982 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServer$Division: 3b0419d0-b93c-4a38-a8f8-ba0374c67423@group-5F40B2F80ADE: ConfigurationManager, init=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-27 17:03:35,016 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-27 17:03:35,020 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-27 17:03:35,034 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-27 17:03:35,035 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-27 17:03:35,047 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-27 17:03:35,060 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-27 17:03:35,065 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-27 17:03:35,080 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-27 17:03:35,148 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-27 17:03:35,175 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-27 17:03:35,175 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-27 17:03:35,178 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-27 17:03:35,209 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-27 17:03:35,215 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-27 17:03:35,230 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade does not exist. Creating ...
datanode2_1  | 2023-06-27 17:03:35,325 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade/in_use.lock acquired by nodename 7@afaecf158e34
datanode2_1  | 2023-06-27 17:03:35,362 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade has been successfully formatted.
datanode2_1  | 2023-06-27 17:03:35,486 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO ratis.ContainerStateMachine: group-5F40B2F80ADE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-27 17:03:35,488 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-27 17:03:35,509 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-27 17:03:35,510 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-27 17:03:35,579 [3b0419d0-b93c-4a38-a8f8-ba0374c67423-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-27 17:03:21,183 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-27 17:03:21,186 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-27 17:03:21,186 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-27 17:03:21,187 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-27 17:03:21,188 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 17:03:21,188 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-27 17:03:21,193 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-27 17:03:21,193 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-27 17:03:21,194 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-27 17:03:21,194 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-27 17:03:21,194 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280 does not exist. Creating ...
datanode1_1  | 2023-06-27 17:03:21,198 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280/in_use.lock acquired by nodename 7@4cab33795ed1
datanode1_1  | 2023-06-27 17:03:21,202 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280 has been successfully formatted.
datanode1_1  | 2023-06-27 17:03:21,205 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO ratis.ContainerStateMachine: group-CBF4F2545280: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-27 17:03:21,205 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-27 17:03:21,253 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-27 17:03:32,280 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode3_1  | 2023-06-27 17:03:32,280 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode3_1  | 2023-06-27 17:03:32,287 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 17:03:32,287 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode3_1  | 2023-06-27 17:03:32,290 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode3_1  | 2023-06-27 17:03:32,291 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 17:03:32,291 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-06-27 17:03:32,353 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO impl.RoleInfo: d17c8765-e162-413f-ae01-cbbd58f77ae9: start d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderStateImpl
datanode3_1  | 2023-06-27 17:03:32,650 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-06-27 17:03:32,901 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-27 17:03:33,329 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LeaderElection1] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280: set configuration 0: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 17:03:33,334 [grpc-default-executor-2] INFO impl.VoteContext: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-LEADER: reject PRE_VOTE from e5cd4d6c-bb68-491f-b474-5dc217379157: this server is the leader and still has leadership
datanode3_1  | 2023-06-27 17:03:33,334 [grpc-default-executor-2] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280 replies to PRE_VOTE vote request: e5cd4d6c-bb68-491f-b474-5dc217379157<-d17c8765-e162-413f-ae01-cbbd58f77ae9#0:FAIL-t1. Peer's state: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280:t1, leader=d17c8765-e162-413f-ae01-cbbd58f77ae9, voted=d17c8765-e162-413f-ae01-cbbd58f77ae9, raftlog=Memoized:d17c8765-e162-413f-ae01-cbbd58f77ae9@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=0: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 17:03:33,865 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280.
datanode3_1  | 2023-06-27 17:03:33,866 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d17c8765-e162-413f-ae01-cbbd58f77ae9: addNew group-33B29E5FEF17:[d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] returns group-33B29E5FEF17:java.util.concurrent.CompletableFuture@f4abe75[Not completed]
datanode3_1  | 2023-06-27 17:03:33,889 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9: new RaftServerImpl for group-33B29E5FEF17:[d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-27 17:03:33,892 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-27 17:03:33,892 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-27 17:03:33,892 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-27 17:03:33,892 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 17:03:33,896 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 17:03:33,896 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-27 17:03:33,896 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17: ConfigurationManager, init=-1: peers:[d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-27 17:03:33,896 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 17:03:33,897 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-27 17:03:33,897 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-27 17:03:33,897 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-27 17:03:33,897 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-27 17:03:33,901 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-27 17:03:33,902 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-27 17:03:33,907 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-27 17:03:33,912 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 17:03:33,912 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 17:03:33,912 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-27 17:03:33,912 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-27 17:03:33,912 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-27 17:03:33,913 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-27 17:03:33,913 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e50c70ba-12fb-4728-8c61-33b29e5fef17 does not exist. Creating ...
datanode3_1  | 2023-06-27 17:03:33,974 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e50c70ba-12fb-4728-8c61-33b29e5fef17/in_use.lock acquired by nodename 7@c4c5f0c38083
datanode3_1  | 2023-06-27 17:03:33,992 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e50c70ba-12fb-4728-8c61-33b29e5fef17 has been successfully formatted.
datanode3_1  | 2023-06-27 17:03:34,050 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO ratis.ContainerStateMachine: group-33B29E5FEF17: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-27 17:03:34,064 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-27 17:03:34,078 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-27 17:03:34,179 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:34,183 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-27 17:03:34,189 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-27 17:03:34,191 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:34,222 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-27 17:03:34,232 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-27 17:03:34,348 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:34,349 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e50c70ba-12fb-4728-8c61-33b29e5fef17
datanode3_1  | 2023-06-27 17:03:34,349 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-27 17:03:34,349 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 17:03:34,350 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:34,350 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-27 17:03:34,350 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-27 17:03:34,380 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-27 17:03:34,384 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-27 17:03:34,390 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-27 17:03:34,401 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:34,431 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:34,984 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-27 17:03:35,287 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-27 17:03:35,298 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-27 17:03:35,299 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO segmented.SegmentedRaftLogWorker: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 17:03:35,310 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO segmented.SegmentedRaftLogWorker: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-27 17:03:35,471 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17: start as a follower, conf=-1: peers:[d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-27 17:03:35,472 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-27 17:03:35,472 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO impl.RoleInfo: d17c8765-e162-413f-ae01-cbbd58f77ae9: start d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17-FollowerState
datanode3_1  | 2023-06-27 17:03:35,475 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-33B29E5FEF17,id=d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode3_1  | 2023-06-27 17:03:35,475 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2023-06-27 17:03:08,774 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1        | 2023-06-27 17:03:08,827 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2023-06-27 17:03:08,827 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2023-06-27 17:03:09,407 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om1_1        | 2023-06-27 17:03:09,844 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-27 17:03:09,893 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-06-27 17:03:09,904 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1        | 2023-06-27 17:03:09,916 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1        | 2023-06-27 17:03:09,931 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1        | 2023-06-27 17:03:09,996 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1        | 2023-06-27 17:03:12,419 [main] INFO reflections.Reflections: Reflections took 3604 ms to scan 8 urls, producing 24 keys and 639 values [using 2 cores]
om1_1        | 2023-06-27 17:03:13,896 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2023-06-27 17:03:13,952 [main] INFO ipc.Server: Listener at om1:9862
om1_1        | 2023-06-27 17:03:13,954 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2023-06-27 17:03:20,445 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2023-06-27 17:03:20,608 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2023-06-27 17:03:20,608 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2023-06-27 17:03:21,585 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2023-06-27 17:03:21,589 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2023-06-27 17:03:21,610 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1        | 2023-06-27 17:03:21,664 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om1
om1_1        | 2023-06-27 17:03:21,743 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1        | 2023-06-27 17:03:21,782 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2023-06-27 17:03:21,897 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2023-06-27 17:03:21,902 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 17:03:21,921 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1        | 2023-06-27 17:03:21,931 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1        | 2023-06-27 17:03:21,979 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-06-27 17:03:22,048 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2023-06-27 17:03:22,049 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2023-06-27 17:03:22,059 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 17:03:22,130 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1        | 2023-06-27 17:03:22,139 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2023-06-27 17:03:22,142 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2023-06-27 17:03:22,145 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-06-27 17:03:22,147 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2023-06-27 17:03:22,152 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1        | 2023-06-27 17:03:22,161 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-27 17:03:21,253 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:21,253 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-27 17:03:21,253 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-27 17:03:21,253 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:21,254 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-27 17:03:21,254 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: new e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-27 17:03:21,283 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-27 17:03:21,311 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:21,312 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:22,746 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-e5cd4d6c-bb68-491f-b474-5dc217379157: Detected pause in JVM or host machine approximately 1.418s with 1.429s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=95ms
datanode1_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1334ms
datanode1_1  | 2023-06-27 17:03:22,764 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-27 17:03:22,769 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-27 17:03:22,769 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-27 17:03:22,771 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 17:03:22,772 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 17:03:22,773 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: start as a follower, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:22,776 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-27 17:03:22,776 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: start e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState
datanode1_1  | 2023-06-27 17:03:22,781 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBF4F2545280,id=e5cd4d6c-bb68-491f-b474-5dc217379157
datanode1_1  | 2023-06-27 17:03:22,782 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-27 17:03:22,782 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-27 17:03:22,783 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-27 17:03:22,783 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-27 17:03:22,787 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 17:03:22,788 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280
datanode1_1  | 2023-06-27 17:03:22,792 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 17:03:22,970 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-27 17:03:26,013 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState] INFO impl.FollowerState: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5222306357ns, electionTimeout:5136ms
datanode1_1  | 2023-06-27 17:03:26,014 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: shutdown e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState
datanode1_1  | 2023-06-27 17:03:26,015 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-27 17:03:26,048 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-27 17:03:26,051 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-FollowerState] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: start e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1
datanode1_1  | 2023-06-27 17:03:26,078 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO impl.LeaderElection: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:26,094 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO impl.LeaderElection: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode1_1  | 2023-06-27 17:03:26,118 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO impl.LeaderElection: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:26,120 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO impl.LeaderElection: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2023-06-27 17:03:26,122 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: shutdown e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1
datanode1_1  | 2023-06-27 17:03:26,126 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2023-06-27 17:03:26,134 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B9564BF6E306 with new leaderId: e5cd4d6c-bb68-491f-b474-5dc217379157
datanode1_1  | 2023-06-27 17:03:26,136 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306: change Leader from null to e5cd4d6c-bb68-491f-b474-5dc217379157 at term 1 for becomeLeader, leader elected after 6866ms
datanode1_1  | 2023-06-27 17:03:26,408 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-06-27 17:03:26,574 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 17:03:26,640 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-06-27 17:03:26,842 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-06-27 17:03:26,847 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-06-27 17:03:26,874 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-06-27 17:03:27,115 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 17:03:27,158 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2023-06-27 17:03:27,190 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: start e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderStateImpl
datanode1_1  | 2023-06-27 17:03:28,078 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-e5cd4d6c-bb68-491f-b474-5dc217379157: Detected pause in JVM or host machine approximately 0.302s with 0.743s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=743ms
datanode1_1  | 2023-06-27 17:03:28,077 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState] INFO impl.FollowerState: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5300067862ns, electionTimeout:5069ms
datanode1_1  | 2023-06-27 17:03:28,109 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: shutdown e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState
datanode1_1  | 2023-06-27 17:03:28,109 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-27 17:03:28,115 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-27 17:03:28,117 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: start e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2
datanode1_1  | 2023-06-27 17:03:28,260 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2] INFO impl.LeaderElection: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:28,558 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-06-27 17:03:28,681 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 17:03:28,703 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-27 17:03:28,748 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 3b0419d0-b93c-4a38-a8f8-ba0374c67423
datanode1_1  | 2023-06-27 17:03:28,761 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode1_1  | 2023-06-27 17:03:30,001 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-LeaderElection1] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306: set configuration 0: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:31,496 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-B9564BF6E306-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bb6b299a-fe5d-49b8-b1dc-b9564bf6e306/current/log_inprogress_0
datanode1_1  | 2023-06-27 17:03:31,691 [grpc-default-executor-0] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: receive requestVote(PRE_VOTE, d17c8765-e162-413f-ae01-cbbd58f77ae9, group-CBF4F2545280, 0, (t:0, i:0))
datanode1_1  | 2023-06-27 17:03:31,704 [grpc-default-executor-6] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: receive requestVote(ELECTION, d17c8765-e162-413f-ae01-cbbd58f77ae9, group-CBF4F2545280, 1, (t:0, i:0))
datanode1_1  | 2023-06-27 17:03:31,730 [grpc-default-executor-0] INFO impl.VoteContext: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-CANDIDATE: accept PRE_VOTE from d17c8765-e162-413f-ae01-cbbd58f77ae9: our priority 0 <= candidate's priority 1
datanode1_1  | 2023-06-27 17:03:31,763 [grpc-default-executor-0] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280 replies to PRE_VOTE vote request: d17c8765-e162-413f-ae01-cbbd58f77ae9<-e5cd4d6c-bb68-491f-b474-5dc217379157#0:OK-t0. Peer's state: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280:t0, leader=null, voted=, raftlog=Memoized:e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:31,763 [grpc-default-executor-6] INFO impl.VoteContext: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-CANDIDATE: accept ELECTION from d17c8765-e162-413f-ae01-cbbd58f77ae9: our priority 0 <= candidate's priority 1
datanode1_1  | 2023-06-27 17:03:31,930 [grpc-default-executor-6] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: changes role from CANDIDATE to FOLLOWER at term 1 for candidate:d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode1_1  | 2023-06-27 17:03:31,930 [grpc-default-executor-6] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: shutdown e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2
datanode1_1  | 2023-06-27 17:03:31,939 [grpc-default-executor-6] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: start e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FollowerState
datanode1_1  | 2023-06-27 17:03:32,042 [grpc-default-executor-6] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280 replies to ELECTION vote request: d17c8765-e162-413f-ae01-cbbd58f77ae9<-e5cd4d6c-bb68-491f-b474-5dc217379157#0:OK-t1. Peer's state: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280:t1, leader=null, voted=d17c8765-e162-413f-ae01-cbbd58f77ae9, raftlog=Memoized:e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:32,508 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2] INFO impl.LeaderElection: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2: PRE_VOTE DISCOVERED_A_NEW_TERM (term=1) received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-06-27 17:03:32,513 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2] INFO impl.LeaderElection:   Response 0: e5cd4d6c-bb68-491f-b474-5dc217379157<-3b0419d0-b93c-4a38-a8f8-ba0374c67423#0:OK-t1
datanode1_1  | 2023-06-27 17:03:32,515 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2] INFO impl.LeaderElection: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-LeaderElection2 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=1)
datanode1_1  | 2023-06-27 17:03:32,573 [grpc-default-executor-6] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: receive requestVote(PRE_VOTE, 3b0419d0-b93c-4a38-a8f8-ba0374c67423, group-CBF4F2545280, 0, (t:0, i:0))
datanode1_1  | 2023-06-27 17:03:32,614 [grpc-default-executor-6] INFO impl.VoteContext: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-FOLLOWER: accept PRE_VOTE from 3b0419d0-b93c-4a38-a8f8-ba0374c67423: our priority 0 <= candidate's priority 0
datanode1_1  | 2023-06-27 17:03:32,614 [grpc-default-executor-6] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280 replies to PRE_VOTE vote request: 3b0419d0-b93c-4a38-a8f8-ba0374c67423<-e5cd4d6c-bb68-491f-b474-5dc217379157#0:OK-t1. Peer's state: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280:t1, leader=null, voted=d17c8765-e162-413f-ae01-cbbd58f77ae9, raftlog=Memoized:e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:32,802 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-27 17:03:33,620 [e5cd4d6c-bb68-491f-b474-5dc217379157-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CBF4F2545280 with new leaderId: d17c8765-e162-413f-ae01-cbbd58f77ae9
datanode1_1  | 2023-06-27 17:03:33,627 [e5cd4d6c-bb68-491f-b474-5dc217379157-server-thread1] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: change Leader from null to d17c8765-e162-413f-ae01-cbbd58f77ae9 at term 1 for appendEntries, leader elected after 12437ms
datanode1_1  | 2023-06-27 17:03:33,637 [e5cd4d6c-bb68-491f-b474-5dc217379157-server-thread1] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280: set configuration 0: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:33,642 [e5cd4d6c-bb68-491f-b474-5dc217379157-server-thread1] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-06-27 17:03:33,651 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-CBF4F2545280-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/df7a4d8c-2676-4111-87bd-cbf4f2545280/current/log_inprogress_0
datanode1_1  | 2023-06-27 17:03:33,949 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280.
datanode1_1  | 2023-06-27 17:03:33,950 [PipelineCommandHandlerThread-0] INFO server.RaftServer: e5cd4d6c-bb68-491f-b474-5dc217379157: addNew group-5F40B2F80ADE:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER] returns group-5F40B2F80ADE:java.util.concurrent.CompletableFuture@28823fe9[Not completed]
datanode1_1  | 2023-06-27 17:03:33,976 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157: new RaftServerImpl for group-5F40B2F80ADE:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-27 17:03:33,980 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-27 17:03:33,982 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-27 17:03:33,983 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-27 17:03:33,983 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-27 17:03:33,986 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-27 17:03:33,987 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-27 17:03:33,988 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE: ConfigurationManager, init=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-27 17:03:33,996 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-27 17:03:34,008 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2023-06-27 17:03:01,044 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1        | 2023-06-27 17:03:04,533 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1        | 2023-06-27 17:03:04,654 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2023-06-27 17:03:04,655 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2023-06-27 17:03:04,924 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om2_1        | 2023-06-27 17:03:05,010 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-27 17:03:05,409 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om2_1        | 2023-06-27 17:03:05,416 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om2_1        | 2023-06-27 17:03:07,128 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om2_1        | 2023-06-27 17:03:07,841 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-27 17:03:07,841 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2023-06-27 17:03:07,943 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om2_1        | 2023-06-27 17:03:07,943 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2023-06-27 17:03:08,898 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2023-06-27 17:03:08,946 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-27 17:03:09,273 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1        | 2023-06-27 17:03:09,379 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2023-06-27 17:03:09,608 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-27 17:03:09,622 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-27 17:03:09,781 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2023-06-27 17:03:09,902 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-27 17:03:09,918 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-27 17:03:09,923 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-27 17:03:09,929 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-27 17:03:09,932 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1        | 2023-06-27 17:03:09,935 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2023-06-27 17:03:09,940 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2023-06-27 17:03:09,968 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 17:03:09,972 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2023-06-27 17:03:09,984 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-06-27 17:03:10,103 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1        | 2023-06-27 17:03:10,156 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1        | 2023-06-27 17:03:10,163 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-06-27 17:03:12,474 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1        | 2023-06-27 17:03:12,494 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1        | 2023-06-27 17:03:12,498 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1        | 2023-06-27 17:03:12,499 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-27 17:03:12,500 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-27 17:03:12,526 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-27 17:03:12,564 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@14df8bee[Not completed]
om2_1        | 2023-06-27 17:03:12,565 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1        | 2023-06-27 17:03:12,607 [main] INFO om.OzoneManager: Creating RPC Server
om2_1        | 2023-06-27 17:03:12,707 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1        | 2023-06-27 17:03:12,728 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2023-06-27 17:03:12,730 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2023-06-27 17:03:12,731 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2023-06-27 17:03:12,731 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-27 17:03:12,733 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-27 17:03:12,736 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2023-06-27 17:03:12,873 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1        | 2023-06-27 17:03:12,875 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-27 17:03:12,958 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2023-06-27 17:03:12,961 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1        | 2023-06-27 17:03:13,043 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2023-06-27 17:03:22,176 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1        | 2023-06-27 17:03:22,178 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2023-06-27 17:03:22,280 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2023-06-27 17:03:22,286 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-27 17:03:22,452 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1        | 2023-06-27 17:03:22,459 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1        | 2023-06-27 17:03:22,461 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1        | 2023-06-27 17:03:22,531 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-27 17:03:22,543 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-27 17:03:22,559 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-27 17:03:22,565 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2023-06-27 17:03:22,580 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-27 17:03:22,611 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-27 17:03:22,648 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1        | 2023-06-27 17:03:22,648 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-06-27 17:03:22,671 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2023-06-27 17:03:22,680 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2023-06-27 17:03:22,683 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2023-06-27 17:03:22,696 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2023-06-27 17:03:22,750 [main] INFO server.RaftServer: om1: start RPC server
om1_1        | 2023-06-27 17:03:23,751 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1        | 2023-06-27 17:03:23,781 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2023-06-27 17:03:23,783 [main] INFO om.OzoneManager: Starting secret key client.
om1_1        | 2023-06-27 17:03:24,440 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z).
om1_1        | 2023-06-27 17:03:24,453 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M51.183552S and interval of PT1M
om1_1        | 2023-06-27 17:03:24,473 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2023-06-27 17:03:24,474 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 385622236209
om1_1        | 2023-06-27 17:03:24,523 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2023-06-27 17:03:24,529 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2023-06-27 17:03:25,719 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1        | 2023-06-27 17:03:25,724 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om1_1        | 2023-06-27 17:03:25,725 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om1_1        | 2023-06-27 17:03:26,123 [main] INFO util.log: Logging initialized @75679ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1        | 2023-06-27 17:03:27,787 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5207329536ns, electionTimeout:5130ms
om1_1        | 2023-06-27 17:03:27,800 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-27 17:03:27,814 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1        | 2023-06-27 17:03:27,840 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1        | 2023-06-27 17:03:27,846 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1        | 2023-06-27 17:03:27,902 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-27 17:03:28,021 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1        | 2023-06-27 17:03:28,180 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1        | 2023-06-27 17:03:28,207 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1        | 2023-06-27 17:03:28,216 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        | 2023-06-27 17:03:28,220 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1        | 2023-06-27 17:03:28,288 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om1_1        | 2023-06-27 17:03:28,442 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-27 17:03:28,443 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-06-27 17:03:28,501 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1        | 2023-06-27 17:03:28,502 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1        | 2023-06-27 17:03:29,222 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om1_1        | 2023-06-27 17:03:29,326 [main] INFO http.HttpServer2: Jetty bound to port 9874
om1_1        | 2023-06-27 17:03:29,356 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om1_1        | 2023-06-27 17:03:30,011 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1        | 2023-06-27 17:03:30,025 [main] INFO server.session: No SessionScavenger set, using defaults
om1_1        | 2023-06-27 17:03:30,092 [main] INFO server.session: node0 Scavenging every 600000ms
om1_1        | 2023-06-27 17:03:30,469 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-06-27 17:03:30,530 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@15aae3b6{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1        | 2023-06-27 17:03:30,557 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@343a80d5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1        | 2023-06-27 17:03:32,230 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1        | 2023-06-27 17:03:32,811 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.555486132s. [buffered_nanos=2559436250, waiting_for_connection]
om1_1        | 2023-06-27 17:03:32,818 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
datanode3_1  | 2023-06-27 17:03:35,475 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-27 17:03:35,475 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-27 17:03:35,475 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-27 17:03:35,476 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-27 17:03:35,476 [d17c8765-e162-413f-ae01-cbbd58f77ae9@group-33B29E5FEF17-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-27 17:03:35,476 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=e50c70ba-12fb-4728-8c61-33b29e5fef17
datanode3_1  | 2023-06-27 17:03:35,476 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e50c70ba-12fb-4728-8c61-33b29e5fef17.
datanode3_1  | 2023-06-27 17:03:35,481 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d17c8765-e162-413f-ae01-cbbd58f77ae9: addNew group-5F40B2F80ADE:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER] returns group-5F40B2F80ADE:java.util.concurrent.CompletableFuture@475c6f58[Not completed]
datanode3_1  | 2023-06-27 17:03:35,484 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9: new RaftServerImpl for group-5F40B2F80ADE:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-27 17:03:35,484 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-27 17:03:35,484 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-27 17:03:35,484 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-27 17:03:35,484 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-27 17:03:35,484 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-27 17:03:35,484 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-27 17:03:35,485 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServer$Division: d17c8765-e162-413f-ae01-cbbd58f77ae9@group-5F40B2F80ADE: ConfigurationManager, init=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-27 17:03:35,485 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-27 17:03:35,485 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-27 17:03:35,485 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-27 17:03:35,487 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-27 17:03:35,487 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-27 17:03:35,532 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-27 17:03:35,532 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-27 17:03:35,556 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-27 17:03:35,565 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-27 17:03:35,565 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-27 17:03:35,565 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-27 17:03:35,565 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-27 17:03:35,565 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-27 17:03:35,565 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-27 17:03:35,567 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade does not exist. Creating ...
datanode3_1  | 2023-06-27 17:03:35,573 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade/in_use.lock acquired by nodename 7@c4c5f0c38083
datanode3_1  | 2023-06-27 17:03:35,586 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade has been successfully formatted.
datanode3_1  | 2023-06-27 17:03:35,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO ratis.ContainerStateMachine: group-5F40B2F80ADE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-27 17:03:35,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-27 17:03:35,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-27 17:03:35,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:35,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-27 17:03:35,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-27 17:03:35,598 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d17c8765-e162-413f-ae01-cbbd58f77ae9@group-5F40B2F80ADE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-27 17:03:35,599 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-27 17:03:35,600 [d17c8765-e162-413f-ae01-cbbd58f77ae9-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | Waiting for the service scm3.org:9894
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | 2023-06-27 17:03:34,011 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-27 17:03:34,013 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-27 17:03:34,047 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-27 17:03:34,056 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-27 17:03:34,057 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-27 17:03:34,058 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-27 17:03:34,092 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-27 17:03:34,129 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-27 17:03:34,143 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-27 17:03:34,147 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-27 17:03:34,147 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-27 17:03:34,148 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-27 17:03:34,150 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade does not exist. Creating ...
datanode1_1  | 2023-06-27 17:03:34,166 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade/in_use.lock acquired by nodename 7@4cab33795ed1
datanode1_1  | 2023-06-27 17:03:34,175 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade has been successfully formatted.
datanode1_1  | 2023-06-27 17:03:34,184 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO ratis.ContainerStateMachine: group-5F40B2F80ADE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-27 17:03:34,190 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-27 17:03:34,192 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-27 17:03:34,196 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:34,199 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-27 17:03:34,200 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-27 17:03:34,253 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:34,260 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-27 17:03:34,330 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-27 17:03:34,340 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:34,350 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: new e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ee002aed-0fb6-4f4a-94f8-5f40b2f80ade
datanode1_1  | 2023-06-27 17:03:34,353 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-27 17:03:34,359 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-27 17:03:34,360 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:34,361 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-27 17:03:34,366 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-27 17:03:34,366 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-27 17:03:34,367 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-27 17:03:34,371 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-27 17:03:34,376 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-27 17:03:34,380 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-27 17:03:34,448 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-27 17:03:34,462 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-27 17:03:34,469 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-27 17:03:34,475 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 17:03:34,475 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO segmented.SegmentedRaftLogWorker: e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-27 17:03:34,510 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE: start as a follower, conf=-1: peers:[e5cd4d6c-bb68-491f-b474-5dc217379157|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, 3b0419d0-b93c-4a38-a8f8-ba0374c67423|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER, d17c8765-e162-413f-ae01-cbbd58f77ae9|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-27 17:03:34,511 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServer$Division: e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-27 17:03:34,511 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO impl.RoleInfo: e5cd4d6c-bb68-491f-b474-5dc217379157: start e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE-FollowerState
datanode1_1  | 2023-06-27 17:03:34,512 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5F40B2F80ADE,id=e5cd4d6c-bb68-491f-b474-5dc217379157
datanode1_1  | 2023-06-27 17:03:34,512 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-27 17:03:34,512 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-27 17:03:34,512 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-27 17:03:34,513 [e5cd4d6c-bb68-491f-b474-5dc217379157-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-27 17:03:34,568 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-27 17:03:34,621 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ee002aed-0fb6-4f4a-94f8-5f40b2f80ade
datanode1_1  | 2023-06-27 17:03:34,661 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-27 17:03:34,705 [e5cd4d6c-bb68-491f-b474-5dc217379157@group-5F40B2F80ADE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2023-06-27 16:59:36,628 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
recon_1      | STARTUP_MSG:   java = 11.0.19
recon_1      | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1      | ************************************************************/
recon_1      | 2023-06-27 16:59:36,675 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-06-27 16:59:39,821 [main] INFO reflections.Reflections: Reflections took 325 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1      | 2023-06-27 16:59:42,974 [main] INFO reflections.Reflections: Reflections took 515 ms to scan 3 urls, producing 131 keys and 286 values 
recon_1      | 2023-06-27 16:59:43,368 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2023-06-27 16:59:43,547 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2023-06-27 16:59:44,294 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file recon.keytab. Keytab auto renewal enabled : false
recon_1      | 2023-06-27 16:59:44,294 [main] INFO recon.ReconServer: Recon login successful.
recon_1      | 2023-06-27 16:59:44,376 [main] INFO recon.ReconServer: ReconStorageConfig initialized.Initializing certificate.
recon_1      | 2023-06-27 16:59:44,376 [main] INFO recon.ReconServer: Initializing secure Recon.
recon_1      | 2023-06-27 16:59:47,060 [main] WARN security.ReconCertificateClient: Certificates could not be loaded.
recon_1      | java.nio.file.NoSuchFileException: /data/metadata/recon/certs
recon_1      | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
recon_1      | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
recon_1      | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
recon_1      | 	at java.base/java.nio.file.Files.list(Files.java:3699)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
recon_1      | 	at org.apache.hadoop.ozone.recon.security.ReconCertificateClient.<init>(ReconCertificateClient.java:62)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.initializeCertificateClient(ReconServer.java:184)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:126)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:71)
recon_1      | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
recon_1      | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
recon_1      | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
recon_1      | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
recon_1      | 	at picocli.CommandLine.execute(CommandLine.java:2078)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:94)
recon_1      | 2023-06-27 16:59:47,080 [main] ERROR security.ReconCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
recon_1      | 2023-06-27 16:59:47,081 [main] INFO security.ReconCertificateClient: Certificate client init case: 0
recon_1      | 2023-06-27 16:59:47,084 [main] INFO security.ReconCertificateClient: Creating keypair for client as keypair and certificate not found.
recon_1      | 2023-06-27 16:59:49,784 [main] INFO recon.ReconServer: Init response: GETCERT
recon_1      | 2023-06-27 16:59:49,785 [main] INFO security.ReconCertificateClient: Creating CSR for Recon.
recon_1      | 2023-06-27 16:59:49,901 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.115,host:recon
recon_1      | 2023-06-27 16:59:49,901 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
recon_1      | 2023-06-27 16:59:49,912 [main] ERROR utils.CertificateSignRequest: Invalid domain recon
recon_1      | 2023-06-27 16:59:56,562 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1      | 2023-06-27 16:59:58,563 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1      | 2023-06-27 17:00:00,565 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-27 17:00:02,567 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
recon_1      | 2023-06-27 17:00:04,569 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
recon_1      | 2023-06-27 17:00:06,571 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-27 17:00:08,576 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
recon_1      | 2023-06-27 17:00:10,577 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
recon_1      | 2023-06-27 17:00:12,579 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-27 17:00:14,754 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:a2237aa5-d04c-471b-bae4-225264529c4c is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1      | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
recon_1      | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1      | , while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
recon_1      | 2023-06-27 17:00:16,756 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
recon_1      | 2023-06-27 17:00:18,758 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
recon_1      | 2023-06-27 17:00:21,106 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 253775241875
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om2_1        | 2023-06-27 17:03:13,071 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1        | 2023-06-27 17:03:13,145 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2023-06-27 17:03:13,154 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2023-06-27 17:03:13,485 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om2_1        | 2023-06-27 17:03:14,084 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-06-27 17:03:14,129 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2023-06-27 17:03:14,133 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1        | 2023-06-27 17:03:14,147 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1        | 2023-06-27 17:03:14,149 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1        | 2023-06-27 17:03:14,154 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        | 2023-06-27 17:03:16,426 [main] INFO reflections.Reflections: Reflections took 3228 ms to scan 8 urls, producing 24 keys and 639 values [using 2 cores]
om2_1        | 2023-06-27 17:03:18,514 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1        | 2023-06-27 17:03:18,598 [main] INFO ipc.Server: Listener at om2:9862
om2_1        | 2023-06-27 17:03:18,631 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2023-06-27 17:03:29,305 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2023-06-27 17:03:29,513 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2023-06-27 17:03:29,513 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2023-06-27 17:03:30,357 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2023-06-27 17:03:30,360 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2023-06-27 17:03:30,409 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om2_1        | 2023-06-27 17:03:30,443 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om2
om2_1        | 2023-06-27 17:03:30,505 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1        | 2023-06-27 17:03:30,531 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2023-06-27 17:03:30,638 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2023-06-27 17:03:30,638 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 17:03:30,646 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1        | 2023-06-27 17:03:30,651 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1        | 2023-06-27 17:03:30,667 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-27 17:03:30,697 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2023-06-27 17:03:30,699 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2023-06-27 17:03:30,701 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 17:03:30,741 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1        | 2023-06-27 17:03:30,745 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2023-06-27 17:03:30,746 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2023-06-27 17:03:30,758 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-27 17:03:30,759 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2023-06-27 17:03:30,766 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1        | 2023-06-27 17:03:30,775 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2023-06-27 17:03:30,775 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2023-06-27 17:03:30,776 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2023-06-27 17:03:30,878 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2023-06-27 17:03:30,882 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-27 17:03:31,128 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1        | 2023-06-27 17:03:31,183 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1        | 2023-06-27 17:03:31,192 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2023-06-27 17:03:31,359 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-27 17:03:31,370 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-27 17:03:31,397 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-27 17:03:31,442 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2023-06-27 17:03:31,495 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-27 17:03:31,537 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-27 17:03:31,585 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om1_1        | 2023-06-27 17:03:32,819 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1        | 2023-06-27 17:03:32,819 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.555486132s. [buffered_nanos=2559436250, waiting_for_connection]
om1_1        | 2023-06-27 17:03:32,821 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result REJECTED
om1_1        | 2023-06-27 17:03:32,832 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
om1_1        | 2023-06-27 17:03:32,833 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
om1_1        | 2023-06-27 17:03:32,834 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-27 17:03:33,023 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-06-27 17:03:33,151 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@79c7d95a{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-11322701750572574957/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1        | 2023-06-27 17:03:33,212 [main] INFO server.AbstractConnector: Started ServerConnector@7d4d32a7{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1        | 2023-06-27 17:03:33,217 [main] INFO server.Server: Started @82772ms
om1_1        | 2023-06-27 17:03:33,234 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1        | 2023-06-27 17:03:33,235 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1        | 2023-06-27 17:03:33,241 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1        | 2023-06-27 17:03:33,241 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1        | 2023-06-27 17:03:33,274 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1        | 2023-06-27 17:03:35,273 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:32945 / 172.25.0.115:32945
om1_1        | 2023-06-27 17:03:35,489 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-06-27 17:01:28,566 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-27 17:01:28,657 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-06-27 17:01:39,350 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-27 17:01:42,375 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-27 17:01:43,441 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-27 17:01:43,458 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
om3_1        | 2023-06-27 17:01:43,459 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-27 17:01:45,004 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-27 17:01:45,006 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-27 17:01:45,087 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 17:01:47,077 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f;layoutVersion=6
om3_1        | 2023-06-27 17:01:51,486 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om3_1        | 2023-06-27 17:01:51,486 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2023-06-27 17:01:52,147 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-27 17:01:58,114 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om3_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om3_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om3_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om3_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om3_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om3_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1374)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1353)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om3_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om3_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om3_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om3_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om3_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om3_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om3_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om3_1        | 2023-06-27 17:01:58,138 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2023-06-27 17:01:58,151 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om3_1        | 2023-06-27 17:01:58,164 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2023-06-27 17:02:06,412 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2023-06-27 17:02:06,848 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2023-06-27 17:02:06,849 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2023-06-27 17:02:06,901 [main] ERROR utils.CertificateSignRequest: Invalid domain om3
om3_1        | 2023-06-27 17:02:06,909 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:a2237aa5-d04c-471b-bae4-225264529c4c,clusterId:CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f,subject:om3
om3_1        | 2023-06-27 17:02:09,149 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 253775241875
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om3_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
om3_1        |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
om3_1        |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
om3_1        |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
om3_1        |                        6031d6bbcce2d911b293b01715f60941bca01f22
om3_1        |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
om3_1        |                        e53925b79b76c1e822589654cf8f339218feaedf
om3_1        |                        29211ff344b921aef19d39fe27b1f1791e557b58
om3_1        |                        de0b8dc672e394da848c5d954638a49ff0bd8708
om3_1        |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
om3_1        |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
om3_1        |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
om3_1        |                        df6e7ecf25455532985737512133afdf952d415e
om3_1        |                        23e015a3d5c6b073bba64e6a6a58ef15
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-253775241875.crt.
om3_1        | 2023-06-27 17:02:09,206 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 386751038214
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Start Date: Tue Jun 27 17:02:07 UTC 2023
om3_1        |            Final Date: Wed Jun 26 17:02:07 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Public Key: RSA Public Key [37:a8:d0:d3:b8:6c:32:3d:03:bc:0c:f5:d7:09:00:12:a4:9d:f3:ce],[56:66:d1:a4]
om3_1        |         modulus: e28e75f018fbcbeae9271d8f3b48a32cbe945ba2a06f83e714df96de015d81859fe829de961a06a6401cf713305d1c5ad6afa37c93c5509153286e6708564fe09eb52618608484f6b39d2b0173eb093c5566513c4b0f0eabd38f8f3744880cf550969cfdc192e8e30fe1ac728bd55a1d40a08bc74d21f35d24139a6a9c1583d346f069e381d12b62d47a7eeccd9768ca07a58caee801e35a2486b1fba465e6e7f5c6573d141ad95ad1b59768d8f7c7d73b56a086828077421c9411daa7385f24e9e52ac252a773f565e8384e158de74ad2cbae6e62eb45060756c10a797d52fdba5cdfc99e813690d048db43dc5045d91a07a930e3ab3852631709eac2e018ff
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 6ac24473027720c27d6122d966da1f5092a6cbf4
om3_1        |                        dbce05789d985d182238f292d7c2873bfa10f808
om3_1        |                        86e46b87f06bca7e8e536ec296e318d94664ff59
om3_1        |                        a4454846ff823f0e6b75297661abf8024b85b5ad
om3_1        |                        1b9e14e68b78315e828a1410dae9904bbffbe6ac
om3_1        |                        d2c8e71536e6777b5a13ac6a9dbd9ff477206155
om3_1        |                        6bbe6c6eb0afbd630de5996b3c3578f125f6b0af
om3_1        |                        9ce00eed13d599368850b224b73f0797516e456c
om3_1        |                        3a5b6c037112d462e5bb2755b44841b1a04d422c
om3_1        |                        077c250e3d6c2e375ba9d6151e433ad7b4f7efed
om3_1        |                        582de475afe906bea585adec8d8ca306d2de2356
om3_1        |                        43b0dab33421252ee4e4d3c3210a4d48b6c738f6
recon_1      |            Start Date: Tue Jun 27 16:59:54 UTC 2023
recon_1      |            Final Date: Fri Aug 04 16:59:54 UTC 2028
recon_1      |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
recon_1      |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
recon_1      |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
recon_1      |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
recon_1      |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
recon_1      |                        6031d6bbcce2d911b293b01715f60941bca01f22
recon_1      |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
recon_1      |                        e53925b79b76c1e822589654cf8f339218feaedf
recon_1      |                        29211ff344b921aef19d39fe27b1f1791e557b58
recon_1      |                        de0b8dc672e394da848c5d954638a49ff0bd8708
recon_1      |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
recon_1      |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
recon_1      |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
recon_1      |                        df6e7ecf25455532985737512133afdf952d415e
recon_1      |                        23e015a3d5c6b073bba64e6a6a58ef15
recon_1      |        Extensions: 
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0xbe
recon_1      |  from file: /data/metadata/recon/certs/CA-253775241875.crt.
recon_1      | 2023-06-27 17:00:21,121 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 279570072952
om2_1        | 2023-06-27 17:03:31,629 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-27 17:03:31,673 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2023-06-27 17:03:31,700 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2023-06-27 17:03:31,741 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2023-06-27 17:03:31,778 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1        | 2023-06-27 17:03:31,894 [main] INFO server.RaftServer: om2: start RPC server
om2_1        | 2023-06-27 17:03:32,642 [main] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2023-06-27 17:03:32,663 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2023-06-27 17:03:32,667 [main] INFO om.OzoneManager: Starting secret key client.
om2_1        | 2023-06-27 17:03:33,574 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z).
om2_1        | 2023-06-27 17:03:33,584 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M42.055893S and interval of PT1M
om2_1        | 2023-06-27 17:03:33,594 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2023-06-27 17:03:33,597 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 389874921037
om2_1        | 2023-06-27 17:03:33,619 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2023-06-27 17:03:33,633 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        |                        a1dc14b5133099ea9542441d8e76e9f3
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/386751038214.crt.
om3_1        | 2023-06-27 17:02:09,245 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om3_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
om3_1        |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
om3_1        |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
om3_1        |                        dcfc06fd580359247f53abb2878410a9ef225129
om3_1        |                        820a87eb87b24799aec84587b5f9d56ce0e93904
om3_1        |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
om3_1        |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-27 16:59:58,140 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-27 16:59:58,158 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-27 16:59:58,282 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 16:59:58,455 [main] INFO reflections.Reflections: Reflections took 135 ms to scan 3 urls, producing 131 keys and 286 values 
scm2.org_1   | 2023-06-27 16:59:58,588 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-27 16:59:58,588 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-27 16:59:58,614 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-27 16:59:58,614 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-27 16:59:58,782 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-27 16:59:58,782 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-27 16:59:58,872 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-27 17:00:03,067 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
scm2.org_1   | 2023-06-27 17:00:05,069 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
scm2.org_1   | 2023-06-27 17:00:07,071 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm2.org_1   | 2023-06-27 17:00:09,079 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
scm2.org_1   | 2023-06-27 17:00:11,871 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:a2237aa5-d04c-471b-bae4-225264529c4c is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
scm2.org_1   | 2023-06-27 17:00:13,873 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
scm2.org_1   | 2023-06-27 17:00:16,026 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm2.org_1   | 2023-06-27 17:00:16,783 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-27 17:00:16,786 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-27 17:00:16,787 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2023-06-27 17:00:16,788 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2023-06-27 17:00:18,058 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm2.org_1   | 2023-06-27 17:00:18,094 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2023-06-27 17:00:18,094 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2023-06-27 17:00:18,097 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:5a021426-02fe-4be7-b9ad-23a479d4b0f6,clusterId:CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f,subject:scm-sub@scm2.org
scm2.org_1   | 2023-06-27 17:00:19,230 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2023-06-27 17:00:19,254 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f, SCMID 5a021426-02fe-4be7-b9ad-23a479d4b0f6
scm2.org_1   | 2023-06-27 17:00:19,254 [main] INFO server.StorageContainerManager: Primary SCM Node ID a2237aa5-d04c-471b-bae4-225264529c4c
scm2.org_1   | 2023-06-27 17:00:19,319 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-27 17:00:24,137 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-27 16:59:41,800 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-27 16:59:41,879 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-06-27 16:59:42,762 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 16:59:44,399 [main] INFO reflections.Reflections: Reflections took 1160 ms to scan 3 urls, producing 131 keys and 286 values 
scm1.org_1   | 2023-06-27 16:59:44,956 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-27 16:59:45,040 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-27 16:59:45,211 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-27 16:59:45,214 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-27 16:59:45,276 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2023-06-27 16:59:50,693 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-27 16:59:50,705 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-27 16:59:50,707 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2023-06-27 16:59:50,707 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2023-06-27 16:59:54,176 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2023-06-27 16:59:54,647 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-27 16:59:54,647 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
om3_1        |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
om3_1        |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
om3_1        |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
om3_1        |                        862deee8bb1ba8320f84b774cf369b4523011f63
om3_1        |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
om3_1        |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-27 17:02:09,306 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797752 ms and interval 86400000 ms.
om3_1        | 2023-06-27 17:02:09,310 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om3_1        | 2023-06-27 17:02:09,412 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
recon_1      |            Start Date: Tue Jun 27 17:00:20 UTC 2023
recon_1      |            Final Date: Wed Jun 26 17:00:20 UTC 2024
recon_1      |             SubjectDN: CN=recon@recon,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
recon_1      |            Public Key: RSA Public Key [26:7a:2c:46:b3:a8:6e:53:c7:69:8d:17:0a:14:34:91:97:f7:08:82],[56:66:d1:a4]
recon_1      |         modulus: c76cb9f4a060bff7d0d03dceae0850a4511e28e24d7fce0096b7853e577ce335983e602c7657cfcae3b5b4f4b5f2db2362a78823a5d5067e69c3c439b46d5a77ad4cdf64fad97860fc576bf231362ddb73bfe5effffec86284fae19a003c79e433725a14cda8163fb8c641896690b0f0cdbf9ab601eec5caf0c48412166daca3b2ec559d9e5be453a598a97b714e9fbe013924008b9954d9f8489c7fe157ef093281e9cdecf4241f3a0fffcae0a3f8cd1f1b260b9c3c380e76edad967e463ed1b408fdcb19a7367feb54b9c415d4258a9cd64031e9eb5a27dc1a4878ee4633031a2ba86640bdd30b7c9f3ae0596d2eef129378bcd349648412f8c7ee8f7a21c7
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 6188dac4bfcce5758818a3cb376e135611bcee50
recon_1      |                        33a8a513f5adcabee3d63419cccd8a4d09127780
recon_1      |                        bec63586889fc631025e3399bb1ced3fa255ef64
recon_1      |                        039c4831eab069eb1c9e8164ed17cad0a64943f2
recon_1      |                        da6504b69abc0fe49e0e1191c412e4e7740b0723
recon_1      |                        fc8e7ceb557e98aa6063891ccd516190b6862394
recon_1      |                        6d255284ca711a558bcaabd8983493e9097d41f5
recon_1      |                        3d041a4b8fc0d90f19d16551644fff3eccade489
recon_1      |                        5a403d6f3269cd7ef62574f9e740a6aed478d5c9
recon_1      |                        159fb2a300e7d1f76e73b174c2ae83d770abec8a
recon_1      |                        e11adc1b5fa2d994322f6bb04e3a1dc21b595fb1
recon_1      |                        48db591d3808b86344ab63a36c2d21d078a28bd8
recon_1      |                        5ae7480955295f6b093cd7a33c8459f2
recon_1      |        Extensions: 
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      | 
recon_1      |                        critical(true) KeyUsage: 0xb8
recon_1      |  from file: /data/metadata/recon/certs/279570072952.crt.
recon_1      | 2023-06-27 17:00:21,128 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 1
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
recon_1      |            Start Date: Tue Jun 27 16:59:54 UTC 2023
recon_1      |            Final Date: Fri Aug 04 16:59:54 UTC 2028
recon_1      |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
recon_1      |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
recon_1      |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
recon_1      |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
recon_1      |                        dcfc06fd580359247f53abb2878410a9ef225129
recon_1      |                        820a87eb87b24799aec84587b5f9d56ce0e93904
recon_1      |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
recon_1      |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
recon_1      |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
recon_1      |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
recon_1      |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
recon_1      |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
recon_1      |                        862deee8bb1ba8320f84b774cf369b4523011f63
recon_1      |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
recon_1      |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
recon_1      |        Extensions: 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0x6
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
recon_1      |  from file: /data/metadata/recon/certs/ROOTCA-1.crt.
recon_1      | 2023-06-27 17:00:21,142 [main] INFO security.ReconCertificateClient: CertificateLifetimeMonitor for recon is started with first delay 29116798868 ms and interval 86400000 ms.
recon_1      | 2023-06-27 17:00:21,146 [main] INFO recon.ReconServer: Successfully stored SCM signed certificate, case:GETCERT.
recon_1      | 2023-06-27 17:00:21,906 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-27 17:00:25,064 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2023-06-27 17:00:26,094 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1      | 2023-06-27 17:00:26,103 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.008 seconds to initialized 0 records to KEY_CONTAINER table
recon_1      | 2023-06-27 17:00:26,242 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-27 17:00:26,275 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2023-06-27 17:00:26,276 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2023-06-27 17:00:28,448 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2023-06-27 17:00:28,451 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2023-06-27 17:00:28,451 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
recon_1      | 2023-06-27 17:00:28,510 [main] INFO util.log: Logging initialized @59823ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2023-06-27 17:00:29,077 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1      | 2023-06-27 17:00:29,107 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-06-27 17:02:25,243 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-27 17:02:25,367 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-06-27 17:02:36,008 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-27 17:02:41,160 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-27 17:02:42,171 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-27 17:02:42,181 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
om3_1        | 2023-06-27 17:02:42,187 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-27 17:02:42,327 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 17:02:43,190 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om3_1        | 2023-06-27 17:02:46,502 [main] INFO reflections.Reflections: Reflections took 2797 ms to scan 1 urls, producing 138 keys and 396 values [using 2 cores]
om3_1        | 2023-06-27 17:02:46,762 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om3_1        | 2023-06-27 17:02:48,803 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-27 17:02:48,803 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-27 17:02:48,807 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 17:02:52,539 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-27 17:02:53,161 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-27 17:02:57,504 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-27 17:02:59,017 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 253775241875
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om3_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
om3_1        |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
om3_1        |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
om3_1        |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
om3_1        |                        6031d6bbcce2d911b293b01715f60941bca01f22
om3_1        |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
om3_1        |                        e53925b79b76c1e822589654cf8f339218feaedf
om3_1        |                        29211ff344b921aef19d39fe27b1f1791e557b58
om3_1        |                        de0b8dc672e394da848c5d954638a49ff0bd8708
om3_1        |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
om3_1        |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
om3_1        |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
om3_1        |                        df6e7ecf25455532985737512133afdf952d415e
om3_1        |                        23e015a3d5c6b073bba64e6a6a58ef15
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-253775241875.crt.
om3_1        | 2023-06-27 17:02:59,084 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 386751038214
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Start Date: Tue Jun 27 17:02:07 UTC 2023
om3_1        |            Final Date: Wed Jun 26 17:02:07 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Public Key: RSA Public Key [37:a8:d0:d3:b8:6c:32:3d:03:bc:0c:f5:d7:09:00:12:a4:9d:f3:ce],[56:66:d1:a4]
om3_1        |         modulus: e28e75f018fbcbeae9271d8f3b48a32cbe945ba2a06f83e714df96de015d81859fe829de961a06a6401cf713305d1c5ad6afa37c93c5509153286e6708564fe09eb52618608484f6b39d2b0173eb093c5566513c4b0f0eabd38f8f3744880cf550969cfdc192e8e30fe1ac728bd55a1d40a08bc74d21f35d24139a6a9c1583d346f069e381d12b62d47a7eeccd9768ca07a58caee801e35a2486b1fba465e6e7f5c6573d141ad95ad1b59768d8f7c7d73b56a086828077421c9411daa7385f24e9e52ac252a773f565e8384e158de74ad2cbae6e62eb45060756c10a797d52fdba5cdfc99e813690d048db43dc5045d91a07a930e3ab3852631709eac2e018ff
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 6ac24473027720c27d6122d966da1f5092a6cbf4
om3_1        |                        dbce05789d985d182238f292d7c2873bfa10f808
om3_1        |                        86e46b87f06bca7e8e536ec296e318d94664ff59
om3_1        |                        a4454846ff823f0e6b75297661abf8024b85b5ad
om3_1        |                        1b9e14e68b78315e828a1410dae9904bbffbe6ac
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2023-06-27 16:59:37,025 [main] INFO security.UserGroupInformation: Login successful for user s3g/s3g@EXAMPLE.COM using keytab file s3g.keytab. Keytab auto renewal enabled : false
s3g_1        | 2023-06-27 16:59:37,036 [main] INFO s3.Gateway: S3Gateway login successful.
s3g_1        | 2023-06-27 16:59:37,848 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2023-06-27 16:59:37,849 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2023-06-27 16:59:37,849 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2023-06-27 16:59:38,123 [main] INFO util.log: Logging initialized @9803ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2023-06-27 16:59:38,930 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2023-06-27 16:59:38,998 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2023-06-27 16:59:39,026 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2023-06-27 16:59:39,026 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2023-06-27 16:59:39,026 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2023-06-27 16:59:39,029 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2023-06-27 16:59:39,260 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir16126954684129977797
s3g_1        | 2023-06-27 16:59:39,871 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-27 17:00:47,940 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-27 17:00:47,992 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-06-27 17:00:48,175 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 17:00:48,724 [main] INFO reflections.Reflections: Reflections took 407 ms to scan 3 urls, producing 131 keys and 286 values 
scm3.org_1   | 2023-06-27 17:00:49,019 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-06-27 17:00:49,020 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-27 17:00:49,106 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-27 17:00:49,110 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-27 17:00:49,626 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-27 17:00:49,629 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-27 17:00:49,939 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-27 17:00:50,800 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2023-06-27 17:00:52,290 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-27 17:00:52,297 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-27 17:00:52,300 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm3.org_1   | 2023-06-27 17:00:52,302 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:35Z
s3g_1        | STARTUP_MSG:   java = 11.0.19
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-27 17:00:24,178 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-27 17:00:24,420 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 17:00:24,991 [main] INFO reflections.Reflections: Reflections took 387 ms to scan 3 urls, producing 131 keys and 286 values 
scm2.org_1   | 2023-06-27 17:00:25,309 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-27 17:00:25,336 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-27 17:00:25,453 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-27 17:00:25,477 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-27 17:00:25,992 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-27 17:00:25,993 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-27 17:00:28,734 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 1
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm2.org_1   |            Start Date: Tue Jun 27 16:59:54 UTC 2023
scm2.org_1   |            Final Date: Fri Aug 04 16:59:54 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm2.org_1   |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
scm2.org_1   |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
scm2.org_1   |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
scm2.org_1   |                        dcfc06fd580359247f53abb2878410a9ef225129
scm2.org_1   |                        820a87eb87b24799aec84587b5f9d56ce0e93904
scm2.org_1   |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
scm2.org_1   |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
scm2.org_1   |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
scm2.org_1   |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
scm2.org_1   |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
scm2.org_1   |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
scm2.org_1   |                        862deee8bb1ba8320f84b774cf369b4523011f63
scm2.org_1   |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
scm2.org_1   |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0x6
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2023-06-27 17:00:28,749 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 277125577290
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm2.org_1   |            Start Date: Tue Jun 27 17:00:18 UTC 2023
scm2.org_1   |            Final Date: Fri Aug 04 17:00:18 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=5a021426-02fe-4be7-b9ad-23a479d4b0f6,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm2.org_1   |            Public Key: RSA Public Key [d8:0e:e1:bd:78:f1:f0:25:57:2d:42:74:8e:e7:3e:78:ff:31:af:95],[56:66:d1:a4]
scm2.org_1   |         modulus: b8ec1946263b0b70bc4a3c8d4ff74fcbae80c09591f18c6f556d4ceb04c1f88fbdad33c44fe550fb74514f0a19c9b4bbca556ff5e632c4db9c736304cfb33a98ee1171265162e6ff26dc14c7d48e10dd98a9c802b7b586309359e573b55e640cd24b564b311cdead6f278c3e2287b705aae9c3fe8c24c37748f2716f23cc38bc7e04c70d4ce58561ce0c9d034344cb05a21cfc123ea05ea0f3f24a4b0e0891c0dd70a9dedc24d09484209022783e51c54d175b32610e6f4b5b1299f1ae6d0a5489c8925edacfd10fa4aed005a148231c3e5a8bd162e17b9de2c1ca85b50a5dd83d7840ebd239851a09f20b972e576a7f49665c3ee1e6e8b1ac2ab0e590333157
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 62f94364abd9b571bf90ede41f50c94d5d2bd589
scm2.org_1   |                        9111d4cb3a73d1a0ff42244ae0a257d1da173c8c
scm2.org_1   |                        3e9bc582f2c7468caf07b35d4010495ef4bffd04
scm2.org_1   |                        baff67d4d1945adcabd7743207a768e7f250c4f2
scm2.org_1   |                        5a1f8388a61e1e5e9f4565fda407d80ab040957e
scm2.org_1   |                        fe6c9616390e1fbfe2e03570faea53e549b8340f
scm2.org_1   |                        b9262770743279cefe76410faf2ed156df3609f5
scm2.org_1   |                        9fe17d3cfd8f124dac669e627d2389aaa169d849
scm2.org_1   |                        43fcf32abc3c8096f2067770404e9c65329e39d0
scm2.org_1   |                        e92e581767c5cdce9414e183811b2cfb942c9d2f
scm2.org_1   |                        6787cb4267ca91127798ad6cda77022c64a1a0bf
scm2.org_1   |                        11fc0a8c8d4ca5d9c84c47cc1b5528450b851894
scm2.org_1   |                        dda5d08a8af7c328d4bfcc034b9edd7c
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2023-06-27 17:00:28,761 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 277125577290
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm2.org_1   |            Start Date: Tue Jun 27 17:00:18 UTC 2023
scm2.org_1   |            Final Date: Fri Aug 04 17:00:18 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=5a021426-02fe-4be7-b9ad-23a479d4b0f6,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm2.org_1   |            Public Key: RSA Public Key [d8:0e:e1:bd:78:f1:f0:25:57:2d:42:74:8e:e7:3e:78:ff:31:af:95],[56:66:d1:a4]
scm2.org_1   |         modulus: b8ec1946263b0b70bc4a3c8d4ff74fcbae80c09591f18c6f556d4ceb04c1f88fbdad33c44fe550fb74514f0a19c9b4bbca556ff5e632c4db9c736304cfb33a98ee1171265162e6ff26dc14c7d48e10dd98a9c802b7b586309359e573b55e640cd24b564b311cdead6f278c3e2287b705aae9c3fe8c24c37748f2716f23cc38bc7e04c70d4ce58561ce0c9d034344cb05a21cfc123ea05ea0f3f24a4b0e0891c0dd70a9dedc24d09484209022783e51c54d175b32610e6f4b5b1299f1ae6d0a5489c8925edacfd10fa4aed005a148231c3e5a8bd162e17b9de2c1ca85b50a5dd83d7840ebd239851a09f20b972e576a7f49665c3ee1e6e8b1ac2ab0e590333157
s3g_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir16126954684129977797, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1        | ************************************************************/
s3g_1        | 2023-06-27 16:59:39,929 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2023-06-27 16:59:40,029 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2023-06-27 16:59:40,649 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1        | 2023-06-27 16:59:41,472 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1        | 2023-06-27 16:59:41,473 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1        | 2023-06-27 16:59:41,708 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1        | 2023-06-27 16:59:41,727 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2023-06-27 16:59:41,732 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1        | 2023-06-27 16:59:41,867 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2023-06-27 16:59:41,870 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2023-06-27 16:59:41,873 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1        | 2023-06-27 16:59:41,955 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-27 16:59:42,017 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3fcdcf{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2023-06-27 16:59:42,042 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@36b6964d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | 2023-06-27 16:59:49,462 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-27 16:59:52,176 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6ca33187{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir16126954684129977797/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-8236107488932391937/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2023-06-27 16:59:52,205 [main] INFO server.AbstractConnector: Started ServerConnector@3359c978{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2023-06-27 16:59:52,213 [main] INFO server.Server: Started @23886ms
s3g_1        | 2023-06-27 16:59:52,229 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1        | 2023-06-27 16:59:52,229 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1        | 2023-06-27 16:59:52,231 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1      | 2023-06-27 17:00:29,110 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2023-06-27 17:00:29,110 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 2023-06-27 17:00:29,111 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2023-06-27 17:00:29,120 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2023-06-27 17:00:29,260 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1      | 2023-06-27 17:00:29,276 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1      | 2023-06-27 17:00:29,882 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2023-06-27 17:00:29,904 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1      | 2023-06-27 17:00:29,916 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1      | 2023-06-27 17:00:30,019 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1      | 2023-06-27 17:00:31,851 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 17:00:32,543 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 17:00:32,814 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2023-06-27 17:00:32,839 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2023-06-27 17:00:33,147 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 17:00:33,538 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1      | 2023-06-27 17:00:33,610 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2023-06-27 17:00:33,764 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2023-06-27 17:00:33,791 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2023-06-27 17:00:33,835 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1      | 2023-06-27 17:00:34,891 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1      | 2023-06-27 17:00:35,017 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2023-06-27 17:00:35,041 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1      | 2023-06-27 17:00:35,064 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2023-06-27 17:00:35,247 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1      | 2023-06-27 17:00:35,609 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1      | 2023-06-27 17:00:35,618 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2023-06-27 17:00:35,618 [main] INFO recon.ReconServer: Starting Recon server
recon_1      | 2023-06-27 17:00:35,939 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2023-06-27 17:00:35,980 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2023-06-27 17:00:35,980 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2023-06-27 17:00:37,171 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2023-06-27 17:00:37,186 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1      | 2023-06-27 17:00:37,292 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2023-06-27 17:00:37,294 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2023-06-27 17:00:37,299 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1      | 2023-06-27 17:00:37,400 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-27 17:00:37,408 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2100d881{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1      | 2023-06-27 17:00:37,412 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@49de2cc4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1      | 2023-06-27 17:00:38,911 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-27 17:00:38,963 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-27 17:00:44,839 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@81ebdb0{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-9517501674976802651/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1      | 2023-06-27 17:00:44,944 [main] INFO server.AbstractConnector: Started ServerConnector@f7e004d{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1      | 2023-06-27 17:00:44,944 [main] INFO server.Server: Started @76257ms
recon_1      | 2023-06-27 17:00:44,947 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1      | 2023-06-27 17:00:44,947 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1      | 2023-06-27 17:00:44,969 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1      | 2023-06-27 17:00:44,970 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1      | 2023-06-27 17:00:45,032 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1      | 2023-06-27 17:00:45,089 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1      | 2023-06-27 17:00:45,095 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1      | 2023-06-27 17:00:45,095 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-27 17:00:45,096 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1      | 2023-06-27 17:00:45,105 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1      | 2023-06-27 17:00:46,790 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1      | 2023-06-27 17:00:46,797 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2023-06-27 17:00:46,797 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1      | 2023-06-27 17:00:46,799 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1      | 2023-06-27 17:00:46,839 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1      | 2023-06-27 17:00:46,847 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1      | 2023-06-27 17:01:05,102 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1      | 2023-06-27 17:01:05,103 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1      | 2023-06-27 17:01:07,315 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately. Current retry count: 3.
recon_1      | 2023-06-27 17:01:07,316 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately. Current retry count: 4.
recon_1      | 2023-06-27 17:01:07,317 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-27 17:01:09,320 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately. Current retry count: 6.
recon_1      | 2023-06-27 17:01:09,321 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately. Current retry count: 7.
recon_1      | 2023-06-27 17:01:09,322 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-27 17:01:11,323 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately. Current retry count: 9.
recon_1      | 2023-06-27 17:01:11,324 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately. Current retry count: 10.
recon_1      | 2023-06-27 17:01:11,325 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-27 17:01:13,327 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately. Current retry count: 12.
recon_1      | 2023-06-27 17:01:13,328 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately. Current retry count: 13.
recon_1      | 2023-06-27 17:01:13,328 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
recon_1      | 2023-06-27 17:01:15,331 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately. Current retry count: 15.
recon_1      | 2023-06-27 17:01:15,338 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately. Current retry count: 16.
om3_1        |                        d2c8e71536e6777b5a13ac6a9dbd9ff477206155
om3_1        |                        6bbe6c6eb0afbd630de5996b3c3578f125f6b0af
om3_1        |                        9ce00eed13d599368850b224b73f0797516e456c
om3_1        |                        3a5b6c037112d462e5bb2755b44841b1a04d422c
om3_1        |                        077c250e3d6c2e375ba9d6151e433ad7b4f7efed
om3_1        |                        582de475afe906bea585adec8d8ca306d2de2356
om3_1        |                        43b0dab33421252ee4e4d3c3210a4d48b6c738f6
om3_1        |                        a1dc14b5133099ea9542441d8e76e9f3
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/386751038214.crt.
om3_1        | 2023-06-27 17:02:59,111 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Start Date: Tue Jun 27 16:59:54 UTC 2023
om3_1        |            Final Date: Fri Aug 04 16:59:54 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
om3_1        |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
om3_1        |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
om3_1        |                        dcfc06fd580359247f53abb2878410a9ef225129
om3_1        |                        820a87eb87b24799aec84587b5f9d56ce0e93904
om3_1        |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
om3_1        |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
om3_1        |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
om3_1        |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
om3_1        |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
om3_1        |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
om3_1        |                        862deee8bb1ba8320f84b774cf369b4523011f63
om3_1        |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
om3_1        |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-27 17:02:59,165 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116747869 ms and interval 86400000 ms.
om3_1        | 2023-06-27 17:02:59,606 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
scm1.org_1   | 2023-06-27 16:59:54,798 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f to CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f, valid from Tue Jun 27 16:59:54 UTC 2023 to Fri Aug 04 16:59:54 UTC 2028
scm1.org_1   | 2023-06-27 16:59:54,869 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-27 16:59:54,869 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-06-27 16:59:54,875 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:a2237aa5-d04c-471b-bae4-225264529c4c,clusterId:CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f,subject:scm-sub@scm1.org
scm1.org_1   | 2023-06-27 16:59:55,057 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2023-06-27 16:59:55,164 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-27 16:59:55,275 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 16:59:55,277 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-27 16:59:55,280 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 16:59:55,281 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-27 16:59:55,281 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-06-27 16:59:55,282 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-27 16:59:55,287 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-06-27 16:59:55,289 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 16:59:55,289 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-06-27 16:59:55,290 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 16:59:55,313 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 16:59:55,319 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-27 16:59:55,322 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-27 16:59:55,587 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-27 16:59:55,591 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-27 16:59:55,591 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-27 16:59:55,592 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-27 16:59:55,592 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 16:59:55,599 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 16:59:55,607 [main] INFO server.RaftServer: a2237aa5-d04c-471b-bae4-225264529c4c: addNew group-BF8E9CD9096F:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] returns group-BF8E9CD9096F:java.util.concurrent.CompletableFuture@50bf795f[Not completed]
scm1.org_1   | 2023-06-27 16:59:55,634 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c: new RaftServerImpl for group-BF8E9CD9096F:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-27 16:59:55,636 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-27 16:59:55,637 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-27 16:59:55,637 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-27 16:59:55,638 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-27 16:59:55,638 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 16:59:55,639 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-27 16:59:55,647 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: ConfigurationManager, init=-1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-27 16:59:55,648 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 16:59:55,652 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-06-27 16:59:55,654 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-27 16:59:55,681 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-27 16:59:55,685 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-27 16:59:55,690 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-27 16:59:55,690 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-27 16:59:55,710 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-27 16:59:55,739 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-27 16:59:55,849 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 16:59:55,852 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 16:59:55,853 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-27 16:59:55,854 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-27 16:59:55,854 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-27 16:59:55,855 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-06-27 16:59:55,856 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f does not exist. Creating ...
scm1.org_1   | 2023-06-27 16:59:55,863 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/in_use.lock acquired by nodename 14@scm1.org
scm1.org_1   | 2023-06-27 16:59:55,876 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f has been successfully formatted.
scm1.org_1   | 2023-06-27 16:59:55,879 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-27 16:59:55,887 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-27 16:59:55,887 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 16:59:55,889 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-27 16:59:55,889 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-27 16:59:55,891 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 16:59:55,896 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-27 16:59:55,897 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-27 16:59:55,897 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 16:59:55,901 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   | 2023-06-27 16:59:55,902 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 16:59:55,902 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 16:59:55,903 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 16:59:55,903 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-27 16:59:55,904 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-27 16:59:55,904 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-27 16:59:55,904 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-27 16:59:55,905 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-27 16:59:55,925 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-27 16:59:55,925 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 16:59:55,947 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 16:59:55,947 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 16:59:55,948 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-06-27 16:59:55,968 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-27 16:59:55,968 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-27 16:59:55,970 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: start as a follower, conf=-1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 16:59:55,970 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2023-06-27 16:59:55,971 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: start a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState
scm1.org_1   | 2023-06-27 16:59:55,975 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-27 16:59:55,976 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-27 16:59:55,978 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF8E9CD9096F,id=a2237aa5-d04c-471b-bae4-225264529c4c
scm1.org_1   | 2023-06-27 16:59:55,980 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-27 16:59:55,981 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-27 16:59:55,981 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-27 16:59:55,981 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-27 16:59:56,000 [main] INFO server.RaftServer: a2237aa5-d04c-471b-bae4-225264529c4c: start RPC server
scm1.org_1   | 2023-06-27 16:59:56,048 [main] INFO server.GrpcService: a2237aa5-d04c-471b-bae4-225264529c4c: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-27 16:59:56,051 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a2237aa5-d04c-471b-bae4-225264529c4c: Started
scm1.org_1   | 2023-06-27 17:00:01,098 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO impl.FollowerState: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5126976703ns, electionTimeout:5121ms
scm1.org_1   | 2023-06-27 17:00:01,099 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: shutdown a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState
scm1.org_1   | 2023-06-27 17:00:01,100 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2023-06-27 17:00:01,103 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-27 17:00:01,103 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: start a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1
scm1.org_1   | 2023-06-27 17:00:01,108 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:01,110 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm1.org_1   | 2023-06-27 17:00:01,112 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:01,112 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-27 17:00:01,112 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: shutdown a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1
scm1.org_1   | 2023-06-27 17:00:01,112 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1.org_1   | 2023-06-27 17:00:01,113 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: change Leader from null to a2237aa5-d04c-471b-bae4-225264529c4c at term 1 for becomeLeader, leader elected after 5432ms
scm1.org_1   | 2023-06-27 17:00:01,119 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-27 17:00:01,149 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 17:00:01,150 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 17:00:01,159 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-27 17:00:01,160 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-27 17:00:01,160 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-27 17:00:01,170 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 17:00:01,178 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-27 17:00:01,185 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: start a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl
scm1.org_1   | 2023-06-27 17:00:01,208 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2023-06-27 17:00:01,245 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:01,307 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_0
scm1.org_1   | 2023-06-27 17:00:02,053 [main] INFO server.RaftServer: a2237aa5-d04c-471b-bae4-225264529c4c: close
scm1.org_1   | 2023-06-27 17:00:02,054 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: shutdown
scm1.org_1   | 2023-06-27 17:00:02,055 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF8E9CD9096F,id=a2237aa5-d04c-471b-bae4-225264529c4c
scm1.org_1   | 2023-06-27 17:00:02,055 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: shutdown a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl
scm1.org_1   | 2023-06-27 17:00:02,061 [main] INFO server.GrpcService: a2237aa5-d04c-471b-bae4-225264529c4c: shutdown server GrpcServerProtocolService now
scm1.org_1   | 2023-06-27 17:00:02,073 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO impl.PendingRequests: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2023-06-27 17:00:02,090 [main] INFO server.GrpcService: a2237aa5-d04c-471b-bae4-225264529c4c: shutdown server GrpcServerProtocolService successfully
scm1.org_1   | 2023-06-27 17:00:02,091 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO impl.StateMachineUpdater: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2023-06-27 17:00:02,092 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO impl.StateMachineUpdater: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2023-06-27 17:00:02,092 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO impl.StateMachineUpdater: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2023-06-27 17:00:02,095 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: closes. applyIndex: 0
scm1.org_1   | 2023-06-27 17:00:02,315 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker close()
scm1.org_1   | 2023-06-27 17:00:02,316 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a2237aa5-d04c-471b-bae4-225264529c4c: Stopped
scm1.org_1   | 2023-06-27 17:00:02,316 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 17:00:02,319 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f; layoutVersion=7; scmId=a2237aa5-d04c-471b-bae4-225264529c4c
scm1.org_1   | 2023-06-27 17:00:02,353 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-27 17:00:04,566 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-27 17:00:04,577 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 62f94364abd9b571bf90ede41f50c94d5d2bd589
scm2.org_1   |                        9111d4cb3a73d1a0ff42244ae0a257d1da173c8c
scm2.org_1   |                        3e9bc582f2c7468caf07b35d4010495ef4bffd04
scm2.org_1   |                        baff67d4d1945adcabd7743207a768e7f250c4f2
scm2.org_1   |                        5a1f8388a61e1e5e9f4565fda407d80ab040957e
scm2.org_1   |                        fe6c9616390e1fbfe2e03570faea53e549b8340f
scm2.org_1   |                        b9262770743279cefe76410faf2ed156df3609f5
scm2.org_1   |                        9fe17d3cfd8f124dac669e627d2389aaa169d849
scm2.org_1   |                        43fcf32abc3c8096f2067770404e9c65329e39d0
scm2.org_1   |                        e92e581767c5cdce9414e183811b2cfb942c9d2f
scm2.org_1   |                        6787cb4267ca91127798ad6cda77022c64a1a0bf
scm2.org_1   |                        11fc0a8c8d4ca5d9c84c47cc1b5528450b851894
scm2.org_1   |                        dda5d08a8af7c328d4bfcc034b9edd7c
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/277125577290.crt.
scm2.org_1   | 2023-06-27 17:00:28,776 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716789235 ms and interval 86400000 ms.
scm2.org_1   | 2023-06-27 17:00:29,044 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 17:00:29,703 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-27 17:00:30,419 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2023-06-27 17:00:30,420 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2023-06-27 17:00:30,615 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2023-06-27 17:00:31,183 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:5a021426-02fe-4be7-b9ad-23a479d4b0f6
scm2.org_1   | 2023-06-27 17:00:31,308 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-27 17:00:31,327 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-27 17:00:31,474 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2023-06-27 17:00:31,515 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-27 17:00:31,526 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-27 17:00:31,531 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-27 17:00:31,531 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-27 17:00:31,533 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2.org_1   | 2023-06-27 17:00:31,533 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2023-06-27 17:00:31,534 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2023-06-27 17:00:31,540 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 17:00:31,543 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2023-06-27 17:00:31,543 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-27 17:00:31,587 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2.org_1   | 2023-06-27 17:00:31,613 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2.org_1   | 2023-06-27 17:00:31,619 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-06-27 17:00:32,573 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2.org_1   | 2023-06-27 17:00:32,592 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2.org_1   | 2023-06-27 17:00:32,593 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2.org_1   | 2023-06-27 17:00:32,598 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-27 17:00:32,598 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-27 17:00:32,604 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-27 17:00:32,636 [main] INFO server.RaftServer: 5a021426-02fe-4be7-b9ad-23a479d4b0f6: addNew group-BF8E9CD9096F:[] returns group-BF8E9CD9096F:java.util.concurrent.CompletableFuture@7455204c[Not completed]
scm2.org_1   | 2023-06-27 17:00:32,756 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6: new RaftServerImpl for group-BF8E9CD9096F:[] with SCMStateMachine:uninitialized
scm2.org_1   | 2023-06-27 17:00:32,764 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2.org_1   | 2023-06-27 17:00:32,764 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2.org_1   | 2023-06-27 17:00:32,765 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2.org_1   | 2023-06-27 17:00:32,773 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-27 17:00:32,774 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-27 17:00:32,774 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2023-06-27 17:00:32,792 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2023-06-27 17:00:32,794 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-27 17:00:32,807 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2023-06-27 17:00:32,816 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2.org_1   | 2023-06-27 17:00:32,859 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2023-06-27 17:00:53,905 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2023-06-27 17:00:53,975 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2023-06-27 17:00:53,975 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2023-06-27 17:00:53,982 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf,clusterId:CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f,subject:scm-sub@scm3.org
scm3.org_1   | 2023-06-27 17:00:54,605 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2023-06-27 17:00:54,633 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f, SCMID 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf
scm3.org_1   | 2023-06-27 17:00:54,633 [main] INFO server.StorageContainerManager: Primary SCM Node ID a2237aa5-d04c-471b-bae4-225264529c4c
scm3.org_1   | 2023-06-27 17:00:54,691 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-27 17:00:58,975 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/6cfd7dc078956122a8546b7833728db1acaf2a08 ; compiled by 'runner' on 2023-06-27T16:34Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-27 17:00:58,989 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-06-27 17:01:15,353 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
recon_1      | 2023-06-27 17:01:17,355 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately. Current retry count: 18.
recon_1      | 2023-06-27 17:01:17,357 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately. Current retry count: 19.
recon_1      | 2023-06-27 17:01:17,358 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 20.
recon_1      | 2023-06-27 17:01:19,360 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately. Current retry count: 21.
recon_1      | 2023-06-27 17:01:19,362 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately. Current retry count: 22.
recon_1      | 2023-06-27 17:01:19,368 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 23.
recon_1      | 2023-06-27 17:01:21,369 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately. Current retry count: 24.
recon_1      | 2023-06-27 17:01:21,370 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately. Current retry count: 25.
recon_1      | 2023-06-27 17:01:21,372 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 26.
recon_1      | 2023-06-27 17:01:23,374 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately. Current retry count: 27.
recon_1      | 2023-06-27 17:01:23,376 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately. Current retry count: 28.
recon_1      | 2023-06-27 17:01:23,378 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 29.
recon_1      | 2023-06-27 17:01:25,379 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately. Current retry count: 30.
recon_1      | 2023-06-27 17:01:25,381 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately. Current retry count: 31.
recon_1      | 2023-06-27 17:01:25,383 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 32.
recon_1      | 2023-06-27 17:01:27,384 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately. Current retry count: 33.
recon_1      | 2023-06-27 17:01:27,385 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately. Current retry count: 34.
recon_1      | 2023-06-27 17:01:27,388 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 35.
recon_1      | 2023-06-27 17:01:29,390 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately. Current retry count: 36.
recon_1      | 2023-06-27 17:01:29,391 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately. Current retry count: 37.
recon_1      | 2023-06-27 17:01:29,393 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 38.
recon_1      | 2023-06-27 17:01:31,394 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately. Current retry count: 39.
recon_1      | 2023-06-27 17:01:31,397 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately. Current retry count: 40.
recon_1      | 2023-06-27 17:01:31,399 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 41.
recon_1      | 2023-06-27 17:01:33,401 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately. Current retry count: 42.
recon_1      | 2023-06-27 17:01:33,403 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately. Current retry count: 43.
recon_1      | 2023-06-27 17:01:33,404 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 44.
recon_1      | 2023-06-27 17:01:35,406 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately. Current retry count: 45.
recon_1      | 2023-06-27 17:01:35,409 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately. Current retry count: 46.
recon_1      | 2023-06-27 17:01:35,411 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 47.
recon_1      | 2023-06-27 17:01:37,412 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover immediately. Current retry count: 48.
recon_1      | 2023-06-27 17:01:37,413 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 49 failover attempts. Trying to failover immediately. Current retry count: 49.
recon_1      | 2023-06-27 17:01:37,414 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 50 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 50.
recon_1      | 2023-06-27 17:01:39,415 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 51 failover attempts. Trying to failover immediately. Current retry count: 51.
recon_1      | 2023-06-27 17:01:39,418 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 52 failover attempts. Trying to failover immediately. Current retry count: 52.
recon_1      | 2023-06-27 17:01:39,419 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 53 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 53.
recon_1      | 2023-06-27 17:01:41,420 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 54 failover attempts. Trying to failover immediately. Current retry count: 54.
recon_1      | 2023-06-27 17:01:41,422 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 55 failover attempts. Trying to failover immediately. Current retry count: 55.
recon_1      | 2023-06-27 17:01:41,423 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 56 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 56.
recon_1      | 2023-06-27 17:01:43,424 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 57 failover attempts. Trying to failover immediately. Current retry count: 57.
recon_1      | 2023-06-27 17:01:43,425 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 58 failover attempts. Trying to failover immediately. Current retry count: 58.
recon_1      | 2023-06-27 17:01:43,425 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 59 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 59.
recon_1      | 2023-06-27 17:01:45,428 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 60 failover attempts. Trying to failover immediately. Current retry count: 60.
recon_1      | 2023-06-27 17:01:45,429 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 61 failover attempts. Trying to failover immediately. Current retry count: 61.
recon_1      | 2023-06-27 17:01:45,430 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 62 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 62.
recon_1      | 2023-06-27 17:01:47,431 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 63 failover attempts. Trying to failover immediately. Current retry count: 63.
recon_1      | 2023-06-27 17:01:47,432 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 64 failover attempts. Trying to failover immediately. Current retry count: 64.
recon_1      | 2023-06-27 17:01:47,435 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 65 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 65.
recon_1      | 2023-06-27 17:01:49,436 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 66 failover attempts. Trying to failover immediately. Current retry count: 66.
recon_1      | 2023-06-27 17:01:49,439 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 67 failover attempts. Trying to failover immediately. Current retry count: 67.
recon_1      | 2023-06-27 17:01:49,443 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 68 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 68.
recon_1      | 2023-06-27 17:01:51,445 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 69 failover attempts. Trying to failover immediately. Current retry count: 69.
recon_1      | 2023-06-27 17:01:51,446 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 70 failover attempts. Trying to failover immediately. Current retry count: 70.
recon_1      | 2023-06-27 17:01:51,446 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 71 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 71.
recon_1      | 2023-06-27 17:01:53,448 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 72 failover attempts. Trying to failover immediately. Current retry count: 72.
recon_1      | 2023-06-27 17:01:53,449 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 73 failover attempts. Trying to failover immediately. Current retry count: 73.
recon_1      | 2023-06-27 17:01:53,450 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 74 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 74.
recon_1      | 2023-06-27 17:01:55,451 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 75 failover attempts. Trying to failover immediately. Current retry count: 75.
recon_1      | 2023-06-27 17:01:55,452 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 76 failover attempts. Trying to failover immediately. Current retry count: 76.
recon_1      | 2023-06-27 17:01:55,454 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 77 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 77.
recon_1      | 2023-06-27 17:01:57,455 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 78 failover attempts. Trying to failover immediately. Current retry count: 78.
recon_1      | 2023-06-27 17:01:57,457 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 79 failover attempts. Trying to failover immediately. Current retry count: 79.
recon_1      | 2023-06-27 17:01:57,457 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 80 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 80.
om3_1        | 2023-06-27 17:02:59,838 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 17:03:01,222 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1        | 2023-06-27 17:03:03,474 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1        | 2023-06-27 17:03:03,605 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2023-06-27 17:03:03,606 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2023-06-27 17:03:03,866 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om3_1        | 2023-06-27 17:03:03,979 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-27 17:03:04,188 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1        | 2023-06-27 17:03:04,195 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om3_1        | 2023-06-27 17:03:05,105 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om3_1        | 2023-06-27 17:03:05,859 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-27 17:03:05,862 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1        | 2023-06-27 17:03:05,936 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om3_1        | 2023-06-27 17:03:05,937 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2023-06-27 17:03:06,853 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2023-06-27 17:03:07,023 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-27 17:03:07,310 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1        | 2023-06-27 17:03:07,381 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2023-06-27 17:03:08,248 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-27 17:03:08,267 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-27 17:03:08,479 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2023-06-27 17:03:08,606 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-27 17:03:08,616 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-27 17:03:08,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-27 17:03:08,623 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-27 17:03:08,625 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1        | 2023-06-27 17:03:08,626 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2023-06-27 17:03:08,628 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2023-06-27 17:03:08,642 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 17:03:08,643 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2023-06-27 17:03:08,652 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-27 17:03:08,713 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-06-27 17:03:08,737 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1        | 2023-06-27 17:03:08,742 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-06-27 17:03:11,569 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1        | 2023-06-27 17:03:11,597 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1        | 2023-06-27 17:03:11,603 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1        | 2023-06-27 17:03:11,605 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-27 17:03:11,605 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-27 17:03:11,648 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-06-27 17:03:11,726 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@14df8bee[Not completed]
om3_1        | 2023-06-27 17:03:11,729 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2023-06-27 17:03:11,755 [main] INFO om.OzoneManager: Creating RPC Server
om3_1        | 2023-06-27 17:03:11,805 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1        | 2023-06-27 17:03:11,825 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2023-06-27 17:03:11,826 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2023-06-27 17:03:11,827 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2023-06-27 17:03:11,827 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-27 17:03:11,830 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-27 17:03:11,830 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2023-06-27 17:03:11,931 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1        | 2023-06-27 17:03:11,932 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-06-27 17:03:11,959 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2023-06-27 17:03:11,963 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1        | 2023-06-27 17:03:12,059 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm3.org_1   | 2023-06-27 17:00:59,077 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 17:00:59,295 [main] INFO reflections.Reflections: Reflections took 177 ms to scan 3 urls, producing 131 keys and 286 values 
scm3.org_1   | 2023-06-27 17:00:59,514 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-06-27 17:00:59,529 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-27 17:00:59,579 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-27 17:00:59,580 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-27 17:00:59,838 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-27 17:00:59,838 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-27 17:01:02,137 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 1
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm3.org_1   |            Start Date: Tue Jun 27 16:59:54 UTC 2023
scm3.org_1   |            Final Date: Fri Aug 04 16:59:54 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm3.org_1   |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
scm3.org_1   |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
scm3.org_1   |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
scm3.org_1   |                        dcfc06fd580359247f53abb2878410a9ef225129
scm3.org_1   |                        820a87eb87b24799aec84587b5f9d56ce0e93904
scm3.org_1   |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
scm3.org_1   |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
scm3.org_1   |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
scm3.org_1   |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
scm3.org_1   |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
scm3.org_1   |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
scm3.org_1   |                        862deee8bb1ba8320f84b774cf369b4523011f63
scm3.org_1   |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
scm3.org_1   |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0x6
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2023-06-27 17:01:02,166 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 312987683773
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm3.org_1   |            Start Date: Tue Jun 27 17:00:54 UTC 2023
scm3.org_1   |            Final Date: Fri Aug 04 17:00:54 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm3.org_1   |            Public Key: RSA Public Key [8e:b7:e9:39:54:5c:72:fa:e1:17:5c:91:fe:bf:70:1a:10:3f:18:45],[56:66:d1:a4]
scm3.org_1   |         modulus: 948a192c55124d94c6405ff519577f7b6e1380ca7e6e71f9b93d91e96bf93a77a3b524e73c581213792dd68a7935a44a56a8436963a7ec6308d27287f182d1ed492aadeae89e96891ff6b8d8a7cb5fc7360d013bda1da9e5f20c4dfb8cd04f9cef843eb4c5a351824b329f27c3c3360799af3cb0f369369db2b95165e335b9c9383b139582ef3cf524849882fd1a68eec3d0d3bc02a857cb8018f143bcdfb1c647288c1a07a2e59ca2f968bc7e3521f8d4ac71ea3b171677e116608f1efbc24b1d6e2382b8541680a468f40375fcd1ba83d3095359865ddc8b2e025b816af9098ad1ceea3d767a92588344ab453b3f957c2ddc2a0ad8de9b0add3eb0079a7543
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: b744a97523b7a60ac46a8bdc771c53dc574b8f12
scm3.org_1   |                        9cd93049455fa40ea1e376dac0582e4e15c7f7d0
scm3.org_1   |                        178477bae137727e128bdd48539748ad2b3fdf9e
scm3.org_1   |                        0bec1ea178676a4a380a7036dd7c47f30a43255f
scm3.org_1   |                        eb61674ad333703ea7aced6935f8e86843c4d2e8
scm3.org_1   |                        08b63242536a3725b1a7578e19d809f47d0e8b9d
scm3.org_1   |                        34e51359ea2747ae187d31a549a111e0b9e9c918
scm3.org_1   |                        b21da4a087a13d4e8ce5b91c980ce7080aae7a26
scm3.org_1   |                        6d1d5740bc2bae8d6c7a3a0210b35ba10b3a709d
scm3.org_1   |                        e78dced9f1d0c1f84e4d1c0257ae3e4d796e49ca
scm3.org_1   |                        2667a09fd72192f502dc0ec847cfcc2b3452ba3b
scm3.org_1   |                        a83adb162267bb4d0ea2dc46836f0be98bbfb4e2
scm3.org_1   |                        ecb47146c25aa8797cfdf94f2f13e29e
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/312987683773.crt.
scm3.org_1   | 2023-06-27 17:01:02,186 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 312987683773
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
om3_1        | 2023-06-27 17:03:12,118 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1        | 2023-06-27 17:03:12,161 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2023-06-27 17:03:12,168 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2023-06-27 17:03:12,466 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1        | 2023-06-27 17:03:12,925 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-27 17:03:12,967 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-06-27 17:03:12,975 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1        | 2023-06-27 17:03:12,986 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1        | 2023-06-27 17:03:13,001 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1        | 2023-06-27 17:03:13,033 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1        | 2023-06-27 17:03:15,190 [main] INFO reflections.Reflections: Reflections took 2951 ms to scan 8 urls, producing 24 keys and 639 values [using 2 cores]
om3_1        | 2023-06-27 17:03:16,784 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2023-06-27 17:03:16,826 [main] INFO ipc.Server: Listener at om3:9862
om3_1        | 2023-06-27 17:03:16,839 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2023-06-27 17:03:26,607 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2023-06-27 17:03:26,817 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2023-06-27 17:03:26,817 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2023-06-27 17:03:27,893 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2023-06-27 17:03:27,901 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2023-06-27 17:03:27,929 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1        | 2023-06-27 17:03:27,961 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om3
om3_1        | 2023-06-27 17:03:28,058 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om3_1        | 2023-06-27 17:03:28,107 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2023-06-27 17:03:28,193 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2023-06-27 17:03:28,201 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 17:03:28,227 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1        | 2023-06-27 17:03:28,244 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1        | 2023-06-27 17:03:28,295 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-06-27 17:03:28,360 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2023-06-27 17:03:28,361 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2023-06-27 17:03:28,369 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 17:03:28,449 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1        | 2023-06-27 17:03:28,454 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1        | 2023-06-27 17:03:28,460 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2023-06-27 17:03:28,477 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-06-27 17:03:28,481 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2023-06-27 17:03:28,492 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1        | 2023-06-27 17:03:28,503 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2023-06-27 17:03:28,508 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2023-06-27 17:03:28,512 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2023-06-27 17:03:28,664 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2023-06-27 17:03:28,665 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-27 17:03:28,803 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1        | 2023-06-27 17:03:28,810 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1        | 2023-06-27 17:03:28,816 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2023-06-27 17:03:28,896 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-27 17:03:28,901 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-27 17:03:28,937 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-06-27 17:03:28,946 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2023-06-27 17:03:28,977 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1        | 2023-06-27 17:03:28,998 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-06-27 17:03:29,013 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:00:32,864 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2.org_1   | 2023-06-27 17:00:32,882 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2023-06-27 17:00:32,886 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2023-06-27 17:00:32,928 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2.org_1   | 2023-06-27 17:00:33,316 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-27 17:00:33,331 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2023-06-27 17:00:33,332 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2.org_1   | 2023-06-27 17:00:33,334 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2.org_1   | 2023-06-27 17:00:33,342 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2.org_1   | 2023-06-27 17:00:33,342 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2.org_1   | 2023-06-27 17:00:33,353 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2023-06-27 17:00:33,353 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2023-06-27 17:00:33,354 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2023-06-27 17:00:33,473 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm2.org_1   | 2023-06-27 17:00:33,640 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2.org_1   | 2023-06-27 17:00:33,651 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2023-06-27 17:00:33,696 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2023-06-27 17:00:33,706 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2023-06-27 17:00:33,968 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2023-06-27 17:00:34,074 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm2.org_1   | 2023-06-27 17:00:34,088 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-27 17:00:34,221 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2023-06-27 17:00:34,322 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2023-06-27 17:00:34,323 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-27 17:00:34,343 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2023-06-27 17:00:34,343 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2023-06-27 17:00:34,381 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2.org_1   | 2023-06-27 17:00:34,383 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2.org_1   | 2023-06-27 17:00:34,397 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2.org_1   | 2023-06-27 17:00:34,407 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2.org_1   | 2023-06-27 17:00:34,567 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-27 17:00:34,571 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-27 17:00:34,667 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2023-06-27 17:00:35,124 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2023-06-27 17:00:35,162 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2023-06-27 17:00:35,178 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm2.org_1   | 2023-06-27 17:00:35,190 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2023-06-27 17:00:35,195 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:00:35,198 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-27 17:00:35,791 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm2.org_1   | 2023-06-27 17:00:35,791 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm2.org_1   | 2023-06-27 17:00:35,894 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2023-06-27 17:00:35,981 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 17:00:36,099 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm2.org_1   | 2023-06-27 17:00:36,120 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2023-06-27 17:00:36,356 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm2.org_1   | 2023-06-27 17:00:36,357 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm2.org_1   | 2023-06-27 17:00:38,537 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-27 17:00:38,574 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 17:00:38,574 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm2.org_1   | 2023-06-27 17:00:38,580 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2023-06-27 17:00:38,682 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-27 17:00:38,715 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 17:00:38,715 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm2.org_1   | 2023-06-27 17:00:38,727 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2023-06-27 17:00:38,902 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-27 17:00:39,085 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 17:00:39,086 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm2.org_1   | 2023-06-27 17:00:39,094 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2023-06-27 17:00:39,474 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2.org_1   | 2023-06-27 17:00:39,477 [main] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        false
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                                                Value
scm2.org_1   | Threshold                                          10
scm2.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2.org_1   | Max Size to Move per Iteration                     500GB
scm2.org_1   | Max Size Entering Target per Iteration             26GB
scm2.org_1   | Max Size Leaving Source per Iteration              26GB
scm2.org_1   | 
scm2.org_1   | 2023-06-27 17:00:39,477 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2023-06-27 17:00:39,487 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-27 17:00:39,495 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2023-06-27 17:00:39,512 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f does not exist. Creating ...
scm2.org_1   | 2023-06-27 17:00:39,534 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/in_use.lock acquired by nodename 7@scm2.org
scm2.org_1   | 2023-06-27 17:00:39,558 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f has been successfully formatted.
scm2.org_1   | 2023-06-27 17:00:39,570 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2023-06-27 17:00:39,705 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2023-06-27 17:00:39,705 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 17:00:39,707 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2.org_1   | 2023-06-27 17:00:39,719 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2.org_1   | 2023-06-27 17:00:39,747 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-27 17:00:39,787 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2023-06-27 17:00:39,790 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2023-06-27 17:00:39,790 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 17:00:39,828 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm2.org_1   | 2023-06-27 17:00:39,834 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2023-06-27 17:00:39,838 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2023-06-27 17:00:39,877 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-27 17:00:39,885 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2.org_1   | 2023-06-27 17:00:39,885 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2023-06-27 17:00:39,897 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2023-06-27 17:00:39,897 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2023-06-27 17:00:39,898 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2023-06-27 17:00:39,975 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2023-06-27 17:00:39,975 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-27 17:00:40,037 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2.org_1   | 2023-06-27 17:00:40,038 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2.org_1   | 2023-06-27 17:00:40,038 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-06-27 17:00:40,057 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-27 17:00:40,057 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-27 17:00:40,077 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:00:40,086 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: changes role from      null to FOLLOWER at term 0 for startInitializing
recon_1      | 2023-06-27 17:01:59,459 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 81 failover attempts. Trying to failover immediately. Current retry count: 81.
recon_1      | 2023-06-27 17:01:59,460 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 82 failover attempts. Trying to failover immediately. Current retry count: 82.
recon_1      | 2023-06-27 17:01:59,461 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 83 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 83.
recon_1      | 2023-06-27 17:02:01,463 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 84 failover attempts. Trying to failover immediately. Current retry count: 84.
recon_1      | 2023-06-27 17:02:01,464 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 85 failover attempts. Trying to failover immediately. Current retry count: 85.
recon_1      | 2023-06-27 17:02:01,465 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 86 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 86.
recon_1      | 2023-06-27 17:02:03,466 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 87 failover attempts. Trying to failover immediately. Current retry count: 87.
recon_1      | 2023-06-27 17:02:03,467 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 88 failover attempts. Trying to failover immediately. Current retry count: 88.
recon_1      | 2023-06-27 17:02:03,468 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 89 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 89.
recon_1      | 2023-06-27 17:02:05,469 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 90 failover attempts. Trying to failover immediately. Current retry count: 90.
recon_1      | 2023-06-27 17:02:05,470 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 91 failover attempts. Trying to failover immediately. Current retry count: 91.
recon_1      | 2023-06-27 17:02:05,471 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 92 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 92.
recon_1      | 2023-06-27 17:02:07,473 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 93 failover attempts. Trying to failover immediately. Current retry count: 93.
recon_1      | 2023-06-27 17:02:07,474 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 94 failover attempts. Trying to failover immediately. Current retry count: 94.
recon_1      | 2023-06-27 17:02:07,474 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 95 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 95.
recon_1      | 2023-06-27 17:02:09,476 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 96 failover attempts. Trying to failover immediately. Current retry count: 96.
recon_1      | 2023-06-27 17:02:09,478 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 97 failover attempts. Trying to failover immediately. Current retry count: 97.
recon_1      | 2023-06-27 17:02:09,480 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 98 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 98.
recon_1      | 2023-06-27 17:02:11,482 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 99 failover attempts. Trying to failover immediately. Current retry count: 99.
recon_1      | 2023-06-27 17:02:11,484 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 100 failover attempts. Trying to failover immediately. Current retry count: 100.
recon_1      | 2023-06-27 17:02:11,485 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 101 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 101.
recon_1      | 2023-06-27 17:02:13,487 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 102 failover attempts. Trying to failover immediately. Current retry count: 102.
recon_1      | 2023-06-27 17:02:13,489 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 103 failover attempts. Trying to failover immediately. Current retry count: 103.
recon_1      | 2023-06-27 17:02:13,490 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 104 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 104.
recon_1      | 2023-06-27 17:02:15,492 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 105 failover attempts. Trying to failover immediately. Current retry count: 105.
recon_1      | 2023-06-27 17:02:15,495 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 106 failover attempts. Trying to failover immediately. Current retry count: 106.
recon_1      | 2023-06-27 17:02:15,497 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 107 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 107.
recon_1      | 2023-06-27 17:02:17,499 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 108 failover attempts. Trying to failover immediately. Current retry count: 108.
recon_1      | 2023-06-27 17:02:17,500 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 109 failover attempts. Trying to failover immediately. Current retry count: 109.
recon_1      | 2023-06-27 17:02:17,501 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 110 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 110.
recon_1      | 2023-06-27 17:02:19,503 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 111 failover attempts. Trying to failover immediately. Current retry count: 111.
recon_1      | 2023-06-27 17:02:19,504 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 112 failover attempts. Trying to failover immediately. Current retry count: 112.
recon_1      | 2023-06-27 17:02:19,505 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 113 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 113.
recon_1      | 2023-06-27 17:02:21,507 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 114 failover attempts. Trying to failover immediately. Current retry count: 114.
recon_1      | 2023-06-27 17:02:21,507 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 115 failover attempts. Trying to failover immediately. Current retry count: 115.
recon_1      | 2023-06-27 17:02:21,508 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 116 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 116.
recon_1      | 2023-06-27 17:02:23,509 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 117 failover attempts. Trying to failover immediately. Current retry count: 117.
recon_1      | 2023-06-27 17:02:23,510 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 118 failover attempts. Trying to failover immediately. Current retry count: 118.
recon_1      | 2023-06-27 17:02:23,511 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 119 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 119.
recon_1      | 2023-06-27 17:02:25,512 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 120 failover attempts. Trying to failover immediately. Current retry count: 120.
recon_1      | 2023-06-27 17:02:25,515 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 121 failover attempts. Trying to failover immediately. Current retry count: 121.
recon_1      | 2023-06-27 17:02:25,528 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 122 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 122.
recon_1      | 2023-06-27 17:02:27,541 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 123 failover attempts. Trying to failover immediately. Current retry count: 123.
recon_1      | 2023-06-27 17:02:27,542 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 124 failover attempts. Trying to failover immediately. Current retry count: 124.
recon_1      | 2023-06-27 17:02:27,543 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 125 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 125.
recon_1      | 2023-06-27 17:02:29,544 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 126 failover attempts. Trying to failover immediately. Current retry count: 126.
recon_1      | 2023-06-27 17:02:29,546 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 127 failover attempts. Trying to failover immediately. Current retry count: 127.
recon_1      | 2023-06-27 17:02:29,547 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 128 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 128.
recon_1      | 2023-06-27 17:02:31,548 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 129 failover attempts. Trying to failover immediately. Current retry count: 129.
recon_1      | 2023-06-27 17:02:31,549 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 130 failover attempts. Trying to failover immediately. Current retry count: 130.
recon_1      | 2023-06-27 17:02:31,550 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 131 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 131.
recon_1      | 2023-06-27 17:02:33,551 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 132 failover attempts. Trying to failover immediately. Current retry count: 132.
recon_1      | 2023-06-27 17:02:33,552 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 133 failover attempts. Trying to failover immediately. Current retry count: 133.
recon_1      | 2023-06-27 17:02:33,553 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 134 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 134.
recon_1      | 2023-06-27 17:02:35,555 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 135 failover attempts. Trying to failover immediately. Current retry count: 135.
recon_1      | 2023-06-27 17:02:35,556 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 136 failover attempts. Trying to failover immediately. Current retry count: 136.
recon_1      | 2023-06-27 17:02:35,557 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 137 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 137.
recon_1      | 2023-06-27 17:02:37,558 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 138 failover attempts. Trying to failover immediately. Current retry count: 138.
recon_1      | 2023-06-27 17:02:37,559 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 139 failover attempts. Trying to failover immediately. Current retry count: 139.
recon_1      | 2023-06-27 17:02:37,560 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 140 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 140.
recon_1      | 2023-06-27 17:02:39,568 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 141 failover attempts. Trying to failover immediately. Current retry count: 141.
recon_1      | 2023-06-27 17:02:39,569 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 142 failover attempts. Trying to failover immediately. Current retry count: 142.
recon_1      | 2023-06-27 17:02:39,569 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 143 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 143.
recon_1      | 2023-06-27 17:02:40,765 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:38734 / 172.25.0.104:38734
recon_1      | 2023-06-27 17:02:40,887 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 17:02:41,688 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 144 failover attempts. Trying to failover immediately. Current retry count: 144.
recon_1      | 2023-06-27 17:02:41,767 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 145 failover attempts. Trying to failover immediately. Current retry count: 145.
recon_1      | 2023-06-27 17:02:41,769 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 146 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 146.
recon_1      | 2023-06-27 17:02:41,832 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:53096 / 172.25.0.102:53096
recon_1      | 2023-06-27 17:02:41,884 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 17:02:42,568 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:41154 / 172.25.0.103:41154
recon_1      | 2023-06-27 17:02:42,743 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 17:02:43,771 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 147 failover attempts. Trying to failover immediately. Current retry count: 147.
recon_1      | 2023-06-27 17:02:43,771 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 148 failover attempts. Trying to failover immediately. Current retry count: 148.
recon_1      | 2023-06-27 17:02:43,772 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 149 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 149.
recon_1      | 2023-06-27 17:02:45,652 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3b0419d0-b93c-4a38-a8f8-ba0374c67423
recon_1      | 2023-06-27 17:02:45,671 [IPC Server handler 0 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d17c8765-e162-413f-ae01-cbbd58f77ae9
recon_1      | 2023-06-27 17:02:45,754 [IPC Server handler 0 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d17c8765-e162-413f-ae01-cbbd58f77ae9{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855], networkLocation: /default-rack, certSerialId: 380923164129, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-27 17:02:45,762 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 3b0419d0-b93c-4a38-a8f8-ba0374c67423{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886], networkLocation: /default-rack, certSerialId: 382350562679, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-27 17:02:45,788 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 150 failover attempts. Trying to failover immediately. Current retry count: 150.
recon_1      | 2023-06-27 17:02:45,813 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 151 failover attempts. Trying to failover immediately. Current retry count: 151.
recon_1      | 2023-06-27 17:02:45,824 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 152 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 152.
recon_1      | 2023-06-27 17:02:46,028 [IPC Server handler 13 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e5cd4d6c-bb68-491f-b474-5dc217379157
recon_1      | 2023-06-27 17:02:46,038 [IPC Server handler 13 on default port 9891] INFO node.SCMNodeManager: Registered Data node : e5cd4d6c-bb68-491f-b474-5dc217379157{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 380187848912, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-27 17:02:47,856 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 153 failover attempts. Trying to failover immediately. Current retry count: 153.
recon_1      | 2023-06-27 17:02:47,876 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 154 failover attempts. Trying to failover immediately. Current retry count: 154.
recon_1      | 2023-06-27 17:02:47,890 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 155 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 155.
recon_1      | 2023-06-27 17:02:48,937 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d17c8765-e162-413f-ae01-cbbd58f77ae9 to Node DB.
recon_1      | 2023-06-27 17:02:48,944 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 3b0419d0-b93c-4a38-a8f8-ba0374c67423 to Node DB.
recon_1      | 2023-06-27 17:02:48,969 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node e5cd4d6c-bb68-491f-b474-5dc217379157 to Node DB.
recon_1      | 2023-06-27 17:02:49,907 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 156 failover attempts. Trying to failover immediately. Current retry count: 156.
recon_1      | 2023-06-27 17:02:49,908 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 157 failover attempts. Trying to failover immediately. Current retry count: 157.
recon_1      | 2023-06-27 17:02:49,909 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 158 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 158.
recon_1      | 2023-06-27 17:02:51,912 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 159 failover attempts. Trying to failover immediately. Current retry count: 159.
recon_1      | 2023-06-27 17:02:51,913 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 160 failover attempts. Trying to failover immediately. Current retry count: 160.
recon_1      | 2023-06-27 17:02:51,914 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 161 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 161.
recon_1      | 2023-06-27 17:02:53,915 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 162 failover attempts. Trying to failover immediately. Current retry count: 162.
recon_1      | 2023-06-27 17:02:53,918 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 163 failover attempts. Trying to failover immediately. Current retry count: 163.
recon_1      | 2023-06-27 17:02:53,920 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 164 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 164.
recon_1      | 2023-06-27 17:02:55,926 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 165 failover attempts. Trying to failover immediately. Current retry count: 165.
recon_1      | 2023-06-27 17:02:55,927 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 166 failover attempts. Trying to failover immediately. Current retry count: 166.
recon_1      | 2023-06-27 17:02:55,927 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 167 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 167.
scm1.org_1   | 2023-06-27 17:00:04,625 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 17:00:04,764 [main] INFO reflections.Reflections: Reflections took 101 ms to scan 3 urls, producing 131 keys and 286 values 
scm1.org_1   | 2023-06-27 17:00:04,842 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-27 17:00:04,857 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-27 17:00:04,888 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-27 17:00:04,889 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-27 17:00:05,089 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2023-06-27 17:00:05,089 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2023-06-27 17:00:06,110 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 1
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   |            Start Date: Tue Jun 27 16:59:54 UTC 2023
scm1.org_1   |            Final Date: Fri Aug 04 16:59:54 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   |            Public Key: RSA Public Key [28:8b:fc:f1:0a:01:78:53:2e:16:95:7f:37:10:c4:2d:61:18:06:83],[56:66:d1:a4]
scm1.org_1   |         modulus: ced6e824e0c018eca38e33209800fd6da0e7009c3c67b2e3f46e680914cd6901380dbe240e27800c67f161db0a6be5977ab7e3f9c5c8a541ced5eff893aab63613310cceeee63ef50f86218b44efc1c11e71c5b63ff7ccea6be9e51b6162ebc4526506d0ea41e39b1512ba7cf6c75280ed4be2a5f45d245573db6010b4117de4b0bdc8b20185ac479dcf0df2d169f5fcfbcc7d12035abc920a83cc18f1d9ac6e65b666a7ee0d671f99a73f95552218e352eaa6941b504f8a3f4131923eee271cdc4f77175cdfecd9dadd2c5fcd342c3d8d8d5dcc812560f94f48c5da592bf1dcb01a16f0e5fbfc59dad2472db2fa8145071caad51954bb8b4a7493c2cb35e2b5
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: aef07cc19f48da38a5af98031ebb54446d816069
scm1.org_1   |                        b83a2c77bf0d58427a00ce8eccd4741f88654b48
scm1.org_1   |                        dcfc06fd580359247f53abb2878410a9ef225129
scm1.org_1   |                        820a87eb87b24799aec84587b5f9d56ce0e93904
scm1.org_1   |                        0c486af57cf3e43d5eb98c92759d73a31e4181f8
scm1.org_1   |                        cccf5c02f601774b64273aff9c2801ae1676e2cc
scm1.org_1   |                        7aeee95f4837c03b1f8c9f8395555aeb32f129d6
scm1.org_1   |                        1c4bbc5a119ae6d99d6f7ff64f55f31d0910d135
scm1.org_1   |                        d6b6ed30e48238681628651c3b15e7d80954ca1f
scm1.org_1   |                        31003e84079cb9b68e05af4eaf9092ca39d3b403
scm1.org_1   |                        862deee8bb1ba8320f84b774cf369b4523011f63
scm1.org_1   |                        9ef2b87de2c94726ea424f966d4c2534b922af9f
scm1.org_1   |                        ab1a5ad0fa3acbc0992b60ce54aa02b3
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0x6
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2023-06-27 17:00:06,124 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 253775241875
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   |            Start Date: Tue Jun 27 16:59:54 UTC 2023
scm1.org_1   |            Final Date: Fri Aug 04 16:59:54 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
scm1.org_1   |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
scm1.org_1   |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
scm1.org_1   |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
scm1.org_1   |                        6031d6bbcce2d911b293b01715f60941bca01f22
scm1.org_1   |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
scm1.org_1   |                        e53925b79b76c1e822589654cf8f339218feaedf
scm1.org_1   |                        29211ff344b921aef19d39fe27b1f1791e557b58
scm1.org_1   |                        de0b8dc672e394da848c5d954638a49ff0bd8708
scm1.org_1   |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
scm1.org_1   |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
scm1.org_1   |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
scm1.org_1   |                        df6e7ecf25455532985737512133afdf952d415e
scm1.org_1   |                        23e015a3d5c6b073bba64e6a6a58ef15
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2023-06-27 17:00:06,131 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 253775241875
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   |            Start Date: Tue Jun 27 16:59:54 UTC 2023
scm1.org_1   |            Final Date: Fri Aug 04 16:59:54 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=a2237aa5-d04c-471b-bae4-225264529c4c,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   |            Public Key: RSA Public Key [49:51:e3:09:69:1e:d4:95:34:c3:3d:03:b2:70:c7:a1:03:b1:c2:c2],[56:66:d1:a4]
scm1.org_1   |         modulus: a84eb4d8648b5f36ca598e90fe14e4e60aa3b3a6cd0c9d206de1177aca4cdc8d61a24477a478f7597adb05440d719de621185fe6de6dd420d7ce33645e2d795833f956250af0b245bae00a80ce2a1616c61451de81965eb1dc9ca7010a39a57cc9d2bd2e543306768f0fc7e5e5dea3b9474e95aa85be5e43a28647d9a2e20547b61d5322a00f2c65003ac4d489954d9d5dcaeaa1798a90e641c37ff8824a2c2056e672b0f983aff34c306828f8167597b59fe2c29fd2445551d4c412a21b68003a3341bf46eeab6aaf086fa0cdf4992ecbe6e494476c392146a14ebfc49299ba0e73bc44b4305a92cd45e8a81736aa289f0ba3db4d545a03af9cedab3ccf5b8f
scm3.org_1   |            Start Date: Tue Jun 27 17:00:54 UTC 2023
scm3.org_1   |            Final Date: Fri Aug 04 17:00:54 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf,O=CID-d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm3.org_1   |            Public Key: RSA Public Key [8e:b7:e9:39:54:5c:72:fa:e1:17:5c:91:fe:bf:70:1a:10:3f:18:45],[56:66:d1:a4]
scm3.org_1   |         modulus: 948a192c55124d94c6405ff519577f7b6e1380ca7e6e71f9b93d91e96bf93a77a3b524e73c581213792dd68a7935a44a56a8436963a7ec6308d27287f182d1ed492aadeae89e96891ff6b8d8a7cb5fc7360d013bda1da9e5f20c4dfb8cd04f9cef843eb4c5a351824b329f27c3c3360799af3cb0f369369db2b95165e335b9c9383b139582ef3cf524849882fd1a68eec3d0d3bc02a857cb8018f143bcdfb1c647288c1a07a2e59ca2f968bc7e3521f8d4ac71ea3b171677e116608f1efbc24b1d6e2382b8541680a468f40375fcd1ba83d3095359865ddc8b2e025b816af9098ad1ceea3d767a92588344ab453b3f957c2ddc2a0ad8de9b0add3eb0079a7543
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: b744a97523b7a60ac46a8bdc771c53dc574b8f12
scm3.org_1   |                        9cd93049455fa40ea1e376dac0582e4e15c7f7d0
scm3.org_1   |                        178477bae137727e128bdd48539748ad2b3fdf9e
scm3.org_1   |                        0bec1ea178676a4a380a7036dd7c47f30a43255f
scm3.org_1   |                        eb61674ad333703ea7aced6935f8e86843c4d2e8
scm3.org_1   |                        08b63242536a3725b1a7578e19d809f47d0e8b9d
scm3.org_1   |                        34e51359ea2747ae187d31a549a111e0b9e9c918
scm3.org_1   |                        b21da4a087a13d4e8ce5b91c980ce7080aae7a26
scm3.org_1   |                        6d1d5740bc2bae8d6c7a3a0210b35ba10b3a709d
scm3.org_1   |                        e78dced9f1d0c1f84e4d1c0257ae3e4d796e49ca
scm3.org_1   |                        2667a09fd72192f502dc0ec847cfcc2b3452ba3b
scm3.org_1   |                        a83adb162267bb4d0ea2dc46836f0be98bbfb4e2
scm3.org_1   |                        ecb47146c25aa8797cfdf94f2f13e29e
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm3.org_1   | 2023-06-27 17:01:02,204 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716791811 ms and interval 86400000 ms.
scm3.org_1   | 2023-06-27 17:01:02,488 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 17:01:02,738 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-27 17:01:03,542 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2023-06-27 17:01:03,545 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2023-06-27 17:01:03,707 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2023-06-27 17:01:04,270 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf
scm3.org_1   | 2023-06-27 17:01:04,423 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-27 17:01:04,434 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-27 17:01:04,634 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2023-06-27 17:01:04,670 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-27 17:01:04,683 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-27 17:01:04,684 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-27 17:01:04,686 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-27 17:01:04,689 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3.org_1   | 2023-06-27 17:01:04,689 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2023-06-27 17:01:04,693 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2023-06-27 17:01:04,697 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 17:01:04,700 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2023-06-27 17:01:04,701 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-27 17:01:04,731 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3.org_1   | 2023-06-27 17:01:04,751 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3.org_1   | 2023-06-27 17:01:04,755 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-06-27 17:01:05,650 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3.org_1   | 2023-06-27 17:01:05,653 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3.org_1   | 2023-06-27 17:01:05,654 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3.org_1   | 2023-06-27 17:01:05,654 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-27 17:01:05,655 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-27 17:01:05,658 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-27 17:01:05,674 [main] INFO server.RaftServer: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: addNew group-BF8E9CD9096F:[] returns group-BF8E9CD9096F:java.util.concurrent.CompletableFuture@7455204c[Not completed]
scm3.org_1   | 2023-06-27 17:01:05,711 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: new RaftServerImpl for group-BF8E9CD9096F:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2023-06-27 17:01:05,715 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2023-06-27 17:01:05,715 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2023-06-27 17:01:05,717 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2023-06-27 17:01:05,718 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-27 17:01:05,718 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-27 17:01:05,719 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2023-06-27 17:03:29,116 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1        | 2023-06-27 17:03:29,162 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2023-06-27 17:03:29,164 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2023-06-27 17:03:29,168 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2023-06-27 17:03:29,178 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2023-06-27 17:03:29,298 [main] INFO server.RaftServer: om3: start RPC server
om3_1        | 2023-06-27 17:03:30,417 [main] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2023-06-27 17:03:30,432 [main] INFO om.OzoneManager: Starting secret key client.
om3_1        | 2023-06-27 17:03:30,455 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2023-06-27 17:03:31,332 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z).
om3_1        | 2023-06-27 17:03:31,371 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M44.2659S and interval of PT1M
om3_1        | 2023-06-27 17:03:31,413 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2023-06-27 17:03:31,416 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 386751038214
om3_1        | 2023-06-27 17:03:31,519 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2023-06-27 17:03:31,521 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        | 2023-06-27 17:03:33,147 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1        | 2023-06-27 17:03:33,148 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om3_1        | 2023-06-27 17:03:33,148 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om3_1        | 2023-06-27 17:03:33,666 [main] INFO util.log: Logging initialized @82631ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1        | 2023-06-27 17:03:34,215 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5242941777ns, electionTimeout:5176ms
om3_1        | 2023-06-27 17:03:34,246 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1        | 2023-06-27 17:03:34,271 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1        | 2023-06-27 17:03:34,291 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1        | 2023-06-27 17:03:34,455 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1        | 2023-06-27 17:03:34,631 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-06-27 17:03:35,750 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-06-27 17:03:35,751 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-06-27 17:03:35,843 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: 2392449bf3d011719b1f78fc729783b0ea3ca306
scm1.org_1   |                        abfb72c82fb390a78fe6a77d79af65a4b9692491
scm1.org_1   |                        f4b87504edce2873c1f84b7f09f4f2a64312ebeb
scm1.org_1   |                        6031d6bbcce2d911b293b01715f60941bca01f22
scm1.org_1   |                        4555d61ee630e71b64d2274f0a8ba61e718fa1d6
scm1.org_1   |                        e53925b79b76c1e822589654cf8f339218feaedf
scm1.org_1   |                        29211ff344b921aef19d39fe27b1f1791e557b58
scm1.org_1   |                        de0b8dc672e394da848c5d954638a49ff0bd8708
scm1.org_1   |                        c80a19917ba4a1d7bd94c29568f298ffca6f7d78
scm1.org_1   |                        d5f0b2f1cb23bc5e81b116505082ac1ea1a0a764
scm1.org_1   |                        82ad9846b726b08da9bf2f6c1d1461d488e0d14c
scm1.org_1   |                        df6e7ecf25455532985737512133afdf952d415e
scm1.org_1   |                        23e015a3d5c6b073bba64e6a6a58ef15
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/253775241875.crt.
scm1.org_1   | 2023-06-27 17:00:06,138 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716787867 ms and interval 86400000 ms.
scm1.org_1   | 2023-06-27 17:00:06,227 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 17:00:06,412 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-27 17:00:06,706 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2023-06-27 17:00:06,707 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2023-06-27 17:00:06,770 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-27 17:00:07,022 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:a2237aa5-d04c-471b-bae4-225264529c4c
scm1.org_1   | 2023-06-27 17:00:07,117 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 17:00:07,127 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 17:00:07,194 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-27 17:00:07,202 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 17:00:07,203 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-27 17:00:07,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-27 17:00:07,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-27 17:00:07,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-06-27 17:00:07,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-27 17:00:07,205 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-06-27 17:00:07,206 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 17:00:07,207 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-06-27 17:00:07,207 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 17:00:07,218 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 17:00:07,221 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-27 17:00:07,222 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-27 17:00:07,595 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-27 17:00:07,598 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-27 17:00:07,599 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-27 17:00:07,599 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-27 17:00:07,600 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 17:00:07,603 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 17:00:07,612 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer: a2237aa5-d04c-471b-bae4-225264529c4c: found a subdirectory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   | 2023-06-27 17:00:07,618 [main] INFO server.RaftServer: a2237aa5-d04c-471b-bae4-225264529c4c: addNew group-BF8E9CD9096F:[] returns group-BF8E9CD9096F:java.util.concurrent.CompletableFuture@7455204c[Not completed]
scm1.org_1   | 2023-06-27 17:00:07,643 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c: new RaftServerImpl for group-BF8E9CD9096F:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-27 17:00:07,645 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-27 17:00:07,645 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-27 17:00:07,645 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-27 17:00:07,645 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-27 17:00:07,645 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-27 17:00:07,645 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-27 17:00:07,653 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-27 17:00:07,653 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-27 17:00:07,656 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-06-27 17:00:07,657 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-27 17:00:07,706 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-27 17:00:07,710 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-27 17:00:07,714 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-27 17:00:07,714 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-27 17:00:07,735 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-27 17:00:07,850 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 17:00:07,859 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 17:00:07,861 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-27 17:00:07,862 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-27 17:00:07,863 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-27 17:00:07,863 [a2237aa5-d04c-471b-bae4-225264529c4c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-06-27 17:00:07,865 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2023-06-27 17:00:07,866 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2023-06-27 17:00:07,866 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2023-06-27 17:00:07,912 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm1.org_1   | 2023-06-27 17:00:07,964 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm1.org_1   | 2023-06-27 17:00:07,965 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2023-06-27 17:00:07,974 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2023-06-27 17:00:07,976 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2023-06-27 17:00:08,097 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2023-06-27 17:00:08,121 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm1.org_1   | 2023-06-27 17:00:08,124 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-27 17:00:08,136 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1.org_1   | 2023-06-27 17:00:08,168 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2023-06-27 17:00:08,168 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-27 17:00:08,176 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2023-06-27 17:00:08,177 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 17:00:08,193 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1.org_1   | 2023-06-27 17:00:08,194 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1.org_1   | 2023-06-27 17:00:08,202 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1.org_1   | 2023-06-27 17:00:08,202 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1.org_1   | 2023-06-27 17:00:08,241 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-27 17:00:08,242 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-27 17:00:08,264 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2023-06-27 17:00:08,362 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2023-06-27 17:00:08,388 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2023-06-27 17:00:08,395 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1.org_1   | 2023-06-27 17:00:08,410 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2023-06-27 17:00:08,422 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:00:08,426 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-27 17:00:08,706 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm1.org_1   | 2023-06-27 17:00:08,707 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm1.org_1   | 2023-06-27 17:00:08,731 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-27 17:00:08,735 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-27 17:00:08,736 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 253775241875 on primary SCM
scm1.org_1   | 2023-06-27 17:00:08,750 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2023-06-27 17:00:08,774 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-27 17:00:08,809 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm1.org_1   | 2023-06-27 17:00:08,812 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2023-06-27 17:00:08,887 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm1.org_1   | 2023-06-27 17:00:08,888 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm1.org_1   | 2023-06-27 17:00:09,584 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-27 17:00:09,655 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-27 17:00:40,102 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF8E9CD9096F,id=5a021426-02fe-4be7-b9ad-23a479d4b0f6
scm2.org_1   | 2023-06-27 17:00:40,106 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2.org_1   | 2023-06-27 17:00:40,113 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2023-06-27 17:00:40,114 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2023-06-27 17:00:40,119 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2023-06-27 17:00:40,131 [main] INFO server.RaftServer: 5a021426-02fe-4be7-b9ad-23a479d4b0f6: start RPC server
scm2.org_1   | 2023-06-27 17:00:40,295 [main] INFO server.GrpcService: 5a021426-02fe-4be7-b9ad-23a479d4b0f6: GrpcService started, listening on 9894
scm2.org_1   | 2023-06-27 17:00:40,296 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5a021426-02fe-4be7-b9ad-23a479d4b0f6: Started
scm2.org_1   | 2023-06-27 17:00:40,362 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-27 17:00:43,040 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: receive installSnapshot: a2237aa5-d04c-471b-bae4-225264529c4c->5a021426-02fe-4be7-b9ad-23a479d4b0f6#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-27 17:00:43,055 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2023-06-27 17:00:43,060 [grpc-default-executor-0] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: change Leader from null to a2237aa5-d04c-471b-bae4-225264529c4c at term 2 for installSnapshot, leader elected after 10196ms
scm2.org_1   | 2023-06-27 17:00:43,075 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: Received notification to install snapshot at index 0
scm2.org_1   | 2023-06-27 17:00:43,080 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2.org_1   | 2023-06-27 17:00:43,619 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "a2237aa5-d04c-471b-bae4-225264529c4c"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |     startupRole: FOLLOWER
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2023-06-27 17:00:43,628 [grpc-default-executor-0] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:00:43,656 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: reply installSnapshot: a2237aa5-d04c-471b-bae4-225264529c4c<-5a021426-02fe-4be7-b9ad-23a479d4b0f6#0:OK-t0,ALREADY_INSTALLED
scm2.org_1   | 2023-06-27 17:00:43,781 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 5a021426-02fe-4be7-b9ad-23a479d4b0f6: Completed INSTALL_SNAPSHOT, lastRequest: a2237aa5-d04c-471b-bae4-225264529c4c->5a021426-02fe-4be7-b9ad-23a479d4b0f6#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-27 17:00:43,782 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 5a021426-02fe-4be7-b9ad-23a479d4b0f6: Completed INSTALL_SNAPSHOT, lastReply: null
scm2.org_1   | 2023-06-27 17:00:44,131 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO impl.RoleInfo: 5a021426-02fe-4be7-b9ad-23a479d4b0f6: start 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState
scm2.org_1   | 2023-06-27 17:00:44,132 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-06-27 17:00:44,142 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:00:44,143 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:00:44,145 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: inconsistency entries. Reply:a2237aa5-d04c-471b-bae4-225264529c4c<-5a021426-02fe-4be7-b9ad-23a479d4b0f6#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-06-27 17:00:44,196 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-06-27 17:00:44,197 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: inconsistency entries. Reply:a2237aa5-d04c-471b-bae4-225264529c4c<-5a021426-02fe-4be7-b9ad-23a479d4b0f6#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-06-27 17:00:44,244 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread2] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:00:44,276 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread2] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:00:44,305 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2023-06-27 17:00:44,504 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2023-06-27 17:00:44,543 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:00:44,544 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:00:44,977 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_0
scm2.org_1   | 2023-06-27 17:00:45,023 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_0 to /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_0-0
scm2.org_1   | 2023-06-27 17:00:45,153 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_1
scm2.org_1   | 2023-06-27 17:00:45,169 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:00:45,174 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2023-06-27 17:00:45,176 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-27 17:00:45,177 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2023-06-27 17:00:45,200 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2023-06-27 17:00:45,230 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 17:00:45,308 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread3] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 9: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-06-27 17:00:45,349 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread3] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 11: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:00:45,767 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)]
scm2.org_1   | 2023-06-27 17:00:45,788 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)
scm2.org_1   | 2023-06-27 17:00:45,821 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-BF8E9CD9096F:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2.org_1   | 2023-06-27 17:00:45,828 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2023-06-27 17:00:45,848 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm2.org_1   | 2023-06-27 17:00:45,852 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm2.org_1   | 2023-06-27 17:00:45,856 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2.org_1   | 2023-06-27 17:00:46,102 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)] to file /data/metadata/scm/keys/secret_keys.json
scm2.org_1   | 2023-06-27 17:00:46,110 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:00:46,110 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-27 17:00:46,110 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2023-06-27 17:00:46,259 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:00:46,283 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:00:46,564 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2023-06-27 17:00:46,666 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2023-06-27 17:00:46,666 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2023-06-27 17:00:47,039 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-27 17:00:47,045 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 17:00:47,047 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2023-06-27 17:00:47,178 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-27 17:00:47,179 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-27 17:00:47,180 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 17:00:47,180 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2023-06-27 17:00:47,272 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2023-06-27 17:00:47,274 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-27 17:00:47,274 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2023-06-27 17:00:47,274 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm2.org_1   | 2023-06-27 17:00:47,802 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 5a021426-02fe-4be7-b9ad-23a479d4b0f6
scm2.org_1   | 2023-06-27 17:00:47,826 [main] INFO server.StorageContainerManager: Persist certificate serialId 253775241875 on Scm Bootstrap Node 5a021426-02fe-4be7-b9ad-23a479d4b0f6
scm3.org_1   | 2023-06-27 17:01:05,730 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2023-06-27 17:01:05,731 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-27 17:01:05,744 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2023-06-27 17:01:05,745 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2023-06-27 17:01:05,766 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2023-06-27 17:01:05,773 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm3.org_1   | 2023-06-27 17:01:05,782 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2023-06-27 17:01:05,784 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2023-06-27 17:01:05,826 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm3.org_1   | 2023-06-27 17:01:06,153 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-27 17:01:06,160 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2023-06-27 17:01:06,163 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3.org_1   | 2023-06-27 17:01:06,164 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3.org_1   | 2023-06-27 17:01:06,165 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3.org_1   | 2023-06-27 17:01:06,166 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3.org_1   | 2023-06-27 17:01:06,170 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2023-06-27 17:01:06,170 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2023-06-27 17:01:06,172 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2023-06-27 17:01:06,298 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm3.org_1   | 2023-06-27 17:01:06,369 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3.org_1   | 2023-06-27 17:01:06,371 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2023-06-27 17:01:06,385 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2023-06-27 17:01:06,387 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2023-06-27 17:01:06,572 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2023-06-27 17:01:06,617 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm3.org_1   | 2023-06-27 17:01:06,623 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-27 17:01:06,664 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2023-06-27 17:01:06,831 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2023-06-27 17:01:06,832 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-27 17:01:06,870 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3.org_1   | 2023-06-27 17:01:06,870 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2023-06-27 17:01:06,890 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3.org_1   | 2023-06-27 17:01:06,897 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3.org_1   | 2023-06-27 17:01:06,948 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3.org_1   | 2023-06-27 17:01:06,953 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3.org_1   | 2023-06-27 17:01:07,064 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-27 17:01:07,065 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-27 17:01:07,125 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2023-06-27 17:01:07,363 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2023-06-27 17:01:07,426 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2023-06-27 17:01:07,430 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm3.org_1   | 2023-06-27 17:01:07,491 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2023-06-27 17:01:07,502 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:01:07,509 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-06-27 17:01:08,207 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm3.org_1   | 2023-06-27 17:01:08,214 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm3.org_1   | 2023-06-27 17:01:08,299 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2023-06-27 17:01:08,359 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 17:01:08,469 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm3.org_1   | 2023-06-27 17:01:08,475 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2023-06-27 17:01:08,569 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm3.org_1   | 2023-06-27 17:01:08,574 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm3.org_1   | 2023-06-27 17:01:10,320 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-27 17:01:10,353 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 17:01:10,355 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm3.org_1   | 2023-06-27 17:01:10,361 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2023-06-27 17:01:10,434 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-27 17:01:10,457 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 17:01:10,459 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm3.org_1   | 2023-06-27 17:01:10,535 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2023-06-27 17:01:10,644 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-27 17:01:10,658 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-27 17:01:10,659 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm3.org_1   | 2023-06-27 17:01:10,660 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2023-06-27 17:01:11,135 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3.org_1   | 2023-06-27 17:01:11,135 [main] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        false
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                                                Value
scm3.org_1   | Threshold                                          10
scm3.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3.org_1   | Max Size to Move per Iteration                     500GB
scm3.org_1   | Max Size Entering Target per Iteration             26GB
scm3.org_1   | Max Size Leaving Source per Iteration              26GB
scm3.org_1   | 
scm3.org_1   | 2023-06-27 17:01:11,136 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2023-06-27 17:01:11,153 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-27 17:01:11,169 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2023-06-27 17:01:11,182 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f does not exist. Creating ...
scm3.org_1   | 2023-06-27 17:01:11,195 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/in_use.lock acquired by nodename 7@scm3.org
scm3.org_1   | 2023-06-27 17:01:11,212 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f has been successfully formatted.
scm3.org_1   | 2023-06-27 17:01:11,224 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2023-06-27 17:01:11,258 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2023-06-27 17:01:11,261 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 17:01:11,266 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3.org_1   | 2023-06-27 17:01:11,269 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3.org_1   | 2023-06-27 17:01:11,282 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-27 17:01:11,304 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2023-06-27 17:01:11,307 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2023-06-27 17:01:11,308 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 17:01:11,341 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm3.org_1   | 2023-06-27 17:01:11,343 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2023-06-27 17:01:11,344 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2023-06-27 17:01:11,348 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-27 17:01:11,351 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3.org_1   | 2023-06-27 17:01:11,351 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2023-06-27 17:01:11,356 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2023-06-27 17:01:11,357 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | 2023-06-27 17:01:11,361 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2023-06-27 17:01:11,399 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2023-06-27 17:01:11,401 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-27 17:01:11,704 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3.org_1   | 2023-06-27 17:01:11,705 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3.org_1   | 2023-06-27 17:01:11,706 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1      | 2023-06-27 17:02:57,928 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 168 failover attempts. Trying to failover immediately. Current retry count: 168.
recon_1      | 2023-06-27 17:02:57,929 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 169 failover attempts. Trying to failover immediately. Current retry count: 169.
recon_1      | 2023-06-27 17:02:57,930 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 170 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 170.
recon_1      | 2023-06-27 17:02:59,931 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 171 failover attempts. Trying to failover immediately. Current retry count: 171.
recon_1      | 2023-06-27 17:02:59,932 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 172 failover attempts. Trying to failover immediately. Current retry count: 172.
recon_1      | 2023-06-27 17:02:59,933 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 173 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 173.
recon_1      | 2023-06-27 17:03:01,934 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 174 failover attempts. Trying to failover immediately. Current retry count: 174.
recon_1      | 2023-06-27 17:03:01,936 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 175 failover attempts. Trying to failover immediately. Current retry count: 175.
recon_1      | 2023-06-27 17:03:01,937 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 176 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 176.
recon_1      | 2023-06-27 17:03:03,938 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 177 failover attempts. Trying to failover immediately. Current retry count: 177.
recon_1      | 2023-06-27 17:03:03,940 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 178 failover attempts. Trying to failover immediately. Current retry count: 178.
recon_1      | 2023-06-27 17:03:03,940 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 179 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 179.
recon_1      | 2023-06-27 17:03:05,942 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 180 failover attempts. Trying to failover immediately. Current retry count: 180.
recon_1      | 2023-06-27 17:03:05,943 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 181 failover attempts. Trying to failover immediately. Current retry count: 181.
recon_1      | 2023-06-27 17:03:05,944 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 182 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 182.
recon_1      | 2023-06-27 17:03:07,945 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 183 failover attempts. Trying to failover immediately. Current retry count: 183.
recon_1      | 2023-06-27 17:03:07,946 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 184 failover attempts. Trying to failover immediately. Current retry count: 184.
recon_1      | 2023-06-27 17:03:07,947 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 185 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 185.
recon_1      | 2023-06-27 17:03:09,949 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 186 failover attempts. Trying to failover immediately. Current retry count: 186.
recon_1      | 2023-06-27 17:03:09,949 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 187 failover attempts. Trying to failover immediately. Current retry count: 187.
recon_1      | 2023-06-27 17:03:09,950 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 188 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 188.
recon_1      | 2023-06-27 17:03:11,952 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 189 failover attempts. Trying to failover immediately. Current retry count: 189.
recon_1      | 2023-06-27 17:03:11,953 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 190 failover attempts. Trying to failover immediately. Current retry count: 190.
recon_1      | 2023-06-27 17:03:11,954 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 191 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 191.
recon_1      | 2023-06-27 17:03:17,495 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:41690 / 172.25.0.102:41690
recon_1      | 2023-06-27 17:03:17,569 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 17:03:18,487 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:55016 / 172.25.0.103:55016
recon_1      | 2023-06-27 17:03:18,510 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:34468 / 172.25.0.104:34468
recon_1      | 2023-06-27 17:03:18,608 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 17:03:18,661 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 17:03:20,451 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=bb6b299a-fe5d-49b8-b1dc-b9564bf6e306. Trying to get from SCM.
recon_1      | 2023-06-27 17:03:21,345 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: bb6b299a-fe5d-49b8-b1dc-b9564bf6e306, Nodes: e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:e5cd4d6c-bb68-491f-b474-5dc217379157, CreationTimestamp2023-06-27T17:02:46.864Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 17:03:21,796 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280. Trying to get from SCM.
recon_1      | 2023-06-27 17:03:21,821 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: df7a4d8c-2676-4111-87bd-cbf4f2545280, Nodes: e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T17:02:47.762Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 17:03:21,870 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 reported by d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-27 17:03:21,871 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 reported by e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 17:03:21,871 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 reported by 3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-27 17:03:26,225 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 reported by e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 17:03:31,494 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:38772 / 172.25.0.104:38772
recon_1      | 2023-06-27 17:03:31,884 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-27 17:03:31,885 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 reported by d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-27 17:03:31,887 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: df7a4d8c-2676-4111-87bd-cbf4f2545280, Nodes: e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d17c8765-e162-413f-ae01-cbbd58f77ae9, CreationTimestamp2023-06-27T17:02:47.762Z[UTC]] moved to OPEN state
recon_1      | 2023-06-27 17:03:34,182 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e50c70ba-12fb-4728-8c61-33b29e5fef17. Trying to get from SCM.
recon_1      | 2023-06-27 17:03:34,702 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e50c70ba-12fb-4728-8c61-33b29e5fef17, Nodes: d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d17c8765-e162-413f-ae01-cbbd58f77ae9, CreationTimestamp2023-06-27T17:02:48.057Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 17:03:34,810 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ee002aed-0fb6-4f4a-94f8-5f40b2f80ade. Trying to get from SCM.
recon_1      | 2023-06-27 17:03:34,876 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ee002aed-0fb6-4f4a-94f8-5f40b2f80ade, Nodes: d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T17:02:48.261Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-27 17:03:34,988 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ee002aed-0fb6-4f4a-94f8-5f40b2f80ade reported by e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-27 17:03:35,893 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:60612 / 172.25.0.103:60612
scm2.org_1   | 2023-06-27 17:00:47,859 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm2.org_1   | 2023-06-27 17:00:48,033 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-5a021426-02fe-4be7-b9ad-23a479d4b0f6: Detected pause in JVM or host machine approximately 0.108s with 0.105s GC time.
scm2.org_1   | GC pool 'ParNew' had collection(s): count=1 time=105ms
scm2.org_1   | 2023-06-27 17:00:48,049 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2023-06-27 17:00:48,052 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2023-06-27 17:00:48,056 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2023-06-27 17:00:48,210 [main] INFO util.log: Logging initialized @28338ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2023-06-27 17:00:48,705 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2023-06-27 17:00:48,723 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2023-06-27 17:00:48,728 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm2.org_1   | 2023-06-27 17:00:48,729 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2023-06-27 17:00:48,729 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2023-06-27 17:00:48,736 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm2.org_1   | 2023-06-27 17:00:48,833 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm2.org_1   | 2023-06-27 17:00:48,837 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm2.org_1   | 2023-06-27 17:00:48,841 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm2.org_1   | 2023-06-27 17:00:48,964 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm2.org_1   | 2023-06-27 17:00:48,968 [main] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2023-06-27 17:00:48,973 [main] INFO server.session: node0 Scavenging every 660000ms
scm2.org_1   | 2023-06-27 17:00:49,056 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-27 17:00:49,063 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63cb06df{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2023-06-27 17:00:49,065 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@126688a7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2.org_1   | 2023-06-27 17:00:49,192 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:00:49,195 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:00:49,485 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-27 17:00:49,573 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8ace41e{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-6745600087712064454/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2023-06-27 17:00:49,629 [main] INFO server.AbstractConnector: Started ServerConnector@3631667d{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2023-06-27 17:00:49,630 [main] INFO server.Server: Started @29759ms
scm2.org_1   | 2023-06-27 17:00:49,636 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2023-06-27 17:00:49,636 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2.org_1   | 2023-06-27 17:00:49,640 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2023-06-27 17:00:54,307 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:00:54,307 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:00:54,382 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:00:59,508 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:00:59,509 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:04,645 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:04,646 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:09,785 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:09,786 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:14,945 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:14,945 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:19,958 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:19,960 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:20,423 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 15: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-06-27 17:01:20,521 [5a021426-02fe-4be7-b9ad-23a479d4b0f6-server-thread1] INFO server.RaftServer$Division: 5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F: set configuration 17: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-27 17:01:25,048 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:25,049 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:30,226 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:30,227 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:35,227 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:35,227 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:40,338 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:40,339 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:45,376 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:45,376 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:50,456 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:50,456 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:01:55,478 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:01:55,478 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:00,555 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:00,555 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:02,035 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:02,423 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:03,843 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:05,628 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:05,629 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:07,212 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:08,395 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:10,719 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:10,720 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:11,424 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:15,888 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:15,891 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:20,912 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-27 17:00:09,655 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm1.org_1   | 2023-06-27 17:00:09,656 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2023-06-27 17:00:09,743 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-27 17:00:09,758 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-27 17:00:09,759 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm1.org_1   | 2023-06-27 17:00:09,770 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2023-06-27 17:00:09,812 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-27 17:00:09,824 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-27 17:00:09,824 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm1.org_1   | 2023-06-27 17:00:09,824 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2023-06-27 17:00:09,917 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1.org_1   | 2023-06-27 17:00:09,918 [main] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        false
scm1.org_1   | Container Balancer Configuration values:
scm1.org_1   | Key                                                Value
scm1.org_1   | Threshold                                          10
scm1.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1.org_1   | Max Size to Move per Iteration                     500GB
scm1.org_1   | Max Size Entering Target per Iteration             26GB
scm1.org_1   | Max Size Leaving Source per Iteration              26GB
scm1.org_1   | 
scm1.org_1   | 2023-06-27 17:00:09,919 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2023-06-27 17:00:09,926 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-27 17:00:09,932 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2023-06-27 17:00:09,939 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/in_use.lock acquired by nodename 8@scm1.org
scm1.org_1   | 2023-06-27 17:00:09,944 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=a2237aa5-d04c-471b-bae4-225264529c4c} from /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/raft-meta
scm1.org_1   | 2023-06-27 17:00:09,989 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:09,991 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-27 17:00:09,999 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-27 17:00:10,000 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 17:00:10,001 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-27 17:00:10,003 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-27 17:00:10,008 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 17:00:10,021 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-27 17:00:10,022 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-27 17:00:10,022 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 17:00:10,073 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f
scm1.org_1   | 2023-06-27 17:00:10,075 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 17:00:10,075 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 17:00:10,077 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-27 17:00:10,079 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-27 17:00:10,080 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-27 17:00:10,081 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-27 17:00:10,082 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-27 17:00:10,082 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-27 17:00:10,092 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-27 17:00:10,092 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 17:00:10,291 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 17:00:10,292 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-27 17:00:10,293 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-06-27 17:00:10,316 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:11,720 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-27 17:01:11,721 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-27 17:01:11,729 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:11,730 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3.org_1   | 2023-06-27 17:01:11,736 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF8E9CD9096F,id=8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf
scm3.org_1   | 2023-06-27 17:01:11,742 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2023-06-27 17:01:11,742 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3.org_1   | 2023-06-27 17:01:11,743 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2023-06-27 17:01:11,745 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2023-06-27 17:01:11,760 [main] INFO server.RaftServer: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: start RPC server
scm3.org_1   | 2023-06-27 17:01:11,874 [main] INFO server.GrpcService: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: GrpcService started, listening on 9894
scm3.org_1   | 2023-06-27 17:01:11,889 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: Started
scm3.org_1   | 2023-06-27 17:01:11,918 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-27 17:01:18,021 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: receive installSnapshot: a2237aa5-d04c-471b-bae4-225264529c4c->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-27 17:01:18,092 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2023-06-27 17:01:18,094 [grpc-default-executor-0] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: change Leader from null to a2237aa5-d04c-471b-bae4-225264529c4c at term 2 for installSnapshot, leader elected after 12326ms
scm3.org_1   | 2023-06-27 17:01:18,144 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: Received notification to install snapshot at index 0
scm3.org_1   | 2023-06-27 17:01:18,149 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3.org_1   | 2023-06-27 17:01:20,033 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set new configuration index: 11
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "a2237aa5-d04c-471b-bae4-225264529c4c"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "5a021426-02fe-4be7-b9ad-23a479d4b0f6"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2023-06-27 17:01:20,067 [grpc-default-executor-0] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 11: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:20,167 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: reply installSnapshot: a2237aa5-d04c-471b-bae4-225264529c4c<-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#0:OK-t0,ALREADY_INSTALLED
scm3.org_1   | 2023-06-27 17:01:20,278 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: Completed INSTALL_SNAPSHOT, lastRequest: a2237aa5-d04c-471b-bae4-225264529c4c->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-27 17:01:20,281 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: Completed INSTALL_SNAPSHOT, lastReply: null
scm3.org_1   | 2023-06-27 17:01:20,574 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO impl.RoleInfo: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: start 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState
scm3.org_1   | 2023-06-27 17:01:20,750 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-06-27 17:01:20,976 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:20,976 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:20,994 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: inconsistency entries. Reply:a2237aa5-d04c-471b-bae4-225264529c4c<-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-27 17:01:21,071 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: Failed appendEntries as previous log entry ((t:2, i:14)) is not found
scm3.org_1   | 2023-06-27 17:01:21,072 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: inconsistency entries. Reply:a2237aa5-d04c-471b-bae4-225264529c4c<-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#2:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-27 17:01:21,129 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: Failed appendEntries as previous log entry ((t:2, i:15)) is not found
scm3.org_1   | 2023-06-27 17:01:21,129 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: inconsistency entries. Reply:a2237aa5-d04c-471b-bae4-225264529c4c<-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#3:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-27 17:01:21,157 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: Failed appendEntries as previous log entry ((t:2, i:17)) is not found
scm3.org_1   | 2023-06-27 17:01:21,157 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: inconsistency entries. Reply:a2237aa5-d04c-471b-bae4-225264529c4c<-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#5:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-27 17:01:21,228 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:21,228 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:21,265 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 9: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:21,265 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 11: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:21,309 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 15: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:21,309 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 17: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:21,618 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO segmented.SegmentedRaftLogWorker: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker: Starting segment from index:0
scm3.org_1   | 2023-06-27 17:01:22,023 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-BF8E9CD9096F:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3.org_1   | 2023-06-27 17:01:22,074 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2023-06-27 17:01:22,029 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO segmented.SegmentedRaftLogWorker: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3.org_1   | 2023-06-27 17:01:22,169 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,170 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,170 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 9: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:22,171 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 11: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,171 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 15: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:22,172 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread1] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 17: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,221 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,270 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,270 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 9: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:22,273 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 11: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,273 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 15: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:22,275 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 17: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,300 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,304 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,310 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 9: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:22,312 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 11: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,315 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 15: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-27 17:01:22,321 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-server-thread2] INFO server.RaftServer$Division: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F: set configuration 17: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-27 17:01:22,416 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm3.org_1   | 2023-06-27 17:01:22,462 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm3.org_1   | 2023-06-27 17:01:22,506 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm3.org_1   | 2023-06-27 17:01:23,952 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_0
scm3.org_1   | 2023-06-27 17:01:24,131 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_0 to /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_0-0
scm3.org_1   | 2023-06-27 17:01:24,567 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_1
scm3.org_1   | 2023-06-27 17:01:24,758 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:01:24,769 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2023-06-27 17:01:24,769 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-27 17:00:10,317 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_0
scm1.org_1   | 2023-06-27 17:00:10,319 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-27 17:00:10,382 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: start as a follower, conf=0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:10,382 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2023-06-27 17:00:10,387 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: start a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState
scm1.org_1   | 2023-06-27 17:00:10,388 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-27 17:00:10,389 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-27 17:00:10,391 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF8E9CD9096F,id=a2237aa5-d04c-471b-bae4-225264529c4c
scm1.org_1   | 2023-06-27 17:00:10,394 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-27 17:00:10,394 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-27 17:00:10,395 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-27 17:00:10,396 [a2237aa5-d04c-471b-bae4-225264529c4c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-27 17:00:10,400 [main] INFO server.RaftServer: a2237aa5-d04c-471b-bae4-225264529c4c: start RPC server
scm1.org_1   | 2023-06-27 17:00:10,446 [main] INFO server.GrpcService: a2237aa5-d04c-471b-bae4-225264529c4c: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-27 17:00:10,447 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a2237aa5-d04c-471b-bae4-225264529c4c: Started
scm1.org_1   | 2023-06-27 17:00:10,458 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-27 17:00:10,459 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2023-06-27 17:00:10,461 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm1.org_1   | 2023-06-27 17:00:10,461 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm1.org_1   | 2023-06-27 17:00:10,462 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm1.org_1   | 2023-06-27 17:00:10,581 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2023-06-27 17:00:10,594 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2023-06-27 17:00:10,595 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2023-06-27 17:00:10,804 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-27 17:00:10,804 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 17:00:10,827 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-27 17:00:10,828 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-27 17:00:10,827 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2023-06-27 17:00:10,829 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 17:00:10,830 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2023-06-27 17:00:10,859 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2023-06-27 17:00:10,860 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 17:00:10,867 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm1.org_1   | 2023-06-27 17:00:10,870 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm1.org_1   | 2023-06-27 17:00:10,932 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm1.org_1   | 2023-06-27 17:00:10,965 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1.org_1   | 2023-06-27 17:00:10,966 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm1.org_1   | 2023-06-27 17:00:10,970 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm1.org_1   | 2023-06-27 17:00:10,996 [main] INFO util.log: Logging initialized @8056ms to org.eclipse.jetty.util.log.Slf4jLog
scm1.org_1   | 2023-06-27 17:00:11,194 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1.org_1   | 2023-06-27 17:00:11,209 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1.org_1   | 2023-06-27 17:00:11,214 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm1.org_1   | 2023-06-27 17:00:11,224 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm1.org_1   | 2023-06-27 17:00:11,224 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm1.org_1   | 2023-06-27 17:00:11,233 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | 2023-06-27 17:00:11,312 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1.org_1   | 2023-06-27 17:00:11,313 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2023-06-27 17:00:11,315 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm1.org_1   | 2023-06-27 17:00:11,446 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2023-06-27 17:00:11,447 [main] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2023-06-27 17:00:11,455 [main] INFO server.session: node0 Scavenging every 660000ms
scm2.org_1   | 2023-06-27 17:02:20,913 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:26,081 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:26,083 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:31,144 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:31,145 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:36,232 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:36,232 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:40,776 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:49872 / 172.25.0.104:49872
scm2.org_1   | 2023-06-27 17:02:40,882 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 17:02:41,421 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:41,421 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:41,757 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:51318 / 172.25.0.102:51318
scm2.org_1   | 2023-06-27 17:02:41,843 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 17:02:42,619 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:58532 / 172.25.0.103:58532
scm2.org_1   | 2023-06-27 17:02:42,741 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 17:02:46,092 [IPC Server handler 69 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e5cd4d6c-bb68-491f-b474-5dc217379157
scm2.org_1   | 2023-06-27 17:02:46,366 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d17c8765-e162-413f-ae01-cbbd58f77ae9
scm2.org_1   | 2023-06-27 17:02:46,384 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3b0419d0-b93c-4a38-a8f8-ba0374c67423
scm2.org_1   | 2023-06-27 17:02:46,408 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3b0419d0-b93c-4a38-a8f8-ba0374c67423{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 382350562679, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-27 17:02:46,391 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d17c8765-e162-413f-ae01-cbbd58f77ae9{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 380923164129, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-27 17:02:46,514 [IPC Server handler 69 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e5cd4d6c-bb68-491f-b474-5dc217379157{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 380187848912, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-27 17:02:46,526 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:46,526 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:46,476 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-27 17:02:46,583 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-27 17:02:46,583 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-27 17:02:46,582 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 17:02:46,614 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 17:02:46,614 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 17:02:46,614 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-27 17:02:46,615 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2.org_1   | 2023-06-27 17:02:46,615 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2.org_1   | 2023-06-27 17:02:46,617 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-27 17:02:47,685 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:48,093 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:48,227 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:48,415 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:48,554 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:02:51,695 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:51,696 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:02:56,767 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:02:56,769 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:03:01,849 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:03:01,849 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:03:06,896 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:03:06,897 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:03:12,002 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:03:12,003 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:03:17,142 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:03:17,143 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:03:17,511 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:36168 / 172.25.0.102:36168
scm2.org_1   | 2023-06-27 17:03:17,583 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 17:03:18,439 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:55116 / 172.25.0.104:55116
scm2.org_1   | 2023-06-27 17:03:18,455 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 17:03:18,481 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:56136 / 172.25.0.103:56136
scm2.org_1   | 2023-06-27 17:03:18,610 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 17:03:20,862 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:03:21,288 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 17:03:22,309 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:03:22,309 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:03:26,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 17:03:27,437 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:03:27,437 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-27 17:03:31,362 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:34610 / 172.25.0.104:34610
scm2.org_1   | 2023-06-27 17:03:31,852 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-27 17:03:31,914 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-27 17:03:32,596 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-27 17:03:32,597 [5a021426-02fe-4be7-b9ad-23a479d4b0f6@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:24,772 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2023-06-27 17:01:24,914 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2023-06-27 17:01:25,279 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2023-06-27 17:01:25,280 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2023-06-27 17:01:26,081 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:26,088 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:26,773 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 17:01:26,824 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2023-06-27 17:01:27,442 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-27 17:01:27,639 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 17:01:27,646 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2023-06-27 17:01:29,326 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)]
scm3.org_1   | 2023-06-27 17:01:29,350 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)
scm3.org_1   | 2023-06-27 17:01:29,775 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-27 17:01:29,776 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-27 17:01:29,844 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 17:01:29,887 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2023-06-27 17:01:30,547 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2023-06-27 17:01:30,557 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2023-06-27 17:01:30,787 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: Detected pause in JVM or host machine approximately 0.205s with 0.166s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=166ms
scm3.org_1   | 2023-06-27 17:01:30,759 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-27 17:01:30,759 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2023-06-27 17:01:31,021 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)] to file /data/metadata/scm/keys/secret_keys.json
scm3.org_1   | 2023-06-27 17:01:31,059 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:01:31,059 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-27 17:01:31,059 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2023-06-27 17:01:31,262 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:31,262 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:31,874 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:01:31,957 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:01:32,043 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:01:33,199 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf
scm3.org_1   | 2023-06-27 17:01:33,236 [main] INFO server.StorageContainerManager: Persist certificate serialId 253775241875 on Scm Bootstrap Node 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf
scm3.org_1   | 2023-06-27 17:01:33,325 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm3.org_1   | 2023-06-27 17:01:33,591 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2023-06-27 17:01:33,593 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2023-06-27 17:01:33,610 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2023-06-27 17:01:34,206 [main] INFO util.log: Logging initialized @38896ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2023-06-27 17:01:36,191 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: Detected pause in JVM or host machine approximately 0.269s with 0.394s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=394ms
scm3.org_1   | 2023-06-27 17:01:36,234 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2023-06-27 17:01:36,376 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:36,377 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:36,393 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2023-06-27 17:01:36,422 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2023-06-27 17:01:36,422 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2023-06-27 17:01:36,422 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2023-06-27 17:01:36,456 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2023-06-27 17:01:37,031 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm3.org_1   | 2023-06-27 17:01:37,052 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2023-06-27 17:01:37,076 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm3.org_1   | 2023-06-27 17:01:37,479 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2023-06-27 17:01:37,485 [main] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2023-06-27 17:01:37,499 [main] INFO server.session: node0 Scavenging every 660000ms
scm3.org_1   | 2023-06-27 17:01:37,829 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-06-27 17:01:37,872 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7abc82fa{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2023-06-27 17:01:37,876 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4aa8504f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2023-06-27 17:01:38,846 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-06-27 17:01:38,919 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@28c319a2{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-14049180506836063936/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2023-06-27 17:01:39,042 [main] INFO server.AbstractConnector: Started ServerConnector@578f8288{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2023-06-27 17:01:39,042 [main] INFO server.Server: Started @43732ms
scm3.org_1   | 2023-06-27 17:01:39,083 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2023-06-27 17:01:39,084 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2023-06-27 17:01:39,092 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2023-06-27 17:01:41,388 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:41,388 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:46,482 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:46,482 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:51,536 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:51,536 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:01:56,572 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:01:56,573 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:01,755 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:01,756 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:02,046 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:02,407 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:03,867 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:06,877 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:06,878 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:07,174 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:08,349 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:11,437 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:12,038 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:12,039 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:17,227 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:17,228 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:22,264 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-27 17:00:11,553 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-27 17:00:11,576 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:38052 / 172.25.0.117:38052
scm1.org_1   | 2023-06-27 17:00:11,588 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@373efaa2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2023-06-27 17:00:11,591 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d98522c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2023-06-27 17:00:11,606 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:00:11,739 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-27 17:00:11,750 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@22608777{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-4136415656350131397/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2023-06-27 17:00:11,757 [main] INFO server.AbstractConnector: Started ServerConnector@69e67993{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2023-06-27 17:00:11,757 [main] INFO server.Server: Started @8818ms
scm1.org_1   | 2023-06-27 17:00:11,772 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2023-06-27 17:00:11,772 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2023-06-27 17:00:11,773 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2023-06-27 17:00:12,670 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:57210 / 172.25.0.116:57210
scm1.org_1   | 2023-06-27 17:00:12,693 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:00:14,698 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:38271 / 172.25.0.115:38271
scm1.org_1   | 2023-06-27 17:00:14,720 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:00:14,727 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#12 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:38271 / 172.25.0.115:38271
scm1.org_1   | org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:a2237aa5-d04c-471b-bae4-225264529c4c is not the leader. Could not determine the leader node.
scm1.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1.org_1   | 2023-06-27 17:00:15,509 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO impl.FollowerState: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5122716403ns, electionTimeout:5119ms
scm1.org_1   | 2023-06-27 17:00:15,510 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: shutdown a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState
scm1.org_1   | 2023-06-27 17:00:15,511 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2023-06-27 17:00:15,514 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-27 17:00:15,515 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-FollowerState] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: start a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1
scm1.org_1   | 2023-06-27 17:00:15,524 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:15,525 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-27 17:00:15,527 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:15,529 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.LeaderElection: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1.org_1   | 2023-06-27 17:00:15,529 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: shutdown a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1
scm1.org_1   | 2023-06-27 17:00:15,530 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1.org_1   | 2023-06-27 17:00:15,530 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1.org_1   | 2023-06-27 17:00:15,530 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm1.org_1   | 2023-06-27 17:00:15,534 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: change Leader from null to a2237aa5-d04c-471b-bae4-225264529c4c at term 2 for becomeLeader, leader elected after 7824ms
scm1.org_1   | 2023-06-27 17:00:15,543 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-27 17:00:15,553 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 17:00:15,554 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-27 17:00:15,559 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-27 17:00:15,559 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-27 17:00:15,560 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-27 17:00:15,578 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-27 17:00:15,580 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-27 17:00:15,581 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO impl.RoleInfo: a2237aa5-d04c-471b-bae4-225264529c4c: start a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl
scm1.org_1   | 2023-06-27 17:00:15,587 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1.org_1   | 2023-06-27 17:00:15,592 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_0 to /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_0-0
scm1.org_1   | 2023-06-27 17:00:15,623 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderElection1] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 1: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:15,625 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/d93048ad-6cc2-478b-8bd8-bf8e9cd9096f/current/log_inprogress_1
scm1.org_1   | 2023-06-27 17:00:15,634 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2023-06-27 17:00:15,636 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1.org_1   | 2023-06-27 17:00:15,637 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
scm1.org_1   | 2023-06-27 17:00:15,638 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637793Z, expire at: 2023-06-27T18:00:15.637793Z)
scm1.org_1   | 2023-06-27 17:00:15,647 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:00:15,649 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1.org_1   | 2023-06-27 17:00:15,650 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-27 17:00:15,650 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1.org_1   | 2023-06-27 17:00:15,657 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-27 17:00:15,661 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2023-06-27 17:00:15,796 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)]
scm1.org_1   | 2023-06-27 17:00:15,799 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)
scm1.org_1   | 2023-06-27 17:00:15,851 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 2d9c5171-e911-4bf5-a2c4-6966c1938b04, creation at: 2023-06-27T17:00:15.637Z, expire at: 2023-06-27T18:00:15.637Z)] to file /data/metadata/scm/keys/secret_keys.json
scm1.org_1   | 2023-06-27 17:00:15,852 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:00:15,853 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-27 17:00:15,853 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2023-06-27 17:00:18,175 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:43778 / 172.25.0.117:43778
scm1.org_1   | 2023-06-27 17:00:18,180 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:00:18,262 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: 5a021426-02fe-4be7-b9ad-23a479d4b0f6
scm1.org_1   | 2023-06-27 17:00:18,413 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 17:00:19,068 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:00:20,772 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: 2f3bcb34-1854-4f82-be39-90ad3aee6a7b
scm1.org_1   | 2023-06-27 17:00:20,847 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:00:40,005 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:33562 / 172.25.0.116:33562
scm1.org_1   | 2023-06-27 17:00:40,095 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:00:41,511 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:37974 / 172.25.0.117:37974
scm1.org_1   | 2023-06-27 17:00:41,675 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:00:41,692 [IPC Server handler 20 on default port 9863] INFO ha.SCMRatisServerImpl: a2237aa5-d04c-471b-bae4-225264529c4c: Submitting SetConfiguration request to Ratis server with new SCM peers list: [a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-27 17:00:41,710 [IPC Server handler 20 on default port 9863] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: receive setConfiguration SetConfigurationRequest:client-524F91E851D9->a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 17:00:41,714 [IPC Server handler 20 on default port 9863] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-524F91E851D9->a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 17:00:41,754 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-27 17:00:41,754 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 17:00:41,754 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-27 17:00:41,792 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-27 17:00:41,794 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-27 17:00:41,795 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 17:00:41,797 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-27 17:00:41,797 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1.org_1   | 2023-06-27 17:00:41,797 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 17:00:41,797 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 17:00:41,825 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-06-27 17:00:41,898 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-GrpcLogAppender: send a2237aa5-d04c-471b-bae4-225264529c4c->5a021426-02fe-4be7-b9ad-23a479d4b0f6#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-27 17:00:41,919 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 5a021426-02fe-4be7-b9ad-23a479d4b0f6
scm1.org_1   | 2023-06-27 17:00:43,753 [grpc-default-executor-2] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-InstallSnapshotResponseHandler: received the first reply a2237aa5-d04c-471b-bae4-225264529c4c<-5a021426-02fe-4be7-b9ad-23a479d4b0f6#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 17:00:43,799 [grpc-default-executor-2] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-27 17:00:43,799 [grpc-default-executor-2] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-27 17:00:43,800 [grpc-default-executor-2] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-27 17:00:43,800 [grpc-default-executor-2] INFO leader.FollowerInfo: Follower a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6 acknowledged installing snapshot
scm1.org_1   | 2023-06-27 17:00:43,800 [grpc-default-executor-2] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 17:00:44,178 [grpc-default-executor-2] WARN server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 17:00:44,189 [grpc-default-executor-2] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6: setNextIndex nextIndex: updateUnconditionally 9 -> 0
scm1.org_1   | 2023-06-27 17:00:44,230 [grpc-default-executor-2] WARN server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 17:00:44,230 [grpc-default-executor-2] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->5a021426-02fe-4be7-b9ad-23a479d4b0f6: setNextIndex nextIndex: updateUnconditionally 9 -> 0
scm1.org_1   | 2023-06-27 17:00:45,284 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 9: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-27 17:00:45,327 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 11: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:00:45,363 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:43489 / 172.25.0.115:43489
scm1.org_1   | 2023-06-27 17:00:45,390 [IPC Server handler 20 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 5a021426-02fe-4be7-b9ad-23a479d4b0f6.
scm1.org_1   | 2023-06-27 17:00:45,463 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:00:47,569 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:59172 / 172.25.0.117:59172
scm1.org_1   | 2023-06-27 17:00:47,608 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:00:50,632 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:46804 / 172.25.0.118:46804
scm1.org_1   | 2023-06-27 17:00:50,686 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:00:54,199 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:60652 / 172.25.0.118:60652
scm1.org_1   | 2023-06-27 17:00:54,209 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:00:54,210 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf
scm1.org_1   | 2023-06-27 17:00:54,252 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-27 17:00:54,383 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:00:57,671 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:50772 / 172.25.0.116:50772
scm1.org_1   | 2023-06-27 17:00:57,773 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:01:14,935 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:50082 / 172.25.0.118:50082
scm1.org_1   | 2023-06-27 17:01:15,199 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:01:15,202 [IPC Server handler 20 on default port 9863] INFO ha.SCMRatisServerImpl: a2237aa5-d04c-471b-bae4-225264529c4c: Submitting SetConfiguration request to Ratis server with new SCM peers list: [a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-27 17:01:15,202 [IPC Server handler 20 on default port 9863] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: receive setConfiguration SetConfigurationRequest:client-524F91E851D9->a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 17:01:15,207 [IPC Server handler 20 on default port 9863] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-524F91E851D9->a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-27 17:01:15,216 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-27 17:01:15,216 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-27 17:01:15,218 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-27 17:01:15,239 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-27 17:01:15,240 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-27 17:01:15,240 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-27 17:01:15,240 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-27 17:01:15,240 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm2.org_1   | 2023-06-27 17:03:34,110 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-27 17:03:34,110 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-27 17:03:34,110 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2.org_1   | 2023-06-27 17:03:34,110 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2.org_1   | 2023-06-27 17:03:34,110 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3.org_1   | 2023-06-27 17:02:22,264 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:27,407 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:27,407 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:32,464 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:32,464 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:37,552 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:37,553 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:41,024 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:33546 / 172.25.0.104:33546
scm3.org_1   | 2023-06-27 17:02:41,131 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 17:02:41,753 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:56526 / 172.25.0.102:56526
scm3.org_1   | 2023-06-27 17:02:41,788 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 17:02:42,590 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:42,591 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:42,775 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:35086 / 172.25.0.103:35086
scm3.org_1   | 2023-06-27 17:02:43,020 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 17:02:46,201 [IPC Server handler 38 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e5cd4d6c-bb68-491f-b474-5dc217379157
scm3.org_1   | 2023-06-27 17:02:46,300 [IPC Server handler 38 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e5cd4d6c-bb68-491f-b474-5dc217379157{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 380187848912, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-27 17:02:46,332 [IPC Server handler 15 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3b0419d0-b93c-4a38-a8f8-ba0374c67423
scm3.org_1   | 2023-06-27 17:02:46,363 [IPC Server handler 15 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3b0419d0-b93c-4a38-a8f8-ba0374c67423{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 382350562679, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-27 17:02:46,401 [IPC Server handler 16 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d17c8765-e162-413f-ae01-cbbd58f77ae9
scm3.org_1   | 2023-06-27 17:02:46,401 [IPC Server handler 16 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d17c8765-e162-413f-ae01-cbbd58f77ae9{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 380923164129, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-27 17:02:46,433 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-27 17:02:46,494 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-27 17:02:46,494 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-27 17:02:46,512 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 17:02:46,512 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 17:02:46,513 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 17:02:46,529 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-27 17:02:46,529 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2023-06-27 17:02:46,530 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2023-06-27 17:02:46,544 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-27 17:02:47,787 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:47,799 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:47,801 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:48,131 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:48,263 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:48,370 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:48,541 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:02:52,910 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:52,911 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:02:58,090 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:02:58,091 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:03,260 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:03:03,260 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:08,433 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:03:08,433 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:13,480 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:03:13,481 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:17,490 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:47798 / 172.25.0.102:47798
scm3.org_1   | 2023-06-27 17:03:17,564 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 17:03:18,512 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:57524 / 172.25.0.104:57524
scm3.org_1   | 2023-06-27 17:03:18,552 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:53776 / 172.25.0.103:53776
scm3.org_1   | 2023-06-27 17:03:18,620 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 17:03:18,638 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:03:18,638 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:18,656 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 17:03:21,050 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: Detected pause in JVM or host machine approximately 0.101s with 0.223s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=223ms
scm3.org_1   | 2023-06-27 17:03:21,117 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:03:21,324 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 17:03:23,756 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:03:23,757 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:26,217 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-27 17:03:28,847 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:03:28,848 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:31,487 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:52138 / 172.25.0.104:52138
scm3.org_1   | 2023-06-27 17:03:31,856 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-27 17:03:31,929 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-27 17:03:31,940 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 17:01:15,240 [IPC Server handler 20 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-27 17:01:15,240 [IPC Server handler 20 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-27 17:01:15,248 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-06-27 17:01:15,264 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-GrpcLogAppender: send a2237aa5-d04c-471b-bae4-225264529c4c->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-27 17:01:15,264 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf
scm1.org_1   | 2023-06-27 17:01:18,526 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:60704 / 172.25.0.116:60704
scm1.org_1   | 2023-06-27 17:01:18,851 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:01:20,268 [grpc-default-executor-0] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-InstallSnapshotResponseHandler: received the first reply a2237aa5-d04c-471b-bae4-225264529c4c<-8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 17:01:20,273 [grpc-default-executor-0] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-27 17:01:20,273 [grpc-default-executor-0] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-27 17:01:20,282 [grpc-default-executor-0] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-27 17:01:20,287 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf acknowledged installing snapshot
scm1.org_1   | 2023-06-27 17:01:20,293 [grpc-default-executor-0] INFO server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm1.org_1   | 2023-06-27 17:01:20,408 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 15: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-27 17:01:20,482 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-LeaderStateImpl] INFO server.RaftServer$Division: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F: set configuration 17: peers:[a2237aa5-d04c-471b-bae4-225264529c4c|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5a021426-02fe-4be7-b9ad-23a479d4b0f6|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-27 17:01:20,575 [IPC Server handler 20 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf.
scm1.org_1   | 2023-06-27 17:01:21,048 [grpc-default-executor-1] WARN server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 17:01:21,085 [grpc-default-executor-1] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-27 17:01:21,096 [grpc-default-executor-1] WARN server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 17:01:21,096 [grpc-default-executor-1] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-27 17:01:21,138 [grpc-default-executor-1] WARN server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 17:01:21,138 [grpc-default-executor-1] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-27 17:01:21,165 [grpc-default-executor-1] WARN server.GrpcLogAppender: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-27 17:01:21,166 [grpc-default-executor-1] INFO leader.FollowerInfo: a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F->8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-27 17:01:32,238 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:50578 / 172.25.0.118:50578
scm1.org_1   | 2023-06-27 17:01:32,315 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:01:47,008 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:34697 / 172.25.0.115:34697
scm1.org_1   | 2023-06-27 17:01:47,099 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:01:50,000 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:40782 / 172.25.0.112:40782
scm1.org_1   | 2023-06-27 17:01:50,222 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:01:50,485 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:49704 / 172.25.0.113:49704
scm1.org_1   | 2023-06-27 17:01:50,736 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:01:51,012 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:49934 / 172.25.0.111:49934
scm1.org_1   | 2023-06-27 17:01:51,123 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:02:00,998 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:53886 / 172.25.0.102:53886
scm1.org_1   | 2023-06-27 17:02:01,140 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:01,160 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 4cab33795ed1, UUID: e5cd4d6c-bb68-491f-b474-5dc217379157
scm1.org_1   | 2023-06-27 17:02:01,657 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:43490 / 172.25.0.104:43490
scm1.org_1   | 2023-06-27 17:02:01,751 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:01,753 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn c4c5f0c38083, UUID: d17c8765-e162-413f-ae01-cbbd58f77ae9
scm1.org_1   | 2023-06-27 17:02:02,050 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:02,386 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:03,403 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:35356 / 172.25.0.103:35356
scm1.org_1   | 2023-06-27 17:02:03,539 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:03,547 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn afaecf158e34, UUID: 3b0419d0-b93c-4a38-a8f8-ba0374c67423
scm1.org_1   | 2023-06-27 17:02:03,837 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:04,521 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:39726 / 172.25.0.102:39726
scm1.org_1   | 2023-06-27 17:02:04,617 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-27 17:02:05,021 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:40082 / 172.25.0.104:40082
scm1.org_1   | 2023-06-27 17:02:05,053 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-27 17:02:06,735 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:35368 / 172.25.0.103:35368
scm1.org_1   | 2023-06-27 17:02:06,748 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:40556 / 172.25.0.111:40556
scm1.org_1   | 2023-06-27 17:02:06,774 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:06,793 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: cc7abbff-3b8f-44b2-bad6-765b4550ec43
scm1.org_1   | 2023-06-27 17:02:06,811 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-27 17:02:07,172 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:07,923 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:40838 / 172.25.0.113:40838
scm1.org_1   | 2023-06-27 17:02:07,961 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:07,962 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: 6d22b3f6-cb84-47b3-9df3-e64110a77ab6
scm1.org_1   | 2023-06-27 17:02:08,320 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:11,060 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:51878 / 172.25.0.112:51878
scm1.org_1   | 2023-06-27 17:02:11,085 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:11,089 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: 9d896ac6-6ad2-4655-a230-d8f738e354eb
scm1.org_1   | 2023-06-27 17:02:11,429 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:29,855 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:57914 / 172.25.0.104:57914
scm1.org_1   | 2023-06-27 17:02:29,883 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:30,535 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:33684 / 172.25.0.102:33684
scm1.org_1   | 2023-06-27 17:02:30,570 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:30,751 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:35980 / 172.25.0.103:35980
scm1.org_1   | 2023-06-27 17:02:30,796 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:02:31,073 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:41110 / 172.25.0.116:41110
scm1.org_1   | 2023-06-27 17:02:31,380 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:02:40,793 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:45514 / 172.25.0.104:45514
scm1.org_1   | 2023-06-27 17:02:40,881 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 17:02:41,810 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:47138 / 172.25.0.102:47138
scm1.org_1   | 2023-06-27 17:02:41,869 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 17:02:42,616 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:46128 / 172.25.0.103:46128
scm1.org_1   | 2023-06-27 17:02:42,749 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 17:02:46,375 [IPC Server handler 50 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d17c8765-e162-413f-ae01-cbbd58f77ae9
scm1.org_1   | 2023-06-27 17:02:46,506 [IPC Server handler 50 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d17c8765-e162-413f-ae01-cbbd58f77ae9{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 380923164129, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-27 17:02:46,507 [IPC Server handler 45 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3b0419d0-b93c-4a38-a8f8-ba0374c67423
scm1.org_1   | 2023-06-27 17:02:46,561 [IPC Server handler 45 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3b0419d0-b93c-4a38-a8f8-ba0374c67423{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 382350562679, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-27 17:02:46,570 [IPC Server handler 58 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e5cd4d6c-bb68-491f-b474-5dc217379157
scm1.org_1   | 2023-06-27 17:02:46,571 [IPC Server handler 58 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e5cd4d6c-bb68-491f-b474-5dc217379157{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 380187848912, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-27 17:02:46,683 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 17:02:46,796 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 17:02:46,796 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 17:02:46,915 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=bb6b299a-fe5d-49b8-b1dc-b9564bf6e306 to datanode:e5cd4d6c-bb68-491f-b474-5dc217379157
scm1.org_1   | 2023-06-27 17:02:46,910 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-27 17:02:46,969 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-27 17:02:46,970 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-27 17:02:46,970 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-27 17:02:46,970 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2023-06-27 17:02:46,970 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2023-06-27 17:02:46,970 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-27 17:02:47,447 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:47,479 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: bb6b299a-fe5d-49b8-b1dc-b9564bf6e306, Nodes: e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T17:02:46.864223Z[UTC]].
scm1.org_1   | 2023-06-27 17:02:47,762 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 to datanode:e5cd4d6c-bb68-491f-b474-5dc217379157
scm1.org_1   | 2023-06-27 17:02:47,769 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 to datanode:3b0419d0-b93c-4a38-a8f8-ba0374c67423
scm3.org_1   | 2023-06-27 17:03:31,940 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-27 17:03:31,948 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm3.org_1   | 2023-06-27 17:03:31,948 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3.org_1   | 2023-06-27 17:03:31,948 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3.org_1   | 2023-06-27 17:03:33,950 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-27 17:03:33,950 [8c6d3de5-7dd6-40e5-a7f9-e0a1c4691cdf@group-BF8E9CD9096F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-27 17:03:35,964 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:33468 / 172.25.0.103:33468
scm1.org_1   | 2023-06-27 17:02:47,790 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 to datanode:d17c8765-e162-413f-ae01-cbbd58f77ae9
scm1.org_1   | 2023-06-27 17:02:47,971 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:48,055 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: df7a4d8c-2676-4111-87bd-cbf4f2545280, Nodes: e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T17:02:47.762586Z[UTC]].
scm1.org_1   | 2023-06-27 17:02:48,057 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e50c70ba-12fb-4728-8c61-33b29e5fef17 to datanode:d17c8765-e162-413f-ae01-cbbd58f77ae9
scm1.org_1   | 2023-06-27 17:02:48,193 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:48,259 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: e50c70ba-12fb-4728-8c61-33b29e5fef17, Nodes: d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T17:02:48.057050Z[UTC]].
scm1.org_1   | 2023-06-27 17:02:48,261 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ee002aed-0fb6-4f4a-94f8-5f40b2f80ade to datanode:d17c8765-e162-413f-ae01-cbbd58f77ae9
scm1.org_1   | 2023-06-27 17:02:48,265 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ee002aed-0fb6-4f4a-94f8-5f40b2f80ade to datanode:3b0419d0-b93c-4a38-a8f8-ba0374c67423
scm1.org_1   | 2023-06-27 17:02:48,270 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ee002aed-0fb6-4f4a-94f8-5f40b2f80ade to datanode:e5cd4d6c-bb68-491f-b474-5dc217379157
scm1.org_1   | 2023-06-27 17:02:48,360 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:48,383 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: ee002aed-0fb6-4f4a-94f8-5f40b2f80ade, Nodes: d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T17:02:48.261528Z[UTC]].
scm1.org_1   | 2023-06-27 17:02:48,392 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=ee002aed-0fb6-4f4a-94f8-5f40b2f80ade contains same datanodes as previous pipelines: PipelineID=df7a4d8c-2676-4111-87bd-cbf4f2545280 nodeIds: d17c8765-e162-413f-ae01-cbbd58f77ae9, 3b0419d0-b93c-4a38-a8f8-ba0374c67423, e5cd4d6c-bb68-491f-b474-5dc217379157
scm1.org_1   | 2023-06-27 17:02:48,395 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e2e8a98a-d1bb-4d37-ad77-14245c896f1b to datanode:3b0419d0-b93c-4a38-a8f8-ba0374c67423
scm1.org_1   | 2023-06-27 17:02:48,456 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:02:48,575 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Added pipeline Pipeline[ Id: e2e8a98a-d1bb-4d37-ad77-14245c896f1b, Nodes: 3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-27T17:02:48.395073Z[UTC]].
scm1.org_1   | 2023-06-27 17:02:48,581 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-27 17:02:48,583 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-27 17:02:50,442 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:46538 / 172.25.0.111:46538
scm1.org_1   | 2023-06-27 17:02:50,553 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:02:54,215 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:46924 / 172.25.0.113:46924
scm1.org_1   | 2023-06-27 17:02:54,270 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:02:54,395 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:57450 / 172.25.0.112:57450
scm1.org_1   | 2023-06-27 17:02:54,493 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-27 17:03:17,509 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:54750 / 172.25.0.102:54750
scm1.org_1   | 2023-06-27 17:03:17,584 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 17:03:18,589 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-27 17:03:18,629 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:53736 / 172.25.0.103:53736
scm1.org_1   | 2023-06-27 17:03:18,673 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 17:03:18,732 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:54058 / 172.25.0.104:54058
scm1.org_1   | 2023-06-27 17:03:18,750 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 17:03:20,653 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:03:20,710 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:46123 / 172.25.0.115:46123
scm1.org_1   | 2023-06-27 17:03:20,883 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: bb6b299a-fe5d-49b8-b1dc-b9564bf6e306, Nodes: e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e5cd4d6c-bb68-491f-b474-5dc217379157, CreationTimestamp2023-06-27T17:02:46.864Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-06-27 17:03:21,030 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 17:03:21,102 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:03:21,267 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 17:03:23,897 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:34930 / 172.25.0.111:34930
scm1.org_1   | 2023-06-27 17:03:23,955 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-27 17:03:24,789 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:34942 / 172.25.0.111:34942
scm1.org_1   | 2023-06-27 17:03:24,843 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:03:26,246 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 17:03:26,366 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:41192 / 172.25.0.116:41192
scm1.org_1   | 2023-06-27 17:03:26,751 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:03:30,594 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:34084 / 172.25.0.113:34084
scm1.org_1   | 2023-06-27 17:03:30,716 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-27 17:03:31,452 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:51166 / 172.25.0.104:51166
scm1.org_1   | 2023-06-27 17:03:31,817 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-27 17:03:31,900 [a2237aa5-d04c-471b-bae4-225264529c4c@group-BF8E9CD9096F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-27 17:03:31,905 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: df7a4d8c-2676-4111-87bd-cbf4f2545280, Nodes: e5cd4d6c-bb68-491f-b474-5dc217379157(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)3b0419d0-b93c-4a38-a8f8-ba0374c67423(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d17c8765-e162-413f-ae01-cbbd58f77ae9, CreationTimestamp2023-06-27T17:02:47.762Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1.org_1   | 2023-06-27 17:03:31,913 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO security.RootCARotationManager: notifyStatusChanged: enable monitor task
scm1.org_1   | 2023-06-27 17:03:31,999 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:34086 / 172.25.0.113:34086
scm1.org_1   | 2023-06-27 17:03:32,046 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1.org_1   | 2023-06-27 17:03:32,058 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm1.org_1   | 2023-06-27 17:03:32,099 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:03:32,967 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:60582 / 172.25.0.112:60582
scm1.org_1   | 2023-06-27 17:03:33,046 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-27 17:03:34,309 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e50c70ba-12fb-4728-8c61-33b29e5fef17, Nodes: d17c8765-e162-413f-ae01-cbbd58f77ae9(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d17c8765-e162-413f-ae01-cbbd58f77ae9, CreationTimestamp2023-06-27T17:02:48.057Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-06-27 17:03:34,451 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:45263 / 172.25.0.115:45263
scm1.org_1   | 2023-06-27 17:03:34,662 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-27 17:03:34,952 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:60586 / 172.25.0.112:60586
scm1.org_1   | 2023-06-27 17:03:35,105 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-27 17:03:35,963 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:34920 / 172.25.0.103:34920
