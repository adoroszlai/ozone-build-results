Attaching to ozonesecure-ha_scm1.org_1, ozonesecure-ha_s3g_1, ozonesecure-ha_recon_1, ozonesecure-ha_scm3.org_1, ozonesecure-ha_kdc_1, ozonesecure-ha_datanode2_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_kms_1, ozonesecure-ha_om3_1, ozonesecure-ha_datanode3_1, ozonesecure-ha_httpfs_1, ozonesecure-ha_datanode1_1, ozonesecure-ha_om1_1, ozonesecure-ha_om2_1
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2023-06-30 10:06:01,094 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = 2b3975d57ed5/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode1_1  | STARTUP_MSG:   java = 11.0.19
datanode1_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode1_1  | ************************************************************/
datanode1_1  | 2023-06-30 10:06:01,209 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2023-06-30 10:06:01,571 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2023-06-30 10:06:02,637 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2023-06-30 10:06:04,280 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2023-06-30 10:06:04,284 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2023-06-30 10:06:05,580 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2b3975d57ed5 ip:172.25.0.102
datanode1_1  | 2023-06-30 10:06:10,549 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2023-06-30 10:06:12,061 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2023-06-30 10:06:12,064 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2023-06-30 10:06:13,896 [main] INFO reflections.Reflections: Reflections took 1258 ms to scan 2 urls, producing 107 keys and 231 values 
datanode1_1  | 2023-06-30 10:06:19,437 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2023-06-30 10:06:22,227 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode1_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode1_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode1_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode1_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode1_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode1_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode1_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode1_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode1_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode1_1  | 2023-06-30 10:06:22,260 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2023-06-30 10:06:22,269 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2023-06-30 10:06:22,281 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2023-06-30 10:06:26,705 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2023-06-30 10:06:27,143 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:2b3975d57ed5
datanode1_1  | 2023-06-30 10:06:27,150 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2023-06-30 10:06:27,221 [main] ERROR utils.CertificateSignRequest: Invalid domain 2b3975d57ed5
datanode1_1  | 2023-06-30 10:06:27,228 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@2b3975d57ed5
datanode1_1  | 2023-06-30 10:06:32,140 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 253598402501
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode1_1  |            Start Date: Fri Jun 30 10:04:21 UTC 2023
datanode1_1  |            Final Date: Mon Aug 07 10:04:21 UTC 2028
datanode1_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode1_1  |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
datanode1_1  |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
datanode1_1  |                        43e78ba4990cf8693d7e311e758874532e0e4770
datanode1_1  |                        6e386edb08789430a89a39ebde59b89ba6e84e94
datanode1_1  |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
datanode1_1  |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
datanode1_1  |                        73af7af9023c681485766dec4627568ceadcae31
datanode1_1  |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
datanode1_1  |                        988b846e620e04205f341894328c4379e192c0f0
datanode1_1  |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
datanode1_1  |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
datanode1_1  |                        d924b613b78a9538512bbc922e5120bb84930f4c
datanode1_1  |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
datanode1_1  |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0xbe
datanode1_1  |  from file: /data/metadata/dn/certs/CA-253598402501.crt.
datanode1_1  | 2023-06-30 10:06:32,184 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 1
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode1_1  |            Start Date: Fri Jun 30 10:04:21 UTC 2023
datanode1_1  |            Final Date: Mon Aug 07 10:04:21 UTC 2028
datanode1_1  |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode1_1  |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
datanode1_1  |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
datanode1_1  |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
datanode1_1  |                        dead23e7b54024421f408e091345739cf1861cb6
datanode1_1  |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
datanode1_1  |                        6385912587b5531778fa6f5c476842a25dc986ad
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2023-06-30 10:05:59,263 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = f831826c9e82/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode2_1  | STARTUP_MSG:   java = 11.0.19
datanode2_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode2_1  | ************************************************************/
datanode2_1  | 2023-06-30 10:05:59,519 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-06-30 10:06:00,250 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2023-06-30 10:06:01,282 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2023-06-30 10:06:03,082 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2023-06-30 10:06:03,082 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2023-06-30 10:06:04,217 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:f831826c9e82 ip:172.25.0.103
datanode2_1  | 2023-06-30 10:06:09,727 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2023-06-30 10:06:11,044 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2023-06-30 10:06:11,052 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2023-06-30 10:06:13,204 [main] INFO reflections.Reflections: Reflections took 1506 ms to scan 2 urls, producing 107 keys and 231 values 
datanode2_1  | 2023-06-30 10:06:18,661 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2023-06-30 10:06:20,689 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode2_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode2_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode2_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode2_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode2_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode2_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode2_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode2_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode2_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode2_1  | 2023-06-30 10:06:20,709 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2023-06-30 10:06:20,716 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2023-06-30 10:06:20,725 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2023-06-30 10:06:32,180 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2023-06-30 10:06:32,668 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:f831826c9e82
datanode2_1  | 2023-06-30 10:06:32,669 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2023-06-30 10:06:32,738 [main] ERROR utils.CertificateSignRequest: Invalid domain f831826c9e82
datanode2_1  | 2023-06-30 10:06:32,748 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@f831826c9e82
datanode2_1  | 2023-06-30 10:06:37,570 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 253598402501
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode2_1  |            Start Date: Fri Jun 30 10:04:21 UTC 2023
datanode2_1  |            Final Date: Mon Aug 07 10:04:21 UTC 2028
datanode2_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode2_1  |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
datanode2_1  |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
datanode2_1  |                        43e78ba4990cf8693d7e311e758874532e0e4770
datanode2_1  |                        6e386edb08789430a89a39ebde59b89ba6e84e94
datanode2_1  |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
datanode2_1  |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
datanode2_1  |                        73af7af9023c681485766dec4627568ceadcae31
datanode2_1  |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
datanode2_1  |                        988b846e620e04205f341894328c4379e192c0f0
datanode2_1  |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
datanode2_1  |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
datanode2_1  |                        d924b613b78a9538512bbc922e5120bb84930f4c
datanode2_1  |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
datanode2_1  |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
kdc_1        | Jun 30 10:03:52 kdc krb5kdc[7](info): Loaded
kdc_1        | Jun 30 10:03:52 kdc krb5kdc[7](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jun 30 10:03:52 kdc krb5kdc[7](info): setting up network...
kdc_1        | Jun 30 10:03:52 kdc krb5kdc[7](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jun 30 10:03:52 kdc krb5kdc[7](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jun 30 10:03:52 kdc krb5kdc[7](info): set up 4 sockets
kdc_1        | Jun 30 10:03:52 kdc krb5kdc[7](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jun 30 10:03:55 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688119435, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:04:00 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.119: ISSUE: authtime 1688119440, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, httpfs/httpfs@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:04:02 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.114: ISSUE: authtime 1688119442, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, s3g/s3g@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:04:10 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.115: ISSUE: authtime 1688119450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:04:26 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1688119466, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:04:32 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1688119472, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:04:39 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1688119466, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:04:40 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688119435, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:04:41 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.115: ISSUE: authtime 1688119450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:04:51 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688119491, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:04:54 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1688119494, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:05:07 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688119491, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:05:10 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1688119494, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:05:12 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688119512, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:05:18 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1688119518, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:05:19 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1688119518, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:05:26 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688119512, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:05:29 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1688119529, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:05:31 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688119531, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2023-06-30 10:05:59,440 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = bda3280202ff/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode3_1  | STARTUP_MSG:   java = 11.0.19
datanode3_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode3_1  | ************************************************************/
datanode3_1  | 2023-06-30 10:05:59,658 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2023-06-30 10:06:00,284 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2023-06-30 10:06:01,452 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2023-06-30 10:06:03,324 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2023-06-30 10:06:03,324 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2023-06-30 10:06:04,645 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:bda3280202ff ip:172.25.0.104
datanode3_1  | 2023-06-30 10:06:09,607 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2023-06-30 10:06:10,975 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2023-06-30 10:06:10,975 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2023-06-30 10:06:12,938 [main] INFO reflections.Reflections: Reflections took 1333 ms to scan 2 urls, producing 107 keys and 231 values 
datanode3_1  | 2023-06-30 10:06:17,547 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode3_1  | 2023-06-30 10:06:20,379 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode3_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode3_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode3_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode3_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode3_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode3_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode3_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode3_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode3_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode3_1  | 2023-06-30 10:06:20,425 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2023-06-30 10:06:20,434 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2023-06-30 10:06:20,442 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2023-06-30 10:06:33,370 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2023-06-30 10:06:33,969 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:bda3280202ff
datanode3_1  | 2023-06-30 10:06:33,971 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2023-06-30 10:06:34,096 [main] ERROR utils.CertificateSignRequest: Invalid domain bda3280202ff
datanode3_1  | 2023-06-30 10:06:34,103 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@bda3280202ff
datanode3_1  | 2023-06-30 10:06:38,368 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 388756064589
datanode3_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode3_1  |            Start Date: Fri Jun 30 10:06:36 UTC 2023
datanode3_1  |            Final Date: Sat Jun 29 10:06:36 UTC 2024
datanode3_1  |             SubjectDN: CN=dn@bda3280202ff,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode3_1  |            Public Key: RSA Public Key [58:67:dd:6c:c3:08:53:31:66:98:bb:b8:28:ba:47:07:a7:38:75:c0],[56:66:d1:a4]
datanode3_1  |         modulus: ba03f6a568dfbbfb6fe2af2838c4c8ccff24c5f5e3ef13d579578edccb06bc72a041252f766457ba68364337283797f3915a28aa59c1374ed6f86c0403c6aec224991a77d75dc82a4c7451e28be48560314ed84acc70353dd26c3a33df7221ccf66737b1c571826379313bede4855ea06973916ec0a7a62a7ac5f1fc70b2061b4820e1407b67564537820e609809979ce2023e54020ae60c869649823592ce9ae5a4f49f35a67361626c240c8ae3c740ef19e98e6fc5825ce2f241d7acad180502421077fa8158a0aa697ecb872af193e2d4c31f4062d92bfb28dedff3a19023c3945efec6cfafcbcb4f1b73a917a0256bdd876d66db164c2105eefadb729ec3
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 1d4e2ed751a4d10285b8cf23274f74a790f7c6ce
datanode3_1  |                        19e8aed9ecaee9e19642c66c8059727f3ba33e30
datanode3_1  |                        b6800fae6e5f5f4368f1ef007cf1ace06b0860c2
datanode3_1  |                        542dec1b97fc9d04ea6a705d9246289c9cc106fc
datanode3_1  |                        4348a23980b9012793151998d0e657d49bc8d7fe
datanode3_1  |                        3bbfffda7e592662850ed8162b76ebbdef4c544f
datanode3_1  |                        24471d02885d6f20338984e9b00bd659ba03d1b6
datanode3_1  |                        f74b8fbce0da41770cefc1b0a90e1156bed325c3
datanode3_1  |                        a3cf141b6698ed97b27b760649214373b5474a11
datanode3_1  |                        a04ea859b884fa117af6ffcb4622b43121f9641e
datanode3_1  |                        21bda98f3a408bb3992c2adc3d9b50b421890d85
datanode3_1  |                        59c5ca1eec3283b2c0ae3b7bb0eb3c92a95be334
datanode3_1  |                        3f7d9cd7bfe6372acb344c91f61c5030
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  | 
datanode3_1  |                        critical(true) KeyUsage: 0xb8
datanode3_1  |  from file: /data/metadata/dn/certs/388756064589.crt.
datanode3_1  | 2023-06-30 10:06:38,435 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 253598402501
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode3_1  |            Start Date: Fri Jun 30 10:04:21 UTC 2023
datanode3_1  |            Final Date: Mon Aug 07 10:04:21 UTC 2028
datanode3_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode3_1  |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
datanode3_1  |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
datanode3_1  |                        43e78ba4990cf8693d7e311e758874532e0e4770
datanode3_1  |                        6e386edb08789430a89a39ebde59b89ba6e84e94
datanode3_1  |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
datanode3_1  |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
datanode3_1  |                        73af7af9023c681485766dec4627568ceadcae31
httpfs_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
httpfs_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
httpfs_1     | 2023-06-30 10:03:56,747 [main] INFO server.HttpFSServerWebServer: STARTUP_MSG: 
httpfs_1     | /************************************************************
httpfs_1     | STARTUP_MSG: Starting HttpFSServerWebServer
httpfs_1     | STARTUP_MSG:   host = httpfs/172.25.0.119
httpfs_1     | STARTUP_MSG:   args = []
httpfs_1     | STARTUP_MSG:   version = 3.3.6
httpfs_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/curator-client-4.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/json-simple-1.1.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/curator-framework-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.5.6.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/zookeeper-jute-3.5.6.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar
httpfs_1     | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
httpfs_1     | STARTUP_MSG:   java = 11.0.19
httpfs_1     | ************************************************************/
httpfs_1     | 2023-06-30 10:03:56,834 [main] INFO server.HttpFSServerWebServer: registered UNIX signal handlers for [TERM, HUP, INT]
httpfs_1     | 2023-06-30 10:03:57,874 [main] INFO util.log: Logging initialized @5586ms to org.eclipse.jetty.util.log.Slf4jLog
httpfs_1     | 2023-06-30 10:03:58,570 [main] INFO http.HttpRequestLog: Http request log for http.requests.webhdfs is not defined
httpfs_1     | 2023-06-30 10:03:58,597 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
httpfs_1     | 2023-06-30 10:03:58,605 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context webhdfs
httpfs_1     | 2023-06-30 10:03:58,629 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
httpfs_1     | 2023-06-30 10:03:58,629 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
httpfs_1     | 2023-06-30 10:03:59,102 [main] INFO http.HttpServer2: Jetty bound to port 14000
httpfs_1     | 2023-06-30 10:03:59,104 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
httpfs_1     | 2023-06-30 10:03:59,184 [main] INFO server.session: DefaultSessionIdManager workerName=node0
httpfs_1     | 2023-06-30 10:03:59,205 [main] INFO server.session: No SessionScavenger set, using defaults
httpfs_1     | 2023-06-30 10:03:59,207 [main] INFO server.session: node0 Scavenging every 660000ms
httpfs_1     | 2023-06-30 10:03:59,320 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4dbb42b7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
httpfs_1     | 2023-06-30 10:03:59,339 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23c30a20{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
httpfs_1     | log4j:WARN No appenders could be found for logger (org.eclipse.jetty.webapp.WebAppClassLoader).
httpfs_1     | log4j:WARN Please initialize the log4j system properly.
httpfs_1     | log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
httpfs_1     | 10:04:00,128  WARN Server:474 - Log4j [/etc/hadoop/httpfs-log4j.properties] configuration file not found, using default configuration from classpath
httpfs_1     | 10:04:00,129  INFO Server:389 - ++++++++++++++++++++++++++++++++++++++++++++++++++++++
httpfs_1     | 10:04:00,129  INFO Server:390 - Server [httpfs] starting
httpfs_1     | 10:04:00,133  INFO Server:391 -   Built information:
httpfs_1     | 10:04:00,133  INFO Server:392 -     Version           : 1.4.0-SNAPSHOT
httpfs_1     | 10:04:00,133  INFO Server:394 -     Source Repository : REPO NOT AVAIL
httpfs_1     | 10:04:00,134  INFO Server:396 -     Source Revision   : REVISION NOT AVAIL
httpfs_1     | 10:04:00,134  INFO Server:398 -     Built by          : runner
httpfs_1     | 10:04:00,134  INFO Server:400 -     Built timestamp   : 2023-06-30T09:40:39+0000
httpfs_1     | 10:04:00,135  INFO Server:402 -   Runtime information:
httpfs_1     | 10:04:00,137  INFO Server:403 -     Home   dir: /opt/hadoop
httpfs_1     | 10:04:00,137  INFO Server:404 -     Config dir: /etc/hadoop
httpfs_1     | 10:04:00,139  INFO Server:405 -     Log    dir: /opt/hadoop/log
httpfs_1     | 10:04:00,139  INFO Server:406 -     Temp   dir: /opt/hadoop/temp
httpfs_1     | 10:04:00,321  INFO Server:544 - System property sets  httpfs.log.dir: /opt/hadoop/log
httpfs_1     | 10:04:00,321  INFO Server:544 - System property sets  httpfs.temp.dir: /opt/hadoop/temp
httpfs_1     | 10:04:00,322  INFO Server:544 - System property sets  httpfs.config.dir: /etc/hadoop
httpfs_1     | 10:04:00,322  INFO Server:544 - System property sets  httpfs.home.dir: /opt/hadoop
httpfs_1     | 10:04:00,366  INFO FileSystemAccessService:162 - Using FileSystemAccess JARs version [3.3.6]
httpfs_1     | 10:04:01,119  INFO UserGroupInformation:1132 - Login successful for user httpfs/httpfs@EXAMPLE.COM using keytab file httpfs.keytab. Keytab auto renewal enabled : false
httpfs_1     | 10:04:01,132  INFO FileSystemAccessService:191 - Using FileSystemAccess Kerberos authentication, principal [httpfs/httpfs@EXAMPLE.COM] keytab [/etc/security/keytabs/httpfs.keytab]
httpfs_1     | 10:04:01,226  INFO Server:413 - Services initialized
httpfs_1     | 10:04:01,227  INFO Server:423 - Server [httpfs] started!, status [NORMAL]
httpfs_1     | 10:04:01,228  INFO HttpFSServerWebApp:107 - Connects to Namenode [ofs://omservice]
httpfs_1     | 10:04:01,229  INFO HttpFSServerWebApp:126 - Initializing HttpFSServerMetrics
httpfs_1     | 10:04:01,437  INFO JvmPauseMonitor:188 - Starting JVM pause monitor
httpfs_1     | 10:04:01,997  INFO MetricsConfig:120 - Loaded properties from hadoop-metrics2.properties
httpfs_1     | 10:04:02,697  INFO MetricsSystemImpl:378 - Scheduled Metric snapshot period at 10 second(s).
httpfs_1     | 10:04:02,697  INFO MetricsSystemImpl:191 - HttpFSServer metrics system started
httpfs_1     | 10:04:02,869  INFO KerberosAuthenticationHandler:175 - Using keytab /etc/security/keytabs/httpfs.keytab, for principal HTTP/httpfs@EXAMPLE.COM
httpfs_1     | 10:04:02,947  INFO AbstractDelegationTokenSecretManager:415 - Updating the current master key for generating delegation tokens
httpfs_1     | 10:04:03,041  INFO AbstractDelegationTokenSecretManager:797 - Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
httpfs_1     | 10:04:03,041  INFO AbstractDelegationTokenSecretManager:415 - Updating the current master key for generating delegation tokens
httpfs_1     | Jun 30, 2023 10:04:03 AM com.sun.jersey.api.core.PackagesResourceConfig init
httpfs_1     | INFO: Scanning for root resource and provider classes in the packages:
httpfs_1     |   org.apache.ozone.fs.http.server
httpfs_1     |   org.apache.ozone.lib.wsrs
httpfs_1     | Jun 30, 2023 10:04:03 AM com.sun.jersey.api.core.ScanningResourceConfig logClasses
httpfs_1     | INFO: Root resource classes found:
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSServer
httpfs_1     | Jun 30, 2023 10:04:03 AM com.sun.jersey.api.core.ScanningResourceConfig logClasses
httpfs_1     | INFO: Provider classes found:
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSExceptionProvider
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSParametersProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONMapProvider
httpfs_1     | Jun 30, 2023 10:04:04 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
httpfs_1     | INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
httpfs_1     | 10:04:08,058  INFO ContextHandler:921 - Started o.e.j.w.WebAppContext@3a175162{webhdfs,/,file:///tmp/hadoop-hadoop/httpfs/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/webhdfs}
httpfs_1     | 10:04:08,114  INFO AbstractConnector:333 - Started ServerConnector@25ce9dc4{HTTP/1.1, (http/1.1)}{0.0.0.0:14000}
httpfs_1     | 10:04:08,114  INFO Server:415 - Started @15828ms
datanode1_1  |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
datanode1_1  |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
datanode1_1  |                        734cec37929dd6f294085e1181a91f1d47304865
datanode1_1  |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
datanode1_1  |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
datanode1_1  |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
datanode1_1  |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
datanode1_1  |                        0e32f359ccbefded3427e57b80709364
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0x6
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
kdc_1        | Jun 30 10:05:42 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1688119529, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:05:49 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688119531, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:06:00 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688119560, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:06:10 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1688119570, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:06:10 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1688119570, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:06:11 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1688119571, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:06:18 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1688119578, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:06:18 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1688119578, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:06:19 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1688119579, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:06:23 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1688119578, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:06:24 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1688119578, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:06:25 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1688119579, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:06:29 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1688119571, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:06:35 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1688119570, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:06:36 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1688119570, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:07:07 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688119560, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:07:13 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1688119571, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 30 10:07:16 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688119636, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:07:16 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1688119570, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 30 10:07:17 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1688119570, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 30 10:07:18 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1688119638, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
om1_1        | Waiting for the service scm3.org:9894
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-30 10:06:00,632 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om1_1        | STARTUP_MSG:   java = 11.0.19
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-30 10:06:00,726 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-06-30 10:06:12,233 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-30 10:06:15,581 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om1_1        | 2023-06-30 10:06:16,584 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-30 10:06:16,584 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-30 10:06:16,584 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1        | 2023-06-30 10:06:18,837 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-30 10:06:18,846 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-30 10:06:19,040 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 10:06:20,905 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-06880496-7012-4073-8c29-45a6b430095b;layoutVersion=6
om1_1        | 2023-06-30 10:06:25,598 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om1_1        | 2023-06-30 10:06:25,602 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2023-06-30 10:06:26,382 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-30 10:06:32,978 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om1_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om1_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om1_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om1_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om1_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om1_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1383)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1362)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om1_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om1_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om1_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om1_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om1_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om1_1        | 2023-06-30 10:06:33,026 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2023-06-30 10:06:33,041 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om1_1        | 2023-06-30 10:06:33,050 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2023-06-30 10:06:36,549 [main] INFO om.OzoneManager: Init response: GETCERT
om1_1        | 2023-06-30 10:06:36,979 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om1_1        | 2023-06-30 10:06:36,986 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2023-06-30 10:06:37,038 [main] ERROR utils.CertificateSignRequest: Invalid domain om1
om1_1        | 2023-06-30 10:06:37,041 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:42abf28c-5926-4498-87c1-8b8ba8a02c3f,clusterId:CID-06880496-7012-4073-8c29-45a6b430095b,subject:om1
om1_1        | 2023-06-30 10:06:40,325 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 390654687721
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Start Date: Fri Jun 30 10:06:38 UTC 2023
om1_1        |            Final Date: Sat Jun 29 10:06:38 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Public Key: RSA Public Key [1f:52:04:a5:d1:46:29:9b:dc:b5:67:cf:d7:aa:4a:5b:9f:5e:5b:f2],[56:66:d1:a4]
om1_1        |         modulus: e4ad5123e64d8decddbf55f535d891b48f1392615859484114575a4ceab62e6f5d0e8dd3af8ba97bda95b1bd8ae0801b545654eeeeb3000976b38ef18af9e953bfc3ac3eba39907ea8296013f2223eb3b34493a8c37c1ab0b8c7f4d0b52fcae1a21eedd9f7a590ff32c6c09b8b13b2a0febd677d7372aa43412fa41662959bf9918cdc7ebc9c2b4fcbae82818534e7ea3485ba5416e4f81917af53ab37cc7cc9faa2af82fe9f19059a59b09040408ccb11539ed4409ef6ebcbefb6461ce33fcfa8764c167905dbafa912ac3f1b3408e43260a1ec4e5050f5ad3de1ff1cc897cb143900b2405a0619750dc9c2321102e34579c9a058babcbb1ec51cb681c01ef5
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: aa971d02f2802f1561abcdd37d50b521426632be
om1_1        |                        520aea54e1f8e9e1c3c76dfeeaf4eaa50f6d70d3
om1_1        |                        0faec9bcd63e5f8fcc884ee1976b06d343f903ff
om1_1        |                        f6087572c36a729732f3df7bdea5b2afef029647
om1_1        |                        83d67955ad4a4499b439bb7459e8b0775124f57c
om1_1        |                        ce36fb79a20218f98665fddf61e01ac6d25607ea
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0xbe
datanode2_1  |  from file: /data/metadata/dn/certs/CA-253598402501.crt.
datanode2_1  | 2023-06-30 10:06:37,601 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 1
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode2_1  |            Start Date: Fri Jun 30 10:04:21 UTC 2023
datanode2_1  |            Final Date: Mon Aug 07 10:04:21 UTC 2028
datanode2_1  |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode2_1  |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
datanode2_1  |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
datanode2_1  |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
datanode2_1  |                        dead23e7b54024421f408e091345739cf1861cb6
datanode2_1  |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
datanode2_1  |                        6385912587b5531778fa6f5c476842a25dc986ad
datanode1_1  | 2023-06-30 10:06:32,219 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 382242999512
datanode1_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode1_1  |            Start Date: Fri Jun 30 10:06:30 UTC 2023
datanode1_1  |            Final Date: Sat Jun 29 10:06:30 UTC 2024
datanode1_1  |             SubjectDN: CN=dn@2b3975d57ed5,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode1_1  |            Public Key: RSA Public Key [82:72:44:ef:47:7f:35:03:2a:ed:48:92:aa:fb:d4:53:a5:45:89:8e],[56:66:d1:a4]
datanode1_1  |         modulus: cdf93f29b62d45c0dd7eb57c6033c6cb84a5b806a0b228871680a1f9e38a38c96205e10823fc197bf0444937c6530cdd71728308e9a38e798eba39f6df9962df9407035ea708e6dc926974ea4d5990ccee261a6de033039f7c47145630979c6df714e9a2d1f2889fbb5d10aba52797933da96ead5f0ca0899d6f55d2736d6665334f83614dd61ce03b278413fa51a0f345e6009b92962dbd567cd1270ad82f6023fa4dc6af12eefc6c7048d6ddfcfb9de8bf831ad42442e29b97d129247f2776d0e8d4bf0a868b8860ee806b563e52e9918cac6025e0406091a4225625e365be6e3dc5e80276098b79ed8e9097a16b52b2ac6754a85b9282313dc63a1aa67fdf
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 9e579382c26a37c7250ba742b6055c6691160b9d
datanode1_1  |                        3d16a8a9aa92364fb1a1543f425e3b9ffe87ef82
datanode1_1  |                        2b78411e78200f10648e61dd57c40ef880e995e9
datanode1_1  |                        adca70b1953355c9e4d3f65e33255fa340bc9e72
datanode1_1  |                        8eda2e60995ef129c06cd702ec5285afd9de8efc
datanode1_1  |                        648fe97335f37fef29cdc43b01ad18a35dc848fd
datanode1_1  |                        f9e2a77f8c8181a947d3df683ca58dacb1f20e54
datanode1_1  |                        3d0940b6bb372d00e5453f10bbf74b4e07a0ca6d
datanode1_1  |                        7302ab8bbfd4431ffae7d81368a5454614c91abc
datanode1_1  |                        e9749e6b32563585ace810857632175bfba3cf11
datanode1_1  |                        71f841089904c1ce88670e6a04f42da9efe3e858
datanode1_1  |                        3f3bf8da693f37b8f9d24df614561fe25cfa7bc5
datanode1_1  |                        ad29e08fcf83ee9fb904ebca9bda12bc
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  | 
datanode1_1  |                        critical(true) KeyUsage: 0xb8
datanode1_1  |  from file: /data/metadata/dn/certs/382242999512.crt.
datanode1_1  | 2023-06-30 10:06:32,276 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797772 ms and interval 86400000 ms.
datanode1_1  | 2023-06-30 10:06:32,866 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2023-06-30 10:06:33,231 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode1_1  | 2023-06-30 10:06:35,193 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z).
datanode1_1  | 2023-06-30 10:06:35,201 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M8.329477S and interval of PT1M
datanode1_1  | 2023-06-30 10:06:35,386 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode1_1  | 2023-06-30 10:06:35,848 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2023-06-30 10:06:37,721 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2023-06-30 10:06:37,887 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode1_1  | 2023-06-30 10:06:37,931 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2023-06-30 10:06:37,942 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2023-06-30 10:06:38,313 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2023-06-30 10:06:38,365 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-06-30 10:06:38,381 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode1_1  | 2023-06-30 10:06:38,393 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode1_1  | 2023-06-30 10:06:38,397 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2023-06-30 10:06:38,398 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode1_1  | 2023-06-30 10:06:38,707 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2023-06-30 10:06:38,715 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2023-06-30 10:06:46,991 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode1_1  | 2023-06-30 10:06:48,268 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode1_1  | 2023-06-30 10:06:48,432 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode1_1  | 2023-06-30 10:06:49,015 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-06-30 10:06:49,547 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2023-06-30 10:06:51,269 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-30 10:06:51,305 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2023-06-30 10:06:51,308 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-30 10:06:51,321 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2023-06-30 10:06:51,354 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode1_1  | 2023-06-30 10:06:51,357 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2023-06-30 10:06:51,365 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2023-06-30 10:06:51,389 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 10:06:51,406 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2023-06-30 10:06:51,414 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-30 10:06:51,658 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-06-30 10:06:51,742 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode1_1  | 2023-06-30 10:06:51,748 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-06-30 10:06:59,035 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode1_1  | 2023-06-30 10:06:59,260 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode1_1  | 2023-06-30 10:06:59,302 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode1_1  | 2023-06-30 10:06:59,303 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode1_1  | 2023-06-30 10:06:59,305 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode1_1  | 2023-06-30 10:06:59,347 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode1_1  | 2023-06-30 10:06:59,358 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode1_1  | 2023-06-30 10:06:59,465 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode1_1  | 2023-06-30 10:06:59,480 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode1_1  | 2023-06-30 10:06:59,507 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode1_1  | 2023-06-30 10:06:59,510 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode1_1  | 2023-06-30 10:07:00,072 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode1_1  | 2023-06-30 10:07:00,080 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode1_1  | 2023-06-30 10:07:00,082 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-30 10:07:00,087 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-30 10:07:00,146 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-30 10:07:00,202 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x1ca0e3cc] REGISTERED
datanode1_1  | 2023-06-30 10:07:00,228 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x1ca0e3cc] BIND: 0.0.0.0/0.0.0.0:9855
datanode1_1  | 2023-06-30 10:07:00,279 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x1ca0e3cc, L:/0.0.0.0:9855] ACTIVE
datanode1_1  | 2023-06-30 10:07:00,566 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode1_1  | 2023-06-30 10:07:01,719 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode1_1  | 2023-06-30 10:07:03,440 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode1_1  | 2023-06-30 10:07:03,442 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2023-06-30 10:07:03,444 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2023-06-30 10:07:03,707 [main] INFO util.log: Logging initialized @79855ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2023-06-30 10:07:04,938 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2023-06-30 10:07:05,013 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2023-06-30 10:07:05,030 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2023-06-30 10:07:05,030 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2023-06-30 10:07:05,037 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode1_1  | 2023-06-30 10:07:05,079 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2023-06-30 10:07:05,584 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode1_1  | 2023-06-30 10:07:05,595 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2023-06-30 10:07:05,619 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode1_1  | 2023-06-30 10:07:05,967 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2023-06-30 10:07:05,967 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2023-06-30 10:07:05,981 [main] INFO server.session: node0 Scavenging every 600000ms
datanode1_1  | 2023-06-30 10:07:06,219 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-30 10:07:06,259 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73daf6a6{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2023-06-30 10:07:06,267 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23389e2b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2023-06-30 10:07:07,506 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-30 10:07:07,707 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4df50829{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-17997185976403514017/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2023-06-30 10:07:07,816 [main] INFO server.AbstractConnector: Started ServerConnector@6a8b94b2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2023-06-30 10:07:07,822 [main] INFO server.Server: Started @83970ms
datanode1_1  | 2023-06-30 10:07:07,858 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2023-06-30 10:07:07,859 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2023-06-30 10:07:07,869 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2023-06-30 10:07:08,112 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode1_1  | 2023-06-30 10:07:08,153 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
kdc_1        | Jun 30 10:07:20 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1688119640, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:07:21 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1688119641, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 10:07:24 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1688119638, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:07:26 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1688119640, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:07:27 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1688119641, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 10:07:59 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688119636, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
datanode2_1  |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
datanode2_1  |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
datanode2_1  |                        734cec37929dd6f294085e1181a91f1d47304865
datanode2_1  |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
datanode2_1  |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
datanode2_1  |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
datanode2_1  |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
datanode2_1  |                        0e32f359ccbefded3427e57b80709364
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0x6
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2023-06-30 10:06:37,664 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 387609916440
datanode2_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode2_1  |            Start Date: Fri Jun 30 10:06:35 UTC 2023
datanode2_1  |            Final Date: Sat Jun 29 10:06:35 UTC 2024
datanode2_1  |             SubjectDN: CN=dn@f831826c9e82,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode2_1  |            Public Key: RSA Public Key [b9:d3:a4:14:46:51:7f:2f:1d:7f:31:74:36:a5:76:42:f3:8e:17:9c],[56:66:d1:a4]
datanode2_1  |         modulus: c00c54785aa1e56f92d6b6b7ae58c094a02bff903d6b0e52a2404ec948db8e0958daf9269060c529a2170017390742273c6a9df67c653796fdcb1ccb268ea99b9b2961d20e528be14b16dbb0220e08f652c540a7cd3cc9f1648de2257f9881455c6835b3ff5840cd5d7c43cb7ad83339f74ca61ee7ed90c54a9171a9cf5902ca1ad3d2b2dcb3d42bdb3ab1c86b2f5ad55ebdfe83fe17ae6b83ce35ef5866ce4c4fcc9267d1778510c5e3753ace1a07d565f2bb318d508a39b1efc6128e6f0da12d6ca5f53292c5d7b72025a935a942a0d338f1524c3a08654abf65689cc70c7c6e004c2a78c9a78dd342899bb7962be4113fe0cea58f7a58049bc42b249b6d2b
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 1bf551205a52e490bfcc422eab5e3d7d5c6aa992
datanode2_1  |                        2fac2488434f02d42c4e426dc8f026cfc24f9335
datanode2_1  |                        88ab5472e95ab25311dcc9408a20853ad19d5451
datanode2_1  |                        90198bf3f2506304a03c48b0a5c55c57cc0ac7a5
datanode2_1  |                        ab74df20f58fee8f9c14cc22b0efb28c8a2356d1
datanode2_1  |                        ce6db2e47f6db34ec3bcef68720d45e393059a85
datanode2_1  |                        80df8097b681c55f58db590014eade3d1e95e14d
datanode2_1  |                        183c0848b49ee24c222d2a9ee991bf1787ccc27f
datanode2_1  |                        41cd61fff7c6cc57406ae57714b9795a8a3d1c87
datanode2_1  |                        7e6e94ca8c2399f9c300c0637d97ebd9c779c7f1
datanode2_1  |                        7aab756de0ca6af7344f983ac1d3d56362a5b536
datanode2_1  |                        99bb5a57977e5ac9ea404229d93c7d1f260b4b71
datanode2_1  |                        e505ce14087b3e3041eb49e9703e2e52
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  | 
datanode2_1  |                        critical(true) KeyUsage: 0xb8
datanode2_1  |  from file: /data/metadata/dn/certs/387609916440.crt.
datanode2_1  | 2023-06-30 10:06:37,705 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797328 ms and interval 86400000 ms.
datanode2_1  | 2023-06-30 10:06:38,235 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2023-06-30 10:06:38,591 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode2_1  | 2023-06-30 10:06:39,690 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z).
datanode2_1  | 2023-06-30 10:06:39,694 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M3.833942S and interval of PT1M
datanode2_1  | 2023-06-30 10:06:39,883 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode2_1  | 2023-06-30 10:06:40,138 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode2_1  | 2023-06-30 10:06:41,860 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2023-06-30 10:06:42,000 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode2_1  | 2023-06-30 10:06:42,027 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2023-06-30 10:06:42,046 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2023-06-30 10:06:42,579 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  | 2023-06-30 10:06:42,634 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-30 10:06:42,643 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2023-06-30 10:06:42,680 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2023-06-30 10:06:42,686 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode2_1  | 2023-06-30 10:06:42,692 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2023-06-30 10:06:42,993 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2023-06-30 10:06:42,999 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2023-06-30 10:06:50,906 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode2_1  | 2023-06-30 10:06:52,106 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode2_1  | 2023-06-30 10:06:52,244 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode2_1  | 2023-06-30 10:06:52,898 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-30 10:06:53,525 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2023-06-30 10:06:54,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-30 10:06:54,628 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2023-06-30 10:06:54,633 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-30 10:06:54,637 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2023-06-30 10:06:54,642 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode2_1  | 2023-06-30 10:06:54,646 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2023-06-30 10:06:54,650 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
datanode3_1  |                        988b846e620e04205f341894328c4379e192c0f0
datanode3_1  |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
datanode3_1  |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
datanode3_1  |                        d924b613b78a9538512bbc922e5120bb84930f4c
datanode3_1  |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
datanode3_1  |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
datanode3_1  | 
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
datanode3_1  |                        critical(true) KeyUsage: 0xbe
datanode3_1  |  from file: /data/metadata/dn/certs/CA-253598402501.crt.
datanode3_1  | 2023-06-30 10:06:38,460 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 1
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode3_1  |            Start Date: Fri Jun 30 10:04:21 UTC 2023
datanode3_1  |            Final Date: Mon Aug 07 10:04:21 UTC 2028
datanode3_1  |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode3_1  |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
datanode3_1  |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
datanode3_1  |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
datanode3_1  |                        dead23e7b54024421f408e091345739cf1861cb6
datanode3_1  |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
datanode3_1  |                        6385912587b5531778fa6f5c476842a25dc986ad
datanode3_1  |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
datanode3_1  |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
datanode3_1  |                        734cec37929dd6f294085e1181a91f1d47304865
datanode3_1  |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
datanode3_1  |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
datanode3_1  |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
datanode3_1  |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
datanode3_1  |                        0e32f359ccbefded3427e57b80709364
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        e0b77aa23c34f2664149871a2c137ab432a1fac9
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-30 10:05:58,396 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om1_1        |                        090a3bd0f9a5e7b7d1509f19bf42634c87cadcaa
om1_1        |                        063ddc6a80c96502e830ea24436173565b1ad34e
om1_1        |                        35450b8e285ef1bb7ac30b712d974b1a70967be7
om1_1        |                        04311ed210808c191c7f0104ec94e189756d001a
om1_1        |                        981f61098b16a4c1ebad2e21b201978058448bc2
om1_1        |                        0f9c867ddf30cbf5f95b367627529a96
om1_1        |        Extensions: 
om2_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-30 10:05:58,611 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-06-30 10:06:10,916 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-30 10:06:15,807 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-30 10:06:16,593 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-06-30 10:06:16,616 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-30 10:06:16,617 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-30 10:06:18,674 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-30 10:06:18,680 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-30 10:06:18,709 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 10:06:19,877 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-06880496-7012-4073-8c29-45a6b430095b;layoutVersion=6
om2_1        | 2023-06-30 10:06:24,376 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/390654687721.crt.
om1_1        | 2023-06-30 10:06:40,377 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 253598402501
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om1_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
om1_1        |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
om1_1        |                        43e78ba4990cf8693d7e311e758874532e0e4770
om1_1        |                        6e386edb08789430a89a39ebde59b89ba6e84e94
om1_1        |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
om1_1        |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
om1_1        |                        73af7af9023c681485766dec4627568ceadcae31
om1_1        |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
om1_1        |                        988b846e620e04205f341894328c4379e192c0f0
om1_1        |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
om1_1        |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
om1_1        |                        d924b613b78a9538512bbc922e5120bb84930f4c
om1_1        |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
om1_1        |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om2_1        | 2023-06-30 10:06:24,376 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2023-06-30 10:06:24,970 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-06-30 10:06:31,525 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om2_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om2_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om2_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode3_1  |                        critical(true) KeyUsage: 0x6
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
om2_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
om2_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
datanode3_1  | 
datanode3_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2023-06-30 10:06:38,509 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797536 ms and interval 86400000 ms.
datanode3_1  | 2023-06-30 10:06:39,443 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2023-06-30 10:06:39,816 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode3_1  | 2023-06-30 10:06:40,402 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z).
datanode3_1  | 2023-06-30 10:06:40,410 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M3.119445S and interval of PT1M
datanode3_1  | 2023-06-30 10:06:40,651 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode3_1  | 2023-06-30 10:06:40,954 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode3_1  | 2023-06-30 10:06:42,528 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2023-06-30 10:06:42,698 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode3_1  | 2023-06-30 10:06:42,747 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2023-06-30 10:06:42,770 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2023-06-30 10:06:43,235 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode3_1  | 2023-06-30 10:06:43,284 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-30 10:06:43,311 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2023-06-30 10:06:43,326 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2023-06-30 10:06:43,326 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2023-06-30 10:06:43,330 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2023-06-30 10:06:43,588 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2023-06-30 10:06:43,596 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2023-06-30 10:06:51,403 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode3_1  | 2023-06-30 10:06:53,196 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode3_1  | 2023-06-30 10:06:53,276 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode3_1  | 2023-06-30 10:06:53,753 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-30 10:06:54,316 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2023-06-30 10:06:55,267 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-30 10:06:55,282 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2023-06-30 10:06:55,288 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-30 10:06:55,311 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2023-06-30 10:06:55,324 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode3_1  | 2023-06-30 10:06:55,324 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2023-06-30 10:06:55,330 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  | 2023-06-30 10:06:55,351 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 10:06:55,364 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2023-06-30 10:06:55,385 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-30 10:06:55,500 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-06-30 10:06:55,672 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode3_1  | 2023-06-30 10:06:55,678 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-06-30 10:07:02,104 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode3_1  | 2023-06-30 10:07:02,361 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode3_1  | 2023-06-30 10:07:02,365 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode3_1  | 2023-06-30 10:07:02,371 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode3_1  | 2023-06-30 10:07:02,379 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode3_1  | 2023-06-30 10:07:02,406 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode3_1  | 2023-06-30 10:07:02,410 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode3_1  | 2023-06-30 10:07:02,488 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode3_1  | 2023-06-30 10:07:02,498 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode3_1  | 2023-06-30 10:07:02,518 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode3_1  | 2023-06-30 10:07:02,521 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode3_1  | 2023-06-30 10:07:03,071 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om2_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1383)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1362)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om2_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om2_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om2_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om2_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om2_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om2_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om2_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om2_1        | 2023-06-30 10:06:31,557 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2023-06-30 10:06:31,569 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om2_1        | 2023-06-30 10:06:31,573 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2023-06-30 10:06:36,559 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2023-06-30 10:06:36,995 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
om2_1        | 2023-06-30 10:06:37,003 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2023-06-30 10:06:37,061 [main] ERROR utils.CertificateSignRequest: Invalid domain om2
om2_1        | 2023-06-30 10:06:37,071 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:42abf28c-5926-4498-87c1-8b8ba8a02c3f,clusterId:CID-06880496-7012-4073-8c29-45a6b430095b,subject:om2
om2_1        | 2023-06-30 10:06:40,546 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 390994128355
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Start Date: Fri Jun 30 10:06:38 UTC 2023
om2_1        |            Final Date: Sat Jun 29 10:06:38 UTC 2024
om2_1        |             SubjectDN: CN=om2,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Public Key: RSA Public Key [fd:de:6e:8d:87:c7:3f:4e:1b:40:ce:08:26:61:b6:e6:61:d7:0a:ce],[56:66:d1:a4]
om2_1        |         modulus: ef4427dba167872479a8a6803cc078d462152f43f1f0c0bd4ca6460f78df38f0e2938f7469a25e9933c1bd207e3e5eeb00cd9e890d137d09244961fd775f7bd3c1c5c0bff9fa62e654a0f16e8360ae43bcd18b547788735f058272c1fde6f6663f89fd37013080a36a79411e541b56958b307bb48a190c5d427857072057ec25bd96d6764fa7726586bcaf7ae675af38c18828b0c563a85b1d48e1297c03c094d6bc0186eec2dfec674e8e507d3029276631f8f73981a33d9bc8e5ef1eb089fca1408eb7544495eaf6b46895b9d82459dae9194b4c1d1c5499588180235d5b47bb550d1d5cc63ade9a34e2c6b2b3e04c6f977851b7c507fc13479e91617dda85
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 06b4d0a20e8e49d08493211d5c4e89ec6914a544
om2_1        |                        641168f8c66263a1dda141f816c49c704cc0c930
om2_1        |                        e5fad9581fa4fe555cf3c2a33e51b656fbcc1039
om2_1        |                        30e92d50c4f4e9d15fca8814db6c736c0f62fb7f
om2_1        |                        682e6e38dc0eeac52d122c471c7e4e7a81b744aa
om2_1        |                        3f937f60e0a03f78a249a93114211e8b11469ffc
om2_1        |                        7c8ad2174d3add265c9fc49cd49833cebe8e8a7b
om2_1        |                        861c5572dd7fd958d8371d14863243d3bcd7bd6d
om2_1        |                        c3377d14c293c0711971d3fd07c95b2166de40ca
om2_1        |                        10821e8c069402758b68ddfd9ae091a931d4d0ea
om2_1        |                        f603849445b63fa48636ef0be91ab4201de13d2a
om2_1        |                        d09bb93f00772c8db9194bf7898fe5b68d30fdda
om2_1        |                        890aa26e6a2787239800793de2d6881e
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
om2_1        |  from file: /data/metadata/om/certs/390994128355.crt.
om2_1        | 2023-06-30 10:06:40,587 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 253598402501
datanode2_1  | 2023-06-30 10:06:54,656 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 10:06:54,662 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2023-06-30 10:06:54,668 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-30 10:06:54,784 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode2_1  | 2023-06-30 10:06:54,830 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode2_1  | 2023-06-30 10:06:54,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-06-30 10:07:01,661 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode2_1  | 2023-06-30 10:07:01,838 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode2_1  | 2023-06-30 10:07:01,846 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode2_1  | 2023-06-30 10:07:01,851 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode2_1  | 2023-06-30 10:07:01,862 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode2_1  | 2023-06-30 10:07:01,882 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode2_1  | 2023-06-30 10:07:01,888 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:01,941 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:01,948 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode2_1  | 2023-06-30 10:07:01,974 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode2_1  | 2023-06-30 10:07:01,978 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode2_1  | 2023-06-30 10:07:02,307 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode2_1  | 2023-06-30 10:07:02,312 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:02,313 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-30 10:07:02,315 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-30 10:07:02,379 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-30 10:07:02,460 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x1999b625] REGISTERED
datanode2_1  | 2023-06-30 10:07:02,495 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x1999b625] BIND: 0.0.0.0/0.0.0.0:9855
datanode2_1  | 2023-06-30 10:07:02,535 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x1999b625, L:/0.0.0.0:9855] ACTIVE
datanode2_1  | 2023-06-30 10:07:02,825 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
om3_1        | Waiting for the service scm3.org:9894
datanode1_1  | 2023-06-30 10:07:08,170 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
datanode1_1  | 2023-06-30 10:07:10,303 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode1_1  | 2023-06-30 10:07:10,303 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  | 2023-06-30 10:07:10,309 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode1_1  | 2023-06-30 10:07:10,324 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
om1_1        |                        critical(true) KeyUsage: 0xbe
datanode1_1  | 2023-06-30 10:07:10,361 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 2023-06-30 10:07:11,145 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
om1_1        |  from file: /data/metadata/om/certs/CA-253598402501.crt.
om1_1        | 2023-06-30 10:06:40,434 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  | 2023-06-30 10:07:11,230 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
om1_1        |          SerialNumber: 1
datanode1_1  | 2023-06-30 10:07:14,776 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode1_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om1_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-06-30 10:06:01,716 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
om1_1        |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
datanode2_1  | 2023-06-30 10:07:03,829 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode2_1  | 2023-06-30 10:07:05,650 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | 2023-06-30 10:07:05,653 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2023-06-30 10:07:05,653 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode2_1  | 2023-06-30 10:07:05,983 [main] INFO util.log: Logging initialized @82012ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2023-06-30 10:07:07,225 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2023-06-30 10:07:07,294 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2023-06-30 10:07:07,319 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2023-06-30 10:07:07,320 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode2_1  | 2023-06-30 10:07:07,322 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode2_1  | 2023-06-30 10:07:07,352 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2023-06-30 10:07:07,874 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-30 10:06:01,825 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-06-30 10:06:12,740 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-30 10:06:17,334 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-30 10:06:18,104 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-30 10:06:18,125 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
om3_1        | 2023-06-30 10:06:18,133 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-30 10:06:20,414 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-30 10:06:20,421 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-30 10:06:20,493 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 10:06:22,009 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-06880496-7012-4073-8c29-45a6b430095b;layoutVersion=6
om3_1        | 2023-06-30 10:06:26,712 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om3_1        | 2023-06-30 10:06:26,712 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2023-06-30 10:06:27,288 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-30 10:06:33,413 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om3_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om3_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om3_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om3_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om3_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om3_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1383)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1362)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om3_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om3_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om3_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om3_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om3_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om3_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om3_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode1_1  | Caused by: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	... 1 more
datanode1_1  | 2023-06-30 10:07:16,971 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-06880496-7012-4073-8c29-45a6b430095b/DS-27d53cd0-1fba-472f-91d2-e771a1c581f1/container.db to cache
datanode1_1  | 2023-06-30 10:07:16,971 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-06880496-7012-4073-8c29-45a6b430095b/DS-27d53cd0-1fba-472f-91d2-e771a1c581f1/container.db for volume DS-27d53cd0-1fba-472f-91d2-e771a1c581f1
datanode1_1  | 2023-06-30 10:07:16,983 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2023-06-30 10:07:17,038 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode1_1  | 2023-06-30 10:07:17,377 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode1_1  | 2023-06-30 10:07:17,380 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 90f061fe-584f-4bf2-a5ce-ab438b7e17a1
datanode1_1  | 2023-06-30 10:07:18,075 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: start RPC server
datanode1_1  | 2023-06-30 10:07:18,119 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: GrpcService started, listening on 9858
datanode1_1  | 2023-06-30 10:07:18,120 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: GrpcService started, listening on 9856
datanode1_1  | 2023-06-30 10:07:18,157 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: GrpcService started, listening on 9857
datanode1_1  | 2023-06-30 10:07:18,183 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 90f061fe-584f-4bf2-a5ce-ab438b7e17a1 is started using port 9858 for RATIS
datanode1_1  | 2023-06-30 10:07:18,185 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 90f061fe-584f-4bf2-a5ce-ab438b7e17a1 is started using port 9857 for RATIS_ADMIN
datanode1_1  | 2023-06-30 10:07:18,185 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 90f061fe-584f-4bf2-a5ce-ab438b7e17a1 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2023-06-30 10:07:18,185 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 90f061fe-584f-4bf2-a5ce-ab438b7e17a1 is started using port 9855 for RATIS_DATASTREAM
datanode1_1  | 2023-06-30 10:07:18,189 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-90f061fe-584f-4bf2-a5ce-ab438b7e17a1: Started
datanode1_1  | 2023-06-30 10:07:18,287 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-30 10:07:18,288 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-30 10:07:18,346 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode1_1  | 2023-06-30 10:07:52,081 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: addNew group-5B762BD02996:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] returns group-5B762BD02996:java.util.concurrent.CompletableFuture@3cdfed96[Not completed]
datanode1_1  | 2023-06-30 10:07:52,255 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: new RaftServerImpl for group-5B762BD02996:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-30 10:07:52,280 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-30 10:07:52,289 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-30 10:07:52,291 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-30 10:07:52,293 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-30 10:07:52,351 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-30 10:07:52,358 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-30 10:07:52,487 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996: ConfigurationManager, init=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-30 10:07:52,493 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-30 10:07:52,614 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-30 10:07:52,618 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-30 10:07:52,804 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-30 10:07:52,832 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-30 10:07:52,880 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-30 10:07:52,883 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-30 10:07:53,223 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-30 10:07:53,432 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-30 10:07:53,485 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-30 10:07:53,491 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-30 10:07:53,492 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1        |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
om1_1        |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
om1_1        |                        dead23e7b54024421f408e091345739cf1861cb6
om1_1        |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
om1_1        |                        6385912587b5531778fa6f5c476842a25dc986ad
om1_1        |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
om1_1        |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
om1_1        |                        734cec37929dd6f294085e1181a91f1d47304865
om1_1        |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
om1_1        |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
om1_1        |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
om1_1        |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
om1_1        |                        0e32f359ccbefded3427e57b80709364
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-30 10:06:40,464 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797564 ms and interval 86400000 ms.
om1_1        | 2023-06-30 10:06:40,464 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | 2023-06-30 10:06:40,594 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-30 10:06:57,087 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode2_1  | 2023-06-30 10:07:07,886 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2023-06-30 10:07:07,899 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode2_1  | 2023-06-30 10:07:08,441 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2023-06-30 10:07:08,446 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2023-06-30 10:07:08,463 [main] INFO server.session: node0 Scavenging every 660000ms
datanode2_1  | 2023-06-30 10:07:08,651 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-30 10:07:03,085 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:08,687 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4f091800{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2023-06-30 10:07:08,696 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5ace5b0e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2023-06-30 10:07:09,780 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-06-30 10:07:09,954 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@51653509{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-621453802640655657/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2023-06-30 10:07:10,064 [main] INFO server.AbstractConnector: Started ServerConnector@3c2654e{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2023-06-30 10:07:10,064 [main] INFO server.Server: Started @86094ms
datanode2_1  | 2023-06-30 10:07:10,125 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2023-06-30 10:07:10,126 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2023-06-30 10:07:10,142 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2023-06-30 10:07:10,431 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode2_1  | 2023-06-30 10:07:10,475 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode2_1  | 2023-06-30 10:07:10,490 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode2_1  | 2023-06-30 10:07:12,965 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode2_1  | 2023-06-30 10:07:12,966 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode2_1  | 2023-06-30 10:07:12,977 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode2_1  | 2023-06-30 10:07:12,992 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode2_1  | 2023-06-30 10:07:13,061 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode2_1  | 2023-06-30 10:07:14,654 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2023-06-30 10:07:14,966 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode2_1  | 2023-06-30 10:07:18,023 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-06880496-7012-4073-8c29-45a6b430095b/DS-32af0e69-7718-4b48-a33f-67e619b197fa/container.db to cache
datanode2_1  | 2023-06-30 10:07:18,023 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-06880496-7012-4073-8c29-45a6b430095b/DS-32af0e69-7718-4b48-a33f-67e619b197fa/container.db for volume DS-32af0e69-7718-4b48-a33f-67e619b197fa
datanode2_1  | 2023-06-30 10:07:18,067 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2023-06-30 10:07:18,100 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode2_1  | 2023-06-30 10:07:18,327 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode2_1  | 2023-06-30 10:07:18,328 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 8585dc18-efaa-4bfa-93ea-e78f13194d9c
datanode2_1  | 2023-06-30 10:07:18,742 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: start RPC server
datanode2_1  | 2023-06-30 10:07:18,806 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: GrpcService started, listening on 9858
datanode2_1  | 2023-06-30 10:07:18,885 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: GrpcService started, listening on 9856
datanode2_1  | 2023-06-30 10:07:18,895 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: GrpcService started, listening on 9857
datanode2_1  | 2023-06-30 10:07:18,941 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8585dc18-efaa-4bfa-93ea-e78f13194d9c is started using port 9858 for RATIS
datanode2_1  | 2023-06-30 10:07:18,957 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8585dc18-efaa-4bfa-93ea-e78f13194d9c is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2023-06-30 10:07:18,957 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8585dc18-efaa-4bfa-93ea-e78f13194d9c is started using port 9856 for RATIS_SERVER
datanode2_1  | 2023-06-30 10:07:18,958 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8585dc18-efaa-4bfa-93ea-e78f13194d9c is started using port 9855 for RATIS_DATASTREAM
datanode2_1  | 2023-06-30 10:07:18,959 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-8585dc18-efaa-4bfa-93ea-e78f13194d9c: Started
datanode2_1  | 2023-06-30 10:07:19,156 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-30 10:07:19,157 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-30 10:07:19,242 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode2_1  | 2023-06-30 10:07:53,675 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: addNew group-47395DCA916D:[8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-47395DCA916D:java.util.concurrent.CompletableFuture@60c95dbb[Not completed]
datanode2_1  | 2023-06-30 10:07:54,191 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: new RaftServerImpl for group-47395DCA916D:[8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-30 10:07:54,210 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-30 10:07:54,221 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-30 10:07:54,226 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-30 10:07:03,089 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-30 10:07:03,095 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-30 10:07:03,239 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-30 10:07:03,240 [45c04ec6-f646-4507-992b-4f7a81b89828-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x17eba4e2] REGISTERED
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om2_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
om2_1        |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
om2_1        |                        43e78ba4990cf8693d7e311e758874532e0e4770
om2_1        |                        6e386edb08789430a89a39ebde59b89ba6e84e94
om2_1        |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
om2_1        |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
om2_1        |                        73af7af9023c681485766dec4627568ceadcae31
om2_1        |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
om2_1        |                        988b846e620e04205f341894328c4379e192c0f0
om2_1        |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
om2_1        |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
om2_1        |                        d924b613b78a9538512bbc922e5120bb84930f4c
om2_1        |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
om2_1        |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
datanode3_1  | 2023-06-30 10:07:03,267 [45c04ec6-f646-4507-992b-4f7a81b89828-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x17eba4e2] BIND: 0.0.0.0/0.0.0.0:9855
datanode3_1  | 2023-06-30 10:07:03,289 [45c04ec6-f646-4507-992b-4f7a81b89828-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x17eba4e2, L:/0.0.0.0:9855] ACTIVE
datanode3_1  | 2023-06-30 10:07:03,752 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode3_1  | 2023-06-30 10:07:04,720 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode3_1  | 2023-06-30 10:07:06,510 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2023-06-30 10:07:06,515 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2023-06-30 10:07:06,518 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2023-06-30 10:07:06,839 [main] INFO util.log: Logging initialized @83275ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2023-06-30 10:07:08,133 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2023-06-30 10:07:08,219 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2023-06-30 10:07:08,236 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2023-06-30 10:07:08,236 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2023-06-30 10:07:08,244 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2023-06-30 10:07:08,279 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2023-06-30 10:07:08,760 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode3_1  | 2023-06-30 10:07:08,769 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2023-06-30 10:07:08,783 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode3_1  | 2023-06-30 10:07:09,122 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2023-06-30 10:07:09,122 [main] INFO server.session: No SessionScavenger set, using defaults
datanode3_1  | 2023-06-30 10:07:09,144 [main] INFO server.session: node0 Scavenging every 600000ms
datanode3_1  | 2023-06-30 10:07:09,281 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-30 10:07:09,337 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@549fc0b3{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2023-06-30 10:07:09,344 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23138317{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2023-06-30 10:07:10,472 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-30 10:07:10,610 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@798ac54d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6608483568110077481/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2023-06-30 10:07:10,697 [main] INFO server.AbstractConnector: Started ServerConnector@1ac32da1{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2023-06-30 10:07:10,697 [main] INFO server.Server: Started @87133ms
datanode3_1  | 2023-06-30 10:07:10,730 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2023-06-30 10:07:10,731 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2023-06-30 10:07:10,738 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2023-06-30 10:07:11,026 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode3_1  | 2023-06-30 10:07:11,088 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode3_1  | 2023-06-30 10:07:11,174 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode3_1  | 2023-06-30 10:07:14,046 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode3_1  | 2023-06-30 10:07:14,046 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode3_1  | 2023-06-30 10:07:14,109 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode3_1  | 2023-06-30 10:07:14,174 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode3_1  | 2023-06-30 10:07:14,377 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode3_1  | 2023-06-30 10:07:16,198 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2023-06-30 10:07:16,328 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode3_1  | 2023-06-30 10:07:18,766 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-06880496-7012-4073-8c29-45a6b430095b/DS-d378cf2a-ba70-4083-8ce5-f6765ff7299f/container.db to cache
datanode3_1  | 2023-06-30 10:07:18,766 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-06880496-7012-4073-8c29-45a6b430095b/DS-d378cf2a-ba70-4083-8ce5-f6765ff7299f/container.db for volume DS-d378cf2a-ba70-4083-8ce5-f6765ff7299f
datanode3_1  | 2023-06-30 10:07:18,797 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2023-06-30 10:07:18,818 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode3_1  | 2023-06-30 10:07:19,213 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode3_1  | 2023-06-30 10:07:19,213 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 45c04ec6-f646-4507-992b-4f7a81b89828
datanode3_1  | 2023-06-30 10:07:19,699 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.RaftServer: 45c04ec6-f646-4507-992b-4f7a81b89828: start RPC server
datanode3_1  | 2023-06-30 10:07:19,733 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: 45c04ec6-f646-4507-992b-4f7a81b89828: GrpcService started, listening on 9858
datanode3_1  | 2023-06-30 10:07:19,735 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: 45c04ec6-f646-4507-992b-4f7a81b89828: GrpcService started, listening on 9856
datanode3_1  | 2023-06-30 10:07:19,739 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: 45c04ec6-f646-4507-992b-4f7a81b89828: GrpcService started, listening on 9857
datanode3_1  | 2023-06-30 10:07:19,841 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 45c04ec6-f646-4507-992b-4f7a81b89828 is started using port 9858 for RATIS
datanode3_1  | 2023-06-30 10:07:19,845 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 45c04ec6-f646-4507-992b-4f7a81b89828 is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2023-06-30 10:07:19,845 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 45c04ec6-f646-4507-992b-4f7a81b89828 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2023-06-30 10:07:53,494 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-30 10:07:53,500 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-30 10:07:53,502 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7ad03ed4-4cc4-4086-b128-5b762bd02996 does not exist. Creating ...
datanode1_1  | 2023-06-30 10:07:53,563 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7ad03ed4-4cc4-4086-b128-5b762bd02996/in_use.lock acquired by nodename 8@2b3975d57ed5
datanode1_1  | 2023-06-30 10:07:53,633 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7ad03ed4-4cc4-4086-b128-5b762bd02996 has been successfully formatted.
datanode1_1  | 2023-06-30 10:07:53,712 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO ratis.ContainerStateMachine: group-5B762BD02996: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-30 10:07:53,770 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-30 10:07:53,936 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-30 10:07:53,939 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 10:07:53,976 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-30 10:07:54,003 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-30 10:07:54,035 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 10:07:54,116 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-30 10:07:54,158 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-30 10:07:54,164 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 10:07:54,229 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7ad03ed4-4cc4-4086-b128-5b762bd02996
datanode1_1  | 2023-06-30 10:07:54,252 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-30 10:07:54,257 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 10:07:54,290 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 10:07:54,300 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-30 10:07:54,303 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-30 10:07:54,304 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-30 10:07:54,308 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-30 10:07:54,310 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2023-06-30 10:04:02,901 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
recon_1      | STARTUP_MSG:   java = 11.0.19
recon_1      | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1      | ************************************************************/
recon_1      | 2023-06-30 10:04:02,954 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-06-30 10:04:06,441 [main] INFO reflections.Reflections: Reflections took 439 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1      | 2023-06-30 10:04:09,592 [main] INFO reflections.Reflections: Reflections took 339 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1      | 2023-06-30 10:04:09,829 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2023-06-30 10:04:10,014 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2023-06-30 10:04:10,658 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file recon.keytab. Keytab auto renewal enabled : false
recon_1      | 2023-06-30 10:04:10,671 [main] INFO recon.ReconServer: Recon login successful.
recon_1      | 2023-06-30 10:04:10,704 [main] INFO recon.ReconServer: ReconStorageConfig initialized.Initializing certificate.
recon_1      | 2023-06-30 10:04:10,704 [main] INFO recon.ReconServer: Initializing secure Recon.
recon_1      | 2023-06-30 10:04:12,906 [main] WARN security.ReconCertificateClient: Certificates could not be loaded.
recon_1      | java.nio.file.NoSuchFileException: /data/metadata/recon/certs
recon_1      | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
recon_1      | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
recon_1      | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
recon_1      | 	at java.base/java.nio.file.Files.list(Files.java:3699)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
recon_1      | 	at org.apache.hadoop.ozone.recon.security.ReconCertificateClient.<init>(ReconCertificateClient.java:62)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.initializeCertificateClient(ReconServer.java:184)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:126)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:71)
recon_1      | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
recon_1      | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
recon_1      | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
recon_1      | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
recon_1      | 	at picocli.CommandLine.execute(CommandLine.java:2078)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:94)
recon_1      | 2023-06-30 10:04:12,913 [main] ERROR security.ReconCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
recon_1      | 2023-06-30 10:04:12,917 [main] INFO security.ReconCertificateClient: Certificate client init case: 0
recon_1      | 2023-06-30 10:04:12,918 [main] INFO security.ReconCertificateClient: Creating keypair for client as keypair and certificate not found.
recon_1      | 2023-06-30 10:04:16,448 [main] INFO recon.ReconServer: Init response: GETCERT
recon_1      | 2023-06-30 10:04:16,457 [main] INFO security.ReconCertificateClient: Creating CSR for Recon.
recon_1      | 2023-06-30 10:04:16,526 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.115,host:recon
recon_1      | 2023-06-30 10:04:16,533 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
recon_1      | 2023-06-30 10:04:16,549 [main] ERROR utils.CertificateSignRequest: Invalid domain recon
recon_1      | 2023-06-30 10:04:23,048 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1      | 2023-06-30 10:04:25,050 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1      | 2023-06-30 10:04:27,052 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-30 10:04:29,055 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
recon_1      | 2023-06-30 10:04:31,059 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
recon_1      | 2023-06-30 10:04:33,082 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-30 10:04:35,084 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
recon_1      | 2023-06-30 10:04:37,087 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
recon_1      | 2023-06-30 10:04:39,090 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-30 10:04:41,269 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:42abf28c-5926-4498-87c1-8b8ba8a02c3f is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1      | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
recon_1      | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1      | , while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
recon_1      | 2023-06-30 10:04:43,271 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
recon_1      | 2023-06-30 10:04:45,274 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
recon_1      | 2023-06-30 10:04:47,691 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 253598402501
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
recon_1      |            Start Date: Fri Jun 30 10:04:21 UTC 2023
recon_1      |            Final Date: Mon Aug 07 10:04:21 UTC 2028
recon_1      |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
recon_1      |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
recon_1      |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
recon_1      |                        43e78ba4990cf8693d7e311e758874532e0e4770
recon_1      |                        6e386edb08789430a89a39ebde59b89ba6e84e94
recon_1      |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
recon_1      |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
recon_1      |                        73af7af9023c681485766dec4627568ceadcae31
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om3_1        | 2023-06-30 10:06:33,437 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2023-06-30 10:06:33,450 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om3_1        | 2023-06-30 10:06:33,458 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2023-06-30 10:06:40,115 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2023-06-30 10:06:40,670 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2023-06-30 10:06:40,673 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2023-06-30 10:06:40,713 [main] ERROR utils.CertificateSignRequest: Invalid domain om3
om3_1        | 2023-06-30 10:06:40,714 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:42abf28c-5926-4498-87c1-8b8ba8a02c3f,clusterId:CID-06880496-7012-4073-8c29-45a6b430095b,subject:om3
om3_1        | 2023-06-30 10:06:43,542 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 253598402501
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om3_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
om3_1        |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
om3_1        |                        43e78ba4990cf8693d7e311e758874532e0e4770
om3_1        |                        6e386edb08789430a89a39ebde59b89ba6e84e94
om3_1        |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
om3_1        |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
om3_1        |                        73af7af9023c681485766dec4627568ceadcae31
om3_1        |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
om3_1        |                        988b846e620e04205f341894328c4379e192c0f0
om3_1        |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
om3_1        |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
om3_1        |                        d924b613b78a9538512bbc922e5120bb84930f4c
om3_1        |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
om3_1        |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-253598402501.crt.
om3_1        | 2023-06-30 10:06:43,578 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om3_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
om3_1        |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
om3_1        |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
om3_1        |                        dead23e7b54024421f408e091345739cf1861cb6
om3_1        |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
om3_1        |                        6385912587b5531778fa6f5c476842a25dc986ad
om3_1        |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
om3_1        |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
om3_1        |                        734cec37929dd6f294085e1181a91f1d47304865
om3_1        |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
om3_1        |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
datanode2_1  | 2023-06-30 10:07:54,227 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-30 10:07:54,245 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-30 10:07:54,245 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-30 10:07:54,412 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D: ConfigurationManager, init=-1: peers:[8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-30 10:07:54,457 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-30 10:07:54,619 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-30 10:07:54,653 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-30 10:07:54,883 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-30 10:07:54,905 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-30 10:07:54,980 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-30 10:07:54,981 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-30 10:07:55,290 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-30 10:07:55,446 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-30 10:07:55,481 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-30 10:07:55,491 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-30 10:07:55,493 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:55,500 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-30 10:07:55,504 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:55,518 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/459a01b6-1a94-49da-be94-47395dca916d does not exist. Creating ...
datanode2_1  | 2023-06-30 10:07:55,585 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/459a01b6-1a94-49da-be94-47395dca916d/in_use.lock acquired by nodename 8@f831826c9e82
datanode2_1  | 2023-06-30 10:07:55,651 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/459a01b6-1a94-49da-be94-47395dca916d has been successfully formatted.
datanode2_1  | 2023-06-30 10:07:55,773 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO ratis.ContainerStateMachine: group-47395DCA916D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-30 10:07:55,801 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-30 10:07:55,948 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-30 10:07:55,949 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 10:07:55,995 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-30 10:07:56,029 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-30 10:07:56,115 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 10:07:56,204 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-30 10:07:56,239 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-06-30 10:07:56,239 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 10:07:56,316 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/459a01b6-1a94-49da-be94-47395dca916d
datanode2_1  | 2023-06-30 10:07:56,332 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-06-30 10:07:56,333 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 10:07:56,346 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 10:07:56,348 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-06-30 10:07:56,353 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-30 10:07:56,370 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-30 10:07:56,376 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-30 10:07:56,382 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-30 10:07:56,556 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-06-30 10:07:56,563 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 10:07:56,767 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-30 10:07:56,768 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-06-30 10:07:56,773 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-30 10:07:56,844 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 10:07:56,846 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 10:07:56,874 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D: start as a follower, conf=-1: peers:[8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 10:07:56,875 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-253598402501.crt.
om2_1        | 2023-06-30 10:06:40,630 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om2_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
om2_1        |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
om2_1        |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
om2_1        |                        dead23e7b54024421f408e091345739cf1861cb6
om2_1        |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
om2_1        |                        6385912587b5531778fa6f5c476842a25dc986ad
om2_1        |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
om2_1        |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
om2_1        |                        734cec37929dd6f294085e1181a91f1d47304865
om2_1        |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
datanode1_1  | 2023-06-30 10:07:54,430 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-30 10:07:54,454 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 10:07:54,797 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-30 10:07:54,808 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-30 10:07:54,812 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-30 10:07:54,890 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-30 10:07:54,890 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-30 10:07:54,913 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996: start as a follower, conf=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 10:07:54,913 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-30 10:07:54,963 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: start 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState
datanode1_1  | 2023-06-30 10:07:55,084 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 10:07:55,086 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-30 10:07:55,165 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B762BD02996,id=90f061fe-584f-4bf2-a5ce-ab438b7e17a1
datanode1_1  | 2023-06-30 10:07:55,218 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-30 10:07:55,222 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-30 10:07:55,239 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-30 10:07:55,253 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-30 10:07:55,786 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=7ad03ed4-4cc4-4086-b128-5b762bd02996
datanode1_1  | 2023-06-30 10:07:55,807 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=7ad03ed4-4cc4-4086-b128-5b762bd02996.
datanode1_1  | 2023-06-30 10:07:55,834 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: addNew group-DD88234141A4:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] returns group-DD88234141A4:java.util.concurrent.CompletableFuture@613187c2[Not completed]
datanode1_1  | 2023-06-30 10:07:55,912 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: new RaftServerImpl for group-DD88234141A4:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-30 10:07:55,915 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-30 10:07:55,917 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-30 10:07:55,932 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-30 10:07:55,935 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-30 10:07:55,935 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-30 10:07:55,937 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-30 10:07:55,946 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4: ConfigurationManager, init=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-30 10:07:55,953 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-30 10:07:55,964 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-30 10:07:55,972 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-30 10:07:55,982 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-30 10:07:55,990 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-30 10:07:55,996 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-30 10:07:55,997 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-30 10:07:56,009 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-30 10:07:56,027 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-30 10:07:56,047 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-30 10:07:56,047 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-30 10:07:56,052 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-30 10:07:56,058 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-30 10:07:56,058 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-30 10:07:56,068 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4 does not exist. Creating ...
datanode1_1  | 2023-06-30 10:07:56,087 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4/in_use.lock acquired by nodename 8@2b3975d57ed5
datanode1_1  | 2023-06-30 10:07:56,121 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4 has been successfully formatted.
datanode1_1  | 2023-06-30 10:07:56,147 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO ratis.ContainerStateMachine: group-DD88234141A4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-30 10:07:56,147 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-30 10:07:56,198 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-30 10:07:56,198 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 10:07:56,198 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-30 10:07:56,198 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om1_1        | STARTUP_MSG:   java = 11.0.19
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-30 10:06:57,258 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-06-30 10:07:07,830 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-30 10:07:11,742 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om1_1        | 2023-06-30 10:07:12,328 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-30 10:07:12,330 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-30 10:07:12,330 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1        | 2023-06-30 10:07:12,375 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 10:07:12,642 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om1_1        | 2023-06-30 10:07:16,243 [main] INFO reflections.Reflections: Reflections took 3036 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om1_1        | 2023-06-30 10:07:16,674 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om1_1        | 2023-06-30 10:07:18,915 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-30 10:07:18,952 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-30 10:07:18,953 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 10:07:22,619 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-30 10:07:23,117 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-30 10:07:28,721 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-30 10:07:30,090 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 390654687721
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Start Date: Fri Jun 30 10:06:38 UTC 2023
om1_1        |            Final Date: Sat Jun 29 10:06:38 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Public Key: RSA Public Key [1f:52:04:a5:d1:46:29:9b:dc:b5:67:cf:d7:aa:4a:5b:9f:5e:5b:f2],[56:66:d1:a4]
om1_1        |         modulus: e4ad5123e64d8decddbf55f535d891b48f1392615859484114575a4ceab62e6f5d0e8dd3af8ba97bda95b1bd8ae0801b545654eeeeb3000976b38ef18af9e953bfc3ac3eba39907ea8296013f2223eb3b34493a8c37c1ab0b8c7f4d0b52fcae1a21eedd9f7a590ff32c6c09b8b13b2a0febd677d7372aa43412fa41662959bf9918cdc7ebc9c2b4fcbae82818534e7ea3485ba5416e4f81917af53ab37cc7cc9faa2af82fe9f19059a59b09040408ccb11539ed4409ef6ebcbefb6461ce33fcfa8764c167905dbafa912ac3f1b3408e43260a1ec4e5050f5ad3de1ff1cc897cb143900b2405a0619750dc9c2321102e34579c9a058babcbb1ec51cb681c01ef5
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: aa971d02f2802f1561abcdd37d50b521426632be
om1_1        |                        520aea54e1f8e9e1c3c76dfeeaf4eaa50f6d70d3
om1_1        |                        0faec9bcd63e5f8fcc884ee1976b06d343f903ff
om1_1        |                        f6087572c36a729732f3df7bdea5b2afef029647
om1_1        |                        83d67955ad4a4499b439bb7459e8b0775124f57c
om1_1        |                        ce36fb79a20218f98665fddf61e01ac6d25607ea
om1_1        |                        e0b77aa23c34f2664149871a2c137ab432a1fac9
om1_1        |                        090a3bd0f9a5e7b7d1509f19bf42634c87cadcaa
om1_1        |                        063ddc6a80c96502e830ea24436173565b1ad34e
datanode3_1  | 2023-06-30 10:07:19,845 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 45c04ec6-f646-4507-992b-4f7a81b89828 is started using port 9855 for RATIS_DATASTREAM
datanode3_1  | 2023-06-30 10:07:19,846 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-45c04ec6-f646-4507-992b-4f7a81b89828: Started
datanode3_1  | 2023-06-30 10:07:20,010 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-30 10:07:20,021 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-30 10:07:20,123 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode3_1  | 2023-06-30 10:07:54,587 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 45c04ec6-f646-4507-992b-4f7a81b89828: addNew group-8D68E5C9EEAE:[45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] returns group-8D68E5C9EEAE:java.util.concurrent.CompletableFuture@48103b58[Not completed]
datanode3_1  | 2023-06-30 10:07:54,852 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828: new RaftServerImpl for group-8D68E5C9EEAE:[45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-30 10:07:54,853 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-30 10:07:54,865 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-30 10:07:54,868 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-30 10:07:54,868 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-30 10:07:54,901 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-30 10:07:54,901 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-30 10:07:55,001 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE: ConfigurationManager, init=-1: peers:[45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-30 10:07:55,004 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-30 10:07:55,076 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-30 10:07:55,079 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-30 10:07:55,249 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-30 10:07:55,273 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-30 10:07:55,319 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-30 10:07:55,319 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-30 10:07:55,718 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-30 10:07:55,899 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-30 10:07:55,929 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-30 10:07:55,933 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-30 10:07:55,936 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-30 10:07:55,938 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-30 10:07:55,946 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
om2_1        |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
om2_1        |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
om2_1        |                        0e32f359ccbefded3427e57b80709364
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-06-30 10:06:40,702 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797356 ms and interval 86400000 ms.
om2_1        | 2023-06-30 10:06:40,703 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | 2023-06-30 10:06:40,804 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-30 10:06:58,456 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om2_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-30 10:06:58,557 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-06-30 10:07:09,109 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-30 10:07:12,046 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-30 10:07:13,369 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-06-30 10:07:13,373 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-30 10:07:13,387 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-30 10:07:13,476 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 10:07:14,074 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om2_1        | 2023-06-30 10:07:18,070 [main] INFO reflections.Reflections: Reflections took 3126 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om2_1        | 2023-06-30 10:07:18,253 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om2_1        | 2023-06-30 10:07:21,203 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-30 10:07:21,203 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-30 10:07:21,249 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 10:07:24,523 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-30 10:07:25,366 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-30 10:07:30,144 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-06-30 10:07:31,882 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 390994128355
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Start Date: Fri Jun 30 10:06:38 UTC 2023
om2_1        |            Final Date: Sat Jun 29 10:06:38 UTC 2024
om2_1        |             SubjectDN: CN=om2,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Public Key: RSA Public Key [fd:de:6e:8d:87:c7:3f:4e:1b:40:ce:08:26:61:b6:e6:61:d7:0a:ce],[56:66:d1:a4]
om2_1        |         modulus: ef4427dba167872479a8a6803cc078d462152f43f1f0c0bd4ca6460f78df38f0e2938f7469a25e9933c1bd207e3e5eeb00cd9e890d137d09244961fd775f7bd3c1c5c0bff9fa62e654a0f16e8360ae43bcd18b547788735f058272c1fde6f6663f89fd37013080a36a79411e541b56958b307bb48a190c5d427857072057ec25bd96d6764fa7726586bcaf7ae675af38c18828b0c563a85b1d48e1297c03c094d6bc0186eec2dfec674e8e507d3029276631f8f73981a33d9bc8e5ef1eb089fca1408eb7544495eaf6b46895b9d82459dae9194b4c1d1c5499588180235d5b47bb550d1d5cc63ade9a34e2c6b2b3e04c6f977851b7c507fc13479e91617dda85
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 06b4d0a20e8e49d08493211d5c4e89ec6914a544
om2_1        |                        641168f8c66263a1dda141f816c49c704cc0c930
om2_1        |                        e5fad9581fa4fe555cf3c2a33e51b656fbcc1039
om2_1        |                        30e92d50c4f4e9d15fca8814db6c736c0f62fb7f
om2_1        |                        682e6e38dc0eeac52d122c471c7e4e7a81b744aa
om2_1        |                        3f937f60e0a03f78a249a93114211e8b11469ffc
om2_1        |                        7c8ad2174d3add265c9fc49cd49833cebe8e8a7b
om2_1        |                        861c5572dd7fd958d8371d14863243d3bcd7bd6d
om2_1        |                        c3377d14c293c0711971d3fd07c95b2166de40ca
om2_1        |                        10821e8c069402758b68ddfd9ae091a931d4d0ea
om2_1        |                        f603849445b63fa48636ef0be91ab4201de13d2a
om2_1        |                        d09bb93f00772c8db9194bf7898fe5b68d30fdda
om2_1        |                        890aa26e6a2787239800793de2d6881e
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
datanode1_1  | 2023-06-30 10:07:56,253 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 10:07:56,260 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-30 10:07:56,260 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-30 10:07:56,269 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 10:07:56,269 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4
datanode1_1  | 2023-06-30 10:07:56,277 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-30 10:07:56,277 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 10:07:56,284 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 10:07:56,284 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-30 10:07:56,286 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-30 10:07:56,301 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-30 10:07:56,301 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-30 10:07:56,302 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-30 10:07:56,317 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-30 10:07:56,326 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 10:07:57,704 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-90f061fe-584f-4bf2-a5ce-ab438b7e17a1: Detected pause in JVM or host machine approximately 1.323s with 1.320s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=90ms
datanode1_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1230ms
datanode1_1  | 2023-06-30 10:07:57,749 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-30 10:07:57,750 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-30 10:07:57,754 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-30 10:07:57,755 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-30 10:07:57,764 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 10:07:56,892 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO impl.RoleInfo: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: start 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState
datanode2_1  | 2023-06-30 10:07:56,917 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-30 10:07:56,922 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-30 10:07:56,923 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-47395DCA916D,id=8585dc18-efaa-4bfa-93ea-e78f13194d9c
datanode2_1  | 2023-06-30 10:07:56,938 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-06-30 10:07:56,945 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-06-30 10:07:56,949 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-06-30 10:07:56,952 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-06-30 10:07:57,171 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=459a01b6-1a94-49da-be94-47395dca916d
datanode2_1  | 2023-06-30 10:07:57,182 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=459a01b6-1a94-49da-be94-47395dca916d.
datanode2_1  | 2023-06-30 10:07:57,192 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: addNew group-DD88234141A4:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] returns group-DD88234141A4:java.util.concurrent.CompletableFuture@2ac729a4[Not completed]
datanode2_1  | 2023-06-30 10:07:57,195 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: new RaftServerImpl for group-DD88234141A4:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-30 10:07:57,209 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-30 10:07:57,210 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-30 10:07:57,210 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-30 10:07:57,215 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-30 10:07:57,215 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-30 10:07:57,217 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-30 10:07:57,217 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4: ConfigurationManager, init=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-30 10:07:57,219 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-30 10:07:57,220 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-30 10:07:57,220 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-30 10:07:57,225 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-30 10:07:57,229 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-30 10:07:57,229 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-30 10:07:57,230 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-30 10:07:57,230 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-30 10:07:57,234 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-30 10:07:57,242 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-30 10:07:57,242 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-30 10:07:57,242 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:57,246 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-30 10:07:57,246 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-30 10:07:57,247 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4 does not exist. Creating ...
datanode3_1  | 2023-06-30 10:07:55,949 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/35f8130a-9766-41b0-b833-8d68e5c9eeae does not exist. Creating ...
datanode3_1  | 2023-06-30 10:07:56,010 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/35f8130a-9766-41b0-b833-8d68e5c9eeae/in_use.lock acquired by nodename 7@bda3280202ff
datanode3_1  | 2023-06-30 10:07:56,104 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/35f8130a-9766-41b0-b833-8d68e5c9eeae has been successfully formatted.
datanode3_1  | 2023-06-30 10:07:56,262 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO ratis.ContainerStateMachine: group-8D68E5C9EEAE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-30 10:07:56,281 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-30 10:07:56,490 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-30 10:07:56,490 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 10:07:56,531 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-30 10:07:56,555 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-30 10:07:56,601 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 10:07:56,680 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-30 10:07:56,684 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-30 10:07:56,687 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 10:07:56,776 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/35f8130a-9766-41b0-b833-8d68e5c9eeae
datanode3_1  | 2023-06-30 10:07:56,785 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-30 10:07:56,788 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 10:07:56,797 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 10:07:56,804 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-30 10:07:56,806 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-30 10:07:56,818 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-30 10:07:56,821 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-30 10:07:56,822 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-30 10:07:56,933 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-30 10:07:56,941 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 10:07:57,158 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-30 10:07:57,167 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-30 10:07:57,174 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-30 10:07:57,265 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO segmented.SegmentedRaftLogWorker: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-30 10:07:57,265 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO segmented.SegmentedRaftLogWorker: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-30 10:07:57,285 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE: start as a follower, conf=-1: peers:[45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 10:07:57,289 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-30 10:07:57,304 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: start 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState
datanode3_1  | 2023-06-30 10:07:57,320 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-30 10:07:57,320 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-30 10:07:57,331 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8D68E5C9EEAE,id=45c04ec6-f646-4507-992b-4f7a81b89828
datanode3_1  | 2023-06-30 10:07:57,360 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-30 10:07:57,369 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-30 10:07:57,370 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-30 10:07:57,380 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-30 10:07:57,757 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=35f8130a-9766-41b0-b833-8d68e5c9eeae
datanode3_1  | 2023-06-30 10:07:57,767 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=35f8130a-9766-41b0-b833-8d68e5c9eeae.
datanode3_1  | 2023-06-30 10:07:57,777 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 45c04ec6-f646-4507-992b-4f7a81b89828: addNew group-DD88234141A4:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] returns group-DD88234141A4:java.util.concurrent.CompletableFuture@15a7ea80[Not completed]
datanode3_1  | 2023-06-30 10:07:57,847 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828: new RaftServerImpl for group-DD88234141A4:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-30 10:07:57,865 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-30 10:07:57,867 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-30 10:07:57,873 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-30 10:07:57,875 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-30 10:07:57,875 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-30 10:07:57,877 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-30 10:07:57,881 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4: ConfigurationManager, init=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-30 10:07:57,886 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-30 10:07:57,896 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-30 10:07:57,897 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-30 10:07:57,899 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-30 10:07:57,902 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-30 10:07:57,903 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-30 10:07:57,913 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-30 10:07:57,917 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-30 10:07:57,942 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-30 10:07:57,942 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-30 10:07:57,945 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-30 10:07:57,947 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-30 10:07:57,949 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-30 10:07:57,951 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-30 10:07:57,956 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4 does not exist. Creating ...
om3_1        |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
om3_1        |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
om3_1        |                        0e32f359ccbefded3427e57b80709364
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-30 10:06:43,600 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 393626542201
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Start Date: Fri Jun 30 10:06:41 UTC 2023
om3_1        |            Final Date: Sat Jun 29 10:06:41 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Public Key: RSA Public Key [f8:a9:1c:5e:38:9b:31:fc:5e:c9:8e:6a:b9:8f:05:9e:fa:18:ba:21],[56:66:d1:a4]
om3_1        |         modulus: cd49282967d00de49ef70fa9d96aa2f862734ac9d083057e1121c7b7bc5f9ba8d4941911572e59ba6ad1a553a8e94fbf060843372b737fd3e7ef5f876dddf520b2006fc0edbe5fd3117d6f7ab62fd802149f5b10b1f86fbfcff66bd36d4f65e85ea5701ce926e8bbc645534d66f1cad67cd43c791900dd130bdd9db39dd5b03a83f638e561a101fc954e33e9427cb937df7a05b280c920c7c91f5d8d63890277ef4c2971f34fd711f71ce8084b010ef3968a4f8e59d162eeb87bec32fb28a1c3874172577635c5f02dbdb237e870031e8c5794afd09bd5b423e8b0460e250184550dc2391d065ddd9af6505a0796b38abfdca9ddfe821fe16767a7a5a1d2adb3
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 14a9ed77a7c5ab4426ce12afc19577b3226aa55e
om3_1        |                        9a5aae8b7b070705c9b97ec9db772f516399350a
om3_1        |                        bfb49ec14ba50bbd8aafb844d350034d692ddda4
om3_1        |                        a0d744b5e0612c8807a975576c541b4123539eb5
om3_1        |                        81b8a62f6f7cf17806d7492a538269b091aa268d
om3_1        |                        3890fb06c01eefe2e1af62c2651ab47f95e8bb4d
om3_1        |                        83ae0163ac8dcb3fad51867afbffb1e41e4a35d9
om3_1        |                        b5a620e31ee8147137b7936232ab0e45f1177674
om3_1        |                        9a8bac24c6561c8468bc558ac05801753373b071
om3_1        |                        440f2d31091bc51545dd6759be9439d2030745c0
om3_1        |                        19f3c00cc300334b553ce14df86178d799b06f1a
om3_1        |                        5faa04873f485dd3ed217142373c9a9014f74d85
om3_1        |                        ab76033935fa2b7d6b9cf114a228c2b9
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
recon_1      |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
recon_1      |                        988b846e620e04205f341894328c4379e192c0f0
recon_1      |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
recon_1      |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
recon_1      |                        d924b613b78a9538512bbc922e5120bb84930f4c
recon_1      |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
recon_1      |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
recon_1      |        Extensions: 
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0xbe
recon_1      |  from file: /data/metadata/recon/certs/CA-253598402501.crt.
datanode1_1  | 2023-06-30 10:07:57,782 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4: start as a follower, conf=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 10:07:57,789 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-30 10:07:57,807 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: start 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState
datanode1_1  | 2023-06-30 10:07:57,814 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DD88234141A4,id=90f061fe-584f-4bf2-a5ce-ab438b7e17a1
om1_1        |                        35450b8e285ef1bb7ac30b712d974b1a70967be7
om1_1        |                        04311ed210808c191c7f0104ec94e189756d001a
datanode1_1  | 2023-06-30 10:07:57,817 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-30 10:07:57,820 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-30 10:07:57,820 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-30 10:07:57,820 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-30 10:07:57,829 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 10:07:57,857 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4
datanode1_1  | 2023-06-30 10:07:57,879 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-30 10:07:58,044 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-30 10:08:00,157 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState] INFO impl.FollowerState: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5194482698ns, electionTimeout:5069ms
datanode1_1  | 2023-06-30 10:08:00,158 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: shutdown 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState
datanode1_1  | 2023-06-30 10:08:00,163 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-30 10:08:00,181 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-30 10:08:00,181 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-FollowerState] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: start 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1
datanode1_1  | 2023-06-30 10:08:00,213 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 10:08:00,227 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode1_1  | 2023-06-30 10:08:00,261 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 10:08:00,261 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2023-06-30 10:08:00,264 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: shutdown 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1
datanode1_1  | 2023-06-30 10:08:00,264 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2023-06-30 10:08:00,269 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5B762BD02996 with new leaderId: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1
datanode1_1  | 2023-06-30 10:08:00,271 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996: change Leader from null to 90f061fe-584f-4bf2-a5ce-ab438b7e17a1 at term 1 for becomeLeader, leader elected after 7479ms
datanode1_1  | 2023-06-30 10:08:00,456 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-06-30 10:08:00,550 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 10:08:00,556 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-06-30 10:08:00,601 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-06-30 10:08:00,602 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-06-30 10:08:00,631 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-06-30 10:08:00,693 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 10:08:00,710 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode2_1  | 2023-06-30 10:07:57,254 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4/in_use.lock acquired by nodename 8@f831826c9e82
datanode2_1  | 2023-06-30 10:07:57,262 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4 has been successfully formatted.
datanode2_1  | 2023-06-30 10:07:57,332 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO ratis.ContainerStateMachine: group-DD88234141A4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-30 10:07:57,332 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-30 10:07:57,346 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-30 10:07:57,346 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 10:07:57,346 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-30 10:07:57,346 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-30 10:07:57,351 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 10:07:57,360 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-30 10:07:57,361 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-06-30 10:07:57,361 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-30 10:07:57,362 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-30 10:07:57,376 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-06-30 10:07:57,382 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 10:07:58,642 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8585dc18-efaa-4bfa-93ea-e78f13194d9c: Detected pause in JVM or host machine approximately 1.010s with 1.246s GC time.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=91ms
datanode2_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1155ms
datanode2_1  | 2023-06-30 10:07:58,726 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-30 10:07:58,733 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1        |                        981f61098b16a4c1ebad2e21b201978058448bc2
om1_1        |                        0f9c867ddf30cbf5f95b367627529a96
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2023-06-30 10:04:03,189 [main] INFO security.UserGroupInformation: Login successful for user s3g/s3g@EXAMPLE.COM using keytab file s3g.keytab. Keytab auto renewal enabled : false
s3g_1        | 2023-06-30 10:04:03,190 [main] INFO s3.Gateway: S3Gateway login successful.
s3g_1        | 2023-06-30 10:04:04,222 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2023-06-30 10:04:04,223 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2023-06-30 10:04:04,241 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2023-06-30 10:04:04,509 [main] INFO util.log: Logging initialized @10119ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2023-06-30 10:04:05,197 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2023-06-30 10:04:05,254 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2023-06-30 10:04:05,256 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2023-06-30 10:04:05,256 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2023-06-30 10:04:05,256 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2023-06-30 10:04:05,265 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2023-06-30 10:04:05,455 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir1825792692991618512
s3g_1        | 2023-06-30 10:04:06,348 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/390654687721.crt.
om1_1        | 2023-06-30 10:07:30,119 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 253598402501
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om1_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
om1_1        |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
om1_1        |                        43e78ba4990cf8693d7e311e758874532e0e4770
om1_1        |                        6e386edb08789430a89a39ebde59b89ba6e84e94
om1_1        |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
om1_1        |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
om1_1        |                        73af7af9023c681485766dec4627568ceadcae31
om1_1        |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
om1_1        |                        988b846e620e04205f341894328c4379e192c0f0
om1_1        |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
om1_1        |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
om1_1        |                        d924b613b78a9538512bbc922e5120bb84930f4c
om1_1        |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
om1_1        |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0xbe
om1_1        |  from file: /data/metadata/om/certs/CA-253598402501.crt.
om1_1        | 2023-06-30 10:07:30,144 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 1
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om1_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om1_1        |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
om1_1        |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
om1_1        |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
om1_1        |                        dead23e7b54024421f408e091345739cf1861cb6
om1_1        |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
om1_1        |                        6385912587b5531778fa6f5c476842a25dc986ad
om1_1        |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
om1_1        |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
om1_1        |                        734cec37929dd6f294085e1181a91f1d47304865
om1_1        |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
om1_1        |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
om1_1        |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
om1_1        |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
om1_1        |                        0e32f359ccbefded3427e57b80709364
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-30 10:07:30,156 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116747851 ms and interval 86400000 ms.
om1_1        | 2023-06-30 10:07:30,469 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
recon_1      | 2023-06-30 10:04:47,709 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 1
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
recon_1      |            Start Date: Fri Jun 30 10:04:21 UTC 2023
recon_1      |            Final Date: Mon Aug 07 10:04:21 UTC 2028
recon_1      |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
recon_1      |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
recon_1      |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
recon_1      |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
recon_1      |                        dead23e7b54024421f408e091345739cf1861cb6
recon_1      |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
recon_1      |                        6385912587b5531778fa6f5c476842a25dc986ad
recon_1      |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
recon_1      |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
recon_1      |                        734cec37929dd6f294085e1181a91f1d47304865
recon_1      |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
recon_1      |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
recon_1      |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
recon_1      |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
recon_1      |                        0e32f359ccbefded3427e57b80709364
recon_1      |        Extensions: 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0x6
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
recon_1      |  from file: /data/metadata/recon/certs/ROOTCA-1.crt.
recon_1      | 2023-06-30 10:04:47,725 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 279121758205
recon_1      |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
recon_1      |            Start Date: Fri Jun 30 10:04:47 UTC 2023
recon_1      |            Final Date: Sat Jun 29 10:04:47 UTC 2024
recon_1      |             SubjectDN: CN=recon@recon,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
recon_1      |            Public Key: RSA Public Key [43:e1:ac:9e:09:bf:9a:c4:50:b3:9c:c9:07:2c:3d:a2:f1:d7:ee:c1],[56:66:d1:a4]
recon_1      |         modulus: b7f42c07dfa2e632d796bbab7d2893945e3e7307e64dc02f934304c187650f1a8b1c06a491c34e728a5df5d93f91da0dd54255e1b530df0083bd8896473f5c6c9ec6e79915882af157446bdc7831665e53913c24f298ae088f1ba41bc2ae3c6ffa82ffe6e1cb65e75a754dba629a0be6a2fb8b57fdccd60dcd9bf9600af148d92393e6f97d2b5390b1b1b2583e423f0fc75e7608fe55422f6f1b28d2042bdcca9a24e39ecbb399d2b5114e8f3217ddf5e7755578996c46a16c6bb64bab8819fd5d471965b24a44706c7fccb70dc05519bea3cf92b9cc1024273686ac5dd806ab7c4f21aa65f12f8ab4632f0e13de5f91e6512ba4271a15c5430c322346d67979
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 7b8cae33a665fd6ca7fc31f23e41ac761d3f82ac
recon_1      |                        936c53c24349d008b610952a69a8a0fd7cd19399
recon_1      |                        4e0eab19ba5c5e4c242f89e7346a2cd187260323
recon_1      |                        d73a295ee11a29d7529fda8e3f11a48438d6995a
recon_1      |                        338aa02ba2f126ad5dbbaab59eade6164fe2c308
recon_1      |                        d2c13cbb6127fd1f5a8578e30f6d572fed36ac82
recon_1      |                        cedf4dbf424dd423d9848f1990d74e07096f1fed
recon_1      |                        47b6a8d4eea0c0877ee0a033281682b864d9e40a
recon_1      |                        21382926ab68f4df4091880b835155122a943cdf
recon_1      |                        a265055e1820e67e7e2cf4c4094612530fc158ad
recon_1      |                        f40c8d8b85b25c029b2b15eb00f9850c85b00e95
recon_1      |                        a765aa9e161b576f4a891dbfbe2ebb156aaca312
recon_1      |                        7e120ef9049acecbdf89811b2fc66be5
recon_1      |        Extensions: 
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      | 
recon_1      |                        critical(true) KeyUsage: 0xb8
recon_1      |  from file: /data/metadata/recon/certs/279121758205.crt.
recon_1      | 2023-06-30 10:04:47,753 [main] INFO security.ReconCertificateClient: CertificateLifetimeMonitor for recon is started with first delay 29116799273 ms and interval 86400000 ms.
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/393626542201.crt.
om3_1        | 2023-06-30 10:06:43,663 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797388 ms and interval 86400000 ms.
om3_1        | 2023-06-30 10:06:43,665 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om3_1        | 2023-06-30 10:06:43,750 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-06-30 10:07:00,029 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-30 10:07:00,123 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-06-30 10:07:09,859 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-30 10:07:13,168 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-30 10:07:14,811 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-30 10:07:14,845 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
om3_1        | 2023-06-30 10:07:14,885 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-30 10:07:15,154 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 10:07:16,383 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om3_1        | 2023-06-30 10:07:19,261 [main] INFO reflections.Reflections: Reflections took 2195 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om3_1        | 2023-06-30 10:07:19,523 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om3_1        | 2023-06-30 10:07:21,729 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-30 10:07:21,729 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-30 10:07:21,729 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 10:07:25,031 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-30 10:07:25,811 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-30 10:07:30,455 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-30 10:07:32,341 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 253598402501
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om3_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
om3_1        |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
om3_1        |                        43e78ba4990cf8693d7e311e758874532e0e4770
om3_1        |                        6e386edb08789430a89a39ebde59b89ba6e84e94
om3_1        |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
om3_1        |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
om3_1        |                        73af7af9023c681485766dec4627568ceadcae31
om3_1        |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
om3_1        |                        988b846e620e04205f341894328c4379e192c0f0
om3_1        |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
om3_1        |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
om3_1        |                        d924b613b78a9538512bbc922e5120bb84930f4c
om3_1        |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
om3_1        |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-253598402501.crt.
om2_1        |  from file: /data/metadata/om/certs/390994128355.crt.
om2_1        | 2023-06-30 10:07:31,956 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 253598402501
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om2_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
datanode2_1  | 2023-06-30 10:07:58,733 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-30 10:07:58,734 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 10:07:58,734 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 10:07:58,740 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4: start as a follower, conf=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 10:07:58,741 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-06-30 10:07:58,741 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO impl.RoleInfo: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: start 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-FollowerState
datanode2_1  | 2023-06-30 10:07:58,747 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DD88234141A4,id=8585dc18-efaa-4bfa-93ea-e78f13194d9c
datanode2_1  | 2023-06-30 10:07:58,748 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-06-30 10:07:58,748 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-06-30 10:07:58,749 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-06-30 10:07:58,749 [8585dc18-efaa-4bfa-93ea-e78f13194d9c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-30 10:08:00,723 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: start 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderStateImpl
datanode1_1  | 2023-06-30 10:08:00,931 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-06-30 10:08:01,581 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-LeaderElection1] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996: set configuration 0: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 10:08:02,100 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-5B762BD02996-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7ad03ed4-4cc4-4086-b128-5b762bd02996/current/log_inprogress_0
datanode1_1  | 2023-06-30 10:08:02,967 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-90f061fe-584f-4bf2-a5ce-ab438b7e17a1: Detected pause in JVM or host machine approximately 0.694s with 0.678s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=678ms
datanode1_1  | 2023-06-30 10:08:03,014 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState] INFO impl.FollowerState: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5207143176ns, electionTimeout:5115ms
datanode1_1  | 2023-06-30 10:08:03,054 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: shutdown 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState
datanode1_1  | 2023-06-30 10:08:03,054 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState] INFO server.RaftServer$Division: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-30 10:08:03,107 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-30 10:08:03,119 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-FollowerState] INFO impl.RoleInfo: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: start 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2
datanode1_1  | 2023-06-30 10:08:03,213 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 10:08:03,511 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 10:08:03,511 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-30 10:08:03,518 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 8585dc18-efaa-4bfa-93ea-e78f13194d9c
datanode1_1  | 2023-06-30 10:08:03,519 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 45c04ec6-f646-4507-992b-4f7a81b89828
datanode1_1  | 2023-06-30 10:08:08,157 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-06-30 10:08:08,175 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO impl.LeaderElection:   Response 0: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1<-8585dc18-efaa-4bfa-93ea-e78f13194d9c#0:OK-t0
datanode1_1  | 2023-06-30 10:08:08,179 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode1_1  | 2023-06-30 10:08:08,208 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO impl.LeaderElection: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 10:08:08,281 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 10:08:08,281 [90f061fe-584f-4bf2-a5ce-ab438b7e17a1@group-DD88234141A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
s3g_1        | STARTUP_MSG:   java = 11.0.19
om2_1        |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
om2_1        |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
om2_1        |                        43e78ba4990cf8693d7e311e758874532e0e4770
om2_1        |                        6e386edb08789430a89a39ebde59b89ba6e84e94
om2_1        |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
om2_1        |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
om2_1        |                        73af7af9023c681485766dec4627568ceadcae31
om2_1        |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
om2_1        |                        988b846e620e04205f341894328c4379e192c0f0
om2_1        |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
om2_1        |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
om2_1        |                        d924b613b78a9538512bbc922e5120bb84930f4c
om2_1        |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
om2_1        |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-253598402501.crt.
om2_1        | 2023-06-30 10:07:31,998 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om2_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om2_1        |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
om2_1        |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
om2_1        |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
om2_1        |                        dead23e7b54024421f408e091345739cf1861cb6
om2_1        |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
om2_1        |                        6385912587b5531778fa6f5c476842a25dc986ad
om2_1        |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
om2_1        |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
om2_1        |                        734cec37929dd6f294085e1181a91f1d47304865
om2_1        |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
datanode3_1  | 2023-06-30 10:07:57,978 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4/in_use.lock acquired by nodename 7@bda3280202ff
datanode3_1  | 2023-06-30 10:07:57,992 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4 has been successfully formatted.
datanode3_1  | 2023-06-30 10:07:58,023 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO ratis.ContainerStateMachine: group-DD88234141A4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-30 10:07:58,023 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-30 10:07:58,057 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-30 10:07:58,057 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 10:07:58,058 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-30 10:07:58,059 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-30 10:07:58,064 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 10:07:58,067 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-30 10:07:58,078 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-30 10:07:58,082 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 10:07:58,085 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5f2976d1-d881-4d67-95db-dd88234141a4
datanode3_1  | 2023-06-30 10:07:58,085 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-30 10:07:58,085 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 10:07:58,095 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 10:07:58,099 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-30 10:07:58,105 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-30 10:07:58,108 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-30 10:07:58,109 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-30 10:07:58,109 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-30 10:07:58,162 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-30 10:07:58,174 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 10:07:59,749 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-45c04ec6-f646-4507-992b-4f7a81b89828: Detected pause in JVM or host machine approximately 1.214s with 1.567s GC time.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=175ms
datanode3_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1392ms
datanode3_1  | 2023-06-30 10:07:59,788 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-30 10:07:59,790 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-30 10:07:59,791 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-30 10:07:59,794 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO segmented.SegmentedRaftLogWorker: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
s3g_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir1825792692991618512, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1        | ************************************************************/
s3g_1        | 2023-06-30 10:04:06,376 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2023-06-30 10:04:06,464 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2023-06-30 10:04:07,061 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1        | 2023-06-30 10:04:07,837 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1        | 2023-06-30 10:04:07,837 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1        | 2023-06-30 10:04:07,989 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1        | 2023-06-30 10:04:08,003 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2023-06-30 10:04:08,017 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1        | 2023-06-30 10:04:08,183 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2023-06-30 10:04:08,183 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2023-06-30 10:04:08,198 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1        | 2023-06-30 10:04:08,382 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-30 10:04:08,453 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@126be319{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2023-06-30 10:04:08,460 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@75201592{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | 2023-06-30 10:04:12,299 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.148s with 0.158s GC time.
s3g_1        | GC pool 'ParNew' had collection(s): count=1 time=158ms
s3g_1        | 2023-06-30 10:04:16,315 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-30 10:04:19,198 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@326706d{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir1825792692991618512/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-7166835309485265700/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2023-06-30 10:04:19,243 [main] INFO server.AbstractConnector: Started ServerConnector@6f330eb9{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2023-06-30 10:04:19,245 [main] INFO server.Server: Started @24869ms
s3g_1        | 2023-06-30 10:04:19,265 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1        | 2023-06-30 10:04:19,265 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1        | 2023-06-30 10:04:19,267 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
datanode2_1  | 2023-06-30 10:07:58,758 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-30 10:07:58,759 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4
datanode2_1  | 2023-06-30 10:07:58,761 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-30 10:07:58,973 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-30 10:08:02,086 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState] INFO impl.FollowerState: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5196629873ns, electionTimeout:5158ms
datanode2_1  | 2023-06-30 10:08:02,087 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState] INFO impl.RoleInfo: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: shutdown 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState
datanode2_1  | 2023-06-30 10:08:02,120 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-06-30 10:08:02,131 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode2_1  | 2023-06-30 10:08:02,134 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-FollowerState] INFO impl.RoleInfo: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: start 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1
datanode2_1  | 2023-06-30 10:08:02,281 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO impl.LeaderElection: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 10:08:02,296 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO impl.LeaderElection: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode2_1  | 2023-06-30 10:08:02,319 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO impl.LeaderElection: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 10:08:02,320 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO impl.LeaderElection: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode2_1  | 2023-06-30 10:08:02,321 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO impl.RoleInfo: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: shutdown 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1
datanode2_1  | 2023-06-30 10:08:02,322 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode2_1  | 2023-06-30 10:08:02,326 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-47395DCA916D with new leaderId: 8585dc18-efaa-4bfa-93ea-e78f13194d9c
datanode2_1  | 2023-06-30 10:08:02,381 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D: change Leader from null to 8585dc18-efaa-4bfa-93ea-e78f13194d9c at term 1 for becomeLeader, leader elected after 7443ms
datanode2_1  | 2023-06-30 10:08:02,494 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2023-06-30 10:08:02,625 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 10:08:02,629 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode2_1  | 2023-06-30 10:08:02,709 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2023-06-30 10:08:02,714 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2023-06-30 10:08:02,716 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2023-06-30 10:08:02,793 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 10:08:02,822 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode2_1  | 2023-06-30 10:08:02,851 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO impl.RoleInfo: 8585dc18-efaa-4bfa-93ea-e78f13194d9c: start 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderStateImpl
datanode2_1  | 2023-06-30 10:08:03,079 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-06-30 10:08:04,284 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8585dc18-efaa-4bfa-93ea-e78f13194d9c: Detected pause in JVM or host machine approximately 0.549s with 0.794s GC time.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=794ms
datanode2_1  | 2023-06-30 10:08:04,280 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-FollowerState] WARN impl.FollowerState: Unexpected long sleep: sleep 5043ms but took extra 475940007ns (> threshold = 300ms)
datanode2_1  | 2023-06-30 10:08:04,472 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-30 10:08:04,472 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-30 10:08:05,406 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-LeaderElection1] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D: set configuration 0: peers:[8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 10:07:59,795 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO segmented.SegmentedRaftLogWorker: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-30 10:07:59,806 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4: start as a follower, conf=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 10:07:59,806 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-30 10:07:59,810 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: start 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState
datanode3_1  | 2023-06-30 10:07:59,865 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DD88234141A4,id=45c04ec6-f646-4507-992b-4f7a81b89828
datanode3_1  | 2023-06-30 10:07:59,865 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-30 10:07:59,870 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-30 10:07:59,870 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-30 10:07:59,872 [45c04ec6-f646-4507-992b-4f7a81b89828-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-06-30 10:08:06,285 [8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-47395DCA916D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/459a01b6-1a94-49da-be94-47395dca916d/current/log_inprogress_0
datanode2_1  | 2023-06-30 10:08:07,311 [grpc-default-executor-2] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4: receive requestVote(PRE_VOTE, 90f061fe-584f-4bf2-a5ce-ab438b7e17a1, group-DD88234141A4, 0, (t:0, i:0))
datanode2_1  | 2023-06-30 10:08:07,338 [grpc-default-executor-2] INFO impl.VoteContext: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-FOLLOWER: accept PRE_VOTE from 90f061fe-584f-4bf2-a5ce-ab438b7e17a1: our priority 0 <= candidate's priority 1
datanode2_1  | 2023-06-30 10:08:07,796 [grpc-default-executor-2] INFO server.RaftServer$Division: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4 replies to PRE_VOTE vote request: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1<-8585dc18-efaa-4bfa-93ea-e78f13194d9c#0:OK-t0. Peer's state: 8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4:t0, leader=null, voted=, raftlog=Memoized:8585dc18-efaa-4bfa-93ea-e78f13194d9c@group-DD88234141A4-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1      | 2023-06-30 10:04:47,790 [main] INFO recon.ReconServer: Successfully stored SCM signed certificate, case:GETCERT.
recon_1      | 2023-06-30 10:04:48,654 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-30 10:04:50,820 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2023-06-30 10:04:52,483 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1      | 2023-06-30 10:04:52,488 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.004 seconds to initialized 0 records to KEY_CONTAINER table
recon_1      | 2023-06-30 10:04:52,684 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-30 10:04:52,734 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2023-06-30 10:04:52,776 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2023-06-30 10:04:54,742 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2023-06-30 10:04:54,743 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2023-06-30 10:04:54,743 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
recon_1      | 2023-06-30 10:04:54,756 [main] INFO util.log: Logging initialized @60077ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2023-06-30 10:04:55,174 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1      | 2023-06-30 10:04:55,218 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1      | 2023-06-30 10:04:55,225 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2023-06-30 10:04:55,225 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
om2_1        |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
om2_1        |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
om2_1        |                        0e32f359ccbefded3427e57b80709364
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-06-30 10:07:32,017 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116745996 ms and interval 86400000 ms.
om2_1        | 2023-06-30 10:07:32,460 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
om1_1        | 2023-06-30 10:07:30,646 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 10:07:31,668 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1        | 2023-06-30 10:07:35,404 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1        | 2023-06-30 10:07:35,525 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2023-06-30 10:07:35,526 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2023-06-30 10:07:35,810 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om1_1        | 2023-06-30 10:07:35,934 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 10:07:36,357 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om1_1        | 2023-06-30 10:07:36,364 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om1_1        | 2023-06-30 10:07:37,983 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om1_1        | 2023-06-30 10:07:38,533 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-30 10:07:38,541 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2023-06-30 10:07:38,640 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om1_1        | 2023-06-30 10:07:38,645 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2023-06-30 10:07:39,545 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1        | 2023-06-30 10:07:39,607 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-30 10:07:39,841 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1        | 2023-06-30 10:07:39,916 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2023-06-30 10:07:40,206 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-30 10:07:40,214 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-30 10:07:40,296 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2023-06-30 10:07:40,352 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-30 10:07:40,359 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-30 10:07:40,362 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-30 10:07:40,363 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-30 10:07:40,363 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1        | 2023-06-30 10:07:40,364 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2023-06-30 10:07:40,368 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1        | 2023-06-30 10:07:40,395 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 10:07:40,398 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2023-06-30 10:07:40,406 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-30 10:07:40,545 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-06-30 10:07:40,573 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1        | 2023-06-30 10:07:40,580 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-06-30 10:07:42,720 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1        | 2023-06-30 10:07:42,745 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1        | 2023-06-30 10:07:42,745 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1        | 2023-06-30 10:07:42,755 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-06-30 10:07:42,755 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-30 10:07:32,381 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Start Date: Fri Jun 30 10:04:21 UTC 2023
om3_1        |            Final Date: Mon Aug 07 10:04:21 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
om3_1        |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
om3_1        |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
om3_1        |                        dead23e7b54024421f408e091345739cf1861cb6
om3_1        |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
om3_1        |                        6385912587b5531778fa6f5c476842a25dc986ad
om3_1        |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
om3_1        |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
om3_1        |                        734cec37929dd6f294085e1181a91f1d47304865
om3_1        |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
om3_1        |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
om3_1        |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
om3_1        |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
om3_1        |                        0e32f359ccbefded3427e57b80709364
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-30 10:07:32,434 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 393626542201
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Start Date: Fri Jun 30 10:06:41 UTC 2023
om3_1        |            Final Date: Sat Jun 29 10:06:41 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
om3_1        |            Public Key: RSA Public Key [f8:a9:1c:5e:38:9b:31:fc:5e:c9:8e:6a:b9:8f:05:9e:fa:18:ba:21],[56:66:d1:a4]
om3_1        |         modulus: cd49282967d00de49ef70fa9d96aa2f862734ac9d083057e1121c7b7bc5f9ba8d4941911572e59ba6ad1a553a8e94fbf060843372b737fd3e7ef5f876dddf520b2006fc0edbe5fd3117d6f7ab62fd802149f5b10b1f86fbfcff66bd36d4f65e85ea5701ce926e8bbc645534d66f1cad67cd43c791900dd130bdd9db39dd5b03a83f638e561a101fc954e33e9427cb937df7a05b280c920c7c91f5d8d63890277ef4c2971f34fd711f71ce8084b010ef3968a4f8e59d162eeb87bec32fb28a1c3874172577635c5f02dbdb237e870031e8c5794afd09bd5b423e8b0460e250184550dc2391d065ddd9af6505a0796b38abfdca9ddfe821fe16767a7a5a1d2adb3
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 14a9ed77a7c5ab4426ce12afc19577b3226aa55e
om3_1        |                        9a5aae8b7b070705c9b97ec9db772f516399350a
om3_1        |                        bfb49ec14ba50bbd8aafb844d350034d692ddda4
om3_1        |                        a0d744b5e0612c8807a975576c541b4123539eb5
om3_1        |                        81b8a62f6f7cf17806d7492a538269b091aa268d
om3_1        |                        3890fb06c01eefe2e1af62c2651ab47f95e8bb4d
om3_1        |                        83ae0163ac8dcb3fad51867afbffb1e41e4a35d9
om3_1        |                        b5a620e31ee8147137b7936232ab0e45f1177674
om3_1        |                        9a8bac24c6561c8468bc558ac05801753373b071
om3_1        |                        440f2d31091bc51545dd6759be9439d2030745c0
om3_1        |                        19f3c00cc300334b553ce14df86178d799b06f1a
om3_1        |                        5faa04873f485dd3ed217142373c9a9014f74d85
om3_1        |                        ab76033935fa2b7d6b9cf114a228c2b9
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
datanode3_1  | 2023-06-30 10:07:59,870 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-30 10:07:59,875 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-30 10:07:59,881 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4
datanode3_1  | 2023-06-30 10:07:59,965 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-30 10:08:02,387 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState] INFO impl.FollowerState: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5083400076ns, electionTimeout:5062ms
datanode3_1  | 2023-06-30 10:08:02,388 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: shutdown 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState
datanode3_1  | 2023-06-30 10:08:02,389 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-06-30 10:08:02,404 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode3_1  | 2023-06-30 10:08:02,405 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-FollowerState] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: start 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1
datanode3_1  | 2023-06-30 10:08:02,444 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO impl.LeaderElection: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 10:08:02,454 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO impl.LeaderElection: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode3_1  | 2023-06-30 10:08:02,490 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO impl.LeaderElection: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 10:08:02,492 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO impl.LeaderElection: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode3_1  | 2023-06-30 10:08:02,493 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: shutdown 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1
datanode3_1  | 2023-06-30 10:08:02,500 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2023-06-30 10:08:02,501 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8D68E5C9EEAE with new leaderId: 45c04ec6-f646-4507-992b-4f7a81b89828
datanode3_1  | 2023-06-30 10:08:02,505 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE: change Leader from null to 45c04ec6-f646-4507-992b-4f7a81b89828 at term 1 for becomeLeader, leader elected after 7259ms
datanode3_1  | 2023-06-30 10:08:02,657 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2023-06-30 10:08:02,755 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 10:08:02,769 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2023-06-30 10:08:02,819 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2023-06-30 10:08:02,825 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2023-06-30 10:08:02,854 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2023-06-30 10:08:02,998 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 10:08:03,036 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2023-06-30 10:08:03,108 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: start 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderStateImpl
datanode3_1  | 2023-06-30 10:08:03,515 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-06-30 10:08:04,926 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-45c04ec6-f646-4507-992b-4f7a81b89828: Detected pause in JVM or host machine approximately 1.150s with 1.156s GC time.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=1156ms
datanode3_1  | 2023-06-30 10:08:04,989 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState] INFO impl.FollowerState: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5179069461ns, electionTimeout:5108ms
datanode3_1  | 2023-06-30 10:08:05,043 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: shutdown 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState
datanode3_1  | 2023-06-30 10:08:05,044 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-06-30 10:08:05,044 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode3_1  | 2023-06-30 10:08:05,044 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-FollowerState] INFO impl.RoleInfo: 45c04ec6-f646-4507-992b-4f7a81b89828: start 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-LeaderElection2
recon_1      | 2023-06-30 10:04:55,225 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2023-06-30 10:04:55,236 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2023-06-30 10:04:55,394 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1      | 2023-06-30 10:04:55,413 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1      | 2023-06-30 10:04:56,026 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2023-06-30 10:04:56,037 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1      | 2023-06-30 10:04:56,065 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1      | 2023-06-30 10:04:56,075 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1      | 2023-06-30 10:04:58,210 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-30 10:04:58,692 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-30 10:04:58,927 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2023-06-30 10:04:58,938 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2023-06-30 10:04:59,465 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-30 10:04:59,857 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1      | 2023-06-30 10:04:59,945 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2023-06-30 10:05:00,059 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2023-06-30 10:05:00,085 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2023-06-30 10:05:00,142 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1      | 2023-06-30 10:05:01,518 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1      | 2023-06-30 10:05:01,637 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2023-06-30 10:05:01,658 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1      | 2023-06-30 10:05:01,668 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2023-06-30 10:05:01,856 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1      | 2023-06-30 10:05:02,344 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1      | 2023-06-30 10:05:02,360 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2023-06-30 10:05:02,361 [main] INFO recon.ReconServer: Starting Recon server
recon_1      | 2023-06-30 10:05:02,590 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2023-06-30 10:05:02,662 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2023-06-30 10:05:02,662 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2023-06-30 10:05:03,786 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2023-06-30 10:05:03,815 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1      | 2023-06-30 10:05:03,981 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2023-06-30 10:05:03,981 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2023-06-30 10:05:03,987 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1      | 2023-06-30 10:05:04,053 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-30 10:05:04,072 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6ddccee4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1      | 2023-06-30 10:05:04,077 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4c2b8df7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1      | 2023-06-30 10:05:05,819 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-30 10:05:05,861 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-30 10:05:12,190 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6cf87ba6{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-17989035410581706072/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1      | 2023-06-30 10:05:12,255 [main] INFO server.AbstractConnector: Started ServerConnector@363bf9d{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1      | 2023-06-30 10:05:12,260 [main] INFO server.Server: Started @77581ms
recon_1      | 2023-06-30 10:05:12,286 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1      | 2023-06-30 10:05:12,287 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1      | 2023-06-30 10:05:12,291 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1      | 2023-06-30 10:05:12,291 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1      | 2023-06-30 10:05:12,384 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1      | 2023-06-30 10:05:12,431 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
om2_1        | 2023-06-30 10:07:32,743 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 10:07:34,129 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1        | 2023-06-30 10:07:37,034 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1        | 2023-06-30 10:07:37,170 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2023-06-30 10:07:37,172 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2023-06-30 10:07:37,428 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om2_1        | 2023-06-30 10:07:37,512 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 10:07:37,786 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om2_1        | 2023-06-30 10:07:37,794 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om2_1        | 2023-06-30 10:07:38,918 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om2_1        | 2023-06-30 10:07:39,821 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-30 10:07:39,826 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2023-06-30 10:07:39,929 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om2_1        | 2023-06-30 10:07:39,934 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2023-06-30 10:07:41,004 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2023-06-30 10:07:41,114 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-30 10:07:41,526 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1        | 2023-06-30 10:07:41,660 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2023-06-30 10:07:42,410 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-30 10:07:42,430 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-30 10:07:42,563 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2023-06-30 10:07:42,630 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-30 10:07:42,642 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-30 10:07:42,644 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-30 10:07:42,648 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-30 10:07:42,649 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1        | 2023-06-30 10:07:42,650 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2023-06-30 10:07:42,651 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2023-06-30 10:07:42,667 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 10:07:42,672 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2023-06-30 10:07:42,690 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-30 10:07:42,778 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-30 10:07:42,840 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@296eda6c[Not completed]
om1_1        | 2023-06-30 10:07:42,841 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1        | 2023-06-30 10:07:42,897 [main] INFO om.OzoneManager: Creating RPC Server
om1_1        | 2023-06-30 10:07:43,118 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1        | 2023-06-30 10:07:43,152 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2023-06-30 10:07:43,163 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2023-06-30 10:07:43,167 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2023-06-30 10:07:43,167 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-06-30 10:07:43,169 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-06-30 10:07:43,169 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2023-06-30 10:07:43,270 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1        | 2023-06-30 10:07:43,271 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-30 10:07:43,363 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2023-06-30 10:07:43,367 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 2023-06-30 10:07:43,508 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
datanode3_1  | 2023-06-30 10:08:05,305 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-LeaderElection2] INFO impl.LeaderElection: 45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[90f061fe-584f-4bf2-a5ce-ab438b7e17a1|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER, 45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, 8585dc18-efaa-4bfa-93ea-e78f13194d9c|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 10:08:05,651 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-30 10:08:05,651 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-30 10:08:05,685 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 90f061fe-584f-4bf2-a5ce-ab438b7e17a1
datanode3_1  | 2023-06-30 10:08:05,685 [45c04ec6-f646-4507-992b-4f7a81b89828@group-DD88234141A4-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 8585dc18-efaa-4bfa-93ea-e78f13194d9c
datanode3_1  | 2023-06-30 10:08:05,877 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-LeaderElection1] INFO server.RaftServer$Division: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE: set configuration 0: peers:[45c04ec6-f646-4507-992b-4f7a81b89828|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 10:08:07,515 [45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 45c04ec6-f646-4507-992b-4f7a81b89828@group-8D68E5C9EEAE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/35f8130a-9766-41b0-b833-8d68e5c9eeae/current/log_inprogress_0
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-30 10:04:08,399 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-30 10:04:08,558 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-06-30 10:04:09,323 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 10:04:10,965 [main] INFO reflections.Reflections: Reflections took 1179 ms to scan 3 urls, producing 132 keys and 287 values 
scm1.org_1   | 2023-06-30 10:04:11,750 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-30 10:04:11,870 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-30 10:04:12,007 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-30 10:04:12,027 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-30 10:04:12,076 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2023-06-30 10:04:17,835 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
om2_1        | 2023-06-30 10:07:42,794 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1        | 2023-06-30 10:07:42,815 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1        | 2023-06-30 10:07:42,820 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-06-30 10:07:44,794 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1        | 2023-06-30 10:07:44,843 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1        | 2023-06-30 10:07:44,848 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1        | 2023-06-30 10:07:44,848 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-30 10:07:44,852 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-30 10:07:44,868 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-30 10:07:44,926 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@291b3e68[Not completed]
om2_1        | 2023-06-30 10:07:44,926 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1        | 2023-06-30 10:07:44,968 [main] INFO om.OzoneManager: Creating RPC Server
om2_1        | 2023-06-30 10:07:45,067 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1        | 2023-06-30 10:07:45,087 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2023-06-30 10:07:45,088 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2023-06-30 10:07:45,089 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2023-06-30 10:07:45,091 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-30 10:07:45,093 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-30 10:07:45,095 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2023-06-30 10:07:45,205 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1        | 2023-06-30 10:07:45,213 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-30 10:07:45,325 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2023-06-30 10:07:45,327 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1        | 2023-06-30 10:07:45,443 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2023-06-30 10:07:43,556 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1        | 2023-06-30 10:07:43,615 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2023-06-30 10:07:43,620 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2023-06-30 10:07:43,971 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om1_1        | 2023-06-30 10:07:44,429 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-30 10:07:44,479 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-06-30 10:07:44,492 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1        | 2023-06-30 10:07:44,494 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1        | 2023-06-30 10:07:44,512 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1        | 2023-06-30 10:07:44,533 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1        | 2023-06-30 10:07:47,259 [main] INFO reflections.Reflections: Reflections took 3579 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om1_1        | 2023-06-30 10:07:48,909 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2023-06-30 10:07:48,949 [main] INFO ipc.Server: Listener at om1:9862
om1_1        | 2023-06-30 10:07:48,973 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2023-06-30 10:07:58,294 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2023-06-30 10:07:58,474 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2023-06-30 10:07:58,476 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2023-06-30 10:07:59,132 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2023-06-30 10:07:59,144 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2023-06-30 10:07:59,185 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1        | 2023-06-30 10:07:59,212 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om1
om1_1        | 2023-06-30 10:07:59,268 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1        | 2023-06-30 10:07:59,302 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2023-06-30 10:07:59,403 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2023-06-30 10:07:59,417 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 10:07:59,435 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1        | 2023-06-30 10:07:59,459 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1        | 2023-06-30 10:07:59,526 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-06-30 10:07:59,588 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2023-06-30 10:07:59,596 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2023-06-30 10:07:59,597 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 10:07:59,669 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1        | 2023-06-30 10:07:59,675 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2023-06-30 10:07:59,679 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2023-06-30 10:07:59,696 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-06-30 10:07:59,700 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2023-06-30 10:07:59,706 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1      | 2023-06-30 10:05:12,431 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1      | 2023-06-30 10:05:12,433 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-30 10:05:12,434 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1      | 2023-06-30 10:05:12,452 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1      | 2023-06-30 10:05:14,227 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1      | 2023-06-30 10:05:14,227 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2023-06-30 10:05:14,228 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1      | 2023-06-30 10:05:14,232 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1      | 2023-06-30 10:05:14,257 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1      | 2023-06-30 10:05:14,259 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1      | 2023-06-30 10:05:32,441 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1      | 2023-06-30 10:05:32,442 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1      | 2023-06-30 10:05:34,697 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately. Current retry count: 3.
recon_1      | 2023-06-30 10:05:34,698 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately. Current retry count: 4.
recon_1      | 2023-06-30 10:05:34,704 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-30 10:05:36,705 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately. Current retry count: 6.
recon_1      | 2023-06-30 10:05:36,708 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately. Current retry count: 7.
recon_1      | 2023-06-30 10:05:36,710 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-30 10:05:38,715 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately. Current retry count: 9.
recon_1      | 2023-06-30 10:05:38,716 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately. Current retry count: 10.
recon_1      | 2023-06-30 10:05:38,718 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-30 10:05:40,720 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately. Current retry count: 12.
recon_1      | 2023-06-30 10:05:40,723 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately. Current retry count: 13.
recon_1      | 2023-06-30 10:05:40,724 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
recon_1      | 2023-06-30 10:05:42,739 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately. Current retry count: 15.
recon_1      | 2023-06-30 10:05:42,748 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately. Current retry count: 16.
recon_1      | 2023-06-30 10:05:42,749 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
recon_1      | 2023-06-30 10:05:44,756 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately. Current retry count: 18.
recon_1      | 2023-06-30 10:05:44,759 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately. Current retry count: 19.
recon_1      | 2023-06-30 10:05:44,766 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 20.
recon_1      | 2023-06-30 10:05:46,772 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately. Current retry count: 21.
recon_1      | 2023-06-30 10:05:46,775 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately. Current retry count: 22.
recon_1      | 2023-06-30 10:05:46,777 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 23.
recon_1      | 2023-06-30 10:05:48,779 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately. Current retry count: 24.
recon_1      | 2023-06-30 10:05:48,780 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately. Current retry count: 25.
recon_1      | 2023-06-30 10:05:48,780 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 26.
recon_1      | 2023-06-30 10:05:50,782 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately. Current retry count: 27.
recon_1      | 2023-06-30 10:05:50,790 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately. Current retry count: 28.
recon_1      | 2023-06-30 10:05:50,791 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 29.
recon_1      | 2023-06-30 10:05:52,793 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately. Current retry count: 30.
recon_1      | 2023-06-30 10:05:52,795 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately. Current retry count: 31.
recon_1      | 2023-06-30 10:05:52,804 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 32.
recon_1      | 2023-06-30 10:05:54,812 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately. Current retry count: 33.
recon_1      | 2023-06-30 10:05:54,815 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately. Current retry count: 34.
recon_1      | 2023-06-30 10:05:54,826 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 35.
recon_1      | 2023-06-30 10:05:56,830 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately. Current retry count: 36.
recon_1      | 2023-06-30 10:05:56,832 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately. Current retry count: 37.
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/393626542201.crt.
om3_1        | 2023-06-30 10:07:32,470 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116748556 ms and interval 86400000 ms.
om3_1        | 2023-06-30 10:07:32,748 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
om2_1        | 2023-06-30 10:07:45,486 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1        | 2023-06-30 10:07:45,538 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2023-06-30 10:07:45,562 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2023-06-30 10:07:46,041 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om2_1        | 2023-06-30 10:07:46,547 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-06-30 10:07:46,606 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2023-06-30 10:07:46,613 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1        | 2023-06-30 10:07:46,618 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1        | 2023-06-30 10:07:46,622 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1        | 2023-06-30 10:07:46,632 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        | 2023-06-30 10:07:48,580 [main] INFO reflections.Reflections: Reflections took 3076 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om2_1        | 2023-06-30 10:07:50,092 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1        | 2023-06-30 10:07:50,153 [main] INFO ipc.Server: Listener at om2:9862
om2_1        | 2023-06-30 10:07:50,163 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2023-06-30 10:07:57,952 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2023-06-30 10:07:58,123 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2023-06-30 10:07:58,134 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2023-06-30 10:07:59,030 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2023-06-30 10:07:59,035 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2023-06-30 10:07:59,081 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om2_1        | 2023-06-30 10:07:59,135 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om2
om2_1        | 2023-06-30 10:07:59,242 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1        | 2023-06-30 10:07:59,285 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2023-06-30 10:07:59,372 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2023-06-30 10:07:59,373 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 10:07:59,391 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1        | 2023-06-30 10:07:59,410 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1        | 2023-06-30 10:07:59,450 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-30 10:07:59,564 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2023-06-30 10:07:59,567 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2023-06-30 10:07:59,567 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 10:07:59,632 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1        | 2023-06-30 10:07:59,636 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2023-06-30 10:07:59,641 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2023-06-30 10:07:59,648 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-30 10:07:59,652 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2023-06-30 10:07:59,654 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1        | 2023-06-30 10:07:59,667 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2023-06-30 10:07:59,671 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2023-06-30 10:07:59,672 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2023-06-30 10:07:59,768 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2023-06-30 10:07:59,769 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 10:07:59,871 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1        | 2023-06-30 10:07:59,905 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1        | 2023-06-30 10:07:59,909 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2023-06-30 10:07:59,962 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-30 10:07:59,967 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-30 10:07:59,991 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-30 10:08:00,000 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2023-06-30 10:08:00,024 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-30 10:08:00,029 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-30 10:08:00,029 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-30 10:04:17,854 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1        | 2023-06-30 10:07:59,714 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1        | 2023-06-30 10:07:59,721 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2023-06-30 10:07:33,088 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 10:07:34,371 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1        | 2023-06-30 10:07:37,825 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1        | 2023-06-30 10:07:37,930 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2023-06-30 10:07:37,930 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2023-06-30 10:07:38,327 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om3_1        | 2023-06-30 10:07:38,448 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 10:07:38,873 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1        | 2023-06-30 10:07:38,882 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om3_1        | 2023-06-30 10:07:40,354 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om3_1        | 2023-06-30 10:07:41,165 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-30 10:07:41,169 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1        | 2023-06-30 10:07:41,286 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om3_1        | 2023-06-30 10:07:41,286 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2023-06-30 10:07:42,130 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2023-06-30 10:07:42,197 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-30 10:07:42,456 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1        | 2023-06-30 10:07:42,553 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2023-06-30 10:07:43,046 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-30 10:07:43,056 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-30 10:07:43,183 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2023-06-30 10:07:43,244 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-30 10:07:43,261 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-30 10:07:43,264 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-30 10:07:43,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-30 10:07:43,267 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1        | 2023-06-30 10:07:43,270 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2023-06-30 10:07:43,274 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2023-06-30 10:07:43,285 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 10:07:43,287 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2023-06-30 10:07:43,293 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-30 10:07:43,411 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-06-30 10:07:43,432 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1        | 2023-06-30 10:07:43,440 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-06-30 10:07:45,730 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1        | 2023-06-30 10:07:45,745 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1        | 2023-06-30 10:07:45,748 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1        | 2023-06-30 10:07:45,751 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-30 10:07:45,752 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-30 10:07:45,784 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-06-30 10:07:45,854 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@42c82f73[Not completed]
om3_1        | 2023-06-30 10:07:45,855 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2023-06-30 10:07:45,902 [main] INFO om.OzoneManager: Creating RPC Server
om3_1        | 2023-06-30 10:07:46,013 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1        | 2023-06-30 10:07:46,036 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2023-06-30 10:07:46,043 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2023-06-30 10:07:46,047 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2023-06-30 10:07:46,047 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-30 10:07:46,049 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-30 10:07:46,049 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2023-06-30 10:07:46,235 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1        | 2023-06-30 10:07:46,235 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-06-30 10:07:46,327 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2023-06-30 10:07:46,335 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1        | 2023-06-30 10:07:46,517 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2023-06-30 10:07:59,721 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2023-06-30 10:07:59,890 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2023-06-30 10:07:59,896 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 10:08:00,221 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1        | 2023-06-30 10:08:00,244 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1        | 2023-06-30 10:08:00,246 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1        | 2023-06-30 10:08:00,436 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-30 10:08:00,452 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-30 10:08:00,565 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-30 10:08:00,579 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2023-06-30 10:08:00,649 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-30 10:08:00,659 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-30 10:08:00,667 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-06-30 10:08:00,696 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1        | 2023-06-30 10:08:00,727 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2023-06-30 10:08:00,732 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2023-06-30 10:08:00,744 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2023-06-30 10:08:00,751 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2023-06-30 10:08:00,844 [main] INFO server.RaftServer: om1: start RPC server
om1_1        | 2023-06-30 10:08:01,697 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1        | 2023-06-30 10:08:01,744 [main] INFO om.OzoneManager: Starting secret key client.
om1_1        | 2023-06-30 10:08:01,748 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2023-06-30 10:08:02,852 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z).
om1_1        | 2023-06-30 10:08:02,930 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M40.597234S and interval of PT1M
om1_1        | 2023-06-30 10:08:02,976 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2023-06-30 10:08:02,978 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 390654687721
om1_1        | 2023-06-30 10:08:03,041 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2023-06-30 10:08:03,048 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2023-06-30 10:08:04,892 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1        | 2023-06-30 10:08:04,896 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om1_1        | 2023-06-30 10:08:04,897 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om1_1        | 2023-06-30 10:08:05,395 [main] INFO util.log: Logging initialized @82897ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1        | 2023-06-30 10:08:05,995 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5369302601ns, electionTimeout:5179ms
om1_1        | 2023-06-30 10:08:06,005 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-30 10:08:06,017 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1        | 2023-06-30 10:08:06,028 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1        | 2023-06-30 10:08:06,028 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1        | 2023-06-30 10:08:06,118 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-30 10:08:06,510 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-30 10:08:06,512 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-06-30 10:08:06,536 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1        | 2023-06-30 10:08:06,537 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1        | 2023-06-30 10:08:07,851 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1        | 2023-06-30 10:08:08,378 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1        | 2023-06-30 10:08:08,380 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1        | 2023-06-30 10:08:08,380 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1        | 2023-06-30 10:08:08,380 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        | 2023-06-30 10:08:08,471 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
recon_1      | 2023-06-30 10:05:56,833 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 38.
recon_1      | 2023-06-30 10:05:58,866 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately. Current retry count: 39.
recon_1      | 2023-06-30 10:05:58,878 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately. Current retry count: 40.
recon_1      | 2023-06-30 10:05:58,963 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 41.
recon_1      | 2023-06-30 10:06:00,971 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately. Current retry count: 42.
recon_1      | 2023-06-30 10:06:00,974 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately. Current retry count: 43.
recon_1      | 2023-06-30 10:06:00,976 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 44.
recon_1      | 2023-06-30 10:06:02,986 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately. Current retry count: 45.
recon_1      | 2023-06-30 10:06:03,005 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately. Current retry count: 46.
recon_1      | 2023-06-30 10:06:03,020 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 47.
recon_1      | 2023-06-30 10:06:05,022 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover immediately. Current retry count: 48.
recon_1      | 2023-06-30 10:06:05,023 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 49 failover attempts. Trying to failover immediately. Current retry count: 49.
recon_1      | 2023-06-30 10:06:05,025 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 50 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 50.
recon_1      | 2023-06-30 10:06:07,026 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 51 failover attempts. Trying to failover immediately. Current retry count: 51.
recon_1      | 2023-06-30 10:06:07,028 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 52 failover attempts. Trying to failover immediately. Current retry count: 52.
recon_1      | 2023-06-30 10:06:07,029 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 53 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 53.
recon_1      | 2023-06-30 10:06:09,031 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 54 failover attempts. Trying to failover immediately. Current retry count: 54.
recon_1      | 2023-06-30 10:06:09,033 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 55 failover attempts. Trying to failover immediately. Current retry count: 55.
recon_1      | 2023-06-30 10:06:09,034 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 56 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 56.
recon_1      | 2023-06-30 10:06:11,036 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 57 failover attempts. Trying to failover immediately. Current retry count: 57.
recon_1      | 2023-06-30 10:06:11,038 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 58 failover attempts. Trying to failover immediately. Current retry count: 58.
recon_1      | 2023-06-30 10:06:11,039 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 59 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 59.
recon_1      | 2023-06-30 10:06:13,061 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 60 failover attempts. Trying to failover immediately. Current retry count: 60.
recon_1      | 2023-06-30 10:06:13,075 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 61 failover attempts. Trying to failover immediately. Current retry count: 61.
recon_1      | 2023-06-30 10:06:13,080 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 62 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 62.
recon_1      | 2023-06-30 10:06:15,082 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 63 failover attempts. Trying to failover immediately. Current retry count: 63.
recon_1      | 2023-06-30 10:06:15,085 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 64 failover attempts. Trying to failover immediately. Current retry count: 64.
recon_1      | 2023-06-30 10:06:15,086 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 65 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 65.
recon_1      | 2023-06-30 10:06:17,092 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 66 failover attempts. Trying to failover immediately. Current retry count: 66.
recon_1      | 2023-06-30 10:06:17,095 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 67 failover attempts. Trying to failover immediately. Current retry count: 67.
recon_1      | 2023-06-30 10:06:17,097 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 68 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 68.
recon_1      | 2023-06-30 10:06:19,099 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 69 failover attempts. Trying to failover immediately. Current retry count: 69.
recon_1      | 2023-06-30 10:06:19,101 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 70 failover attempts. Trying to failover immediately. Current retry count: 70.
recon_1      | 2023-06-30 10:06:19,102 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 71 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 71.
recon_1      | 2023-06-30 10:06:21,104 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 72 failover attempts. Trying to failover immediately. Current retry count: 72.
recon_1      | 2023-06-30 10:06:21,105 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 73 failover attempts. Trying to failover immediately. Current retry count: 73.
recon_1      | 2023-06-30 10:06:21,106 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 74 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 74.
recon_1      | 2023-06-30 10:06:23,127 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 75 failover attempts. Trying to failover immediately. Current retry count: 75.
recon_1      | 2023-06-30 10:06:23,133 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 76 failover attempts. Trying to failover immediately. Current retry count: 76.
recon_1      | 2023-06-30 10:06:23,141 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 77 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 77.
recon_1      | 2023-06-30 10:06:25,143 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 78 failover attempts. Trying to failover immediately. Current retry count: 78.
recon_1      | 2023-06-30 10:06:25,144 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 79 failover attempts. Trying to failover immediately. Current retry count: 79.
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-30 10:04:17,856 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2023-06-30 10:04:17,866 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2023-06-30 10:04:20,325 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2023-06-30 10:04:21,459 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-30 10:04:21,459 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-06-30 10:04:21,643 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b to CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b, valid from Fri Jun 30 10:04:21 UTC 2023 to Mon Aug 07 10:04:21 UTC 2028
scm1.org_1   | 2023-06-30 10:04:21,688 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-30 10:04:21,688 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-06-30 10:04:21,689 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:42abf28c-5926-4498-87c1-8b8ba8a02c3f,clusterId:CID-06880496-7012-4073-8c29-45a6b430095b,subject:scm-sub@scm1.org
scm1.org_1   | 2023-06-30 10:04:21,851 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2023-06-30 10:04:22,178 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-30 10:04:22,428 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 10:04:22,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 10:04:22,430 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 10:04:22,430 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 10:04:22,430 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-06-30 10:04:22,430 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-30 10:04:22,431 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
om2_1        | 2023-06-30 10:08:00,050 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1        | 2023-06-30 10:08:00,077 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2023-06-30 10:08:00,079 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2023-06-30 10:08:00,079 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2023-06-30 10:08:00,084 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-30 10:04:26,059 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-30 10:04:26,070 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-30 10:04:26,199 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 10:04:26,416 [main] INFO reflections.Reflections: Reflections took 173 ms to scan 3 urls, producing 132 keys and 287 values 
scm2.org_1   | 2023-06-30 10:04:26,517 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-30 10:04:26,518 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-30 10:04:26,544 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-30 10:04:26,544 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-30 10:04:26,794 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-30 10:04:26,795 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-30 10:04:26,875 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-30 10:04:31,130 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
scm2.org_1   | 2023-06-30 10:04:33,132 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
scm2.org_1   | 2023-06-30 10:04:35,135 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm2.org_1   | 2023-06-30 10:04:37,144 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
scm2.org_1   | 2023-06-30 10:04:39,966 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:42abf28c-5926-4498-87c1-8b8ba8a02c3f is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1        | 2023-06-30 10:07:46,566 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1        | 2023-06-30 10:07:46,615 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2023-06-30 10:07:46,619 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2023-06-30 10:07:47,022 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1        | 2023-06-30 10:07:47,491 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-30 10:07:47,542 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-06-30 10:07:47,576 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1        | 2023-06-30 10:07:47,577 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1        | 2023-06-30 10:07:47,600 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1        | 2023-06-30 10:07:47,621 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1        | 2023-06-30 10:07:49,817 [main] INFO reflections.Reflections: Reflections took 3136 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om3_1        | 2023-06-30 10:07:51,804 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2023-06-30 10:07:51,873 [main] INFO ipc.Server: Listener at om3:9862
om3_1        | 2023-06-30 10:07:51,889 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2023-06-30 10:08:02,589 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2023-06-30 10:08:02,723 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2023-06-30 10:08:02,726 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2023-06-30 10:08:03,616 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2023-06-30 10:08:03,619 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2023-06-30 10:08:03,660 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1        | 2023-06-30 10:08:03,732 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om3
om3_1        | 2023-06-30 10:08:03,833 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om3_1        | 2023-06-30 10:08:03,907 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2023-06-30 10:08:04,002 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2023-06-30 10:08:04,009 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 10:08:04,030 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1        | 2023-06-30 10:08:04,037 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1        | 2023-06-30 10:08:04,070 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-06-30 10:08:04,168 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2023-06-30 10:08:04,169 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2023-06-30 10:08:04,175 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 10:08:04,254 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1        | 2023-06-30 10:08:04,261 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1        | 2023-06-30 10:08:04,264 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2023-06-30 10:08:04,277 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-06-30 10:08:04,289 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2023-06-30 10:08:04,290 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1        | 2023-06-30 10:08:04,300 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2023-06-30 10:08:04,308 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2023-06-30 10:08:04,313 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2023-06-30 10:08:04,482 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2023-06-30 10:08:04,484 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 10:08:04,572 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1        | 2023-06-30 10:08:04,580 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1        | 2023-06-30 10:08:04,582 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2023-06-30 10:08:04,705 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-30 10:08:04,707 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-30 10:08:04,741 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-06-30 10:08:04,751 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2023-06-30 10:08:04,779 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1        | 2023-06-30 10:08:04,801 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-06-30 10:08:04,802 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-30 10:08:00,100 [main] INFO server.RaftServer: om2: start RPC server
om2_1        | 2023-06-30 10:08:00,403 [main] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2023-06-30 10:08:00,424 [main] INFO om.OzoneManager: Starting secret key client.
om2_1        | 2023-06-30 10:08:00,458 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2023-06-30 10:08:01,266 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z).
om2_1        | 2023-06-30 10:08:01,308 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M42.219946S and interval of PT1M
om2_1        | 2023-06-30 10:08:01,354 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2023-06-30 10:08:01,355 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 390994128355
om2_1        | 2023-06-30 10:08:01,409 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2023-06-30 10:08:01,413 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om2_1        | 2023-06-30 10:08:03,069 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1        | 2023-06-30 10:08:03,076 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om2_1        | 2023-06-30 10:08:03,078 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om2_1        | 2023-06-30 10:08:03,495 [main] INFO util.log: Logging initialized @80411ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1        | 2023-06-30 10:08:05,159 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5136161185ns, electionTimeout:5098ms
om2_1        | 2023-06-30 10:08:05,191 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-30 10:08:05,211 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1        | 2023-06-30 10:08:05,269 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om2_1        | 2023-06-30 10:08:05,292 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1        | 2023-06-30 10:08:05,307 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-30 10:08:05,605 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-30 10:08:05,606 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-30 10:08:05,636 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1        | 2023-06-30 10:08:05,636 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1        | 2023-06-30 10:08:05,980 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1        | 2023-06-30 10:08:06,099 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2023-06-30 10:08:06,124 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1        | 2023-06-30 10:08:06,125 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1        | 2023-06-30 10:08:06,135 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        | 2023-06-30 10:08:06,210 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om2_1        | 2023-06-30 10:08:06,993 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om2: Detected pause in JVM or host machine approximately 0.184s without any GCs.
om2_1        | 2023-06-30 10:08:07,752 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om2_1        | 2023-06-30 10:08:08,076 [main] INFO http.HttpServer2: Jetty bound to port 9874
om2_1        | 2023-06-30 10:08:08,294 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1      | 2023-06-30 10:06:25,145 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 80 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 80.
recon_1      | 2023-06-30 10:06:27,146 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 81 failover attempts. Trying to failover immediately. Current retry count: 81.
recon_1      | 2023-06-30 10:06:27,147 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 82 failover attempts. Trying to failover immediately. Current retry count: 82.
recon_1      | 2023-06-30 10:06:27,148 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 83 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 83.
recon_1      | 2023-06-30 10:06:29,149 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 84 failover attempts. Trying to failover immediately. Current retry count: 84.
recon_1      | 2023-06-30 10:06:29,150 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 85 failover attempts. Trying to failover immediately. Current retry count: 85.
recon_1      | 2023-06-30 10:06:29,150 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 86 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 86.
recon_1      | 2023-06-30 10:06:31,154 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 87 failover attempts. Trying to failover immediately. Current retry count: 87.
recon_1      | 2023-06-30 10:06:31,155 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 88 failover attempts. Trying to failover immediately. Current retry count: 88.
recon_1      | 2023-06-30 10:06:31,155 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 89 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 89.
recon_1      | 2023-06-30 10:06:33,157 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 90 failover attempts. Trying to failover immediately. Current retry count: 90.
recon_1      | 2023-06-30 10:06:33,158 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 91 failover attempts. Trying to failover immediately. Current retry count: 91.
recon_1      | 2023-06-30 10:06:33,159 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 92 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 92.
recon_1      | 2023-06-30 10:06:35,161 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 93 failover attempts. Trying to failover immediately. Current retry count: 93.
recon_1      | 2023-06-30 10:06:35,162 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 94 failover attempts. Trying to failover immediately. Current retry count: 94.
recon_1      | 2023-06-30 10:06:35,163 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 95 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 95.
recon_1      | 2023-06-30 10:06:37,165 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 96 failover attempts. Trying to failover immediately. Current retry count: 96.
recon_1      | 2023-06-30 10:06:37,166 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 97 failover attempts. Trying to failover immediately. Current retry count: 97.
recon_1      | 2023-06-30 10:06:37,167 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 98 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 98.
recon_1      | 2023-06-30 10:06:39,169 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 99 failover attempts. Trying to failover immediately. Current retry count: 99.
recon_1      | 2023-06-30 10:06:39,170 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 100 failover attempts. Trying to failover immediately. Current retry count: 100.
recon_1      | 2023-06-30 10:06:39,171 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 101 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 101.
recon_1      | 2023-06-30 10:06:41,173 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 102 failover attempts. Trying to failover immediately. Current retry count: 102.
recon_1      | 2023-06-30 10:06:41,174 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 103 failover attempts. Trying to failover immediately. Current retry count: 103.
recon_1      | 2023-06-30 10:06:41,175 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 104 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 104.
recon_1      | 2023-06-30 10:06:43,177 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 105 failover attempts. Trying to failover immediately. Current retry count: 105.
recon_1      | 2023-06-30 10:06:43,178 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 106 failover attempts. Trying to failover immediately. Current retry count: 106.
recon_1      | 2023-06-30 10:06:43,179 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 107 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 107.
recon_1      | 2023-06-30 10:06:45,182 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 108 failover attempts. Trying to failover immediately. Current retry count: 108.
recon_1      | 2023-06-30 10:06:45,185 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 109 failover attempts. Trying to failover immediately. Current retry count: 109.
recon_1      | 2023-06-30 10:06:45,186 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 110 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 110.
recon_1      | 2023-06-30 10:06:47,196 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 111 failover attempts. Trying to failover immediately. Current retry count: 111.
recon_1      | 2023-06-30 10:06:47,197 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 112 failover attempts. Trying to failover immediately. Current retry count: 112.
recon_1      | 2023-06-30 10:06:47,210 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 113 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 113.
recon_1      | 2023-06-30 10:06:49,212 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 114 failover attempts. Trying to failover immediately. Current retry count: 114.
recon_1      | 2023-06-30 10:06:49,215 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 115 failover attempts. Trying to failover immediately. Current retry count: 115.
recon_1      | 2023-06-30 10:06:49,216 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 116 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 116.
recon_1      | 2023-06-30 10:06:51,218 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 117 failover attempts. Trying to failover immediately. Current retry count: 117.
recon_1      | 2023-06-30 10:06:51,221 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 118 failover attempts. Trying to failover immediately. Current retry count: 118.
recon_1      | 2023-06-30 10:06:51,223 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 119 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 119.
recon_1      | 2023-06-30 10:06:53,231 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 120 failover attempts. Trying to failover immediately. Current retry count: 120.
scm1.org_1   | 2023-06-30 10:04:22,442 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:22,443 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-06-30 10:04:22,444 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 10:04:22,468 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 10:04:22,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:22,485 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-30 10:04:23,031 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-30 10:04:23,034 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:23,035 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-30 10:04:23,035 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 10:04:23,036 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 10:04:23,039 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 10:04:23,052 [main] INFO server.RaftServer: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: addNew group-45A6B430095B:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] returns group-45A6B430095B:java.util.concurrent.CompletableFuture@1282f784[Not completed]
scm1.org_1   | 2023-06-30 10:04:23,087 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: new RaftServerImpl for group-45A6B430095B:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-30 10:04:23,098 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-30 10:04:23,098 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-30 10:04:23,099 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-30 10:04:23,099 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 10:04:23,100 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 10:04:23,100 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-30 10:04:23,110 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: ConfigurationManager, init=-1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-30 10:04:23,110 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 10:04:23,115 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-06-30 10:04:23,116 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-30 10:04:23,144 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-30 10:04:23,149 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-30 10:04:23,152 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-30 10:04:23,153 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-30 10:04:23,174 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-30 10:04:23,180 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-30 10:04:23,285 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 10:04:23,288 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-30 10:04:23,288 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:23,288 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-30 10:04:23,289 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:23,289 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
scm2.org_1   | 2023-06-30 10:04:41,968 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
scm2.org_1   | 2023-06-30 10:04:44,138 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm2.org_1   | 2023-06-30 10:04:44,898 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-30 10:05:16,166 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-30 10:05:16,200 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-06-30 10:05:16,518 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 10:05:17,042 [main] INFO reflections.Reflections: Reflections took 380 ms to scan 3 urls, producing 132 keys and 287 values 
scm3.org_1   | 2023-06-30 10:05:17,379 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-06-30 10:05:17,381 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-30 10:05:17,503 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-30 10:05:17,504 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-30 10:05:18,162 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-30 10:05:18,162 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-30 10:05:18,403 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-30 10:05:19,556 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2023-06-30 10:05:21,118 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-30 10:05:21,133 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-30 10:04:44,907 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-30 10:04:44,909 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2023-06-30 10:04:44,911 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2023-06-30 10:04:45,625 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm2.org_1   | 2023-06-30 10:04:45,670 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2023-06-30 10:04:45,670 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2023-06-30 10:04:45,674 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:dc4bcfae-13a7-46ed-9c3e-1369a026857d,clusterId:CID-06880496-7012-4073-8c29-45a6b430095b,subject:scm-sub@scm2.org
scm2.org_1   | 2023-06-30 10:04:46,872 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2023-06-30 10:04:46,895 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-06880496-7012-4073-8c29-45a6b430095b, SCMID dc4bcfae-13a7-46ed-9c3e-1369a026857d
scm2.org_1   | 2023-06-30 10:04:46,895 [main] INFO server.StorageContainerManager: Primary SCM Node ID 42abf28c-5926-4498-87c1-8b8ba8a02c3f
scm2.org_1   | 2023-06-30 10:04:46,924 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-30 10:04:52,382 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
om3_1        | 2023-06-30 10:08:04,810 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
scm1.org_1   | 2023-06-30 10:04:23,290 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b does not exist. Creating ...
scm1.org_1   | 2023-06-30 10:04:23,297 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/in_use.lock acquired by nodename 13@scm1.org
scm1.org_1   | 2023-06-30 10:04:23,304 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b has been successfully formatted.
om3_1        | 2023-06-30 10:08:04,814 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2023-06-30 10:08:04,815 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2023-06-30 10:08:04,816 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2023-06-30 10:08:04,817 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2023-06-30 10:08:04,826 [main] INFO server.RaftServer: om3: start RPC server
om3_1        | 2023-06-30 10:08:05,668 [main] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2023-06-30 10:08:05,689 [main] INFO om.OzoneManager: Starting secret key client.
om3_1        | 2023-06-30 10:08:05,714 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2023-06-30 10:08:06,533 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z).
om3_1        | 2023-06-30 10:08:06,557 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M36.970665S and interval of PT1M
om3_1        | 2023-06-30 10:08:06,586 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2023-06-30 10:08:06,590 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 393626542201
om3_1        | 2023-06-30 10:08:06,645 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2023-06-30 10:08:06,649 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
recon_1      | 2023-06-30 10:06:53,234 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 121 failover attempts. Trying to failover immediately. Current retry count: 121.
recon_1      | 2023-06-30 10:06:53,234 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 122 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 122.
recon_1      | 2023-06-30 10:06:55,236 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 123 failover attempts. Trying to failover immediately. Current retry count: 123.
recon_1      | 2023-06-30 10:06:55,238 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 124 failover attempts. Trying to failover immediately. Current retry count: 124.
recon_1      | 2023-06-30 10:06:55,239 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 125 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 125.
recon_1      | 2023-06-30 10:06:57,242 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 126 failover attempts. Trying to failover immediately. Current retry count: 126.
recon_1      | 2023-06-30 10:06:57,247 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 127 failover attempts. Trying to failover immediately. Current retry count: 127.
recon_1      | 2023-06-30 10:06:57,257 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 128 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 128.
recon_1      | 2023-06-30 10:06:59,273 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 129 failover attempts. Trying to failover immediately. Current retry count: 129.
recon_1      | 2023-06-30 10:06:59,274 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 130 failover attempts. Trying to failover immediately. Current retry count: 130.
recon_1      | 2023-06-30 10:06:59,286 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 131 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 131.
recon_1      | 2023-06-30 10:07:01,287 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 132 failover attempts. Trying to failover immediately. Current retry count: 132.
recon_1      | 2023-06-30 10:07:01,288 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 133 failover attempts. Trying to failover immediately. Current retry count: 133.
recon_1      | 2023-06-30 10:07:01,292 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 134 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 134.
recon_1      | 2023-06-30 10:07:03,293 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 135 failover attempts. Trying to failover immediately. Current retry count: 135.
recon_1      | 2023-06-30 10:07:03,295 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 136 failover attempts. Trying to failover immediately. Current retry count: 136.
recon_1      | 2023-06-30 10:07:03,296 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 137 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 137.
recon_1      | 2023-06-30 10:07:05,298 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 138 failover attempts. Trying to failover immediately. Current retry count: 138.
recon_1      | 2023-06-30 10:07:05,300 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 139 failover attempts. Trying to failover immediately. Current retry count: 139.
recon_1      | 2023-06-30 10:07:05,301 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 140 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 140.
recon_1      | 2023-06-30 10:07:07,303 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 141 failover attempts. Trying to failover immediately. Current retry count: 141.
recon_1      | 2023-06-30 10:07:07,304 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 142 failover attempts. Trying to failover immediately. Current retry count: 142.
recon_1      | 2023-06-30 10:07:07,305 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 143 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 143.
recon_1      | 2023-06-30 10:07:09,310 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 144 failover attempts. Trying to failover immediately. Current retry count: 144.
recon_1      | 2023-06-30 10:07:09,318 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 145 failover attempts. Trying to failover immediately. Current retry count: 145.
recon_1      | 2023-06-30 10:07:09,319 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 146 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 146.
recon_1      | 2023-06-30 10:07:11,327 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 147 failover attempts. Trying to failover immediately. Current retry count: 147.
recon_1      | 2023-06-30 10:07:11,330 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 148 failover attempts. Trying to failover immediately. Current retry count: 148.
recon_1      | 2023-06-30 10:07:11,338 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 149 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 149.
recon_1      | 2023-06-30 10:07:13,266 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:59292 / 172.25.0.102:59292
recon_1      | 2023-06-30 10:07:13,305 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 10:07:13,339 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 150 failover attempts. Trying to failover immediately. Current retry count: 150.
recon_1      | 2023-06-30 10:07:13,340 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 151 failover attempts. Trying to failover immediately. Current retry count: 151.
recon_1      | 2023-06-30 10:07:13,341 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 152 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 152.
recon_1      | 2023-06-30 10:07:15,374 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 153 failover attempts. Trying to failover immediately. Current retry count: 153.
recon_1      | 2023-06-30 10:07:15,445 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 154 failover attempts. Trying to failover immediately. Current retry count: 154.
recon_1      | 2023-06-30 10:07:15,527 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 155 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 155.
recon_1      | 2023-06-30 10:07:16,479 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:34500 / 172.25.0.103:34500
recon_1      | 2023-06-30 10:07:16,617 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 10:07:17,364 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:35632 / 172.25.0.104:35632
recon_1      | 2023-06-30 10:07:17,556 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 10:07:17,589 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 156 failover attempts. Trying to failover immediately. Current retry count: 156.
recon_1      | 2023-06-30 10:07:17,606 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 157 failover attempts. Trying to failover immediately. Current retry count: 157.
recon_1      | 2023-06-30 10:07:17,617 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 158 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 158.
recon_1      | 2023-06-30 10:07:18,145 [IPC Server handler 6 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/90f061fe-584f-4bf2-a5ce-ab438b7e17a1
recon_1      | 2023-06-30 10:07:18,261 [IPC Server handler 6 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 90f061fe-584f-4bf2-a5ce-ab438b7e17a1{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886], networkLocation: /default-rack, certSerialId: 382242999512, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-30 10:07:18,943 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8585dc18-efaa-4bfa-93ea-e78f13194d9c
recon_1      | 2023-06-30 10:07:18,974 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 8585dc18-efaa-4bfa-93ea-e78f13194d9c{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886], networkLocation: /default-rack, certSerialId: 387609916440, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-30 10:07:19,625 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 159 failover attempts. Trying to failover immediately. Current retry count: 159.
recon_1      | 2023-06-30 10:07:19,626 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 160 failover attempts. Trying to failover immediately. Current retry count: 160.
recon_1      | 2023-06-30 10:07:19,627 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 161 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 161.
recon_1      | 2023-06-30 10:07:20,126 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 90f061fe-584f-4bf2-a5ce-ab438b7e17a1 to Node DB.
recon_1      | 2023-06-30 10:07:20,138 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 8585dc18-efaa-4bfa-93ea-e78f13194d9c to Node DB.
recon_1      | 2023-06-30 10:07:20,157 [IPC Server handler 4 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/45c04ec6-f646-4507-992b-4f7a81b89828
recon_1      | 2023-06-30 10:07:20,157 [IPC Server handler 4 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 45c04ec6-f646-4507-992b-4f7a81b89828{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 388756064589, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-30 10:07:20,174 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 45c04ec6-f646-4507-992b-4f7a81b89828 to Node DB.
recon_1      | 2023-06-30 10:07:21,634 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 162 failover attempts. Trying to failover immediately. Current retry count: 162.
recon_1      | 2023-06-30 10:07:21,636 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 163 failover attempts. Trying to failover immediately. Current retry count: 163.
recon_1      | 2023-06-30 10:07:21,636 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 164 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 164.
recon_1      | 2023-06-30 10:07:23,638 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 165 failover attempts. Trying to failover immediately. Current retry count: 165.
recon_1      | 2023-06-30 10:07:23,639 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 166 failover attempts. Trying to failover immediately. Current retry count: 166.
recon_1      | 2023-06-30 10:07:23,640 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 167 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 167.
recon_1      | 2023-06-30 10:07:25,641 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 168 failover attempts. Trying to failover immediately. Current retry count: 168.
recon_1      | 2023-06-30 10:07:25,642 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 169 failover attempts. Trying to failover immediately. Current retry count: 169.
recon_1      | 2023-06-30 10:07:25,643 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 170 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 170.
recon_1      | 2023-06-30 10:07:27,645 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 171 failover attempts. Trying to failover immediately. Current retry count: 171.
recon_1      | 2023-06-30 10:07:27,647 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 172 failover attempts. Trying to failover immediately. Current retry count: 172.
recon_1      | 2023-06-30 10:07:27,649 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 173 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 173.
recon_1      | 2023-06-30 10:07:29,651 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 174 failover attempts. Trying to failover immediately. Current retry count: 174.
recon_1      | 2023-06-30 10:07:29,654 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 175 failover attempts. Trying to failover immediately. Current retry count: 175.
recon_1      | 2023-06-30 10:07:29,659 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 176 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 176.
recon_1      | 2023-06-30 10:07:31,661 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 177 failover attempts. Trying to failover immediately. Current retry count: 177.
recon_1      | 2023-06-30 10:07:31,663 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 178 failover attempts. Trying to failover immediately. Current retry count: 178.
recon_1      | 2023-06-30 10:07:31,664 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 179 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 179.
recon_1      | 2023-06-30 10:07:33,667 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 180 failover attempts. Trying to failover immediately. Current retry count: 180.
recon_1      | 2023-06-30 10:07:33,670 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 181 failover attempts. Trying to failover immediately. Current retry count: 181.
recon_1      | 2023-06-30 10:07:33,671 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 182 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 182.
recon_1      | 2023-06-30 10:07:35,672 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 183 failover attempts. Trying to failover immediately. Current retry count: 183.
recon_1      | 2023-06-30 10:07:35,675 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 184 failover attempts. Trying to failover immediately. Current retry count: 184.
recon_1      | 2023-06-30 10:07:35,676 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 185 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 185.
recon_1      | 2023-06-30 10:07:37,678 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 186 failover attempts. Trying to failover immediately. Current retry count: 186.
recon_1      | 2023-06-30 10:07:37,679 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 187 failover attempts. Trying to failover immediately. Current retry count: 187.
recon_1      | 2023-06-30 10:07:37,680 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 188 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 188.
recon_1      | 2023-06-30 10:07:39,682 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 189 failover attempts. Trying to failover immediately. Current retry count: 189.
recon_1      | 2023-06-30 10:07:39,683 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 190 failover attempts. Trying to failover immediately. Current retry count: 190.
recon_1      | 2023-06-30 10:07:39,689 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 191 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 191.
recon_1      | 2023-06-30 10:07:41,691 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 192 failover attempts. Trying to failover immediately. Current retry count: 192.
recon_1      | 2023-06-30 10:07:41,695 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 193 failover attempts. Trying to failover immediately. Current retry count: 193.
recon_1      | 2023-06-30 10:07:41,704 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 194 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 194.
recon_1      | 2023-06-30 10:07:43,708 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 195 failover attempts. Trying to failover immediately. Current retry count: 195.
recon_1      | 2023-06-30 10:07:43,709 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 196 failover attempts. Trying to failover immediately. Current retry count: 196.
recon_1      | 2023-06-30 10:07:43,711 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 197 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 197.
recon_1      | 2023-06-30 10:07:45,714 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 198 failover attempts. Trying to failover immediately. Current retry count: 198.
recon_1      | 2023-06-30 10:07:45,715 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 199 failover attempts. Trying to failover immediately. Current retry count: 199.
scm1.org_1   | 2023-06-30 10:04:23,308 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-30 10:04:23,316 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-30 10:04:23,317 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:23,318 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-30 10:04:23,319 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-30 10:04:23,321 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 10:04:23,327 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-30 10:04:23,328 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-30 10:04:23,329 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:23,334 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   | 2023-06-30 10:04:23,335 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 10:04:23,335 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 10:04:23,336 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 10:04:23,340 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-30 10:04:23,340 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-30 10:04:23,341 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-30 10:04:23,341 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-30 10:04:23,343 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-30 10:04:23,354 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-30 10:04:23,354 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:23,378 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 10:04:23,378 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 10:04:23,379 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-06-30 10:04:23,391 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-30 10:04:23,391 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-30 10:04:23,396 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: start as a follower, conf=-1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:23,397 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2023-06-30 10:04:23,398 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState
scm1.org_1   | 2023-06-30 10:04:23,404 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-30 10:04:23,405 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-30 10:04:23,408 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-45A6B430095B,id=42abf28c-5926-4498-87c1-8b8ba8a02c3f
scm1.org_1   | 2023-06-30 10:04:23,410 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-30 10:04:23,413 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-30 10:04:23,413 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-30 10:04:23,414 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-30 10:04:23,423 [main] INFO server.RaftServer: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start RPC server
scm1.org_1   | 2023-06-30 10:04:23,471 [main] INFO server.GrpcService: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-30 10:04:23,475 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-42abf28c-5926-4498-87c1-8b8ba8a02c3f: Started
scm1.org_1   | 2023-06-30 10:04:28,519 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO impl.FollowerState: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5121385242ns, electionTimeout:5113ms
scm1.org_1   | 2023-06-30 10:04:28,520 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: shutdown 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState
scm1.org_1   | 2023-06-30 10:04:28,521 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2023-06-30 10:04:28,524 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-30 10:04:28,524 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1
scm1.org_1   | 2023-06-30 10:04:28,531 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:28,532 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm1.org_1   | 2023-06-30 10:04:28,539 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:28,540 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-30 10:04:28,540 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: shutdown 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1
scm1.org_1   | 2023-06-30 10:04:28,540 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-30 10:05:21,140 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm3.org_1   | 2023-06-30 10:05:21,143 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm3.org_1   | 2023-06-30 10:05:23,560 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2023-06-30 10:05:23,684 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2023-06-30 10:05:23,684 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2023-06-30 10:05:23,688 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:0e7462e2-d318-4860-bc91-2d8e1c77938e,clusterId:CID-06880496-7012-4073-8c29-45a6b430095b,subject:scm-sub@scm3.org
scm3.org_1   | 2023-06-30 10:05:24,313 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2023-06-30 10:05:24,355 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-06880496-7012-4073-8c29-45a6b430095b, SCMID 0e7462e2-d318-4860-bc91-2d8e1c77938e
scm3.org_1   | 2023-06-30 10:05:24,355 [main] INFO server.StorageContainerManager: Primary SCM Node ID 42abf28c-5926-4498-87c1-8b8ba8a02c3f
scm3.org_1   | 2023-06-30 10:05:24,409 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-30 10:05:28,227 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-30 10:05:28,238 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-06-30 10:04:28,541 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: change Leader from null to 42abf28c-5926-4498-87c1-8b8ba8a02c3f at term 1 for becomeLeader, leader elected after 5396ms
scm1.org_1   | 2023-06-30 10:04:28,550 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-30 10:04:28,554 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 10:04:28,555 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 10:04:28,560 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-30 10:04:28,560 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-30 10:04:28,561 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-30 10:04:28,578 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 10:04:28,584 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-30 10:04:28,586 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl
scm1.org_1   | 2023-06-30 10:04:28,622 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2023-06-30 10:04:28,728 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:28,756 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_0
scm1.org_1   | 2023-06-30 10:04:29,476 [main] INFO server.RaftServer: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: close
scm1.org_1   | 2023-06-30 10:04:29,477 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: shutdown
scm1.org_1   | 2023-06-30 10:04:29,477 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-45A6B430095B,id=42abf28c-5926-4498-87c1-8b8ba8a02c3f
scm1.org_1   | 2023-06-30 10:04:29,477 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: shutdown 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl
scm1.org_1   | 2023-06-30 10:04:29,484 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO impl.PendingRequests: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2023-06-30 10:04:29,485 [main] INFO server.GrpcService: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: shutdown server GrpcServerProtocolService now
scm1.org_1   | 2023-06-30 10:04:29,499 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO impl.StateMachineUpdater: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2023-06-30 10:04:29,499 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO impl.StateMachineUpdater: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2023-06-30 10:04:29,504 [main] INFO server.GrpcService: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: shutdown server GrpcServerProtocolService successfully
scm1.org_1   | 2023-06-30 10:04:29,504 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO impl.StateMachineUpdater: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2023-06-30 10:04:29,504 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: closes. applyIndex: 0
scm1.org_1   | 2023-06-30 10:04:29,762 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker close()
scm1.org_1   | 2023-06-30 10:04:29,765 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-42abf28c-5926-4498-87c1-8b8ba8a02c3f: Stopped
scm1.org_1   | 2023-06-30 10:04:29,766 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 10:04:29,769 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-06880496-7012-4073-8c29-45a6b430095b; layoutVersion=7; scmId=42abf28c-5926-4498-87c1-8b8ba8a02c3f
scm1.org_1   | 2023-06-30 10:04:29,796 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-30 10:04:32,055 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-30 10:04:32,069 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-06-30 10:04:32,150 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 10:04:32,379 [main] INFO reflections.Reflections: Reflections took 183 ms to scan 3 urls, producing 132 keys and 287 values 
scm1.org_1   | 2023-06-30 10:04:32,514 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-30 10:04:32,528 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-30 10:04:32,558 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-30 10:04:32,565 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-30 10:04:32,768 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2023-06-30 10:04:32,769 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2023-06-30 10:04:33,674 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 253598402501
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   |            Start Date: Fri Jun 30 10:04:21 UTC 2023
scm1.org_1   |            Final Date: Mon Aug 07 10:04:21 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
scm1.org_1   |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
scm1.org_1   |                        43e78ba4990cf8693d7e311e758874532e0e4770
scm1.org_1   |                        6e386edb08789430a89a39ebde59b89ba6e84e94
scm1.org_1   |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
scm1.org_1   |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
scm1.org_1   |                        73af7af9023c681485766dec4627568ceadcae31
scm1.org_1   |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
scm1.org_1   |                        988b846e620e04205f341894328c4379e192c0f0
scm1.org_1   |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
scm1.org_1   |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
scm1.org_1   |                        d924b613b78a9538512bbc922e5120bb84930f4c
scm1.org_1   |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
scm1.org_1   |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/253598402501.crt.
scm1.org_1   | 2023-06-30 10:04:33,683 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 1
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   |            Start Date: Fri Jun 30 10:04:21 UTC 2023
scm1.org_1   |            Final Date: Mon Aug 07 10:04:21 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
scm1.org_1   |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
scm1.org_1   |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
scm1.org_1   |                        dead23e7b54024421f408e091345739cf1861cb6
scm1.org_1   |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
scm1.org_1   |                        6385912587b5531778fa6f5c476842a25dc986ad
scm1.org_1   |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
scm1.org_1   |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
scm1.org_1   |                        734cec37929dd6f294085e1181a91f1d47304865
scm1.org_1   |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-30 10:04:52,403 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-30 10:04:52,618 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 10:04:53,192 [main] INFO reflections.Reflections: Reflections took 425 ms to scan 3 urls, producing 132 keys and 287 values 
scm2.org_1   | 2023-06-30 10:04:53,553 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-30 10:04:53,592 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-30 10:04:53,681 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-30 10:04:53,686 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-30 10:04:54,236 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-30 10:04:54,245 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-30 10:04:56,875 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 1
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm2.org_1   |            Start Date: Fri Jun 30 10:04:21 UTC 2023
scm2.org_1   |            Final Date: Mon Aug 07 10:04:21 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm2.org_1   |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
scm2.org_1   |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
scm2.org_1   |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
scm2.org_1   |                        dead23e7b54024421f408e091345739cf1861cb6
scm2.org_1   |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
scm2.org_1   |                        6385912587b5531778fa6f5c476842a25dc986ad
scm2.org_1   |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
scm2.org_1   |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
scm2.org_1   |                        734cec37929dd6f294085e1181a91f1d47304865
scm2.org_1   |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
scm2.org_1   |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
scm2.org_1   |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
scm2.org_1   |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
scm2.org_1   |                        0e32f359ccbefded3427e57b80709364
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0x6
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2023-06-30 10:04:56,894 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 277817119890
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm2.org_1   |            Start Date: Fri Jun 30 10:04:45 UTC 2023
scm2.org_1   |            Final Date: Mon Aug 07 10:04:45 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=dc4bcfae-13a7-46ed-9c3e-1369a026857d,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm2.org_1   |            Public Key: RSA Public Key [8e:eb:f2:46:26:06:16:49:fd:88:fc:37:8f:84:d3:0a:64:81:b3:bf],[56:66:d1:a4]
scm2.org_1   |         modulus: 97b0a8423b4810f16e08fad7b2dd99ea808bffa472b21569a8c75f9810470e1009ae8ffc973ed626a4ec4674718d3c0752bfeb53ddcfebb5c64be7cbff491f27167cefc82bef2dbc1f74d598a98b48d4cbda37337c9f4ab5cacdf1871c0b0235eb69ce1f198865bf0041b22009c1135c7ffdbfb7d52cbc3590b2baed507f623f32cc726e9218a733578ca0d3565ff3cadf1f0d6e4012f58347d68d6a04f7631825344bdd9759cbb0f6dc0f3e73dc7e77a0b78c58f58ee8cc7926a4c097bcdd67c33f4cb55fe6b6372ca00ed8e16720487e977667bed402b64b95cef47c65eb319b408778355020d977981d761d7889658d83ab1f704f4e3e6ad97e0c1da4b17b
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: b5be244a1bf76224512c9dcb52e509736d7809bc
scm2.org_1   |                        b8f3318f55ea0ba1f46d431eb66ac55404080450
scm2.org_1   |                        4c1c97ec89309f2388c601fb20cf48abf30638f6
scm2.org_1   |                        a6c27476f156f170dd8af5cde0a9ba89f2bf7da9
scm2.org_1   |                        f203697b948582c714ae4bdfb5f367ca0c32291b
scm2.org_1   |                        ab3a97c74d30e6378bc8aa5f04cc8192af2befc2
scm2.org_1   |                        30917cc83df8db18d0dc8ec2db4400c348e4bc02
scm2.org_1   |                        6ed40312f0e2f00d36e69f46268b081f0c19eeaf
scm2.org_1   |                        f2676fd59a27e13cdb96d1c09d07ff2f06a9d8ee
scm2.org_1   |                        4290ddd79da3384e11b23308bbfa6831c04df506
scm2.org_1   |                        179a6185e69c28f65c9b7e1345eafb1394698dec
scm2.org_1   |                        754d2d3a7a7e3bde6ee503f6e0b3c6771e371a76
scm2.org_1   |                        7e820446d199f7272f5bb60ae4636ac3
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/277817119890.crt.
scm2.org_1   | 2023-06-30 10:04:56,917 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 277817119890
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm2.org_1   |            Start Date: Fri Jun 30 10:04:45 UTC 2023
scm2.org_1   |            Final Date: Mon Aug 07 10:04:45 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=dc4bcfae-13a7-46ed-9c3e-1369a026857d,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm2.org_1   |            Public Key: RSA Public Key [8e:eb:f2:46:26:06:16:49:fd:88:fc:37:8f:84:d3:0a:64:81:b3:bf],[56:66:d1:a4]
scm3.org_1   | 2023-06-30 10:05:28,330 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 10:05:28,633 [main] INFO reflections.Reflections: Reflections took 238 ms to scan 3 urls, producing 132 keys and 287 values 
scm3.org_1   | 2023-06-30 10:05:28,841 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-06-30 10:05:28,866 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-30 10:05:28,934 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-30 10:05:28,935 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-30 10:05:29,325 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-30 10:05:29,325 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-30 10:05:31,521 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 315832229995
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm3.org_1   |            Start Date: Fri Jun 30 10:05:23 UTC 2023
scm3.org_1   |            Final Date: Mon Aug 07 10:05:23 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=0e7462e2-d318-4860-bc91-2d8e1c77938e,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm3.org_1   |            Public Key: RSA Public Key [d3:e6:13:fa:1b:95:a7:70:77:fc:31:7f:75:fc:50:64:a0:6b:dd:7e],[56:66:d1:a4]
scm3.org_1   |         modulus: a4cb668e77b355ad9004f4283aece3de3d3cb76c294a0cf9ab5dd6d5fbb8a54eacf889fad7b87d1f9e3fbcc9d8e3ba6470697d679e08fe11bbd8916ca1acaa485f50e2fe08d89432666e6aa62dd84fa6e2f95264279e7ff512068680aa6e6bd250355f1a4f1005f3258dc332944ecfbeecce9c9dc313c0915b836cfb9eec58c5545491c1816f9a5c746cc57674189a9b929487bb0159d20fbe8eb573c901c8f362a412e7a7a8d0fd2a241b8e30becd416c14e1dff321d14be282c348ae1c872dda80f3f15c28a9cfb26fac7475211652704164898cd52b80f3d218b4b9ddec97e2983bc6b914bb4c3f33396d261dfa5cba387d4dfaafc9282d1f2923dfef3f61
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: 59b885cfaf51c988c45abf0fc84dfb87e192c6fb
scm3.org_1   |                        2c9ec3e76552c71377e9321a2da19b727f9b79b9
scm3.org_1   |                        80233a4df6ef8233cc7745bcd9e009ac3b220cf7
scm3.org_1   |                        20707ecfe9c2dcb2718fb2e38f527920379c6a86
scm3.org_1   |                        3ee47fdd451675c4fc7ce4dbe24b82d972808633
scm3.org_1   |                        f30b52a37fed9d14bdcd249248483e292cc5d0e6
scm3.org_1   |                        d64f6eecc7712648971e9d0e253b7e49e2551c2b
scm3.org_1   |                        3dfb0a4c4cd4a54f12bc351d1f9fa3cc1e859208
scm3.org_1   |                        572a4901953c6bbe5f02281ef518d80c74280b60
scm3.org_1   |                        2afa62d7f7f5fe790f0a5eccabcea0ec1660383d
scm3.org_1   |                        139fb1eead4a5f2fc5021ddbafac8d3e43efea2e
scm3.org_1   |                        eb4f79bef44065d2e98a74dae25af6675d32c992
scm3.org_1   |                        03f39f366e840663628c66579ff21028
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/315832229995.crt.
scm3.org_1   | 2023-06-30 10:05:31,532 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 1
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm3.org_1   |            Start Date: Fri Jun 30 10:04:21 UTC 2023
scm3.org_1   |            Final Date: Mon Aug 07 10:04:21 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm3.org_1   |            Public Key: RSA Public Key [8c:18:f8:fc:21:84:78:e9:7c:bc:64:91:dd:5a:13:a8:d2:1e:73:70],[56:66:d1:a4]
scm3.org_1   |         modulus: bda2caadb7c1a6789b3b0fc346866d49bb75b1427393772330478fe855c23a0f124d7094152176317d5008a720eb8aed402d694c8abbbba8fcb2fc912965ef03e1aa77247dd883dad7afb21836060a91b52cba75f5a1a8d94cd991c10aaa27efaba9409e4990e8b5050af14685a82143b1a1c9fbb3ecfa6253f1452f0334c673632a7a4a22c139fa0f60b3e8291991bd4be75a60fbfb0db36a7efd87dcb85597cf29f24330d9506b0c20578d7078c26fe79edbf19b0428b66669e0f02c3044596d4d6a83547f29ec982400b65ca37d7e4e34188fa6983cdfbb5fee694c98b8e060b1330caba9efab4357f3fc386c6fd0e62b7e1f629e3430d1aa3d97b74d3e41
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: 0ba954e9035b0232a420d98ab454ae27faa3c64e
scm3.org_1   |                        061e6754da1487a29ddda6adaf95d0e3dc76a9d6
scm3.org_1   |                        dead23e7b54024421f408e091345739cf1861cb6
scm3.org_1   |                        a8b3ab7ae0d58d7208892f5173ddf34cbc6688b4
scm3.org_1   |                        6385912587b5531778fa6f5c476842a25dc986ad
scm3.org_1   |                        d51d47d153264041e6a3eb5c17b5b4ab7181f186
scm3.org_1   |                        a1e08d798c659c9a1c686010ef8b8494ddce2412
scm3.org_1   |                        734cec37929dd6f294085e1181a91f1d47304865
scm3.org_1   |                        8c542e6927303a5dbcac4c1b460b6f7b5d13d16a
scm3.org_1   |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
scm3.org_1   |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
scm3.org_1   |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
scm3.org_1   |                        0e32f359ccbefded3427e57b80709364
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0x6
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2023-06-30 10:05:31,545 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 315832229995
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
recon_1      | 2023-06-30 10:07:45,716 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 200 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 200.
recon_1      | 2023-06-30 10:07:47,721 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 201 failover attempts. Trying to failover immediately. Current retry count: 201.
recon_1      | 2023-06-30 10:07:47,724 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 202 failover attempts. Trying to failover immediately. Current retry count: 202.
recon_1      | 2023-06-30 10:07:47,726 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 203 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 203.
recon_1      | 2023-06-30 10:07:51,064 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:49236 / 172.25.0.102:49236
recon_1      | 2023-06-30 10:07:51,208 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 10:07:51,968 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:35708 / 172.25.0.103:35708
recon_1      | 2023-06-30 10:07:52,044 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 10:07:53,098 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:34654 / 172.25.0.104:34654
recon_1      | 2023-06-30 10:07:53,287 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 10:07:54,010 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=7ad03ed4-4cc4-4086-b128-5b762bd02996. Trying to get from SCM.
recon_1      | 2023-06-30 10:07:54,622 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 7ad03ed4-4cc4-4086-b128-5b762bd02996, Nodes: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:90f061fe-584f-4bf2-a5ce-ab438b7e17a1, CreationTimestamp2023-06-30T10:07:19.582Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 10:07:56,014 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=459a01b6-1a94-49da-be94-47395dca916d. Trying to get from SCM.
recon_1      | 2023-06-30 10:07:56,031 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 459a01b6-1a94-49da-be94-47395dca916d, Nodes: 8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:07:20.472Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 10:07:56,035 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=459a01b6-1a94-49da-be94-47395dca916d reported by 8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-30 10:07:56,230 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4. Trying to get from SCM.
recon_1      | 2023-06-30 10:07:56,291 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5f2976d1-d881-4d67-95db-dd88234141a4, Nodes: 8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:07:21.240Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 10:07:56,302 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 reported by 90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-30 10:07:56,438 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=35f8130a-9766-41b0-b833-8d68e5c9eeae. Trying to get from SCM.
recon_1      | 2023-06-30 10:07:56,448 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 35f8130a-9766-41b0-b833-8d68e5c9eeae, Nodes: 45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:45c04ec6-f646-4507-992b-4f7a81b89828, CreationTimestamp2023-06-30T10:07:20.974Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 10:07:56,451 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=35f8130a-9766-41b0-b833-8d68e5c9eeae reported by 45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-30 10:07:57,292 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 reported by 8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-30 10:07:58,033 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 reported by 45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-30 10:08:00,326 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 reported by 90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-30 10:08:02,350 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 reported by 8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-30 10:08:02,520 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 reported by 45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
scm1.org_1   |                        f62f1af2f1a8fae20b2c1cb59ad78e3cebc0f838
scm1.org_1   |                        fbd3473a5d90ff2fc8d2b041d3d4b3e2f860ffab
scm1.org_1   |                        ab82c9756ba4c4933e8d9188d39b2d0f49ad65ce
scm1.org_1   |                        0e32f359ccbefded3427e57b80709364
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0x6
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2023-06-30 10:04:33,689 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 253598402501
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   |            Start Date: Fri Jun 30 10:04:21 UTC 2023
scm1.org_1   |            Final Date: Mon Aug 07 10:04:21 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=42abf28c-5926-4498-87c1-8b8ba8a02c3f,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   |            Public Key: RSA Public Key [b8:60:8a:d1:45:c5:62:87:77:5e:77:d4:38:39:7e:10:9d:99:3d:f9],[56:66:d1:a4]
scm1.org_1   |         modulus: ad178cf1cf7bf10e31270af8a24e5f5646af35cb199f27430f2515cac5615b148cb66d3c227d1ce280b473195fe6c4afd6ac8e33fc0ec9ba36526594db8de76d4568d597dcf184d0b5b33cb2a4aa5a85732a6bde2e108ddc3e0630eb7e2fb5d2f304db0c543ced8b9f42c861f68bfae511e926de26a84e2c4975b04adbcb52bc9ddad5f06e697c632030a15f4dffb4cf6efb4861d1b71143c78cadc9a8dcaaaacfe391fce8a2e6ea64d02d6b7774a76536b5999aad9887e99604586f2cbbce0d0c505bd68789b2aee642c6f4da24b0e06f16bc279737c09195617e756789cd3fcbd025365a9ff232daf8f887a17ddc71de01a392cb4077874b49a8e86f2815f1
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: b108edabc3bdbb8b0880b3ac1aa22226792d0ef4
scm1.org_1   |                        43e78ba4990cf8693d7e311e758874532e0e4770
scm1.org_1   |                        6e386edb08789430a89a39ebde59b89ba6e84e94
scm1.org_1   |                        82772ab785fa7db7cd5da6c648f8e80fc541c678
scm1.org_1   |                        a19c575c481ef3eb7f69f049a10384c8a8683b75
scm1.org_1   |                        73af7af9023c681485766dec4627568ceadcae31
scm1.org_1   |                        0e52db1bd4cb6b91d478d0aa35d420d120a0da86
scm1.org_1   |                        988b846e620e04205f341894328c4379e192c0f0
scm1.org_1   |                        05a270792a15afe82cdd3f4e6fa4d59f9efea27a
scm1.org_1   |                        7c681483cbdefcd6f0f12dd80f9762a1b0ff932f
scm1.org_1   |                        d924b613b78a9538512bbc922e5120bb84930f4c
scm1.org_1   |                        59246d9b1234e50822ff2ed3ecabf3c70a9739e1
scm1.org_1   |                        a3067b7440b2b3c1c9c6b2ffb4aefc66
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2023-06-30 10:04:33,694 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716787309 ms and interval 86400000 ms.
scm1.org_1   | 2023-06-30 10:04:33,780 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 10:04:33,943 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 10:04:34,232 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2023-06-30 10:04:34,234 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2023-06-30 10:04:34,347 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-30 10:04:34,600 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:42abf28c-5926-4498-87c1-8b8ba8a02c3f
scm1.org_1   | 2023-06-30 10:04:34,665 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 10:04:34,668 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 10:04:34,748 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-30 10:04:34,756 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 10:04:34,762 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 10:04:34,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 10:04:34,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 10:04:34,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-06-30 10:04:34,764 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-30 10:04:34,765 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-06-30 10:04:34,767 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:34,768 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-06-30 10:04:34,769 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 10:04:34,781 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 10:04:34,792 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:34,797 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-30 10:04:35,213 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-30 10:04:35,217 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:35,223 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-30 10:04:35,223 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 10:04:35,223 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 10:04:35,227 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 10:04:35,233 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: found a subdirectory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   | 2023-06-30 10:04:35,241 [main] INFO server.RaftServer: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: addNew group-45A6B430095B:[] returns group-45A6B430095B:java.util.concurrent.CompletableFuture@1bbef3c7[Not completed]
scm1.org_1   | 2023-06-30 10:04:35,284 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: new RaftServerImpl for group-45A6B430095B:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-30 10:04:35,286 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-30 10:04:35,286 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-30 10:04:35,287 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-30 10:04:35,287 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 10:04:35,287 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 10:04:35,288 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-30 10:04:35,309 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-30 10:04:35,312 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 10:04:35,351 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   |            Start Date: Fri Jun 30 10:05:23 UTC 2023
scm3.org_1   |            Final Date: Mon Aug 07 10:05:23 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=0e7462e2-d318-4860-bc91-2d8e1c77938e,O=CID-06880496-7012-4073-8c29-45a6b430095b
scm3.org_1   |            Public Key: RSA Public Key [d3:e6:13:fa:1b:95:a7:70:77:fc:31:7f:75:fc:50:64:a0:6b:dd:7e],[56:66:d1:a4]
scm3.org_1   |         modulus: a4cb668e77b355ad9004f4283aece3de3d3cb76c294a0cf9ab5dd6d5fbb8a54eacf889fad7b87d1f9e3fbcc9d8e3ba6470697d679e08fe11bbd8916ca1acaa485f50e2fe08d89432666e6aa62dd84fa6e2f95264279e7ff512068680aa6e6bd250355f1a4f1005f3258dc332944ecfbeecce9c9dc313c0915b836cfb9eec58c5545491c1816f9a5c746cc57674189a9b929487bb0159d20fbe8eb573c901c8f362a412e7a7a8d0fd2a241b8e30becd416c14e1dff321d14be282c348ae1c872dda80f3f15c28a9cfb26fac7475211652704164898cd52b80f3d218b4b9ddec97e2983bc6b914bb4c3f33396d261dfa5cba387d4dfaafc9282d1f2923dfef3f61
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: 59b885cfaf51c988c45abf0fc84dfb87e192c6fb
scm3.org_1   |                        2c9ec3e76552c71377e9321a2da19b727f9b79b9
scm3.org_1   |                        80233a4df6ef8233cc7745bcd9e009ac3b220cf7
scm3.org_1   |                        20707ecfe9c2dcb2718fb2e38f527920379c6a86
scm3.org_1   |                        3ee47fdd451675c4fc7ce4dbe24b82d972808633
scm3.org_1   |                        f30b52a37fed9d14bdcd249248483e292cc5d0e6
scm3.org_1   |                        d64f6eecc7712648971e9d0e253b7e49e2551c2b
scm3.org_1   |                        3dfb0a4c4cd4a54f12bc351d1f9fa3cc1e859208
scm3.org_1   |                        572a4901953c6bbe5f02281ef518d80c74280b60
scm3.org_1   |                        2afa62d7f7f5fe790f0a5eccabcea0ec1660383d
scm3.org_1   |                        139fb1eead4a5f2fc5021ddbafac8d3e43efea2e
scm3.org_1   |                        eb4f79bef44065d2e98a74dae25af6675d32c992
scm3.org_1   |                        03f39f366e840663628c66579ff21028
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm3.org_1   | 2023-06-30 10:05:31,551 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716791453 ms and interval 86400000 ms.
scm3.org_1   | 2023-06-30 10:05:31,728 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 10:05:32,184 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 10:05:33,034 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2023-06-30 10:05:33,036 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2023-06-30 10:05:33,254 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2023-06-30 10:05:33,568 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:0e7462e2-d318-4860-bc91-2d8e1c77938e
scm3.org_1   | 2023-06-30 10:05:33,680 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-30 10:05:33,690 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-30 10:05:33,752 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2023-06-30 10:05:33,762 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-30 10:05:33,764 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-30 10:05:33,765 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-30 10:05:33,765 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-30 10:05:33,766 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3.org_1   | 2023-06-30 10:05:33,766 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2023-06-30 10:05:33,767 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2023-06-30 10:05:33,769 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 10:05:33,770 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2023-06-30 10:05:33,771 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-30 10:05:33,811 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3.org_1   | 2023-06-30 10:05:33,822 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3.org_1   | 2023-06-30 10:05:33,824 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-06-30 10:05:34,808 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3.org_1   | 2023-06-30 10:05:34,814 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3.org_1   | 2023-06-30 10:05:34,816 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3.org_1   | 2023-06-30 10:05:34,817 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-30 10:05:34,818 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-30 10:05:34,832 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-30 10:05:34,865 [main] INFO server.RaftServer: 0e7462e2-d318-4860-bc91-2d8e1c77938e: addNew group-45A6B430095B:[] returns group-45A6B430095B:java.util.concurrent.CompletableFuture@129c322f[Not completed]
scm3.org_1   | 2023-06-30 10:05:34,915 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e: new RaftServerImpl for group-45A6B430095B:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2023-06-30 10:05:34,917 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2023-06-30 10:05:34,918 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2023-06-30 10:05:34,918 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2023-06-30 10:05:34,919 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-30 10:05:34,920 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-30 10:05:34,922 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   |         modulus: 97b0a8423b4810f16e08fad7b2dd99ea808bffa472b21569a8c75f9810470e1009ae8ffc973ed626a4ec4674718d3c0752bfeb53ddcfebb5c64be7cbff491f27167cefc82bef2dbc1f74d598a98b48d4cbda37337c9f4ab5cacdf1871c0b0235eb69ce1f198865bf0041b22009c1135c7ffdbfb7d52cbc3590b2baed507f623f32cc726e9218a733578ca0d3565ff3cadf1f0d6e4012f58347d68d6a04f7631825344bdd9759cbb0f6dc0f3e73dc7e77a0b78c58f58ee8cc7926a4c097bcdd67c33f4cb55fe6b6372ca00ed8e16720487e977667bed402b64b95cef47c65eb319b408778355020d977981d761d7889658d83ab1f704f4e3e6ad97e0c1da4b17b
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: b5be244a1bf76224512c9dcb52e509736d7809bc
scm2.org_1   |                        b8f3318f55ea0ba1f46d431eb66ac55404080450
scm2.org_1   |                        4c1c97ec89309f2388c601fb20cf48abf30638f6
scm2.org_1   |                        a6c27476f156f170dd8af5cde0a9ba89f2bf7da9
scm2.org_1   |                        f203697b948582c714ae4bdfb5f367ca0c32291b
scm2.org_1   |                        ab3a97c74d30e6378bc8aa5f04cc8192af2befc2
scm2.org_1   |                        30917cc83df8db18d0dc8ec2db4400c348e4bc02
scm2.org_1   |                        6ed40312f0e2f00d36e69f46268b081f0c19eeaf
scm2.org_1   |                        f2676fd59a27e13cdb96d1c09d07ff2f06a9d8ee
scm2.org_1   |                        4290ddd79da3384e11b23308bbfa6831c04df506
scm2.org_1   |                        179a6185e69c28f65c9b7e1345eafb1394698dec
scm2.org_1   |                        754d2d3a7a7e3bde6ee503f6e0b3c6771e371a76
scm2.org_1   |                        7e820446d199f7272f5bb60ae4636ac3
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2023-06-30 10:04:56,940 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716788078 ms and interval 86400000 ms.
scm2.org_1   | 2023-06-30 10:04:57,196 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 10:04:57,741 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 10:04:58,666 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2023-06-30 10:04:58,673 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2023-06-30 10:04:58,922 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2023-06-30 10:04:59,547 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:dc4bcfae-13a7-46ed-9c3e-1369a026857d
scm2.org_1   | 2023-06-30 10:04:59,658 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-30 10:04:59,667 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-30 10:04:59,798 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2023-06-30 10:04:59,815 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-30 10:04:59,830 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-30 10:04:59,831 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-30 10:04:59,832 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-30 10:04:59,834 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2.org_1   | 2023-06-30 10:04:59,834 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2023-06-30 10:04:59,835 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2023-06-30 10:04:59,837 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 10:04:59,841 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2023-06-30 10:04:59,843 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-30 10:04:59,876 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2.org_1   | 2023-06-30 10:04:59,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2.org_1   | 2023-06-30 10:04:59,894 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-06-30 10:05:01,101 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2.org_1   | 2023-06-30 10:05:01,118 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2.org_1   | 2023-06-30 10:05:01,119 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2.org_1   | 2023-06-30 10:05:01,120 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-30 10:05:01,127 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-30 10:05:01,138 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-30 10:05:01,173 [main] INFO server.RaftServer: dc4bcfae-13a7-46ed-9c3e-1369a026857d: addNew group-45A6B430095B:[] returns group-45A6B430095B:java.util.concurrent.CompletableFuture@129c322f[Not completed]
scm2.org_1   | 2023-06-30 10:05:01,230 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d: new RaftServerImpl for group-45A6B430095B:[] with SCMStateMachine:uninitialized
scm2.org_1   | 2023-06-30 10:05:01,242 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2.org_1   | 2023-06-30 10:05:01,242 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2.org_1   | 2023-06-30 10:05:01,245 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2.org_1   | 2023-06-30 10:05:01,246 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-30 10:05:01,246 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-30 10:05:01,247 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2023-06-30 10:05:01,256 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2023-06-30 10:05:01,288 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-30 10:05:01,307 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2023-06-30 10:05:01,308 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-30 10:04:35,351 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-30 10:04:35,373 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-30 10:04:35,378 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-30 10:04:35,383 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-30 10:04:35,383 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-30 10:04:35,407 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-30 10:04:35,538 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 10:04:35,545 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-30 10:04:35,547 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:35,549 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-30 10:04:35,549 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-30 10:04:35,550 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-06-30 10:04:35,552 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2023-06-30 10:04:35,552 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2023-06-30 10:04:35,553 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2023-06-30 10:04:35,594 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm1.org_1   | 2023-06-30 10:04:35,671 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm1.org_1   | 2023-06-30 10:04:35,679 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2023-06-30 10:04:35,694 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2023-06-30 10:04:35,697 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2023-06-30 10:04:35,820 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2023-06-30 10:04:35,840 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm1.org_1   | 2023-06-30 10:04:35,842 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-30 10:04:35,854 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1.org_1   | 2023-06-30 10:04:35,872 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2023-06-30 10:04:35,872 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-30 10:04:35,879 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2023-06-30 10:04:35,879 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 10:04:35,882 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1.org_1   | 2023-06-30 10:04:35,883 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1.org_1   | 2023-06-30 10:04:35,891 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1.org_1   | 2023-06-30 10:04:35,893 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1.org_1   | 2023-06-30 10:04:35,921 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-30 10:04:35,922 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-30 10:04:35,943 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2023-06-30 10:04:36,087 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2023-06-30 10:04:36,114 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2023-06-30 10:04:36,117 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1.org_1   | 2023-06-30 10:04:36,134 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2023-06-30 10:04:36,140 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:04:36,142 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-30 10:04:36,470 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm1.org_1   | 2023-06-30 10:04:36,471 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm1.org_1   | 2023-06-30 10:04:36,497 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-30 10:04:36,501 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-30 10:04:36,503 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 253598402501 on primary SCM
scm1.org_1   | 2023-06-30 10:04:36,517 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2023-06-30 10:04:36,550 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 10:04:36,603 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm1.org_1   | 2023-06-30 10:04:36,607 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2023-06-30 10:04:36,654 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm1.org_1   | 2023-06-30 10:04:36,655 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm1.org_1   | 2023-06-30 10:04:37,506 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-30 10:04:37,541 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 10:04:37,542 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm1.org_1   | 2023-06-30 10:04:37,551 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2023-06-30 10:04:37,588 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-30 10:04:37,595 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 10:04:37,595 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm1.org_1   | 2023-06-30 10:04:37,597 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2023-06-30 10:04:37,649 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-30 10:04:37,661 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 10:04:37,661 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm1.org_1   | 2023-06-30 10:04:37,662 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2023-06-30 10:04:37,823 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1.org_1   | 2023-06-30 10:04:37,825 [main] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        false
scm1.org_1   | Container Balancer Configuration values:
scm1.org_1   | Key                                                Value
scm1.org_1   | Threshold                                          10
scm1.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1.org_1   | Max Size to Move per Iteration                     500GB
scm1.org_1   | Max Size Entering Target per Iteration             26GB
scm1.org_1   | Max Size Leaving Source per Iteration              26GB
scm1.org_1   | 
scm1.org_1   | 2023-06-30 10:04:37,825 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2023-06-30 10:04:37,831 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-30 10:04:37,854 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2023-06-30 10:04:37,870 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/in_use.lock acquired by nodename 7@scm1.org
scm1.org_1   | 2023-06-30 10:04:37,879 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=42abf28c-5926-4498-87c1-8b8ba8a02c3f} from /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/raft-meta
scm1.org_1   | 2023-06-30 10:04:37,928 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:37,932 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-30 10:04:37,949 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-30 10:04:37,950 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:37,952 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-30 10:04:37,953 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-30 10:04:37,957 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 10:04:37,967 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-30 10:04:37,975 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-30 10:04:37,975 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:37,981 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b
scm1.org_1   | 2023-06-30 10:04:37,983 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 10:04:37,984 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 10:04:37,986 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 10:04:37,988 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-30 10:04:37,989 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-30 10:04:37,990 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-30 10:04:37,990 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-30 10:04:37,990 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-30 10:04:38,013 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-30 10:04:38,013 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:04:38,211 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 10:04:38,212 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 10:04:38,212 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3.org_1   | 2023-06-30 10:05:34,960 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2023-06-30 10:05:34,961 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-30 10:05:34,982 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2023-06-30 10:05:34,984 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2023-06-30 10:05:35,022 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2023-06-30 10:05:35,043 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm3.org_1   | 2023-06-30 10:05:35,057 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2023-06-30 10:05:35,058 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2023-06-30 10:05:35,151 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm3.org_1   | 2023-06-30 10:05:35,463 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-30 10:05:35,471 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2023-06-30 10:05:35,472 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3.org_1   | 2023-06-30 10:05:35,473 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3.org_1   | 2023-06-30 10:05:35,475 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3.org_1   | 2023-06-30 10:05:35,475 [0e7462e2-d318-4860-bc91-2d8e1c77938e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3.org_1   | 2023-06-30 10:05:35,481 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2023-06-30 10:05:35,482 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2023-06-30 10:05:35,482 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2023-06-30 10:05:35,587 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm3.org_1   | 2023-06-30 10:05:35,741 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3.org_1   | 2023-06-30 10:05:35,746 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2023-06-30 10:05:35,779 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2023-06-30 10:05:35,795 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2023-06-30 10:05:36,081 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2023-06-30 10:05:36,120 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm3.org_1   | 2023-06-30 10:05:36,126 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-30 10:05:36,163 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2023-06-30 10:05:36,311 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2023-06-30 10:05:36,315 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-30 10:05:36,355 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3.org_1   | 2023-06-30 10:05:36,357 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2023-06-30 10:05:36,372 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3.org_1   | 2023-06-30 10:05:36,384 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3.org_1   | 2023-06-30 10:05:36,401 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3.org_1   | 2023-06-30 10:05:36,411 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3.org_1   | 2023-06-30 10:05:36,523 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-30 10:05:36,532 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-30 10:05:36,602 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2023-06-30 10:05:36,884 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2023-06-30 10:05:37,040 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2023-06-30 10:05:37,040 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm3.org_1   | 2023-06-30 10:05:37,084 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2023-06-30 10:05:37,094 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:05:37,099 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-06-30 10:05:37,717 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm3.org_1   | 2023-06-30 10:05:37,717 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm3.org_1   | 2023-06-30 10:05:37,760 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2023-06-30 10:05:37,804 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 10:05:37,884 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm3.org_1   | 2023-06-30 10:05:37,887 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2023-06-30 10:05:38,005 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm3.org_1   | 2023-06-30 10:05:38,008 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm2.org_1   | 2023-06-30 10:05:01,373 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2.org_1   | 2023-06-30 10:05:01,393 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2.org_1   | 2023-06-30 10:05:01,407 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2023-06-30 10:05:01,408 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2023-06-30 10:05:01,486 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2.org_1   | 2023-06-30 10:05:01,967 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-30 10:05:01,972 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2023-06-30 10:05:01,976 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2.org_1   | 2023-06-30 10:05:01,977 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2.org_1   | 2023-06-30 10:05:01,981 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2.org_1   | 2023-06-30 10:05:01,984 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2.org_1   | 2023-06-30 10:05:01,986 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2023-06-30 10:05:01,993 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2023-06-30 10:05:01,994 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2023-06-30 10:05:02,110 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm2.org_1   | 2023-06-30 10:05:02,284 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2.org_1   | 2023-06-30 10:05:02,285 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2023-06-30 10:05:02,326 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2023-06-30 10:05:02,338 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2023-06-30 10:05:02,523 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2023-06-30 10:05:02,580 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm2.org_1   | 2023-06-30 10:05:02,587 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-30 10:05:02,677 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2023-06-30 10:05:02,828 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2023-06-30 10:05:02,833 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-30 10:05:02,871 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2023-06-30 10:05:02,874 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2023-06-30 10:05:02,908 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2.org_1   | 2023-06-30 10:05:02,911 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2.org_1   | 2023-06-30 10:05:02,956 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2.org_1   | 2023-06-30 10:05:02,960 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2.org_1   | 2023-06-30 10:05:03,129 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-30 10:05:03,130 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-30 10:05:03,243 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2023-06-30 10:05:03,683 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2023-06-30 10:05:03,763 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2023-06-30 10:05:03,765 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm2.org_1   | 2023-06-30 10:05:03,808 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2023-06-30 10:05:03,823 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:05:03,829 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-30 10:05:04,675 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm2.org_1   | 2023-06-30 10:05:04,686 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm2.org_1   | 2023-06-30 10:05:04,742 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2023-06-30 10:05:04,856 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 10:05:04,988 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm2.org_1   | 2023-06-30 10:05:05,006 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2023-06-30 10:05:05,155 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm2.org_1   | 2023-06-30 10:05:05,159 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm2.org_1   | 2023-06-30 10:05:07,110 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-30 10:05:07,146 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 10:05:07,147 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm2.org_1   | 2023-06-30 10:05:07,164 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2023-06-30 10:05:07,321 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-30 10:05:07,343 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 10:05:07,346 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm2.org_1   | 2023-06-30 10:05:07,352 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2023-06-30 10:05:07,518 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-30 10:05:07,749 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 10:05:07,755 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm2.org_1   | 2023-06-30 10:05:07,766 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2023-06-30 10:05:08,292 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2.org_1   | 2023-06-30 10:05:08,292 [main] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        false
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                                                Value
scm2.org_1   | Threshold                                          10
scm2.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2.org_1   | Max Size to Move per Iteration                     500GB
scm2.org_1   | Max Size Entering Target per Iteration             26GB
scm2.org_1   | Max Size Leaving Source per Iteration              26GB
scm2.org_1   | 
scm2.org_1   | 2023-06-30 10:05:08,301 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2023-06-30 10:05:08,334 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-30 10:05:08,370 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2023-06-30 10:05:08,378 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b does not exist. Creating ...
scm2.org_1   | 2023-06-30 10:05:08,408 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/in_use.lock acquired by nodename 7@scm2.org
scm2.org_1   | 2023-06-30 10:05:08,439 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b has been successfully formatted.
scm2.org_1   | 2023-06-30 10:05:08,472 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2023-06-30 10:05:08,487 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2023-06-30 10:05:08,487 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 10:05:08,489 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2.org_1   | 2023-06-30 10:05:08,499 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2.org_1   | 2023-06-30 10:05:08,506 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-30 10:05:08,523 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2023-06-30 10:05:08,524 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2023-06-30 10:05:08,525 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 10:05:08,541 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b
scm2.org_1   | 2023-06-30 10:05:08,543 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2023-06-30 10:05:08,544 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2023-06-30 10:05:08,551 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-30 10:05:08,551 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2.org_1   | 2023-06-30 10:05:08,554 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2023-06-30 10:05:08,566 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2023-06-30 10:05:08,567 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2023-06-30 10:05:08,568 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2023-06-30 10:05:08,617 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2023-06-30 10:05:08,618 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 10:05:08,992 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2.org_1   | 2023-06-30 10:05:08,999 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2.org_1   | 2023-06-30 10:05:09,000 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-06-30 10:05:09,049 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-30 10:05:09,051 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-30 10:05:09,068 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2.org_1   | 2023-06-30 10:05:09,069 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: changes role from      null to FOLLOWER at term 0 for startInitializing
scm1.org_1   | 2023-06-30 10:04:38,245 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:38,246 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_0
scm1.org_1   | 2023-06-30 10:04:38,251 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-30 10:04:38,307 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: start as a follower, conf=0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:38,307 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2023-06-30 10:04:38,309 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState
scm1.org_1   | 2023-06-30 10:04:38,311 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-45A6B430095B,id=42abf28c-5926-4498-87c1-8b8ba8a02c3f
scm1.org_1   | 2023-06-30 10:04:38,314 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-30 10:04:38,314 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-30 10:04:38,315 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-30 10:04:38,315 [42abf28c-5926-4498-87c1-8b8ba8a02c3f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-30 10:04:38,317 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-30 10:04:38,317 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-30 10:04:38,319 [main] INFO server.RaftServer: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start RPC server
scm1.org_1   | 2023-06-30 10:04:38,391 [main] INFO server.GrpcService: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-30 10:04:38,401 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-42abf28c-5926-4498-87c1-8b8ba8a02c3f: Started
scm1.org_1   | 2023-06-30 10:04:38,407 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-30 10:04:38,407 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2023-06-30 10:04:38,410 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm1.org_1   | 2023-06-30 10:04:38,410 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm1.org_1   | 2023-06-30 10:04:38,410 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm1.org_1   | 2023-06-30 10:04:38,536 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2023-06-30 10:04:38,549 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2023-06-30 10:04:38,549 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2023-06-30 10:04:38,835 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-30 10:04:38,835 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 10:04:38,853 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2023-06-30 10:04:38,881 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-30 10:04:38,882 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-30 10:04:38,883 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 10:04:38,883 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2023-06-30 10:04:38,924 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2023-06-30 10:04:38,926 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 10:04:38,929 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm1.org_1   | 2023-06-30 10:04:38,930 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm1.org_1   | 2023-06-30 10:04:38,991 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm1.org_1   | 2023-06-30 10:04:39,020 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1.org_1   | 2023-06-30 10:04:39,020 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm1.org_1   | 2023-06-30 10:04:39,022 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm1.org_1   | 2023-06-30 10:04:39,053 [main] INFO util.log: Logging initialized @8647ms to org.eclipse.jetty.util.log.Slf4jLog
scm1.org_1   | 2023-06-30 10:04:39,267 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1.org_1   | 2023-06-30 10:04:39,277 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1.org_1   | 2023-06-30 10:04:39,281 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm1.org_1   | 2023-06-30 10:04:39,282 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm1.org_1   | 2023-06-30 10:04:39,283 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm1.org_1   | 2023-06-30 10:04:39,287 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | 2023-06-30 10:04:39,387 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1.org_1   | 2023-06-30 10:04:39,391 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2023-06-30 10:04:39,402 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm1.org_1   | 2023-06-30 10:04:39,531 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2023-06-30 10:04:39,532 [main] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2023-06-30 10:04:39,534 [main] INFO server.session: node0 Scavenging every 600000ms
scm1.org_1   | 2023-06-30 10:04:39,551 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-30 10:04:39,560 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1c63aa5d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2023-06-30 10:04:39,562 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@28728558{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2023-06-30 10:04:39,594 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:51222 / 172.25.0.117:51222
scm1.org_1   | 2023-06-30 10:04:39,715 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:04:39,796 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-30 10:04:39,812 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1f6b9ab7{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-17696163212439736472/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2023-06-30 10:04:39,830 [main] INFO server.AbstractConnector: Started ServerConnector@7a1b4074{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2023-06-30 10:04:39,831 [main] INFO server.Server: Started @9425ms
scm1.org_1   | 2023-06-30 10:04:39,840 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2023-06-30 10:04:39,840 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2023-06-30 10:04:39,842 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2023-06-30 10:04:40,367 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:44514 / 172.25.0.116:44514
scm1.org_1   | 2023-06-30 10:04:40,405 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:04:41,206 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:43493 / 172.25.0.115:43493
scm1.org_1   | 2023-06-30 10:04:41,237 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:04:41,244 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#12 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:43493 / 172.25.0.115:43493
scm1.org_1   | org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:42abf28c-5926-4498-87c1-8b8ba8a02c3f is not the leader. Could not determine the leader node.
scm1.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1.org_1   | 2023-06-30 10:04:43,430 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO impl.FollowerState: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5121333327ns, electionTimeout:5111ms
scm1.org_1   | 2023-06-30 10:04:43,432 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: shutdown 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState
scm1.org_1   | 2023-06-30 10:04:43,433 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2023-06-30 10:04:43,436 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-30 10:04:43,436 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-FollowerState] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1
scm1.org_1   | 2023-06-30 10:04:43,439 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:43,440 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-30 10:04:43,443 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:43,444 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.LeaderElection: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1.org_1   | 2023-06-30 10:04:43,444 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: shutdown 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1
scm1.org_1   | 2023-06-30 10:04:43,445 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1.org_1   | 2023-06-30 10:04:43,445 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1.org_1   | 2023-06-30 10:04:43,445 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm1.org_1   | 2023-06-30 10:04:43,448 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: change Leader from null to 42abf28c-5926-4498-87c1-8b8ba8a02c3f at term 2 for becomeLeader, leader elected after 8072ms
scm1.org_1   | 2023-06-30 10:04:43,455 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-30 10:04:43,460 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 10:04:43,460 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 10:04:43,467 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-30 10:04:43,468 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-30 10:04:43,468 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-30 10:04:43,474 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 10:04:43,475 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-30 10:04:43,477 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO impl.RoleInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: start 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl
scm1.org_1   | 2023-06-30 10:04:43,483 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1.org_1   | 2023-06-30 10:04:43,490 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_0 to /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_0-0
scm1.org_1   | 2023-06-30 10:04:43,492 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderElection1] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:04:43,515 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_1
scm1.org_1   | 2023-06-30 10:04:43,522 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2023-06-30 10:04:43,526 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1.org_1   | 2023-06-30 10:04:43,527 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
scm1.org_1   | 2023-06-30 10:04:43,528 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:04:43,529 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1.org_1   | 2023-06-30 10:04:43,529 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-30 10:04:43,529 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1.org_1   | 2023-06-30 10:04:43,529 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528156Z, expire at: 2023-06-30T11:04:43.528156Z)
scm1.org_1   | 2023-06-30 10:04:43,542 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 10:04:43,545 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2023-06-30 10:04:43,719 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)]
scm1.org_1   | 2023-06-30 10:04:43,725 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)
scm1.org_1   | 2023-06-30 10:04:43,781 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)] to file /data/metadata/scm/keys/secret_keys.json
scm1.org_1   | 2023-06-30 10:04:43,783 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:04:43,783 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-30 10:04:43,783 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2023-06-30 10:04:45,839 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:42790 / 172.25.0.117:42790
scm1.org_1   | 2023-06-30 10:04:45,842 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:04:45,886 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: dc4bcfae-13a7-46ed-9c3e-1369a026857d
scm3.org_1   | 2023-06-30 10:05:39,771 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-30 10:05:39,783 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 10:05:39,785 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm3.org_1   | 2023-06-30 10:05:39,786 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2023-06-30 10:05:39,956 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-30 10:05:39,997 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 10:05:40,003 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm3.org_1   | 2023-06-30 10:05:40,005 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2023-06-30 10:05:40,104 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-30 10:05:40,121 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 10:05:40,121 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm3.org_1   | 2023-06-30 10:05:40,122 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2023-06-30 10:05:40,611 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3.org_1   | 2023-06-30 10:05:40,617 [main] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        false
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                                                Value
scm3.org_1   | Threshold                                          10
scm3.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3.org_1   | Max Size to Move per Iteration                     500GB
scm3.org_1   | Max Size Entering Target per Iteration             26GB
scm3.org_1   | Max Size Leaving Source per Iteration              26GB
scm3.org_1   | 
scm3.org_1   | 2023-06-30 10:05:40,617 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2023-06-30 10:05:40,661 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-30 10:05:40,668 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2023-06-30 10:05:40,671 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b does not exist. Creating ...
scm3.org_1   | 2023-06-30 10:05:40,692 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/in_use.lock acquired by nodename 7@scm3.org
scm3.org_1   | 2023-06-30 10:05:40,737 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b has been successfully formatted.
scm3.org_1   | 2023-06-30 10:05:40,762 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2023-06-30 10:05:40,819 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2023-06-30 10:05:40,819 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 10:05:40,831 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3.org_1   | 2023-06-30 10:05:40,837 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3.org_1   | 2023-06-30 10:05:40,846 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-30 10:05:40,859 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2023-06-30 10:05:40,859 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2023-06-30 10:05:40,859 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 10:05:40,880 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b
scm3.org_1   | 2023-06-30 10:05:40,881 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2023-06-30 10:05:40,881 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2023-06-30 10:05:40,884 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-30 10:05:40,887 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3.org_1   | 2023-06-30 10:05:40,889 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2023-06-30 10:05:40,893 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2023-06-30 10:05:40,893 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | 2023-06-30 10:05:40,896 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2023-06-30 10:05:40,937 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2023-06-30 10:05:40,937 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 10:05:41,007 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3.org_1   | 2023-06-30 10:05:41,009 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3.org_1   | 2023-06-30 10:05:41,010 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-06-30 10:05:09,072 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-45A6B430095B,id=dc4bcfae-13a7-46ed-9c3e-1369a026857d
scm2.org_1   | 2023-06-30 10:05:09,077 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2.org_1   | 2023-06-30 10:05:09,078 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2023-06-30 10:05:09,078 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2023-06-30 10:05:09,080 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2023-06-30 10:05:09,091 [main] INFO server.RaftServer: dc4bcfae-13a7-46ed-9c3e-1369a026857d: start RPC server
scm2.org_1   | 2023-06-30 10:05:09,299 [main] INFO server.GrpcService: dc4bcfae-13a7-46ed-9c3e-1369a026857d: GrpcService started, listening on 9894
scm2.org_1   | 2023-06-30 10:05:09,327 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-dc4bcfae-13a7-46ed-9c3e-1369a026857d: Started
scm2.org_1   | 2023-06-30 10:05:09,378 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-30 10:05:12,330 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: receive installSnapshot: 42abf28c-5926-4498-87c1-8b8ba8a02c3f->dc4bcfae-13a7-46ed-9c3e-1369a026857d#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-30 10:05:12,375 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2023-06-30 10:05:12,376 [grpc-default-executor-0] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: change Leader from null to 42abf28c-5926-4498-87c1-8b8ba8a02c3f at term 2 for installSnapshot, leader elected after 11002ms
scm2.org_1   | 2023-06-30 10:05:12,417 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: Received notification to install snapshot at index 0
scm2.org_1   | 2023-06-30 10:05:12,423 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2.org_1   | 2023-06-30 10:05:12,916 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "42abf28c-5926-4498-87c1-8b8ba8a02c3f"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |     startupRole: FOLLOWER
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2023-06-30 10:05:12,919 [grpc-default-executor-0] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set configuration 1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 10:05:12,929 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: reply installSnapshot: 42abf28c-5926-4498-87c1-8b8ba8a02c3f<-dc4bcfae-13a7-46ed-9c3e-1369a026857d#0:OK-t0,ALREADY_INSTALLED
scm2.org_1   | 2023-06-30 10:05:12,997 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: dc4bcfae-13a7-46ed-9c3e-1369a026857d: Completed INSTALL_SNAPSHOT, lastRequest: 42abf28c-5926-4498-87c1-8b8ba8a02c3f->dc4bcfae-13a7-46ed-9c3e-1369a026857d#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-30 10:05:12,998 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: dc4bcfae-13a7-46ed-9c3e-1369a026857d: Completed INSTALL_SNAPSHOT, lastReply: null
scm2.org_1   | 2023-06-30 10:05:13,262 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread1] INFO impl.RoleInfo: dc4bcfae-13a7-46ed-9c3e-1369a026857d: start dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState
scm2.org_1   | 2023-06-30 10:05:13,269 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread1] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-06-30 10:05:13,272 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread1] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: inconsistency entries. Reply:42abf28c-5926-4498-87c1-8b8ba8a02c3f<-dc4bcfae-13a7-46ed-9c3e-1369a026857d#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-06-30 10:05:13,276 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread2] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-06-30 10:05:13,276 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread2] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: inconsistency entries. Reply:42abf28c-5926-4498-87c1-8b8ba8a02c3f<-dc4bcfae-13a7-46ed-9c3e-1369a026857d#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-06-30 10:05:13,283 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:13,283 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:13,329 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread2] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 10:05:13,329 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread2] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set configuration 1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 10:05:13,349 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread2] INFO segmented.SegmentedRaftLogWorker: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2023-06-30 10:05:13,500 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread2] INFO segmented.SegmentedRaftLogWorker: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2023-06-30 10:05:13,543 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread1] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set configuration 9: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-06-30 10:05:14,013 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_0
scm2.org_1   | 2023-06-30 10:05:14,033 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_0 to /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_0-0
scm2.org_1   | 2023-06-30 10:05:14,098 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_1
scm2.org_1   | 2023-06-30 10:05:14,144 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:05:14,198 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2023-06-30 10:05:14,204 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-30 10:05:14,214 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2023-06-30 10:05:14,228 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread2] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set configuration 11: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 10:05:14,231 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 10:05:14,359 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2023-06-30 10:05:14,741 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-45A6B430095B:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2.org_1   | 2023-06-30 10:05:14,742 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2023-06-30 10:05:14,850 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm2.org_1   | 2023-06-30 10:05:14,862 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm2.org_1   | 2023-06-30 10:05:14,884 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2.org_1   | 2023-06-30 10:05:15,116 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)]
scm2.org_1   | 2023-06-30 10:05:15,128 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)
scm2.org_1   | 2023-06-30 10:05:15,586 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2023-06-30 10:05:15,762 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2023-06-30 10:05:15,762 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2023-06-30 10:05:15,806 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)] to file /data/metadata/scm/keys/secret_keys.json
scm2.org_1   | 2023-06-30 10:05:15,812 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:05:15,813 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-30 10:05:15,814 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2023-06-30 10:05:16,059 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-30 10:05:16,070 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 10:05:16,125 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2023-06-30 10:05:16,223 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-30 10:05:16,224 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-30 10:05:16,227 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 10:05:16,227 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2023-06-30 10:05:16,390 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2023-06-30 10:05:16,505 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:05:16,521 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 10:05:16,526 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:05:16,527 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2023-06-30 10:05:16,545 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm2.org_1   | 2023-06-30 10:05:16,970 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node dc4bcfae-13a7-46ed-9c3e-1369a026857d
scm2.org_1   | 2023-06-30 10:05:16,980 [main] INFO server.StorageContainerManager: Persist certificate serialId 253598402501 on Scm Bootstrap Node dc4bcfae-13a7-46ed-9c3e-1369a026857d
scm2.org_1   | 2023-06-30 10:05:16,989 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm2.org_1   | 2023-06-30 10:05:17,097 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2023-06-30 10:05:17,098 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2023-06-30 10:05:17,100 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2023-06-30 10:05:17,174 [main] INFO util.log: Logging initialized @29814ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2023-06-30 10:05:17,745 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2023-06-30 10:05:17,785 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2023-06-30 10:05:17,789 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm2.org_1   | 2023-06-30 10:05:17,791 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2023-06-30 10:05:17,791 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2023-06-30 10:05:17,801 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm2.org_1   | 2023-06-30 10:05:18,047 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm2.org_1   | 2023-06-30 10:05:18,051 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm2.org_1   | 2023-06-30 10:05:18,055 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm2.org_1   | 2023-06-30 10:05:18,210 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm2.org_1   | 2023-06-30 10:05:18,210 [main] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2023-06-30 10:05:18,213 [main] INFO server.session: node0 Scavenging every 600000ms
scm2.org_1   | 2023-06-30 10:05:18,277 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-30 10:05:18,284 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4aa8504f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2023-06-30 10:05:18,287 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@319267ad{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2.org_1   | 2023-06-30 10:05:18,367 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:18,367 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:18,702 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-30 10:05:18,768 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@bdc2f96{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-4636829269986740176/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2023-06-30 10:05:18,841 [main] INFO server.AbstractConnector: Started ServerConnector@49136fb{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2023-06-30 10:05:18,845 [main] INFO server.Server: Started @31484ms
scm2.org_1   | 2023-06-30 10:05:18,868 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2023-06-30 10:05:18,868 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2.org_1   | 2023-06-30 10:05:18,880 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2023-06-30 10:05:23,385 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:23,385 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:24,149 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:05:28,514 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:28,514 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:33,537 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:33,537 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:38,628 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:38,629 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:43,766 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:43,766 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:48,809 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:48,811 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:50,243 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread1] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set configuration 15: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:41,025 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-30 10:05:41,025 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-30 10:05:41,028 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:41,038 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3.org_1   | 2023-06-30 10:05:41,043 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-45A6B430095B,id=0e7462e2-d318-4860-bc91-2d8e1c77938e
scm3.org_1   | 2023-06-30 10:05:41,052 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2023-06-30 10:05:41,056 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3.org_1   | 2023-06-30 10:05:41,057 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2023-06-30 10:05:41,066 [0e7462e2-d318-4860-bc91-2d8e1c77938e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2023-06-30 10:05:41,089 [main] INFO server.RaftServer: 0e7462e2-d318-4860-bc91-2d8e1c77938e: start RPC server
scm3.org_1   | 2023-06-30 10:05:41,340 [main] INFO server.GrpcService: 0e7462e2-d318-4860-bc91-2d8e1c77938e: GrpcService started, listening on 9894
scm3.org_1   | 2023-06-30 10:05:41,362 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0e7462e2-d318-4860-bc91-2d8e1c77938e: Started
scm3.org_1   | 2023-06-30 10:05:41,389 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-30 10:05:46,510 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: receive installSnapshot: 42abf28c-5926-4498-87c1-8b8ba8a02c3f->0e7462e2-d318-4860-bc91-2d8e1c77938e#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-30 10:05:46,622 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2023-06-30 10:05:46,652 [grpc-default-executor-0] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: change Leader from null to 42abf28c-5926-4498-87c1-8b8ba8a02c3f at term 2 for installSnapshot, leader elected after 11600ms
scm3.org_1   | 2023-06-30 10:05:46,770 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: Received notification to install snapshot at index 0
scm3.org_1   | 2023-06-30 10:05:46,796 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3.org_1   | 2023-06-30 10:05:49,214 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set new configuration index: 11
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "42abf28c-5926-4498-87c1-8b8ba8a02c3f"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "dc4bcfae-13a7-46ed-9c3e-1369a026857d"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2023-06-30 10:05:49,275 [grpc-default-executor-0] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 11: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:49,397 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: reply installSnapshot: 42abf28c-5926-4498-87c1-8b8ba8a02c3f<-0e7462e2-d318-4860-bc91-2d8e1c77938e#0:OK-t0,ALREADY_INSTALLED
scm3.org_1   | 2023-06-30 10:05:49,562 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 0e7462e2-d318-4860-bc91-2d8e1c77938e: Completed INSTALL_SNAPSHOT, lastRequest: 42abf28c-5926-4498-87c1-8b8ba8a02c3f->0e7462e2-d318-4860-bc91-2d8e1c77938e#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-30 10:05:49,563 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 0e7462e2-d318-4860-bc91-2d8e1c77938e: Completed INSTALL_SNAPSHOT, lastReply: null
scm3.org_1   | 2023-06-30 10:05:50,351 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO impl.RoleInfo: 0e7462e2-d318-4860-bc91-2d8e1c77938e: start 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState
scm3.org_1   | 2023-06-30 10:05:50,394 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-06-30 10:05:50,401 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:05:50,417 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:05:50,685 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: inconsistency entries. Reply:42abf28c-5926-4498-87c1-8b8ba8a02c3f<-0e7462e2-d318-4860-bc91-2d8e1c77938e#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-30 10:05:50,766 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: Failed appendEntries as previous log entry ((t:2, i:14)) is not found
scm3.org_1   | 2023-06-30 10:05:50,766 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: inconsistency entries. Reply:42abf28c-5926-4498-87c1-8b8ba8a02c3f<-0e7462e2-d318-4860-bc91-2d8e1c77938e#2:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-30 10:05:50,928 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: Failed appendEntries as previous log entry ((t:2, i:15)) is not found
scm3.org_1   | 2023-06-30 10:05:50,939 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: inconsistency entries. Reply:42abf28c-5926-4498-87c1-8b8ba8a02c3f<-0e7462e2-d318-4860-bc91-2d8e1c77938e#3:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-30 10:05:51,012 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: Failed appendEntries as previous log entry ((t:2, i:17)) is not found
scm3.org_1   | 2023-06-30 10:05:51,015 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: inconsistency entries. Reply:42abf28c-5926-4498-87c1-8b8ba8a02c3f<-0e7462e2-d318-4860-bc91-2d8e1c77938e#6:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-30 10:05:51,018 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-45A6B430095B:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3.org_1   | 2023-06-30 10:05:51,018 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2023-06-30 10:05:51,103 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:51,131 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:51,132 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 9: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:51,132 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 11: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:51,132 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 15: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:51,132 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 17: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:51,219 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm3.org_1   | 2023-06-30 10:05:51,224 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm3.org_1   | 2023-06-30 10:05:51,224 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm3.org_1   | 2023-06-30 10:05:51,301 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO segmented.SegmentedRaftLogWorker: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker: Starting segment from index:0
scm3.org_1   | 2023-06-30 10:05:52,039 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread1] INFO segmented.SegmentedRaftLogWorker: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3.org_1   | 2023-06-30 10:05:52,449 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread2] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,449 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread2] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,450 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread2] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 9: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:52,450 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread2] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 11: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,450 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread2] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 15: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:52,450 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread2] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 17: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,481 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,481 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,481 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 9: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:52,481 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 11: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,482 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 15: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:52,482 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 17: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,502 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 0: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,509 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 1: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,543 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 9: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:52,545 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 11: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:52,550 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 15: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 10:05:52,557 [0e7462e2-d318-4860-bc91-2d8e1c77938e-server-thread3] INFO server.RaftServer$Division: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B: set configuration 17: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 10:05:53,814 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-0e7462e2-d318-4860-bc91-2d8e1c77938e: Detected pause in JVM or host machine approximately 0.171s with 0.154s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=154ms
scm3.org_1   | 2023-06-30 10:05:53,923 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2023-06-30 10:05:54,400 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2023-06-30 10:05:54,400 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2023-06-30 10:05:54,427 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_0
scm3.org_1   | 2023-06-30 10:05:54,609 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_0 to /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_0-0
scm3.org_1   | 2023-06-30 10:05:54,889 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/06880496-7012-4073-8c29-45a6b430095b/current/log_inprogress_1
scm3.org_1   | 2023-06-30 10:05:55,107 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:05:55,109 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2023-06-30 10:05:55,113 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-06-30 10:05:55,113 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2023-06-30 10:05:55,470 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:05:55,470 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:05:55,489 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 10:05:55,489 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2023-06-30 10:05:56,121 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-30 10:05:56,218 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 10:05:56,234 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2023-06-30 10:05:56,579 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-30 10:05:56,582 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-30 10:05:56,583 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 10:05:56,585 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2023-06-30 10:05:57,243 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2023-06-30 10:05:57,315 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 10:05:57,331 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2023-06-30 10:05:57,442 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2023-06-30 10:05:57,479 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-0e7462e2-d318-4860-bc91-2d8e1c77938e: Detected pause in JVM or host machine approximately 0.116s without any GCs.
scm3.org_1   | 2023-06-30 10:05:58,116 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)]
scm3.org_1   | 2023-06-30 10:05:58,141 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)
scm3.org_1   | 2023-06-30 10:06:00,633 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:00,633 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:00,846 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = c4a83dff-b6fd-48fe-af48-409ccf215c30, creation at: 2023-06-30T10:04:43.528Z, expire at: 2023-06-30T11:04:43.528Z)] to file /data/metadata/scm/keys/secret_keys.json
scm3.org_1   | 2023-06-30 10:06:00,868 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:00,889 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-30 10:06:00,889 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2023-06-30 10:06:01,087 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 0e7462e2-d318-4860-bc91-2d8e1c77938e
scm3.org_1   | 2023-06-30 10:06:01,560 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:01,579 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:01,616 [main] INFO server.StorageContainerManager: Persist certificate serialId 253598402501 on Scm Bootstrap Node 0e7462e2-d318-4860-bc91-2d8e1c77938e
scm3.org_1   | 2023-06-30 10:06:01,642 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:01,747 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm3.org_1   | 2023-06-30 10:06:02,074 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2023-06-30 10:06:02,074 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2023-06-30 10:06:02,094 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2023-06-30 10:06:02,854 [main] INFO util.log: Logging initialized @37865ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2023-06-30 10:06:04,454 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2023-06-30 10:06:04,607 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2023-06-30 10:06:04,646 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2023-06-30 10:06:04,647 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2023-06-30 10:06:04,675 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2023-06-30 10:06:04,698 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2023-06-30 10:06:05,480 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm3.org_1   | 2023-06-30 10:06:05,505 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2023-06-30 10:04:46,032 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 10:04:46,648 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:04:47,278 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: d115352d-937c-46e2-a5d9-c03683f1ef0d
scm1.org_1   | 2023-06-30 10:04:47,359 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:05:07,817 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:58084 / 172.25.0.116:58084
scm1.org_1   | 2023-06-30 10:05:07,858 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:05:10,588 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:58456 / 172.25.0.117:58456
scm1.org_1   | 2023-06-30 10:05:10,678 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:05:10,682 [IPC Server handler 15 on default port 9863] INFO ha.SCMRatisServerImpl: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: Submitting SetConfiguration request to Ratis server with new SCM peers list: [42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-30 10:05:10,698 [IPC Server handler 15 on default port 9863] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: receive setConfiguration SetConfigurationRequest:client-A51DB9B41E7E->42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 10:05:10,699 [IPC Server handler 15 on default port 9863] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-A51DB9B41E7E->42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 10:05:10,779 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-30 10:05:10,780 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:05:10,781 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-30 10:05:10,793 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-30 10:05:10,812 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-30 10:05:10,816 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 10:05:10,817 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-30 10:05:10,819 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1.org_1   | 2023-06-30 10:05:10,821 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-30 10:05:10,821 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 10:05:10,850 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-06-30 10:05:10,895 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-GrpcLogAppender: send 42abf28c-5926-4498-87c1-8b8ba8a02c3f->dc4bcfae-13a7-46ed-9c3e-1369a026857d#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-30 10:05:10,902 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for dc4bcfae-13a7-46ed-9c3e-1369a026857d
scm1.org_1   | 2023-06-30 10:05:12,936 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:41087 / 172.25.0.115:41087
scm1.org_1   | 2023-06-30 10:05:12,983 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:05:12,986 [grpc-default-executor-0] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-InstallSnapshotResponseHandler: received the first reply 42abf28c-5926-4498-87c1-8b8ba8a02c3f<-dc4bcfae-13a7-46ed-9c3e-1369a026857d#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-30 10:05:13,012 [grpc-default-executor-0] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-30 10:05:13,033 [grpc-default-executor-0] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-30 10:05:13,033 [grpc-default-executor-0] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-30 10:05:13,033 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d acknowledged installing snapshot
scm1.org_1   | 2023-06-30 10:05:13,038 [grpc-default-executor-0] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm2.org_1   | 2023-06-30 10:05:50,293 [dc4bcfae-13a7-46ed-9c3e-1369a026857d-server-thread1] INFO server.RaftServer$Division: dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B: set configuration 17: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 10:05:53,878 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:53,880 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:05:58,914 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-dc4bcfae-13a7-46ed-9c3e-1369a026857d: Detected pause in JVM or host machine approximately 0.282s with 0.377s GC time.
scm2.org_1   | GC pool 'ParNew' had collection(s): count=1 time=377ms
scm2.org_1   | 2023-06-30 10:05:58,921 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:05:58,921 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:04,013 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:04,013 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:09,100 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:09,101 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:14,291 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:14,293 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:19,430 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:19,431 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:24,596 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:24,597 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:29,623 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:29,624 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:30,966 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:06:34,719 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:34,719 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:36,346 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:06:37,291 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:06:39,114 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:06:39,630 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:06:39,775 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:39,775 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:42,203 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:06:44,778 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:44,779 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:49,785 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:49,785 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:06:54,922 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:06:54,922 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:00,067 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:00,067 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:05,125 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:05,125 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:10,134 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:10,134 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:13,413 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:45446 / 172.25.0.102:45446
scm2.org_1   | 2023-06-30 10:07:13,478 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 10:07:15,137 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:15,137 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:16,357 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:51534 / 172.25.0.103:51534
scm2.org_1   | 2023-06-30 10:07:16,514 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 10:07:17,266 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:53448 / 172.25.0.104:53448
scm2.org_1   | 2023-06-30 10:07:17,546 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 10:07:19,048 [IPC Server handler 76 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/90f061fe-584f-4bf2-a5ce-ab438b7e17a1
scm2.org_1   | 2023-06-30 10:07:19,080 [IPC Server handler 76 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 90f061fe-584f-4bf2-a5ce-ab438b7e17a1{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 382242999512, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-30 10:07:19,122 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-30 10:07:19,189 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 10:07:19,981 [IPC Server handler 77 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm2.org_1   | 2023-06-30 10:07:20,006 [IPC Server handler 77 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8585dc18-efaa-4bfa-93ea-e78f13194d9c{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 387609916440, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-30 10:07:20,006 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 10:07:20,007 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-30 10:07:20,172 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:20,173 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:20,864 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-dc4bcfae-13a7-46ed-9c3e-1369a026857d: Detected pause in JVM or host machine approximately 0.245s with 0.365s GC time.
scm2.org_1   | GC pool 'ParNew' had collection(s): count=1 time=365ms
scm2.org_1   | 2023-06-30 10:07:20,923 [IPC Server handler 75 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/45c04ec6-f646-4507-992b-4f7a81b89828
scm2.org_1   | 2023-06-30 10:07:20,933 [IPC Server handler 75 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 45c04ec6-f646-4507-992b-4f7a81b89828{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 388756064589, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-30 10:07:21,013 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 10:07:21,013 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-30 10:07:21,013 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-30 10:07:21,013 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2.org_1   | 2023-06-30 10:07:21,013 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2.org_1   | 2023-06-30 10:07:21,013 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 10:07:21,461 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:21,476 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:21,519 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:21,727 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:21,762 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:25,328 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:25,329 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:30,370 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:30,371 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:35,386 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:35,387 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:40,517 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:40,518 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:45,602 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:45,602 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:50,679 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:50,679 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:51,038 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:32894 / 172.25.0.102:32894
scm2.org_1   | 2023-06-30 10:07:51,200 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 10:07:52,039 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:56850 / 172.25.0.103:56850
scm2.org_1   | 2023-06-30 10:07:52,217 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 10:07:53,067 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:50054 / 172.25.0.104:50054
scm2.org_1   | 2023-06-30 10:07:53,294 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 10:07:54,455 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2.org_1   |   id: "7ad03ed4-4cc4-4086-b128-5b762bd02996"
scm2.org_1   |   uuid128 {
scm2.org_1   |     mostSigBits: 8849642349324943494
scm2.org_1   |     leastSigBits: -5681190366828549738
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   | isLeader: false
scm2.org_1   | bytesWritten: 0
scm2.org_1   |  from dn=90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102).
scm2.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm1.org_1   | 2023-06-30 10:05:13,297 [grpc-default-executor-0] WARN server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 10:05:13,299 [grpc-default-executor-0] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d: setNextIndex nextIndex: updateUnconditionally 9 -> 0
scm1.org_1   | 2023-06-30 10:05:13,299 [grpc-default-executor-2] WARN server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->dc4bcfae-13a7-46ed-9c3e-1369a026857d-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 10:05:13,389 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 9: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-30 10:05:14,191 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 11: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:05:14,434 [IPC Server handler 15 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: dc4bcfae-13a7-46ed-9c3e-1369a026857d.
scm1.org_1   | 2023-06-30 10:05:16,716 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:41842 / 172.25.0.117:41842
scm1.org_1   | 2023-06-30 10:05:16,756 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:05:19,398 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:40466 / 172.25.0.118:40466
scm1.org_1   | 2023-06-30 10:05:19,429 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:05:23,974 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:60114 / 172.25.0.118:60114
scm1.org_1   | 2023-06-30 10:05:23,991 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:05:23,994 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: 0e7462e2-d318-4860-bc91-2d8e1c77938e
scm1.org_1   | 2023-06-30 10:05:24,045 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 10:05:24,139 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:05:26,394 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:37208 / 172.25.0.116:37208
scm1.org_1   | 2023-06-30 10:05:26,452 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:05:43,176 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:53104 / 172.25.0.118:53104
scm1.org_1   | 2023-06-30 10:05:43,606 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:05:43,620 [IPC Server handler 22 on default port 9863] INFO ha.SCMRatisServerImpl: 42abf28c-5926-4498-87c1-8b8ba8a02c3f: Submitting SetConfiguration request to Ratis server with new SCM peers list: [42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-30 10:05:43,647 [IPC Server handler 22 on default port 9863] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: receive setConfiguration SetConfigurationRequest:client-A51DB9B41E7E->42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 10:05:43,656 [IPC Server handler 22 on default port 9863] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-A51DB9B41E7E->42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 10:05:43,675 [IPC Server handler 22 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-30 10:05:43,677 [IPC Server handler 22 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 10:05:43,677 [IPC Server handler 22 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-30 10:05:43,687 [IPC Server handler 22 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-30 10:05:43,688 [IPC Server handler 22 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-30 10:05:43,688 [IPC Server handler 22 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 10:05:43,692 [IPC Server handler 22 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-30 10:05:43,692 [IPC Server handler 22 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1.org_1   | 2023-06-30 10:05:43,692 [IPC Server handler 22 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2023-06-30 10:06:05,543 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm3.org_1   | 2023-06-30 10:06:05,652 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:05,652 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:06,033 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2023-06-30 10:06:06,034 [main] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2023-06-30 10:06:06,077 [main] INFO server.session: node0 Scavenging every 660000ms
scm3.org_1   | 2023-06-30 10:06:06,388 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-06-30 10:06:06,425 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@46fc79ed{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2023-06-30 10:06:06,436 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2ea55742{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2023-06-30 10:06:08,317 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-06-30 10:06:08,465 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@21618f32{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-7968508668535278409/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2023-06-30 10:06:08,658 [main] INFO server.AbstractConnector: Started ServerConnector@f6776e2{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2023-06-30 10:06:08,668 [main] INFO server.Server: Started @43679ms
scm3.org_1   | 2023-06-30 10:06:08,714 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2023-06-30 10:06:08,714 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2023-06-30 10:06:08,732 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2023-06-30 10:06:10,693 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:10,693 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:15,719 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:15,720 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:20,744 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:20,744 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:25,919 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:25,919 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:30,940 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:30,987 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:30,988 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:36,128 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:36,129 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:36,273 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:37,213 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:39,141 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:39,678 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:41,196 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:41,197 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:42,313 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:06:46,389 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:46,390 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:51,404 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:51,404 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:06:56,409 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:06:56,409 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:01,412 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:01,413 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:06,487 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:06,487 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:11,594 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:11,594 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:13,382 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:43570 / 172.25.0.102:43570
scm3.org_1   | 2023-06-30 10:07:13,467 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 10:07:15,630 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-0e7462e2-d318-4860-bc91-2d8e1c77938e: Detected pause in JVM or host machine approximately 0.874s with 0.721s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=721ms
scm3.org_1   | 2023-06-30 10:07:16,374 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:53358 / 172.25.0.103:53358
scm3.org_1   | 2023-06-30 10:07:16,428 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 10:07:16,689 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:16,697 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:17,343 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:39466 / 172.25.0.104:39466
scm3.org_1   | 2023-06-30 10:07:17,543 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 10:07:19,175 [IPC Server handler 39 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/90f061fe-584f-4bf2-a5ce-ab438b7e17a1
scm3.org_1   | 2023-06-30 10:07:19,468 [IPC Server handler 39 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 90f061fe-584f-4bf2-a5ce-ab438b7e17a1{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 382242999512, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-30 10:07:19,581 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-30 10:07:19,596 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-30 10:07:19,946 [IPC Server handler 41 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm3.org_1   | 2023-06-30 10:07:19,948 [IPC Server handler 41 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8585dc18-efaa-4bfa-93ea-e78f13194d9c{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 387609916440, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-30 10:07:19,979 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-30 10:07:19,980 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-30 10:07:20,919 [IPC Server handler 37 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/45c04ec6-f646-4507-992b-4f7a81b89828
scm3.org_1   | 2023-06-30 10:07:20,925 [IPC Server handler 37 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 45c04ec6-f646-4507-992b-4f7a81b89828{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 388756064589, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-30 10:07:20,927 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-30 10:07:20,927 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-30 10:07:20,927 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2023-06-30 10:07:20,927 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2023-06-30 10:07:20,938 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-30 10:07:21,122 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1.org_1   | 2023-06-30 10:05:43,692 [IPC Server handler 22 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 10:05:43,697 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-06-30 10:05:43,713 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-GrpcLogAppender: send 42abf28c-5926-4498-87c1-8b8ba8a02c3f->0e7462e2-d318-4860-bc91-2d8e1c77938e#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-30 10:05:43,713 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 0e7462e2-d318-4860-bc91-2d8e1c77938e
scm1.org_1   | 2023-06-30 10:05:49,554 [grpc-default-executor-2] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-InstallSnapshotResponseHandler: received the first reply 42abf28c-5926-4498-87c1-8b8ba8a02c3f<-0e7462e2-d318-4860-bc91-2d8e1c77938e#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-30 10:05:49,558 [grpc-default-executor-2] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-30 10:05:49,559 [grpc-default-executor-2] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-30 10:05:49,567 [grpc-default-executor-2] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-30 10:05:49,568 [grpc-default-executor-2] INFO leader.FollowerInfo: Follower 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e acknowledged installing snapshot
scm1.org_1   | 2023-06-30 10:05:49,571 [grpc-default-executor-2] INFO server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm1.org_1   | 2023-06-30 10:05:50,162 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 15: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-30 10:05:50,272 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-LeaderStateImpl] INFO server.RaftServer$Division: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B: set configuration 17: peers:[42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, dc4bcfae-13a7-46ed-9c3e-1369a026857d|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 0e7462e2-d318-4860-bc91-2d8e1c77938e|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 10:05:50,336 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:52872 / 172.25.0.116:52872
scm1.org_1   | 2023-06-30 10:05:50,433 [IPC Server handler 22 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 0e7462e2-d318-4860-bc91-2d8e1c77938e.
scm1.org_1   | 2023-06-30 10:05:50,737 [grpc-default-executor-4] WARN server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 10:05:50,740 [grpc-default-executor-4] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-30 10:05:50,874 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:05:50,905 [grpc-default-executor-4] WARN server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 10:05:50,934 [grpc-default-executor-4] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-30 10:05:50,974 [grpc-default-executor-4] WARN server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 10:05:50,975 [grpc-default-executor-4] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-30 10:05:51,060 [grpc-default-executor-4] WARN server.GrpcLogAppender: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 10:05:51,062 [grpc-default-executor-4] INFO leader.FollowerInfo: 42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B->0e7462e2-d318-4860-bc91-2d8e1c77938e: setNextIndex nextIndex: updateUnconditionally 19 -> 0
scm1.org_1   | 2023-06-30 10:05:59,087 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:34480 / 172.25.0.118:34480
scm1.org_1   | 2023-06-30 10:05:59,610 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:06:14,389 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:39523 / 172.25.0.115:39523
scm1.org_1   | 2023-06-30 10:06:14,497 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:06:23,745 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:46218 / 172.25.0.112:46218
scm1.org_1   | 2023-06-30 10:06:23,887 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:06:24,641 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:34604 / 172.25.0.111:34604
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2.org_1   | 	... 3 more
scm2.org_1   | 2023-06-30 10:07:54,583 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:55,854 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:07:55,854 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:07:56,016 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2.org_1   |   id: "459a01b6-1a94-49da-be94-47395dca916d"
scm2.org_1   |   uuid128 {
scm2.org_1   |     mostSigBits: 5015323016672070106
scm2.org_1   |     leastSigBits: -4714064598237802131
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   | isLeader: false
scm2.org_1   | bytesWritten: 0
scm2.org_1   |  from dn=8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103).
scm2.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2.org_1   | 	... 3 more
scm2.org_1   | 2023-06-30 10:07:56,173 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:56,227 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 10:07:56,447 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2.org_1   |   id: "35f8130a-9766-41b0-b833-8d68e5c9eeae"
scm2.org_1   |   uuid128 {
scm2.org_1   |     mostSigBits: 3888879214445085104
scm2.org_1   |     leastSigBits: -5173636065247236434
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   | isLeader: false
scm2.org_1   | bytesWritten: 0
scm2.org_1   |  from dn=45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104).
scm2.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2.org_1   | 	... 3 more
scm1.org_1   | 2023-06-30 10:06:24,780 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:06:25,931 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:33528 / 172.25.0.113:33528
scm1.org_1   | 2023-06-30 10:06:26,113 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:06:30,012 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:35308 / 172.25.0.102:35308
scm1.org_1   | 2023-06-30 10:06:30,308 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:06:30,321 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 2b3975d57ed5, UUID: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1
scm1.org_1   | 2023-06-30 10:06:30,913 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:06:33,926 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:56980 / 172.25.0.102:56980
scm1.org_1   | 2023-06-30 10:06:33,979 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-30 10:06:35,443 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:49392 / 172.25.0.103:49392
scm1.org_1   | 2023-06-30 10:06:35,571 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:06:35,581 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn f831826c9e82, UUID: 8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm1.org_1   | 2023-06-30 10:06:36,273 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:06:36,720 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:46260 / 172.25.0.104:46260
scm1.org_1   | 2023-06-30 10:06:36,884 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:06:36,886 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn bda3280202ff, UUID: 45c04ec6-f646-4507-992b-4f7a81b89828
scm1.org_1   | 2023-06-30 10:06:37,223 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:06:38,150 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:56652 / 172.25.0.112:56652
scm1.org_1   | 2023-06-30 10:06:38,410 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:51632 / 172.25.0.111:51632
scm1.org_1   | 2023-06-30 10:06:38,454 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:06:38,579 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: 71fc24b6-4cb0-4e17-9350-87ca9153a7ed
scm1.org_1   | 2023-06-30 10:06:38,614 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:06:38,614 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: 67d65376-ab17-44d8-b7d9-46ab6ee767ad
scm1.org_1   | 2023-06-30 10:06:39,075 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:49406 / 172.25.0.103:49406
scm1.org_1   | 2023-06-30 10:06:39,103 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:06:39,131 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-30 10:06:39,611 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:06:40,205 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:46918 / 172.25.0.104:46918
scm1.org_1   | 2023-06-30 10:06:40,250 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-30 10:06:41,701 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:38988 / 172.25.0.113:38988
scm1.org_1   | 2023-06-30 10:06:41,728 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:06:41,733 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: 8eb5f88d-b0fc-4bc4-b4bd-82e9b3d4934c
scm1.org_1   | 2023-06-30 10:06:42,218 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:02,141 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:41158 / 172.25.0.102:41158
scm1.org_1   | 2023-06-30 10:07:02,168 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:07:04,180 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:56504 / 172.25.0.103:56504
scm1.org_1   | 2023-06-30 10:07:04,194 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:07:05,167 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:55070 / 172.25.0.104:55070
scm1.org_1   | 2023-06-30 10:07:05,209 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:07:07,453 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:55554 / 172.25.0.116:55554
scm1.org_1   | 2023-06-30 10:07:07,746 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:07:13,159 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:32802 / 172.25.0.102:32802
scm1.org_1   | 2023-06-30 10:07:13,220 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 10:07:16,464 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:34134 / 172.25.0.103:34134
scm1.org_1   | 2023-06-30 10:07:16,610 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 10:07:17,330 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:52984 / 172.25.0.104:52984
scm1.org_1   | 2023-06-30 10:07:17,532 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 10:07:19,294 [IPC Server handler 22 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/90f061fe-584f-4bf2-a5ce-ab438b7e17a1
scm1.org_1   | 2023-06-30 10:07:19,398 [IPC Server handler 22 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 90f061fe-584f-4bf2-a5ce-ab438b7e17a1{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 382242999512, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-30 10:07:19,466 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 10:07:19,583 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7ad03ed4-4cc4-4086-b128-5b762bd02996 to datanode:90f061fe-584f-4bf2-a5ce-ab438b7e17a1
scm1.org_1   | 2023-06-30 10:07:19,727 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-30 10:07:19,994 [IPC Server handler 85 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm1.org_1   | 2023-06-30 10:07:20,097 [IPC Server handler 85 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8585dc18-efaa-4bfa-93ea-e78f13194d9c{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 387609916440, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-30 10:07:20,105 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 10:07:20,110 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-30 10:07:20,369 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:20,443 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 7ad03ed4-4cc4-4086-b128-5b762bd02996, Nodes: 90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:07:19.582194Z[UTC]]
scm1.org_1   | 2023-06-30 10:07:20,472 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=459a01b6-1a94-49da-be94-47395dca916d to datanode:8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm1.org_1   | 2023-06-30 10:07:20,613 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:20,616 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 459a01b6-1a94-49da-be94-47395dca916d, Nodes: 8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:07:20.472611Z[UTC]]
scm1.org_1   | 2023-06-30 10:07:20,919 [IPC Server handler 16 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/45c04ec6-f646-4507-992b-4f7a81b89828
scm1.org_1   | 2023-06-30 10:07:20,956 [IPC Server handler 16 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 45c04ec6-f646-4507-992b-4f7a81b89828{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 388756064589, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-30 10:07:20,973 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 10:07:20,974 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=35f8130a-9766-41b0-b833-8d68e5c9eeae to datanode:45c04ec6-f646-4507-992b-4f7a81b89828
scm1.org_1   | 2023-06-30 10:07:20,978 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-30 10:07:20,978 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-30 10:07:20,978 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2023-06-30 10:07:20,980 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2023-06-30 10:07:20,980 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 10:07:21,161 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:21,162 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 35f8130a-9766-41b0-b833-8d68e5c9eeae, Nodes: 45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:07:20.974722Z[UTC]]
scm1.org_1   | 2023-06-30 10:07:21,240 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 to datanode:8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm1.org_1   | 2023-06-30 10:07:21,245 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 to datanode:90f061fe-584f-4bf2-a5ce-ab438b7e17a1
scm1.org_1   | 2023-06-30 10:07:21,245 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 to datanode:45c04ec6-f646-4507-992b-4f7a81b89828
scm1.org_1   | 2023-06-30 10:07:21,355 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:21,366 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 5f2976d1-d881-4d67-95db-dd88234141a4, Nodes: 8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:07:21.240111Z[UTC]]
scm1.org_1   | 2023-06-30 10:07:21,399 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aca37863-94f4-4444-99ad-6ba0df2c41eb to datanode:45c04ec6-f646-4507-992b-4f7a81b89828
scm1.org_1   | 2023-06-30 10:07:21,434 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aca37863-94f4-4444-99ad-6ba0df2c41eb to datanode:90f061fe-584f-4bf2-a5ce-ab438b7e17a1
scm1.org_1   | 2023-06-30 10:07:21,434 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aca37863-94f4-4444-99ad-6ba0df2c41eb to datanode:8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm1.org_1   | 2023-06-30 10:07:21,545 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:21,584 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=aca37863-94f4-4444-99ad-6ba0df2c41eb contains same datanodes as previous pipelines: PipelineID=5f2976d1-d881-4d67-95db-dd88234141a4 nodeIds: 45c04ec6-f646-4507-992b-4f7a81b89828, 90f061fe-584f-4bf2-a5ce-ab438b7e17a1, 8585dc18-efaa-4bfa-93ea-e78f13194d9c
scm1.org_1   | 2023-06-30 10:07:21,587 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: aca37863-94f4-4444-99ad-6ba0df2c41eb, Nodes: 45c04ec6-f646-4507-992b-4f7a81b89828(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:07:21.399785Z[UTC]]
scm1.org_1   | 2023-06-30 10:07:21,588 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-30 10:07:21,588 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-30 10:07:24,603 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:56066 / 172.25.0.111:56066
scm1.org_1   | 2023-06-30 10:07:24,745 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:07:26,562 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:54492 / 172.25.0.112:54492
scm1.org_1   | 2023-06-30 10:07:26,655 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:07:27,148 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:46524 / 172.25.0.113:46524
scm1.org_1   | 2023-06-30 10:07:27,226 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 10:07:51,119 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:34028 / 172.25.0.102:34028
scm1.org_1   | 2023-06-30 10:07:51,221 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 10:07:51,594 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-30 10:07:52,053 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:45542 / 172.25.0.103:45542
scm1.org_1   | 2023-06-30 10:07:52,199 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 10:07:53,082 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:46552 / 172.25.0.104:46552
scm1.org_1   | 2023-06-30 10:07:53,284 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 10:07:54,291 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:37539 / 172.25.0.115:37539
scm1.org_1   | 2023-06-30 10:07:54,371 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:54,385 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=7ad03ed4-4cc4-4086-b128-5b762bd02996
scm1.org_1   | 2023-06-30 10:07:54,396 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:07:54,418 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:07:56,155 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:56,160 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=459a01b6-1a94-49da-be94-47395dca916d
scm1.org_1   | 2023-06-30 10:07:56,182 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:07:56,271 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:07:56,517 [42abf28c-5926-4498-87c1-8b8ba8a02c3f@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 10:07:56,539 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=35f8130a-9766-41b0-b833-8d68e5c9eeae
scm1.org_1   | 2023-06-30 10:07:56,540 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:07:57,287 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:07:58,042 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:07:59,393 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:38280 / 172.25.0.116:38280
scm1.org_1   | 2023-06-30 10:07:59,601 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 10:08:00,398 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:08:00,686 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:50640 / 172.25.0.112:50640
scm1.org_1   | 2023-06-30 10:08:00,717 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-30 10:08:02,037 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:50650 / 172.25.0.112:50650
scm1.org_1   | 2023-06-30 10:08:02,087 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:08:02,259 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:58312 / 172.25.0.111:58312
scm1.org_1   | 2023-06-30 10:08:02,366 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:08:02,393 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-30 10:08:02,535 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 10:08:03,647 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:58316 / 172.25.0.111:58316
scm1.org_1   | 2023-06-30 10:08:03,733 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 10:08:05,924 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:47532 / 172.25.0.113:47532
scm1.org_1   | 2023-06-30 10:08:05,957 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-30 10:08:07,217 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:47542 / 172.25.0.113:47542
scm1.org_1   | 2023-06-30 10:08:07,329 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm3.org_1   | 2023-06-30 10:07:21,132 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:21,233 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:21,248 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:21,410 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:21,553 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:21,780 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:21,780 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:26,844 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:26,847 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:31,905 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:31,907 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:36,929 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:36,929 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:42,049 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:42,049 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:47,135 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:47,135 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:51,063 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:36764 / 172.25.0.102:36764
scm3.org_1   | 2023-06-30 10:07:51,187 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 10:07:52,036 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:56614 / 172.25.0.103:56614
scm3.org_1   | 2023-06-30 10:07:52,208 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 10:07:52,211 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:52,211 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:53,054 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:40172 / 172.25.0.104:40172
scm3.org_1   | 2023-06-30 10:07:53,278 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 10:07:54,839 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3.org_1   |   id: "7ad03ed4-4cc4-4086-b128-5b762bd02996"
scm3.org_1   |   uuid128 {
scm3.org_1   |     mostSigBits: 8849642349324943494
scm3.org_1   |     leastSigBits: -5681190366828549738
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   | isLeader: false
scm3.org_1   | bytesWritten: 0
scm3.org_1   |  from dn=90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102).
scm3.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2.org_1   | 2023-06-30 10:07:56,526 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 10:07:57,334 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 10:07:58,032 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 10:08:00,377 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 10:08:00,958 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:08:00,958 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 10:08:02,367 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 10:08:02,527 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 10:08:06,011 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 10:08:06,012 [dc4bcfae-13a7-46ed-9c3e-1369a026857d@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3.org_1   | 	... 3 more
scm3.org_1   | 2023-06-30 10:07:54,971 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:55,986 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3.org_1   |   id: "459a01b6-1a94-49da-be94-47395dca916d"
scm3.org_1   |   uuid128 {
scm3.org_1   |     mostSigBits: 5015323016672070106
scm3.org_1   |     leastSigBits: -4714064598237802131
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   | isLeader: false
scm3.org_1   | bytesWritten: 0
scm3.org_1   |  from dn=8585dc18-efaa-4bfa-93ea-e78f13194d9c(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103).
scm3.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3.org_1   | 	... 3 more
scm3.org_1   | 2023-06-30 10:07:56,165 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:56,235 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 10:07:56,500 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 10:07:56,533 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 10:07:57,337 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 10:07:57,348 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:07:57,348 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:07:58,030 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 10:08:00,379 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 10:08:02,351 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 10:08:02,379 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:08:02,379 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:08:02,532 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 10:08:07,573 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 10:08:07,573 [0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 10:08:09,049 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3.org_1   |   id: "5f2976d1-d881-4d67-95db-dd88234141a4"
scm3.org_1   |   uuid128 {
scm3.org_1   |     mostSigBits: 6857142551300885863
scm3.org_1   |     leastSigBits: -7648275965381885532
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   | isLeader: true
scm3.org_1   | bytesWritten: 0
scm3.org_1   |  from dn=90f061fe-584f-4bf2-a5ce-ab438b7e17a1(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102).
scm3.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 0e7462e2-d318-4860-bc91-2d8e1c77938e@group-45A6B430095B is not the leader 42abf28c-5926-4498-87c1-8b8ba8a02c3f|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3.org_1   | 	... 3 more
scm3.org_1   | 2023-06-30 10:08:09,075 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
