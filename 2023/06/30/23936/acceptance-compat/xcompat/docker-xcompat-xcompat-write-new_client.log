Attaching to xcompat_new_client_1, xcompat_recon_1, xcompat_old_client_1_1_0_1, xcompat_scm_1, xcompat_om_1, xcompat_old_client_1_2_1_1, xcompat_datanode_2, xcompat_datanode_1, xcompat_datanode_3, xcompat_old_client_1_0_0_1, xcompat_s3g_1, xcompat_old_client_1_3_0_1
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-30 10:16:01,898 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = f0e5e42931dd/172.19.0.5
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-30 10:16:03,962 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = ef09c5769810/172.19.0.7
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-06-30 10:16:01,966 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-30 10:16:02,221 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-30 10:16:02,747 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-30 10:16:03,488 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-30 10:16:03,488 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-30 10:16:04,231 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:f0e5e42931dd ip:172.19.0.5
datanode_2          | 2023-06-30 10:16:05,141 [main] INFO reflections.Reflections: Reflections took 604 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_2          | 2023-06-30 10:16:07,692 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-06-30 10:16:08,033 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-30 10:16:09,118 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-30 10:16:09,187 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-30 10:16:09,212 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-30 10:16:09,213 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-30 10:16:09,430 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:16:09,450 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:16:09,451 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-30 10:16:09,492 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-30 10:16:09,492 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-30 10:16:09,493 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-06-30 10:16:04,038 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-30 10:16:04,268 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-30 10:16:04,833 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-30 10:16:05,725 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-30 10:16:05,725 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-30 10:16:06,415 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ef09c5769810 ip:172.19.0.7
datanode_1          | 2023-06-30 10:16:07,365 [main] INFO reflections.Reflections: Reflections took 667 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_1          | 2023-06-30 10:16:09,757 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-06-30 10:16:10,051 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-30 10:16:11,253 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-30 10:16:11,362 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-30 10:16:11,373 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-30 10:16:11,389 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-30 10:16:11,499 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:16:11,577 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:16:11,584 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-30 10:16:11,586 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-30 10:16:11,586 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-30 10:16:11,587 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-30 10:16:09,626 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:16:09,626 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-30 10:16:17,757 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-30 10:16:18,165 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:16:18,426 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-30 10:16:18,782 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-30 10:16:18,812 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-30 10:16:18,822 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-30 10:16:18,825 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-30 10:16:18,827 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-30 10:16:18,827 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-30 10:16:18,828 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-30 10:16:18,832 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:16:18,836 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-30 10:16:18,840 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:16:19,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-30 10:16:19,181 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-30 10:16:19,196 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-30 10:16:20,857 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-30 10:16:20,925 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-30 10:16:20,928 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-30 10:16:20,928 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:16:20,928 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:16:20,931 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:16:21,375 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-30 10:16:21,625 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-06-30 10:16:22,669 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:16:22,734 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-30 10:16:22,873 [main] INFO util.log: Logging initialized @29338ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-30 10:16:23,415 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-30 10:16:23,430 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-30 10:16:23,463 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-30 10:16:23,472 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-30 10:16:23,475 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-30 10:16:23,476 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-30 10:16:23,618 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-06-30 10:16:23,642 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-30 10:16:23,652 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-06-30 10:16:23,841 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-30 10:16:23,843 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-30 10:16:23,858 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-06-30 10:16:23,905 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d77d269{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-30 10:16:23,915 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@52afcdd8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-30 10:16:24,400 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@726dd367{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-11809201447852213048/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-30 10:16:24,467 [main] INFO server.AbstractConnector: Started ServerConnector@16279a5d{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-30 10:16:24,467 [main] INFO server.Server: Started @30932ms
datanode_2          | 2023-06-30 10:16:24,469 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-30 10:16:24,483 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-30 10:16:24,487 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:16:24,634 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-06-30 10:16:24,745 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-06-30 10:16:24,780 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-06-30 10:16:25,947 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-06-30 10:16:25,947 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-06-30 10:16:25,974 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-06-30 10:16:25,984 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-06-30 10:16:26,026 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-30 10:16:26,534 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.12:9891
datanode_1          | 2023-06-30 10:16:11,697 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:16:11,698 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-30 10:16:18,858 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-30 10:16:19,327 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:16:19,575 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-30 10:16:19,973 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-30 10:16:20,004 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-30 10:16:20,004 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-30 10:16:20,004 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-30 10:16:20,005 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-30 10:16:20,005 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-30 10:16:20,008 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-30 10:16:20,021 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:20,167 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-30 10:16:20,168 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:16:20,235 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-30 10:16:20,286 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-30 10:16:20,301 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-30 10:16:22,020 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-30 10:16:22,049 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-30 10:16:22,051 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-30 10:16:22,059 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:16:22,059 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:16:22,087 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:16:22,520 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-30 10:16:22,840 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-06-30 10:16:23,636 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:16:23,741 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-30 10:16:23,932 [main] INFO util.log: Logging initialized @28921ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-30 10:16:24,459 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-30 10:16:24,500 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-30 10:16:24,553 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-30 10:16:24,573 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-30 10:16:24,573 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-30 10:16:24,573 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-30 10:16:24,740 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-06-30 10:16:24,743 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-30 10:16:24,744 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-06-30 10:16:25,005 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-30 10:16:25,011 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-30 10:16:25,013 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-06-30 10:16:25,069 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7e24a2e2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-30 10:16:25,070 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e349258{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-30 10:16:25,622 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@729de0e1{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-9740245162783040607/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-30 10:16:25,681 [main] INFO server.AbstractConnector: Started ServerConnector@457d3f54{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-30 10:16:25,682 [main] INFO server.Server: Started @30670ms
datanode_1          | 2023-06-30 10:16:25,710 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-30 10:16:25,710 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-30 10:16:25,711 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:16:26,010 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-06-30 10:16:26,261 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-06-30 10:16:26,274 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-06-30 10:16:27,359 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-06-30 10:16:27,359 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-06-30 10:16:27,359 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-06-30 10:16:27,408 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-06-30 10:16:27,417 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-30 10:16:27,767 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.12:9891
datanode_2          | 2023-06-30 10:16:26,785 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-30 10:16:29,304 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:29,312 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:30,305 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:30,313 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:31,305 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:31,313 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.12:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:32,306 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:33,307 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:34,308 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:35,309 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:36,310 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:36,345 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From f0e5e42931dd/172.19.0.5 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:55560 remote=recon/172.19.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:55560 remote=recon/172.19.0.12:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-30 10:16:37,311 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:38,312 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:16:43,322 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From f0e5e42931dd/172.19.0.5 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:33712 remote=scm/172.19.0.10:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:33712 remote=scm/172.19.0.10:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-30 10:16:43,934 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f7626177-b26e-48a1-bda4-27683d55e918/DS-3271f261-1751-4bdb-9cb2-97720c190869/container.db to cache
datanode_2          | 2023-06-30 10:16:43,934 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f7626177-b26e-48a1-bda4-27683d55e918/DS-3271f261-1751-4bdb-9cb2-97720c190869/container.db for volume DS-3271f261-1751-4bdb-9cb2-97720c190869
datanode_2          | 2023-06-30 10:16:43,941 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-30 10:16:43,946 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-06-30 10:16:44,015 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-06-30 10:16:44,015 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d2a04bcb-d332-412c-bff4-3560f6749473
datanode_2          | 2023-06-30 10:16:44,034 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.RaftServer: d2a04bcb-d332-412c-bff4-3560f6749473: start RPC server
datanode_2          | 2023-06-30 10:16:44,041 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: d2a04bcb-d332-412c-bff4-3560f6749473: GrpcService started, listening on 9858
datanode_2          | 2023-06-30 10:16:44,042 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: d2a04bcb-d332-412c-bff4-3560f6749473: GrpcService started, listening on 9856
datanode_2          | 2023-06-30 10:16:44,044 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: d2a04bcb-d332-412c-bff4-3560f6749473: GrpcService started, listening on 9857
datanode_2          | 2023-06-30 10:16:44,053 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d2a04bcb-d332-412c-bff4-3560f6749473 is started using port 9858 for RATIS
datanode_2          | 2023-06-30 10:16:44,053 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d2a04bcb-d332-412c-bff4-3560f6749473 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-30 10:16:44,053 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d2a04bcb-d332-412c-bff4-3560f6749473 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-30 10:16:44,054 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d2a04bcb-d332-412c-bff4-3560f6749473: Started
datanode_2          | 2023-06-30 10:16:44,089 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:16:53,941 [grpc-default-executor-1] INFO server.RaftServer: d2a04bcb-d332-412c-bff4-3560f6749473: addNew group-D309D49F7363:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-D309D49F7363:java.util.concurrent.CompletableFuture@34db918d[Not completed]
datanode_1          | 2023-06-30 10:16:27,981 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-30 10:16:30,717 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:30,722 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:31,723 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:32,724 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:33,724 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:34,725 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:35,726 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:35,757 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From ef09c5769810/172.19.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.7:55078 remote=recon/172.19.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.7:55078 remote=recon/172.19.0.12:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-30 10:16:36,727 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:37,728 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:16:42,739 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From ef09c5769810/172.19.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.7:32774 remote=scm/172.19.0.10:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 2023-06-30 10:16:54,127 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473: new RaftServerImpl for group-D309D49F7363:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:16:54,131 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:16:54,135 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:16:54,135 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:16:54,138 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:16:54,138 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:16:54,139 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:16:54,174 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: ConfigurationManager, init=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:16:54,174 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:16:54,193 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:16:54,194 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:16:54,254 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:16:54,267 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-30 10:16:54,278 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:16:54,281 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:16:54,355 [grpc-default-executor-0] INFO server.RaftServer: d2a04bcb-d332-412c-bff4-3560f6749473: addNew group-75D1BC6DB22B:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER] returns group-75D1BC6DB22B:java.util.concurrent.CompletableFuture@3bbc3c8c[Not completed]
datanode_2          | 2023-06-30 10:16:54,388 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-30 10:16:54,423 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:16:54,435 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:16:54,438 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-30 10:16:54,442 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-30 10:16:54,456 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-30 10:16:54,456 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-30 10:16:54,469 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473: new RaftServerImpl for group-75D1BC6DB22B:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:16:54,470 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:16:54,470 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:16:54,470 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:16:54,470 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:16:54,472 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:16:54,472 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:16:54,473 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: ConfigurationManager, init=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.7:32774 remote=scm/172.19.0.10:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-30 10:16:43,994 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f7626177-b26e-48a1-bda4-27683d55e918/DS-4e46bc56-4445-4eb0-9f53-9d408c4c6aa2/container.db to cache
datanode_1          | 2023-06-30 10:16:43,999 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f7626177-b26e-48a1-bda4-27683d55e918/DS-4e46bc56-4445-4eb0-9f53-9d408c4c6aa2/container.db for volume DS-4e46bc56-4445-4eb0-9f53-9d408c4c6aa2
datanode_1          | 2023-06-30 10:16:44,002 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-30 10:16:44,018 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-06-30 10:16:44,160 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-06-30 10:16:44,160 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4898f574-1037-4fa8-9e86-7f94f4ca3111
datanode_1          | 2023-06-30 10:16:44,210 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.RaftServer: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start RPC server
datanode_1          | 2023-06-30 10:16:44,217 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: 4898f574-1037-4fa8-9e86-7f94f4ca3111: GrpcService started, listening on 9858
datanode_1          | 2023-06-30 10:16:44,218 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: 4898f574-1037-4fa8-9e86-7f94f4ca3111: GrpcService started, listening on 9856
datanode_1          | 2023-06-30 10:16:44,220 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: 4898f574-1037-4fa8-9e86-7f94f4ca3111: GrpcService started, listening on 9857
datanode_1          | 2023-06-30 10:16:44,221 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4898f574-1037-4fa8-9e86-7f94f4ca3111 is started using port 9858 for RATIS
datanode_1          | 2023-06-30 10:16:44,222 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4898f574-1037-4fa8-9e86-7f94f4ca3111 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-30 10:16:44,222 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4898f574-1037-4fa8-9e86-7f94f4ca3111 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-30 10:16:44,222 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4898f574-1037-4fa8-9e86-7f94f4ca3111: Started
datanode_1          | 2023-06-30 10:16:44,298 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:16:48,630 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4898f574-1037-4fa8-9e86-7f94f4ca3111: addNew group-39DAE38C5442:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:1|startupRole:FOLLOWER] returns group-39DAE38C5442:java.util.concurrent.CompletableFuture@20394d76[Not completed]
datanode_1          | 2023-06-30 10:16:48,713 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111: new RaftServerImpl for group-39DAE38C5442:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:16:48,719 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:16:48,721 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:16:48,721 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:16:48,722 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:16:48,724 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:16:48,724 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:16:48,745 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442: ConfigurationManager, init=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:16:48,746 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:16:48,761 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:16:48,765 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:16:48,811 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:16:48,820 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-30 10:16:48,843 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:16:48,846 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:16:48,952 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-30 10:16:49,050 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:16:49,070 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:16:49,072 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:16:49,075 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:16:49,078 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:16:49,080 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:16:49,082 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/57c9d9bb-d884-4f1a-8cfe-39dae38c5442 does not exist. Creating ...
datanode_1          | 2023-06-30 10:16:49,099 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/57c9d9bb-d884-4f1a-8cfe-39dae38c5442/in_use.lock acquired by nodename 6@ef09c5769810
datanode_1          | 2023-06-30 10:16:49,119 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/57c9d9bb-d884-4f1a-8cfe-39dae38c5442 has been successfully formatted.
datanode_1          | 2023-06-30 10:16:49,192 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO ratis.ContainerStateMachine: group-39DAE38C5442: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:16:49,285 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:16:49,384 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:16:49,392 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:49,403 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:16:49,408 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:16:49,463 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:49,491 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:16:49,507 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:16:49,510 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:49,551 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/57c9d9bb-d884-4f1a-8cfe-39dae38c5442
datanode_1          | 2023-06-30 10:16:49,556 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:16:49,558 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:16:49,566 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:49,566 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:16:49,566 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:16:49,570 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:16:49,572 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:16:49,575 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:16:49,621 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:49,623 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:49,683 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:16:49,686 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:16:49,718 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:16:49,767 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-30 10:16:03,786 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 713d54bfe484/172.19.0.6
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-06-30 10:16:03,828 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-30 10:16:04,049 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-30 10:16:04,576 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-30 10:16:05,316 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-30 10:16:05,316 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-30 10:16:06,096 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:713d54bfe484 ip:172.19.0.6
datanode_3          | 2023-06-30 10:16:07,006 [main] INFO reflections.Reflections: Reflections took 656 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_3          | 2023-06-30 10:16:09,764 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-06-30 10:16:10,031 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-30 10:16:11,253 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-30 10:16:11,392 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-30 10:16:11,407 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-30 10:16:11,450 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-30 10:16:11,630 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:16:11,647 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:16:11,698 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-30 10:16:11,712 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-30 10:16:11,712 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-30 10:16:11,716 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-30 10:16:11,840 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:16:11,841 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-30 10:16:18,512 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-30 10:16:18,921 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:16:19,173 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-30 10:16:19,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-30 10:16:19,854 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-30 10:16:19,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-30 10:16:19,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-30 10:16:19,871 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-30 10:16:19,873 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-30 10:16:19,880 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-30 10:16:19,880 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:19,885 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-30 10:16:19,890 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:16:19,963 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-30 10:16:19,981 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-30 10:16:20,012 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-30 10:16:21,938 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-30 10:16:21,972 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-30 10:16:21,972 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-30 10:16:21,972 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:16:21,972 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:16:21,997 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:16:22,175 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-30 10:16:22,663 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-06-30 10:16:23,553 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:16:23,621 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-30 10:16:23,770 [main] INFO util.log: Logging initialized @28974ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-30 10:16:24,270 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-30 10:16:24,320 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-30 10:16:24,360 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-30 10:16:24,373 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-30 10:16:24,381 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-30 10:16:24,381 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-30 10:16:24,572 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-06-30 10:16:24,591 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-30 10:16:24,598 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-06-30 10:16:24,786 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-30 10:16:24,786 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-30 10:16:24,789 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-30 10:16:24,860 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7528089c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-30 10:16:24,872 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6f9b5f01{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-30 10:16:25,630 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@13f10967{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16968930016954792499/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-30 10:16:25,743 [main] INFO server.AbstractConnector: Started ServerConnector@5f1a3c4{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-30 10:16:25,744 [main] INFO server.Server: Started @30947ms
datanode_3          | 2023-06-30 10:16:25,751 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-30 10:16:25,751 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-30 10:16:25,756 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:16:25,919 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-06-30 10:16:26,023 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-06-30 10:16:26,041 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-06-30 10:16:27,293 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-06-30 10:16:27,294 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-06-30 10:16:27,313 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-06-30 10:16:27,315 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-06-30 10:16:27,343 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-30 10:16:27,683 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.12:9891
datanode_1          | 2023-06-30 10:16:49,770 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:16:49,787 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442: start as a follower, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:49,791 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:16:49,799 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState
datanode_1          | 2023-06-30 10:16:49,820 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-39DAE38C5442,id=4898f574-1037-4fa8-9e86-7f94f4ca3111
datanode_1          | 2023-06-30 10:16:49,824 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:16:49,824 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:16:49,833 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:16:49,843 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:16:49,878 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:16:49,880 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:16:49,958 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=57c9d9bb-d884-4f1a-8cfe-39dae38c5442
datanode_1          | 2023-06-30 10:16:49,959 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=57c9d9bb-d884-4f1a-8cfe-39dae38c5442.
datanode_1          | 2023-06-30 10:16:49,959 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4898f574-1037-4fa8-9e86-7f94f4ca3111: addNew group-D309D49F7363:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-D309D49F7363:java.util.concurrent.CompletableFuture@6d341a90[Not completed]
datanode_1          | 2023-06-30 10:16:50,007 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111: new RaftServerImpl for group-D309D49F7363:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:16:50,007 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:16:50,007 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:16:50,007 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:16:50,007 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:16:50,007 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:16:50,011 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:16:50,011 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: ConfigurationManager, init=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:16:50,011 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:16:50,016 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:16:50,016 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:16:50,017 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:16:50,017 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-30 10:16:50,017 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:16:50,017 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:16:50,019 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-30 10:16:50,026 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:16:50,026 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:16:54,473 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:16:54,473 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:16:54,473 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:16:54,476 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:16:54,476 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-30 10:16:54,476 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:16:54,477 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:16:54,477 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-30 10:16:54,482 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:16:54,483 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:16:54,483 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-30 10:16:54,483 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-30 10:16:54,483 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-30 10:16:54,483 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-30 10:16:54,483 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363 does not exist. Creating ...
datanode_2          | 2023-06-30 10:16:54,496 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363/in_use.lock acquired by nodename 7@f0e5e42931dd
datanode_2          | 2023-06-30 10:16:54,502 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363 has been successfully formatted.
datanode_2          | 2023-06-30 10:16:54,511 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO ratis.ContainerStateMachine: group-D309D49F7363: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:16:54,534 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:16:54,598 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:16:54,599 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:16:54,600 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-30 10:16:54,601 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-30 10:16:54,607 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:16:54,618 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:16:54,621 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:16:54,621 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:16:54,640 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363
datanode_2          | 2023-06-30 10:16:54,640 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-30 10:16:54,641 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:16:54,644 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:16:54,647 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:16:54,647 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:16:54,650 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:16:54,650 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:16:54,650 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:16:54,665 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:16:54,669 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:16:54,746 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:16:54,746 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:16:54,747 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:16:54,762 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:16:50,026 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:16:50,026 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:16:50,027 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:16:50,027 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:16:50,030 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363 does not exist. Creating ...
datanode_1          | 2023-06-30 10:16:50,033 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363/in_use.lock acquired by nodename 6@ef09c5769810
datanode_1          | 2023-06-30 10:16:50,044 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363 has been successfully formatted.
datanode_1          | 2023-06-30 10:16:50,044 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO ratis.ContainerStateMachine: group-D309D49F7363: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:16:50,044 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:16:50,045 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:16:50,045 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:50,045 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:16:50,045 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:16:50,045 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:16:50,066 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:16:50,067 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:16:50,067 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:16:50,069 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:50,075 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:50,559 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-4898f574-1037-4fa8-9e86-7f94f4ca3111: Detected pause in JVM or host machine approximately 0.313s with 0.481s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=56ms
datanode_1          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=425ms
datanode_1          | 2023-06-30 10:16:50,569 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:16:50,569 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:16:50,569 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:16:50,569 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:16:50,569 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:16:50,571 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: start as a follower, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:50,572 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:16:50,572 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState
datanode_2          | 2023-06-30 10:16:54,763 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:16:54,764 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: start as a follower, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:16:54,764 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:16:54,765 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FollowerState
datanode_2          | 2023-06-30 10:16:54,775 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D309D49F7363,id=d2a04bcb-d332-412c-bff4-3560f6749473
datanode_2          | 2023-06-30 10:16:54,776 [d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:16:54,782 [d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:16:54,777 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:16:54,782 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:16:54,782 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:16:54,783 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:16:54,808 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b does not exist. Creating ...
datanode_2          | 2023-06-30 10:16:54,811 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b/in_use.lock acquired by nodename 7@f0e5e42931dd
datanode_2          | 2023-06-30 10:16:54,813 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b has been successfully formatted.
datanode_2          | 2023-06-30 10:16:54,814 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO ratis.ContainerStateMachine: group-75D1BC6DB22B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:16:54,815 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:16:54,815 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:16:54,816 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:16:54,816 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-30 10:16:54,816 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-30 10:16:54,817 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:16:54,817 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:16:54,817 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:16:54,817 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:16:54,817 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b
datanode_2          | 2023-06-30 10:16:54,817 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-30 10:16:54,817 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:16:54,818 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:16:54,818 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:16:54,818 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:16:54,818 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:16:54,818 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:16:54,819 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:16:54,819 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:16:54,836 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:16:55,357 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d2a04bcb-d332-412c-bff4-3560f6749473: Detected pause in JVM or host machine approximately 0.255s with 0.511s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=65ms
datanode_2          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=446ms
datanode_2          | 2023-06-30 10:16:55,374 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:16:55,375 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:16:55,375 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:16:55,377 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:16:55,378 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:16:55,378 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: start as a follower, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:16:55,378 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:16:55,378 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState
datanode_2          | 2023-06-30 10:16:55,381 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-75D1BC6DB22B,id=d2a04bcb-d332-412c-bff4-3560f6749473
datanode_2          | 2023-06-30 10:16:55,381 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:16:55,381 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:16:55,381 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:16:55,381 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:16:55,387 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:16:55,431 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:16:55,708 [grpc-default-executor-0] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: receive requestVote(PRE_VOTE, a4810344-5fc9-4e68-93f9-7973f1df7364, group-D309D49F7363, 0, (t:0, i:0))
datanode_2          | 2023-06-30 10:16:55,710 [grpc-default-executor-0] INFO impl.VoteContext: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FOLLOWER: accept PRE_VOTE from a4810344-5fc9-4e68-93f9-7973f1df7364: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-30 10:16:55,752 [grpc-default-executor-0] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363 replies to PRE_VOTE vote request: a4810344-5fc9-4e68-93f9-7973f1df7364<-d2a04bcb-d332-412c-bff4-3560f6749473#0:OK-t0. Peer's state: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363:t0, leader=null, voted=, raftlog=Memoized:d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:16:56,080 [grpc-default-executor-0] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: receive requestVote(ELECTION, a4810344-5fc9-4e68-93f9-7973f1df7364, group-D309D49F7363, 1, (t:0, i:0))
datanode_2          | 2023-06-30 10:16:56,082 [grpc-default-executor-0] INFO impl.VoteContext: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FOLLOWER: accept ELECTION from a4810344-5fc9-4e68-93f9-7973f1df7364: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-30 10:16:56,082 [grpc-default-executor-0] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_2          | 2023-06-30 10:16:56,082 [grpc-default-executor-0] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: shutdown d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FollowerState
datanode_2          | 2023-06-30 10:16:56,085 [d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FollowerState] INFO impl.FollowerState: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FollowerState was interrupted
datanode_2          | 2023-06-30 10:16:56,086 [grpc-default-executor-0] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-FollowerState
datanode_2          | 2023-06-30 10:16:56,094 [grpc-default-executor-0] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363 replies to ELECTION vote request: a4810344-5fc9-4e68-93f9-7973f1df7364<-d2a04bcb-d332-412c-bff4-3560f6749473#0:OK-t1. Peer's state: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363:t1, leader=null, voted=a4810344-5fc9-4e68-93f9-7973f1df7364, raftlog=Memoized:d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:16:56,509 [d2a04bcb-d332-412c-bff4-3560f6749473-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D309D49F7363 with new leaderId: a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_2          | 2023-06-30 10:16:56,509 [d2a04bcb-d332-412c-bff4-3560f6749473-server-thread2] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: change Leader from null to a4810344-5fc9-4e68-93f9-7973f1df7364 at term 1 for appendEntries, leader elected after 2255ms
datanode_1          | 2023-06-30 10:16:50,573 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D309D49F7363,id=4898f574-1037-4fa8-9e86-7f94f4ca3111
datanode_1          | 2023-06-30 10:16:50,574 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:16:50,574 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:16:50,574 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:16:50,574 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:16:50,575 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:16:50,583 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:16:50,583 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363
datanode_1          | 2023-06-30 10:16:52,481 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-4898f574-1037-4fa8-9e86-7f94f4ca3111: Detected pause in JVM or host machine approximately 0.421s with 0.445s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=445ms
datanode_1          | 2023-06-30 10:16:54,145 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363.
datanode_1          | 2023-06-30 10:16:54,146 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4898f574-1037-4fa8-9e86-7f94f4ca3111: addNew group-75D1BC6DB22B:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER] returns group-75D1BC6DB22B:java.util.concurrent.CompletableFuture@5695ac73[Not completed]
datanode_1          | 2023-06-30 10:16:54,151 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111: new RaftServerImpl for group-75D1BC6DB22B:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:16:54,151 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:16:54,151 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:16:54,151 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:16:54,151 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:16:54,151 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:16:54,154 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: ConfigurationManager, init=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:16:54,155 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-30 10:16:54,164 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:16:54,164 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:16:54,164 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:16:54,164 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:16:54,164 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:16:54,164 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:16:54,168 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b does not exist. Creating ...
datanode_1          | 2023-06-30 10:16:54,174 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b/in_use.lock acquired by nodename 6@ef09c5769810
datanode_3          | 2023-06-30 10:16:27,884 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-30 10:16:30,676 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:30,684 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:31,677 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:31,685 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:32,686 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:33,686 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:34,687 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:35,688 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:36,689 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:36,708 [EndpointStateMachine task thread for recon/172.19.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 713d54bfe484/172.19.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.6:40292 remote=recon/172.19.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.6:40292 remote=recon/172.19.0.12:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-30 10:16:37,690 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.10:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:16:42,705 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 713d54bfe484/172.19.0.6 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.6:53388 remote=scm/172.19.0.10:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 2023-06-30 10:16:54,179 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b has been successfully formatted.
datanode_1          | 2023-06-30 10:16:54,179 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO ratis.ContainerStateMachine: group-75D1BC6DB22B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:16:54,181 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:16:54,181 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:16:54,182 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:16:54,204 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:54,204 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:16:54,204 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:16:54,204 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:16:54,204 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:16:54,204 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:16:54,205 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:16:54,205 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:16:54,291 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:16:54,292 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:16:54,293 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:16:54,297 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:16:54,298 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:16:54,300 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: start as a follower, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:54,301 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:16:54,301 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState
datanode_1          | 2023-06-30 10:16:54,302 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-75D1BC6DB22B,id=4898f574-1037-4fa8-9e86-7f94f4ca3111
datanode_1          | 2023-06-30 10:16:54,302 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:16:54,302 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:16:54,302 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:16:54,302 [4898f574-1037-4fa8-9e86-7f94f4ca3111-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:16:54,303 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:16:54,305 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b
datanode_1          | 2023-06-30 10:16:54,328 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:16:54,915 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState] INFO impl.FollowerState: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5119725608ns, electionTimeout:5090ms
datanode_1          | 2023-06-30 10:16:54,915 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: shutdown 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState
datanode_1          | 2023-06-30 10:16:54,915 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:16:54,918 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-30 10:16:54,918 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-FollowerState] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1
datanode_1          | 2023-06-30 10:16:54,920 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO impl.LeaderElection: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:54,921 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO impl.LeaderElection: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-06-30 10:16:54,923 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO impl.LeaderElection: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:54,923 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO impl.LeaderElection: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-30 10:16:54,923 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: shutdown 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1
datanode_1          | 2023-06-30 10:16:54,925 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-30 10:16:54,925 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-39DAE38C5442 with new leaderId: 4898f574-1037-4fa8-9e86-7f94f4ca3111
datanode_1          | 2023-06-30 10:16:54,931 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442: change Leader from null to 4898f574-1037-4fa8-9e86-7f94f4ca3111 at term 1 for becomeLeader, leader elected after 6114ms
datanode_1          | 2023-06-30 10:16:54,963 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:16:54,989 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:16:55,000 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-30 10:16:55,019 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:16:55,020 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:16:55,021 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:16:55,051 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:16:55,057 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-30 10:16:55,067 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderStateImpl
datanode_1          | 2023-06-30 10:16:55,124 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:16:55,221 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-LeaderElection1] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442: set configuration 0: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:55,389 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-39DAE38C5442-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/57c9d9bb-d884-4f1a-8cfe-39dae38c5442/current/log_inprogress_0
datanode_1          | 2023-06-30 10:16:55,730 [grpc-default-executor-0] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: receive requestVote(PRE_VOTE, a4810344-5fc9-4e68-93f9-7973f1df7364, group-D309D49F7363, 0, (t:0, i:0))
datanode_1          | 2023-06-30 10:16:55,744 [grpc-default-executor-0] INFO impl.VoteContext: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FOLLOWER: accept PRE_VOTE from a4810344-5fc9-4e68-93f9-7973f1df7364: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:16:55,777 [grpc-default-executor-0] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363 replies to PRE_VOTE vote request: a4810344-5fc9-4e68-93f9-7973f1df7364<-4898f574-1037-4fa8-9e86-7f94f4ca3111#0:OK-t0. Peer's state: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363:t0, leader=null, voted=, raftlog=Memoized:4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:16:56,526 [d2a04bcb-d332-412c-bff4-3560f6749473-server-thread2] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363: set configuration 0: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:16:56,573 [d2a04bcb-d332-412c-bff4-3560f6749473-server-thread2] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:16:56,867 [d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-D309D49F7363-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363/current/log_inprogress_0
datanode_2          | 2023-06-30 10:16:59,609 [grpc-default-executor-0] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: receive requestVote(PRE_VOTE, 4898f574-1037-4fa8-9e86-7f94f4ca3111, group-75D1BC6DB22B, 0, (t:0, i:0))
datanode_2          | 2023-06-30 10:16:59,609 [grpc-default-executor-0] INFO impl.VoteContext: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FOLLOWER: reject PRE_VOTE from 4898f574-1037-4fa8-9e86-7f94f4ca3111: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-30 10:16:59,609 [grpc-default-executor-0] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B replies to PRE_VOTE vote request: 4898f574-1037-4fa8-9e86-7f94f4ca3111<-d2a04bcb-d332-412c-bff4-3560f6749473#0:FAIL-t0. Peer's state: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B:t0, leader=null, voted=, raftlog=Memoized:d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:00,439 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState] INFO impl.FollowerState: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5061379883ns, electionTimeout:5007ms
datanode_2          | 2023-06-30 10:17:00,440 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: shutdown d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState
datanode_2          | 2023-06-30 10:17:00,440 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:17:00,443 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-30 10:17:00,443 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-FollowerState] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1
datanode_2          | 2023-06-30 10:17:00,446 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:00,450 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 4898f574-1037-4fa8-9e86-7f94f4ca3111
datanode_2          | 2023-06-30 10:17:00,454 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:17:00,464 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:17:00,454 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_2          | 2023-06-30 10:17:00,795 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-30 10:17:00,795 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection:   Response 0: d2a04bcb-d332-412c-bff4-3560f6749473<-4898f574-1037-4fa8-9e86-7f94f4ca3111#0:OK-t0
datanode_2          | 2023-06-30 10:17:00,795 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_2          | 2023-06-30 10:17:00,803 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:00,810 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:17:00,810 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:17:00,828 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-30 10:17:00,829 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection:   Response 0: d2a04bcb-d332-412c-bff4-3560f6749473<-a4810344-5fc9-4e68-93f9-7973f1df7364#0:OK-t1
datanode_2          | 2023-06-30 10:17:00,829 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1 ELECTION round 0: result PASSED
datanode_2          | 2023-06-30 10:17:00,829 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: shutdown d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1
datanode_2          | 2023-06-30 10:17:00,829 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-30 10:17:00,829 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-75D1BC6DB22B with new leaderId: d2a04bcb-d332-412c-bff4-3560f6749473
datanode_2          | 2023-06-30 10:17:00,832 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: change Leader from null to d2a04bcb-d332-412c-bff4-3560f6749473 at term 1 for becomeLeader, leader elected after 6352ms
datanode_2          | 2023-06-30 10:17:00,835 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:17:00,870 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:17:00,872 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-30 10:17:00,883 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:17:00,883 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:17:00,883 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:17:00,896 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:17:00,898 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-30 10:17:00,922 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-30 10:17:00,922 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:17:00,922 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-30 10:17:00,925 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-30 10:17:00,928 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-30 10:17:00,928 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:17:00,928 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-06-30 10:17:00,928 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-06-30 10:17:00,929 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:17:00,929 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:17:00,934 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-30 10:17:00,939 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderStateImpl
datanode_2          | 2023-06-30 10:17:00,941 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:17:00,945 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b/current/log_inprogress_0
datanode_2          | 2023-06-30 10:17:00,957 [d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B-LeaderElection1] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-75D1BC6DB22B: set configuration 0: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:17,215 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473: new RaftServerImpl for group-7CDB92B9D242:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:17:17,216 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:17:17,216 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:17:17,216 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:17:17,217 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:17:17,217 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:17:17,217 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:17:17,217 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242: ConfigurationManager, init=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:17:17,217 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:17:17,217 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:17:17,220 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:17:17,220 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:17:17,220 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-30 10:17:17,220 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:17:17,220 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:17:17,220 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-30 10:17:17,221 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:17:17,223 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:17:17,223 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:16:55,777 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:16:55,777 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:16:56,032 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b.
datanode_1          | 2023-06-30 10:16:56,062 [grpc-default-executor-0] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: receive requestVote(ELECTION, a4810344-5fc9-4e68-93f9-7973f1df7364, group-D309D49F7363, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:16:56,062 [grpc-default-executor-0] INFO impl.VoteContext: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FOLLOWER: accept ELECTION from a4810344-5fc9-4e68-93f9-7973f1df7364: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:16:56,063 [grpc-default-executor-0] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_1          | 2023-06-30 10:16:56,063 [grpc-default-executor-0] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: shutdown 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState
datanode_1          | 2023-06-30 10:16:56,063 [grpc-default-executor-0] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState
datanode_1          | 2023-06-30 10:16:56,063 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState] INFO impl.FollowerState: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-FollowerState was interrupted
datanode_1          | 2023-06-30 10:16:56,074 [grpc-default-executor-0] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363 replies to ELECTION vote request: a4810344-5fc9-4e68-93f9-7973f1df7364<-4898f574-1037-4fa8-9e86-7f94f4ca3111#0:OK-t1. Peer's state: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363:t1, leader=null, voted=a4810344-5fc9-4e68-93f9-7973f1df7364, raftlog=Memoized:4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:56,534 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D309D49F7363 with new leaderId: a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_1          | 2023-06-30 10:16:56,534 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: change Leader from null to a4810344-5fc9-4e68-93f9-7973f1df7364 at term 1 for appendEntries, leader elected after 6517ms
datanode_1          | 2023-06-30 10:16:56,537 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363: set configuration 0: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:56,544 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:16:56,557 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-D309D49F7363-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363/current/log_inprogress_0
datanode_1          | 2023-06-30 10:16:59,530 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO impl.FollowerState: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5228985183ns, electionTimeout:5196ms
datanode_1          | 2023-06-30 10:16:59,530 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: shutdown 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState
datanode_1          | 2023-06-30 10:16:59,531 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:16:59,531 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-30 10:16:59,531 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2
datanode_1          | 2023-06-30 10:16:59,532 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO impl.LeaderElection: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:16:59,541 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:16:59,541 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:16:59,543 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for d2a04bcb-d332-412c-bff4-3560f6749473
datanode_1          | 2023-06-30 10:16:59,543 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_1          | 2023-06-30 10:16:59,625 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO impl.LeaderElection: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:16:59,625 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO impl.LeaderElection:   Response 0: 4898f574-1037-4fa8-9e86-7f94f4ca3111<-d2a04bcb-d332-412c-bff4-3560f6749473#0:FAIL-t0
datanode_1          | 2023-06-30 10:16:59,625 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO impl.LeaderElection:   Response 1: 4898f574-1037-4fa8-9e86-7f94f4ca3111<-a4810344-5fc9-4e68-93f9-7973f1df7364#0:OK-t0
datanode_1          | 2023-06-30 10:16:59,625 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO impl.LeaderElection: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2 PRE_VOTE round 0: result REJECTED
datanode_1          | 2023-06-30 10:16:59,625 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_1          | 2023-06-30 10:16:59,625 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: shutdown 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2
datanode_1          | 2023-06-30 10:16:59,625 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-LeaderElection2] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState
datanode_1          | 2023-06-30 10:17:00,770 [grpc-default-executor-2] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: receive requestVote(PRE_VOTE, d2a04bcb-d332-412c-bff4-3560f6749473, group-75D1BC6DB22B, 0, (t:0, i:0))
datanode_1          | 2023-06-30 10:17:00,771 [grpc-default-executor-2] INFO impl.VoteContext: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FOLLOWER: accept PRE_VOTE from d2a04bcb-d332-412c-bff4-3560f6749473: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:17:00,772 [grpc-default-executor-2] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B replies to PRE_VOTE vote request: d2a04bcb-d332-412c-bff4-3560f6749473<-4898f574-1037-4fa8-9e86-7f94f4ca3111#0:OK-t0. Peer's state: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B:t0, leader=null, voted=, raftlog=Memoized:4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:17:00,814 [grpc-default-executor-2] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: receive requestVote(ELECTION, d2a04bcb-d332-412c-bff4-3560f6749473, group-75D1BC6DB22B, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:17:00,814 [grpc-default-executor-2] INFO impl.VoteContext: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FOLLOWER: accept ELECTION from d2a04bcb-d332-412c-bff4-3560f6749473: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:17:00,814 [grpc-default-executor-2] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d2a04bcb-d332-412c-bff4-3560f6749473
datanode_1          | 2023-06-30 10:17:00,814 [grpc-default-executor-2] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: shutdown 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState
datanode_1          | 2023-06-30 10:17:00,814 [grpc-default-executor-2] INFO impl.RoleInfo: 4898f574-1037-4fa8-9e86-7f94f4ca3111: start 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState
datanode_1          | 2023-06-30 10:17:00,815 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState] INFO impl.FollowerState: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-FollowerState was interrupted
datanode_1          | 2023-06-30 10:17:00,817 [grpc-default-executor-2] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B replies to ELECTION vote request: d2a04bcb-d332-412c-bff4-3560f6749473<-4898f574-1037-4fa8-9e86-7f94f4ca3111#0:OK-t1. Peer's state: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B:t1, leader=null, voted=d2a04bcb-d332-412c-bff4-3560f6749473, raftlog=Memoized:4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:17:01,067 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-75D1BC6DB22B with new leaderId: d2a04bcb-d332-412c-bff4-3560f6749473
datanode_1          | 2023-06-30 10:17:01,068 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: change Leader from null to d2a04bcb-d332-412c-bff4-3560f6749473 at term 1 for appendEntries, leader elected after 6912ms
datanode_1          | 2023-06-30 10:17:01,075 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO server.RaftServer$Division: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B: set configuration 0: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:17:01,077 [4898f574-1037-4fa8-9e86-7f94f4ca3111-server-thread1] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:17:01,079 [4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4898f574-1037-4fa8-9e86-7f94f4ca3111@group-75D1BC6DB22B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b/current/log_inprogress_0
datanode_1          | 2023-06-30 10:17:15,865 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-4898f574-1037-4fa8-9e86-7f94f4ca3111: Detected pause in JVM or host machine approximately 0.346s with 0.595s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=595ms
datanode_1          | 2023-06-30 10:17:44,299 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:18:44,299 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:19:44,300 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:20:44,300 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:21:44,301 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:17:17,223 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-30 10:17:17,223 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-30 10:17:17,223 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-30 10:17:17,224 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d2a04bcb-d332-412c-bff4-3560f6749473: addNew group-7CDB92B9D242:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] returns      null d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
datanode_2          | 2023-06-30 10:17:17,225 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242 does not exist. Creating ...
datanode_2          | 2023-06-30 10:17:17,227 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242/in_use.lock acquired by nodename 7@f0e5e42931dd
datanode_2          | 2023-06-30 10:17:17,228 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242 has been successfully formatted.
datanode_2          | 2023-06-30 10:17:17,230 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO ratis.ContainerStateMachine: group-7CDB92B9D242: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:17:17,230 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:17:17,230 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:17:17,230 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:17:17,230 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-30 10:17:17,230 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-30 10:17:17,230 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:17:17,231 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:17:17,231 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:17:17,231 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:17:17,235 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.6:53388 remote=scm/172.19.0.10:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-30 10:16:43,960 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f7626177-b26e-48a1-bda4-27683d55e918/DS-958dbb59-a1e3-405b-8406-b67a4c169bfb/container.db to cache
datanode_3          | 2023-06-30 10:16:43,960 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f7626177-b26e-48a1-bda4-27683d55e918/DS-958dbb59-a1e3-405b-8406-b67a4c169bfb/container.db for volume DS-958dbb59-a1e3-405b-8406-b67a4c169bfb
datanode_3          | 2023-06-30 10:16:43,983 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-30 10:16:43,992 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-06-30 10:16:44,144 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-06-30 10:16:44,148 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_3          | 2023-06-30 10:16:44,189 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.RaftServer: a4810344-5fc9-4e68-93f9-7973f1df7364: start RPC server
datanode_3          | 2023-06-30 10:16:44,197 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: a4810344-5fc9-4e68-93f9-7973f1df7364: GrpcService started, listening on 9858
datanode_3          | 2023-06-30 10:16:44,199 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: a4810344-5fc9-4e68-93f9-7973f1df7364: GrpcService started, listening on 9856
datanode_3          | 2023-06-30 10:16:44,203 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO server.GrpcService: a4810344-5fc9-4e68-93f9-7973f1df7364: GrpcService started, listening on 9857
datanode_3          | 2023-06-30 10:16:44,212 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a4810344-5fc9-4e68-93f9-7973f1df7364 is started using port 9858 for RATIS
datanode_3          | 2023-06-30 10:16:44,212 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a4810344-5fc9-4e68-93f9-7973f1df7364 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-30 10:16:44,212 [EndpointStateMachine task thread for scm/172.19.0.10:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a4810344-5fc9-4e68-93f9-7973f1df7364 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-30 10:16:44,237 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a4810344-5fc9-4e68-93f9-7973f1df7364: Started
datanode_3          | 2023-06-30 10:16:44,254 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:16:48,678 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a4810344-5fc9-4e68-93f9-7973f1df7364: addNew group-FFE6261BA92C:[a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-FFE6261BA92C:java.util.concurrent.CompletableFuture@36b1bd3d[Not completed]
datanode_3          | 2023-06-30 10:16:48,758 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364: new RaftServerImpl for group-FFE6261BA92C:[a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:16:48,764 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:16:48,765 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:16:48,768 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:16:48,769 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:16:48,769 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:16:48,771 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:16:48,791 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C: ConfigurationManager, init=-1: peers:[a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:16:48,792 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:16:48,801 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:16:48,803 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:16:48,824 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:16:48,830 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-30 10:16:48,848 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:16:48,849 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:16:48,875 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-30 10:16:48,927 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:16:48,937 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:16:48,943 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-30 10:16:48,945 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-30 10:16:48,945 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-30 10:16:48,950 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-30 10:16:48,951 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/15fb554b-03cb-4515-a830-ffe6261ba92c does not exist. Creating ...
datanode_3          | 2023-06-30 10:16:48,967 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/15fb554b-03cb-4515-a830-ffe6261ba92c/in_use.lock acquired by nodename 7@713d54bfe484
datanode_3          | 2023-06-30 10:16:48,984 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/15fb554b-03cb-4515-a830-ffe6261ba92c has been successfully formatted.
datanode_3          | 2023-06-30 10:16:49,021 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO ratis.ContainerStateMachine: group-FFE6261BA92C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:16:49,040 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:16:49,060 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:16:49,063 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:49,065 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-30 10:16:49,068 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-30 10:16:49,077 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:49,098 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:16:49,100 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:16:49,100 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:49,141 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/15fb554b-03cb-4515-a830-ffe6261ba92c
datanode_3          | 2023-06-30 10:16:49,141 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-30 10:16:49,148 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:16:49,151 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:49,151 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:16:49,156 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:16:49,156 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:16:49,163 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:16:49,164 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:16:49,220 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:49,227 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:49,390 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:16:49,395 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:16:49,396 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:16:49,497 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:16:49,497 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:16:49,505 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C: start as a follower, conf=-1: peers:[a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:49,510 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:16:49,520 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState
datanode_3          | 2023-06-30 10:16:49,534 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:16:49,534 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:16:49,584 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FFE6261BA92C,id=a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_3          | 2023-06-30 10:16:49,592 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:16:49,596 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:16:49,596 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:16:49,597 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:16:49,691 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=15fb554b-03cb-4515-a830-ffe6261ba92c
datanode_3          | 2023-06-30 10:16:49,692 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=15fb554b-03cb-4515-a830-ffe6261ba92c.
datanode_3          | 2023-06-30 10:16:49,693 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a4810344-5fc9-4e68-93f9-7973f1df7364: addNew group-D309D49F7363:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-D309D49F7363:java.util.concurrent.CompletableFuture@8f91b49[Not completed]
datanode_3          | 2023-06-30 10:16:49,735 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364: new RaftServerImpl for group-D309D49F7363:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:16:49,736 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:16:49,736 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:16:49,736 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:16:49,741 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:16:49,741 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:16:49,741 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:16:49,743 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363: ConfigurationManager, init=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:16:49,744 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:16:49,756 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:16:49,768 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:16:49,775 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:16:49,775 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-30 10:16:49,777 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:16:49,777 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:16:49,778 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-30 10:16:49,784 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:16:49,784 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:16:49,789 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-30 10:16:49,792 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-30 10:16:49,792 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-30 10:16:49,792 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-30 10:16:49,796 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363 does not exist. Creating ...
datanode_3          | 2023-06-30 10:16:49,800 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363/in_use.lock acquired by nodename 7@713d54bfe484
datanode_3          | 2023-06-30 10:16:49,805 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363 has been successfully formatted.
datanode_3          | 2023-06-30 10:16:49,808 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO ratis.ContainerStateMachine: group-D309D49F7363: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:16:03,635 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = c623f9b3e3a7/172.19.0.9
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-30 10:16:03,422 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = e005690fc6b8/172.19.0.12
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
recon_1             | STARTUP_MSG:   java = 11.0.19
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
recon_1             | 2023-06-30 10:16:03,483 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-30 10:16:06,459 [main] INFO reflections.Reflections: Reflections took 365 ms to scan 1 urls, producing 20 keys and 75 values 
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-30 10:16:02,780 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-30 10:16:02,795 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-30 10:16:03,041 [main] INFO util.log: Logging initialized @9684ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-30 10:16:03,896 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-06-30 10:16:04,004 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-30 10:16:04,066 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-30 10:16:04,081 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-30 10:16:04,091 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-30 10:16:04,096 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-30 10:16:04,345 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir375248243786173481
s3g_1               | 2023-06-30 10:16:04,869 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 6d29dea2b8c3/172.19.0.4
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 2023-06-30 10:16:10,063 [main] INFO reflections.Reflections: Reflections took 417 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1             | 2023-06-30 10:16:10,489 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-30 10:16:12,753 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:16:18,416 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-30 10:16:20,198 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-30 10:16:20,215 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.011 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-30 10:16:20,542 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:16:20,704 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:16:20,709 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-30 10:16:23,721 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-30 10:16:23,784 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-30 10:16:23,855 [main] INFO util.log: Logging initialized @28857ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-30 10:16:24,339 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-30 10:16:24,393 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-06-30 10:16:24,425 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-30 10:16:24,442 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-30 10:16:24,442 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-30 10:16:24,442 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-30 10:16:24,729 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-06-30 10:16:24,746 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-30 10:16:25,660 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-30 10:16:25,717 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-06-30 10:16:25,746 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-30 10:16:25,902 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-30 10:16:25,902 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-30 10:16:28,847 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:16:29,348 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:16:29,673 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-06-30 10:16:29,675 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-30 10:16:29,991 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:16:30,205 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-06-30 10:16:30,252 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-30 10:16:30,312 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-30 10:16:30,320 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-30 10:16:30,332 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-06-30 10:16:30,650 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-30 10:16:30,678 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-30 10:16:30,707 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1             | 2023-06-30 10:16:30,710 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-30 10:16:30,768 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-30 10:16:30,854 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-06-30 10:16:30,856 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-30 10:16:30,856 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-30 10:16:30,935 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-30 10:16:30,971 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-30 10:16:30,971 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-30 10:16:31,303 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-30 10:16:31,307 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-06-30 10:16:31,403 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-30 10:16:31,409 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-30 10:16:31,411 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-06-30 10:16:31,437 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@404566e7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om_1                | STARTUP_MSG:   java = 11.0.19
datanode_2          | 2023-06-30 10:17:17,235 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-30 10:17:17,235 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:17:17,236 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:17:17,236 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:17:17,236 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:17:17,236 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:17:17,236 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:17:17,236 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:17:17,248 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:17:17,249 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:17:17,270 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:17:17,270 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:17:17,270 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:17:17,270 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:17:17,271 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:17:17,271 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242: start as a follower, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:17,271 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:17:17,271 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState
datanode_2          | 2023-06-30 10:17:17,272 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7CDB92B9D242,id=d2a04bcb-d332-412c-bff4-3560f6749473
datanode_2          | 2023-06-30 10:17:17,275 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:17:17,275 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:17:17,275 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:17:17,275 [d2a04bcb-d332-412c-bff4-3560f6749473-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:17:17,282 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:17:17,288 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:17:17,288 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242
datanode_2          | 2023-06-30 10:17:17,289 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242.
datanode_2          | 2023-06-30 10:17:22,402 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState] INFO impl.FollowerState: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5130702988ns, electionTimeout:5114ms
datanode_2          | 2023-06-30 10:17:22,403 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: shutdown d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState
datanode_2          | 2023-06-30 10:17:22,403 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:17:22,403 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-30 10:17:22,404 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-FollowerState] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2
datanode_2          | 2023-06-30 10:17:22,405 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:22,405 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-06-30 10:17:22,406 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:22,406 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO impl.LeaderElection: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-30 10:17:22,407 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: shutdown d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2
datanode_2          | 2023-06-30 10:17:22,407 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-30 10:17:22,407 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7CDB92B9D242 with new leaderId: d2a04bcb-d332-412c-bff4-3560f6749473
datanode_2          | 2023-06-30 10:17:22,407 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242: change Leader from null to d2a04bcb-d332-412c-bff4-3560f6749473 at term 1 for becomeLeader, leader elected after 5186ms
datanode_2          | 2023-06-30 10:17:22,407 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:17:22,407 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:17:22,408 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-30 10:17:22,408 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:17:22,408 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:17:22,408 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:17:22,408 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:17:22,408 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-30 10:17:22,408 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO impl.RoleInfo: d2a04bcb-d332-412c-bff4-3560f6749473: start d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderStateImpl
datanode_2          | 2023-06-30 10:17:22,409 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:17:22,410 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242/current/log_inprogress_0
datanode_2          | 2023-06-30 10:17:22,424 [d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242-LeaderElection2] INFO server.RaftServer$Division: d2a04bcb-d332-412c-bff4-3560f6749473@group-7CDB92B9D242: set configuration 0: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:17:44,089 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:18:44,096 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:19:44,096 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:20:44,096 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:21:44,097 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | 2023-06-30 10:16:31,439 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3d763ae5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-30 10:16:35,034 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1631da37{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-17224759975694736356/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-06-30 10:16:35,058 [main] INFO server.AbstractConnector: Started ServerConnector@4c5c0306{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-30 10:16:35,060 [main] INFO server.Server: Started @40060ms
recon_1             | 2023-06-30 10:16:35,066 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-30 10:16:35,066 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-30 10:16:35,071 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-30 10:16:35,071 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-30 10:16:35,088 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-30 10:16:35,094 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-30 10:16:35,094 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-30 10:16:35,094 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:16:35,097 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-30 10:16:35,101 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:16:37,252 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e005690fc6b8/172.19.0.12 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-06-30 10:16:39,313 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6ace6ea5-447f-4e9d-beaf-9e0258503780 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
recon_1             | 2023-06-30 10:16:41,317 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6ace6ea5-447f-4e9d-beaf-9e0258503780 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1             | 2023-06-30 10:16:43,329 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6ace6ea5-447f-4e9d-beaf-9e0258503780 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-30 10:16:03,698 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:16:09,260 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-30 10:16:11,000 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:16:11,411 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.9:9862
om_1                | 2023-06-30 10:16:11,412 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:16:11,412 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:16:11,710 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:16:12,827 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863]
om_1                | 2023-06-30 10:16:16,180 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-06-30 10:16:18,185 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-06-30 10:16:20,186 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-06-30 10:16:22,190 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-06-30 10:16:24,196 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-06-30 10:16:26,199 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om_1                | 2023-06-30 10:16:28,204 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-06-30 10:16:30,206 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-06-30 10:16:32,208 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-06-30 10:16:34,210 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-06-30 10:16:36,211 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c623f9b3e3a7/172.19.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-06-30 10:16:39,073 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6ace6ea5-447f-4e9d-beaf-9e0258503780 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-06-30 10:16:41,076 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6ace6ea5-447f-4e9d-beaf-9e0258503780 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-06-30 10:16:43,080 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6ace6ea5-447f-4e9d-beaf-9e0258503780 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_3          | 2023-06-30 10:16:49,808 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:16:49,809 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:16:49,867 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:49,867 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-30 10:16:49,867 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-30 10:16:49,867 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:16:49,868 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:16:49,871 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:16:49,871 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:16:49,871 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:16:49,871 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:49,905 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:50,417 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a4810344-5fc9-4e68-93f9-7973f1df7364: Detected pause in JVM or host machine approximately 0.169s with 0.508s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=142ms
datanode_3          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=366ms
datanode_3          | 2023-06-30 10:16:50,418 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:16:50,421 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:16:50,423 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:16:50,423 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:16:50,423 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:16:50,427 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363: start as a follower, conf=-1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:50,427 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:16:50,427 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState
datanode_3          | 2023-06-30 10:16:50,436 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D309D49F7363,id=a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_3          | 2023-06-30 10:16:50,436 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:16:50,436 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:16:50,436 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:16:50,436 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:16:50,438 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:16:50,448 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:16:50,448 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363
datanode_3          | 2023-06-30 10:16:52,529 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a4810344-5fc9-4e68-93f9-7973f1df7364: Detected pause in JVM or host machine approximately 0.105s with 0.468s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=468ms
datanode_3          | 2023-06-30 10:16:54,581 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState] INFO impl.FollowerState: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5061923476ns, electionTimeout:5046ms
datanode_3          | 2023-06-30 10:16:54,582 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: shutdown a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState
datanode_3          | 2023-06-30 10:16:54,584 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:16:54,589 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-30 10:16:54,592 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-FollowerState] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1
datanode_3          | 2023-06-30 10:16:54,622 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:54,623 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-06-30 10:16:54,632 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:54,633 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-30 10:16:54,633 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: shutdown a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1
datanode_3          | 2023-06-30 10:16:54,633 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:16:54,634 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FFE6261BA92C with new leaderId: a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_3          | 2023-06-30 10:16:54,635 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C: change Leader from null to a4810344-5fc9-4e68-93f9-7973f1df7364 at term 1 for becomeLeader, leader elected after 5811ms
datanode_3          | 2023-06-30 10:16:54,692 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:16:54,732 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:16:54,735 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-30 10:16:54,765 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:16:54,773 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:16:54,776 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:16:54,807 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:16:54,820 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-30 10:16:54,841 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderStateImpl
datanode_3          | 2023-06-30 10:16:54,906 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-f7626177-b26e-48a1-bda4-27683d55e918;layoutVersion=6
om_1                | 2023-06-30 10:16:45,189 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at c623f9b3e3a7/172.19.0.9
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:16:50,786 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = c623f9b3e3a7/172.19.0.9
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | 2023-06-30 10:16:54,970 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-LeaderElection1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C: set configuration 0: peers:[a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:55,388 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-FFE6261BA92C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/15fb554b-03cb-4515-a830-ffe6261ba92c/current/log_inprogress_0
datanode_3          | 2023-06-30 10:16:55,476 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState] INFO impl.FollowerState: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5049097049ns, electionTimeout:5027ms
datanode_3          | 2023-06-30 10:16:55,479 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: shutdown a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState
datanode_3          | 2023-06-30 10:16:55,480 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:16:55,480 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-30 10:16:55,481 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-FollowerState] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2
datanode_3          | 2023-06-30 10:16:55,517 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:55,553 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 4898f574-1037-4fa8-9e86-7f94f4ca3111
datanode_3          | 2023-06-30 10:16:55,590 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:16:55,607 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for d2a04bcb-d332-412c-bff4-3560f6749473
datanode_3          | 2023-06-30 10:16:55,611 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:16:55,673 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363.
datanode_3          | 2023-06-30 10:16:55,675 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a4810344-5fc9-4e68-93f9-7973f1df7364: addNew group-75D1BC6DB22B:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER] returns group-75D1BC6DB22B:java.util.concurrent.CompletableFuture@342c0633[Not completed]
datanode_3          | 2023-06-30 10:16:55,680 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364: new RaftServerImpl for group-75D1BC6DB22B:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:16:55,681 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:16:55,681 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:16:55,681 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:16:55,681 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:16:55,681 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:16:55,681 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:16:55,681 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: ConfigurationManager, init=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:16:55,682 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:16:55,682 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:16:55,682 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:16:55,683 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:16:55,683 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-30 10:16:55,683 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:16:55,683 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
s3g_1               | STARTUP_MSG:   java = 11.0.19
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir375248243786173481, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1               | ************************************************************/
s3g_1               | 2023-06-30 10:16:04,923 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-30 10:16:05,020 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-30 10:16:05,681 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-06-30 10:16:06,576 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-06-30 10:16:06,576 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-30 10:16:06,865 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1               | 2023-06-30 10:16:06,942 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-30 10:16:06,949 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-06-30 10:16:07,214 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-30 10:16:07,222 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-30 10:16:07,223 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-06-30 10:16:07,377 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e33c391{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-30 10:16:07,378 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@20312893{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1               | 2023-06-30 10:16:24,221 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1a139347{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir375248243786173481/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-2981498407772617637/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1               | 2023-06-30 10:16:24,342 [main] INFO server.AbstractConnector: Started ServerConnector@106faf11{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-30 10:16:24,342 [main] INFO server.Server: Started @30986ms
s3g_1               | 2023-06-30 10:16:24,363 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-06-30 10:16:24,363 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-06-30 10:16:24,381 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
datanode_3          | 2023-06-30 10:16:55,683 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-30 10:16:55,686 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:16:55,686 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:16:55,686 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-30 10:16:55,686 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-30 10:16:55,686 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-30 10:16:55,686 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-30 10:16:55,705 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b does not exist. Creating ...
datanode_3          | 2023-06-30 10:16:55,719 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b/in_use.lock acquired by nodename 7@713d54bfe484
datanode_3          | 2023-06-30 10:16:55,737 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b has been successfully formatted.
datanode_3          | 2023-06-30 10:16:55,738 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO ratis.ContainerStateMachine: group-75D1BC6DB22B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:16:55,738 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:16:55,738 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:16:55,738 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:55,738 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-30 10:16:55,738 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:16:55,739 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:16:55,740 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:16:55,747 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:55,998 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:16:56,035 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection:   Response 0: a4810344-5fc9-4e68-93f9-7973f1df7364<-d2a04bcb-d332-412c-bff4-3560f6749473#0:OK-t0
datanode_3          | 2023-06-30 10:16:56,037 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode_3          | 2023-06-30 10:16:55,998 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:16:56,040 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:16:56,040 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:16:56,041 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-30 10:16:50,818 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:16:54,283 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-30 10:16:55,740 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:16:56,057 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.9:9862
om_1                | 2023-06-30 10:16:56,057 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:16:56,068 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:16:56,226 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:16:56,475 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-06-30 10:16:57,064 [main] INFO reflections.Reflections: Reflections took 399 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1                | 2023-06-30 10:16:57,097 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-06-30 10:16:57,166 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:16:57,785 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863]
om_1                | 2023-06-30 10:16:57,923 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9863]
om_1                | 2023-06-30 10:16:59,187 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-06-30 10:16:59,261 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:16:59,763 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-06-30 10:17:00,658 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-30 10:17:00,779 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-06-30 10:17:00,836 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:17:00,962 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-06-30 10:17:00,966 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-06-30 10:17:01,596 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-30 10:17:01,733 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:17:01,735 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-30 10:17:01,767 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-30 10:17:01,795 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:17:01,903 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-30 10:17:01,927 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
datanode_3          | 2023-06-30 10:16:56,044 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:16:56,044 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:16:56,063 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:16:56,083 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:16:56,078 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: start as a follower, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:56,083 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:16:56,087 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FollowerState
datanode_3          | 2023-06-30 10:16:56,094 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:16:56,100 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection:   Response 0: a4810344-5fc9-4e68-93f9-7973f1df7364<-4898f574-1037-4fa8-9e86-7f94f4ca3111#0:OK-t1
datanode_3          | 2023-06-30 10:16:56,100 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.LeaderElection: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-06-30 10:16:56,100 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: shutdown a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2
datanode_3          | 2023-06-30 10:16:56,100 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:16:56,100 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D309D49F7363 with new leaderId: a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_3          | 2023-06-30 10:16:56,100 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363: change Leader from null to a4810344-5fc9-4e68-93f9-7973f1df7364 at term 1 for becomeLeader, leader elected after 6324ms
datanode_3          | 2023-06-30 10:16:56,101 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:16:56,102 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:16:56,103 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-30 10:16:56,108 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:16:56,108 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:16:56,108 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:16:56,110 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:16:56,110 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-30 10:16:56,095 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-75D1BC6DB22B,id=a4810344-5fc9-4e68-93f9-7973f1df7364
datanode_3          | 2023-06-30 10:16:56,111 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:16:56,112 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:16:56,134 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:16:56,134 [a4810344-5fc9-4e68-93f9-7973f1df7364-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:16:56,095 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:16:56,140 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:16:56,149 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b
datanode_3          | 2023-06-30 10:16:56,195 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:16:56,195 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:56,199 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:16:56,203 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-30 10:16:56,214 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om_1                | 2023-06-30 10:17:01,979 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-30 10:17:01,987 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-30 10:17:01,988 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-30 10:17:01,989 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-30 10:17:01,989 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-30 10:17:01,989 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-30 10:17:01,989 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:17:01,990 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-30 10:17:01,991 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:17:01,992 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-30 10:17:01,992 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-30 10:17:02,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-30 10:17:02,013 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-30 10:17:02,016 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-30 10:17:02,499 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-30 10:17:02,505 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-30 10:17:02,508 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-30 10:17:02,509 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:17:02,509 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:17:02,517 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:17:02,538 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@74a15f4[Not completed]
om_1                | 2023-06-30 10:17:02,539 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-30 10:17:02,560 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-30 10:17:02,579 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-30 10:17:02,594 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-30 10:17:02,595 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-30 10:17:02,595 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-30 10:17:02,595 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:17:02,595 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:17:02,595 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-30 10:17:02,606 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-30 10:17:02,607 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:17:02,620 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-30 10:17:02,622 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-30 10:17:02,684 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-30 10:17:02,689 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-06-30 10:17:02,697 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-30 10:17:02,697 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-30 10:17:02,798 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-06-30 10:17:03,005 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-30 10:17:03,070 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-30 10:17:03,070 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-30 10:17:03,071 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-30 10:17:03,071 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-30 10:17:03,072 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-30 10:17:04,035 [main] INFO reflections.Reflections: Reflections took 1283 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om_1                | 2023-06-30 10:17:04,407 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-30 10:17:04,413 [main] INFO ipc.Server: Listener at om:9862
om_1                | 2023-06-30 10:17:04,415 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-30 10:17:05,587 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-30 10:17:05,622 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-30 10:17:05,622 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-30 10:17:05,792 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.19.0.9:9862
om_1                | 2023-06-30 10:17:05,794 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-30 10:17:05,803 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-30 10:17:05,813 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@c623f9b3e3a7
om_1                | 2023-06-30 10:17:05,822 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
datanode_3          | 2023-06-30 10:16:56,214 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:16:56,215 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-30 10:16:56,216 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-30 10:16:56,216 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:16:56,216 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-30 10:16:56,245 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:16:56,246 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:16:56,246 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:16:56,246 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-30 10:16:56,247 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-30 10:16:56,247 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:16:56,250 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-30 10:16:56,250 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-30 10:16:56,251 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:16:56,251 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-30 10:16:56,258 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderStateImpl
datanode_3          | 2023-06-30 10:16:56,260 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:16:56,266 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dfbf3335-fc72-48b8-a89c-d309d49f7363/current/log_inprogress_0
datanode_3          | 2023-06-30 10:16:56,309 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363-LeaderElection2] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-D309D49F7363: set configuration 0: peers:[4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:16:56,614 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b.
datanode_3          | 2023-06-30 10:16:59,589 [grpc-default-executor-1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: receive requestVote(PRE_VOTE, 4898f574-1037-4fa8-9e86-7f94f4ca3111, group-75D1BC6DB22B, 0, (t:0, i:0))
datanode_3          | 2023-06-30 10:16:59,593 [grpc-default-executor-1] INFO impl.VoteContext: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FOLLOWER: accept PRE_VOTE from 4898f574-1037-4fa8-9e86-7f94f4ca3111: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-30 10:16:59,601 [grpc-default-executor-1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B replies to PRE_VOTE vote request: 4898f574-1037-4fa8-9e86-7f94f4ca3111<-a4810344-5fc9-4e68-93f9-7973f1df7364#0:OK-t0. Peer's state: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B:t0, leader=null, voted=, raftlog=Memoized:a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:17:00,784 [grpc-default-executor-1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: receive requestVote(PRE_VOTE, d2a04bcb-d332-412c-bff4-3560f6749473, group-75D1BC6DB22B, 0, (t:0, i:0))
datanode_3          | 2023-06-30 10:17:00,784 [grpc-default-executor-1] INFO impl.VoteContext: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FOLLOWER: accept PRE_VOTE from d2a04bcb-d332-412c-bff4-3560f6749473: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-30 10:17:00,785 [grpc-default-executor-1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B replies to PRE_VOTE vote request: d2a04bcb-d332-412c-bff4-3560f6749473<-a4810344-5fc9-4e68-93f9-7973f1df7364#0:OK-t0. Peer's state: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B:t0, leader=null, voted=, raftlog=Memoized:a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:17:00,818 [grpc-default-executor-1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: receive requestVote(ELECTION, d2a04bcb-d332-412c-bff4-3560f6749473, group-75D1BC6DB22B, 1, (t:0, i:0))
datanode_3          | 2023-06-30 10:17:00,819 [grpc-default-executor-1] INFO impl.VoteContext: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FOLLOWER: accept ELECTION from d2a04bcb-d332-412c-bff4-3560f6749473: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-30 10:17:00,819 [grpc-default-executor-1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d2a04bcb-d332-412c-bff4-3560f6749473
datanode_3          | 2023-06-30 10:17:00,819 [grpc-default-executor-1] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: shutdown a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FollowerState
datanode_3          | 2023-06-30 10:17:00,819 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FollowerState] INFO impl.FollowerState: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FollowerState was interrupted
datanode_3          | 2023-06-30 10:17:00,820 [grpc-default-executor-1] INFO impl.RoleInfo: a4810344-5fc9-4e68-93f9-7973f1df7364: start a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-FollowerState
datanode_3          | 2023-06-30 10:17:00,825 [grpc-default-executor-1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B replies to ELECTION vote request: d2a04bcb-d332-412c-bff4-3560f6749473<-a4810344-5fc9-4e68-93f9-7973f1df7364#0:OK-t1. Peer's state: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B:t1, leader=null, voted=d2a04bcb-d332-412c-bff4-3560f6749473, raftlog=Memoized:a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:17:01,157 [a4810344-5fc9-4e68-93f9-7973f1df7364-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-75D1BC6DB22B with new leaderId: d2a04bcb-d332-412c-bff4-3560f6749473
datanode_3          | 2023-06-30 10:17:01,162 [a4810344-5fc9-4e68-93f9-7973f1df7364-server-thread1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: change Leader from null to d2a04bcb-d332-412c-bff4-3560f6749473 at term 1 for appendEntries, leader elected after 5473ms
datanode_3          | 2023-06-30 10:17:01,172 [a4810344-5fc9-4e68-93f9-7973f1df7364-server-thread1] INFO server.RaftServer$Division: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B: set configuration 0: peers:[d2a04bcb-d332-412c-bff4-3560f6749473|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 4898f574-1037-4fa8-9e86-7f94f4ca3111|rpc:172.19.0.7:9856|admin:172.19.0.7:9857|client:172.19.0.7:9858|dataStream:172.19.0.7:9858|priority:0|startupRole:FOLLOWER, a4810344-5fc9-4e68-93f9-7973f1df7364|rpc:172.19.0.6:9856|admin:172.19.0.6:9857|client:172.19.0.6:9858|dataStream:172.19.0.6:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:17:01,177 [a4810344-5fc9-4e68-93f9-7973f1df7364-server-thread1] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:17:01,181 [a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a4810344-5fc9-4e68-93f9-7973f1df7364@group-75D1BC6DB22B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5a93fe8d-013f-4021-82fd-75d1bc6db22b/current/log_inprogress_0
datanode_3          | 2023-06-30 10:17:44,254 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:18:44,255 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:19:44,260 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:20:44,260 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:21:44,260 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.10:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1             | 2023-06-30 10:16:46,279 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1             | 2023-06-30 10:16:46,286 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:16:46,287 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=57c9d9bb-d884-4f1a-8cfe-39dae38c5442 from SCM.
recon_1             | 2023-06-30 10:16:46,838 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 from SCM.
recon_1             | 2023-06-30 10:16:46,841 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b from SCM.
recon_1             | 2023-06-30 10:16:46,849 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242 from SCM.
recon_1             | 2023-06-30 10:16:46,850 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=15fb554b-03cb-4515-a830-ffe6261ba92c from SCM.
recon_1             | 2023-06-30 10:16:46,852 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-30 10:16:46,857 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:16:46,887 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-30 10:16:46,928 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-30 10:16:47,543 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:40292 / 172.19.0.6:40292: output error
recon_1             | 2023-06-30 10:16:47,543 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:58286 / 172.19.0.7:58286: output error
recon_1             | 2023-06-30 10:16:47,545 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,545 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,548 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:42168 / 172.19.0.5:42168: output error
recon_1             | 2023-06-30 10:16:47,550 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,549 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:41490 / 172.19.0.6:41490: output error
recon_1             | 2023-06-30 10:16:47,549 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:55560 / 172.19.0.5:55560: output error
recon_1             | 2023-06-30 10:16:47,549 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:58272 / 172.19.0.7:58272: output error
recon_1             | 2023-06-30 10:16:47,548 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:41486 / 172.19.0.6:41486: output error
recon_1             | 2023-06-30 10:16:47,548 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:42182 / 172.19.0.5:42182: output error
recon_1             | 2023-06-30 10:16:47,551 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,551 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,551 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,551 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,551 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,627 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:55078 / 172.19.0.7:55078: output error
recon_1             | 2023-06-30 10:16:47,633 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:16:47,665 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4898f574-1037-4fa8-9e86-7f94f4ca3111
recon_1             | 2023-06-30 10:16:47,713 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 4898f574-1037-4fa8-9e86-7f94f4ca3111{ip: 172.19.0.7, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:16:47,832 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 4898f574-1037-4fa8-9e86-7f94f4ca3111 to Node DB.
recon_1             | 2023-06-30 10:16:49,039 [IPC Server handler 8 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a4810344-5fc9-4e68-93f9-7973f1df7364
recon_1             | 2023-06-30 10:16:49,054 [IPC Server handler 8 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a4810344-5fc9-4e68-93f9-7973f1df7364{ip: 172.19.0.6, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:16:49,055 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a4810344-5fc9-4e68-93f9-7973f1df7364 to Node DB.
recon_1             | 2023-06-30 10:16:49,056 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=15fb554b-03cb-4515-a830-ffe6261ba92c reported by a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)
recon_1             | 2023-06-30 10:16:49,481 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=57c9d9bb-d884-4f1a-8cfe-39dae38c5442 reported by 4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)
recon_1             | 2023-06-30 10:16:49,918 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)
recon_1             | 2023-06-30 10:16:50,068 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by 4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)
recon_1             | 2023-06-30 10:16:54,198 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by 4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)
recon_1             | 2023-06-30 10:16:54,199 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b reported by 4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)
recon_1             | 2023-06-30 10:16:54,530 [IPC Server handler 3 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d2a04bcb-d332-412c-bff4-3560f6749473
recon_1             | 2023-06-30 10:16:54,531 [IPC Server handler 3 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d2a04bcb-d332-412c-bff4-3560f6749473{ip: 172.19.0.5, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:16:54,532 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d2a04bcb-d332-412c-bff4-3560f6749473 to Node DB.
recon_1             | 2023-06-30 10:16:54,533 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5)
om_1                | 2023-06-30 10:17:05,827 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-30 10:17:05,841 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-30 10:17:05,843 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:17:05,845 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-30 10:17:05,847 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-30 10:17:05,852 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:17:05,862 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-30 10:17:05,862 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-30 10:17:05,862 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:17:05,873 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-30 10:17:05,874 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:17:05,875 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-30 10:17:05,878 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:17:05,879 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-30 10:17:05,880 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-30 10:17:05,881 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-30 10:17:05,885 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-30 10:17:05,885 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-30 10:17:05,903 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-30 10:17:05,904 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:17:05,946 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-30 10:17:05,946 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-30 10:17:05,947 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-30 10:17:05,972 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:17:05,972 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:17:05,973 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:17:05,976 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-30 10:17:05,979 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:17:05,986 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-30 10:17:05,990 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-30 10:17:05,990 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-30 10:17:05,991 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-30 10:17:05,991 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-30 10:17:05,991 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-30 10:17:05,992 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-30 10:17:05,994 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-30 10:17:06,095 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-30 10:17:06,102 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-30 10:17:06,107 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-30 10:17:06,246 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-30 10:17:06,248 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-30 10:17:06,334 [main] INFO util.log: Logging initialized @20699ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-30 10:17:06,661 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-30 10:17:06,673 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-30 10:17:06,695 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-30 10:17:06,698 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-30 10:17:06,699 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-30 10:17:06,699 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-30 10:17:06,805 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1                | 2023-06-30 10:17:06,809 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-30 10:17:06,822 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-06-30 10:17:06,905 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-30 10:17:06,910 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-30 10:17:06,912 [main] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-30 10:17:06,934 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@58d9d844{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-30 10:17:06,942 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d30304f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-30 10:17:07,235 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4b86b189{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-15769255002739109700/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-06-30 10:17:07,269 [main] INFO server.AbstractConnector: Started ServerConnector@43ae3c30{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-30 10:17:07,270 [main] INFO server.Server: Started @21635ms
om_1                | 2023-06-30 10:17:07,279 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-30 10:17:07,279 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-30 10:17:07,281 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-30 10:17:07,286 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-30 10:17:07,289 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-30 10:17:07,385 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-30 10:17:07,772 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-06-30 10:17:11,169 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5190678793ns, electionTimeout:5178ms
om_1                | 2023-06-30 10:17:11,170 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:17:11,172 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-30 10:17:11,175 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-06-30 10:17:11,176 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:17:11,188 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:17:11,189 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-06-30 10:17:11,194 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:17:11,194 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-30 10:17:11,194 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:17:11,195 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-30 10:17:11,202 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 8510ms
om_1                | 2023-06-30 10:17:11,214 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-30 10:17:11,225 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:17:11,227 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:17:11,235 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-30 10:17:11,236 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-30 10:17:11,237 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-30 10:17:11,246 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:17:11,249 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-30 10:17:11,253 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-30 10:17:11,283 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-30 10:17:11,336 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:17:11,400 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-30 10:17:11,526 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
recon_1             | 2023-06-30 10:16:54,534 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b reported by d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5)
recon_1             | 2023-06-30 10:16:54,637 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)
recon_1             | 2023-06-30 10:16:54,930 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by 4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)
recon_1             | 2023-06-30 10:16:54,930 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b reported by 4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)
recon_1             | 2023-06-30 10:16:55,358 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5)
recon_1             | 2023-06-30 10:16:55,359 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b reported by d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5)
recon_1             | 2023-06-30 10:16:55,770 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)
recon_1             | 2023-06-30 10:16:55,770 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b reported by a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)
recon_1             | 2023-06-30 10:16:56,128 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 reported by a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)
recon_1             | 2023-06-30 10:16:56,129 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b reported by a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)
recon_1             | 2023-06-30 10:17:00,861 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b reported by d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5)
recon_1             | 2023-06-30 10:17:15,554 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-30 10:17:15,701 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-30 10:17:16,987 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-30 10:17:16,988 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-30 10:17:17,024 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-30 10:17:17,029 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-30 10:17:17,021 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-06-30 10:17:17,064 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 57 milliseconds.
recon_1             | 2023-06-30 10:17:17,074 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1             | 2023-06-30 10:17:17,075 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-06-30 10:17:17,104 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-06-30 10:17:17,135 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 93 milliseconds to process 0 existing database records.
recon_1             | 2023-06-30 10:17:17,150 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 15 milliseconds for processing 1 containers.
recon_1             | 2023-06-30 10:17:17,244 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242 reported by d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5)
recon_1             | 2023-06-30 10:17:26,396 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-30 10:17:26,403 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-30 10:17:35,097 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-30 10:17:35,098 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-30 10:17:36,890 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688120255098
recon_1             | 2023-06-30 10:17:36,911 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-06-30 10:17:37,719 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688120255098.
recon_1             | 2023-06-30 10:17:37,786 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-30 10:17:38,178 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1             | 2023-06-30 10:17:38,192 [pool-52-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-30 10:17:38,201 [pool-52-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-30 10:17:38,201 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:17:38,201 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-30 10:17:38,204 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-30 10:17:38,287 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:17:38,287 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.085 seconds to process 3 keys.
recon_1             | 2023-06-30 10:17:38,337 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-30 10:17:38,390 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-30 10:18:17,123 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1             | 2023-06-30 10:18:17,155 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:18:17,156 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 32
recon_1             | 2023-06-30 10:19:17,156 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:19:17,156 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-30 10:20:17,156 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:20:17,157 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-30 10:21:17,157 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:21:17,157 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-30 10:22:17,152 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
recon_1             | 2023-06-30 10:22:17,154 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 2 milliseconds for processing 2 containers.
recon_1             | 2023-06-30 10:22:17,158 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:22:17,158 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-30 10:22:17,165 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-06-30 10:22:17,172 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 30 milliseconds.
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-30 10:17:12,735 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-30 10:17:12,851 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-06-30 10:17:35,999 [qtp2120525656-53] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-06-30 10:17:36,724 [qtp2120525656-53] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1688120256692 in 31 milliseconds
om_1                | 2023-06-30 10:17:36,768 [qtp2120525656-53] INFO db.RDBCheckpointUtils: Waited for 34 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1688120256692 availability.
om_1                | 2023-06-30 10:17:36,830 [qtp2120525656-53] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 60 milliseconds
om_1                | 2023-06-30 10:17:36,830 [qtp2120525656-53] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
om_1                | 2023-06-30 10:17:36,831 [qtp2120525656-53] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688120256692
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:16:09,985 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 50b5cf80bed9/172.19.0.10
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:16:10,148 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:16:11,092 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:16:13,166 [main] INFO reflections.Reflections: Reflections took 1670 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1               | 2023-06-30 10:16:14,564 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:16:14,730 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:16:16,400 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-30 10:16:17,377 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:16:17,485 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:16:17,489 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:16:17,490 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:16:17,515 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-30 10:16:17,519 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-30 10:16:17,520 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-30 10:16:17,529 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:17,548 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-30 10:16:17,548 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:16:17,693 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-30 10:16:17,703 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-30 10:16:17,737 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-30 10:16:19,785 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-30 10:16:19,820 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-30 10:16:19,820 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-30 10:16:19,821 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:16:19,827 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:16:19,830 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:16:19,896 [main] INFO server.RaftServer: 6ace6ea5-447f-4e9d-beaf-9e0258503780: addNew group-27683D55E918:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|priority:0|startupRole:FOLLOWER] returns group-27683D55E918:java.util.concurrent.CompletableFuture@784abd3e[Not completed]
scm_1               | 2023-06-30 10:16:20,065 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780: new RaftServerImpl for group-27683D55E918:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-30 10:16:20,105 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-30 10:16:20,105 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-30 10:16:20,106 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-30 10:16:20,106 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:16:20,106 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:16:20,107 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-30 10:16:20,236 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: ConfigurationManager, init=-1: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-30 10:16:20,243 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:16:20,381 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-30 10:16:20,381 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-30 10:16:20,525 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-30 10:16:20,571 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-30 10:16:20,622 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-30 10:16:20,643 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-30 10:16:20,850 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-30 10:16:20,910 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-30 10:16:22,105 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:16:22,134 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-30 10:16:22,164 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-30 10:16:22,165 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-30 10:16:22,165 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-30 10:16:22,171 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-30 10:16:22,184 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918 does not exist. Creating ...
scm_1               | 2023-06-30 10:16:22,243 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/in_use.lock acquired by nodename 13@50b5cf80bed9
scm_1               | 2023-06-30 10:16:22,296 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918 has been successfully formatted.
scm_1               | 2023-06-30 10:16:22,386 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-30 10:16:22,482 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-30 10:16:22,495 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:22,507 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-30 10:16:22,562 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-30 10:16:22,608 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:16:22,704 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-30 10:16:22,705 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-30 10:16:22,705 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:22,742 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918
scm_1               | 2023-06-30 10:16:22,742 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:16:22,742 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:16:22,787 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:16:22,788 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-30 10:16:22,792 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-30 10:16:22,793 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-30 10:16:22,797 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-30 10:16:22,798 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-30 10:16:22,980 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-30 10:16:22,991 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:23,185 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-30 10:16:23,213 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-30 10:16:23,231 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-30 10:16:23,362 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:16:23,387 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:16:23,403 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: start as a follower, conf=-1: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:23,443 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-06-30 10:16:23,445 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState
scm_1               | 2023-06-30 10:16:23,464 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-30 10:16:23,473 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-30 10:16:23,602 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-27683D55E918,id=6ace6ea5-447f-4e9d-beaf-9e0258503780
scm_1               | 2023-06-30 10:16:23,604 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-30 10:16:23,618 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-30 10:16:23,623 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-30 10:16:23,632 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-30 10:16:23,679 [main] INFO server.RaftServer: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start RPC server
scm_1               | 2023-06-30 10:16:24,273 [main] INFO server.GrpcService: 6ace6ea5-447f-4e9d-beaf-9e0258503780: GrpcService started, listening on 9894
scm_1               | 2023-06-30 10:16:24,335 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6ace6ea5-447f-4e9d-beaf-9e0258503780: Started
scm_1               | 2023-06-30 10:16:28,581 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO impl.FollowerState: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5136302352ns, electionTimeout:5104ms
scm_1               | 2023-06-30 10:16:28,583 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: shutdown 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState
scm_1               | 2023-06-30 10:16:28,606 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-06-30 10:16:28,617 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-30 10:16:28,619 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1
scm_1               | 2023-06-30 10:16:28,646 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:28,648 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-06-30 10:16:28,671 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:28,673 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-06-30 10:16:28,673 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: shutdown 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1
scm_1               | 2023-06-30 10:16:28,674 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-06-30 10:16:28,674 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: change Leader from null to 6ace6ea5-447f-4e9d-beaf-9e0258503780 at term 1 for becomeLeader, leader elected after 8149ms
scm_1               | 2023-06-30 10:16:28,721 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-30 10:16:28,732 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:16:28,736 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:16:28,775 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-30 10:16:28,781 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-30 10:16:28,785 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-30 10:16:28,824 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:16:28,844 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-30 10:16:28,854 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderStateImpl
scm_1               | 2023-06-30 10:16:29,055 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-30 10:16:29,184 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: set configuration 0: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:29,476 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/current/log_inprogress_0
scm_1               | 2023-06-30 10:16:30,339 [main] INFO server.RaftServer: 6ace6ea5-447f-4e9d-beaf-9e0258503780: close
scm_1               | 2023-06-30 10:16:30,340 [main] INFO server.GrpcService: 6ace6ea5-447f-4e9d-beaf-9e0258503780: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-30 10:16:30,340 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: shutdown
scm_1               | 2023-06-30 10:16:30,341 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-27683D55E918,id=6ace6ea5-447f-4e9d-beaf-9e0258503780
scm_1               | 2023-06-30 10:16:30,341 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: shutdown 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderStateImpl
scm_1               | 2023-06-30 10:16:30,345 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO impl.PendingRequests: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-30 10:16:30,363 [main] INFO server.GrpcService: 6ace6ea5-447f-4e9d-beaf-9e0258503780: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-30 10:16:30,364 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO impl.StateMachineUpdater: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-30 10:16:30,365 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO impl.StateMachineUpdater: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-30 10:16:30,365 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO impl.StateMachineUpdater: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-30 10:16:30,372 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: closes. applyIndex: 0
scm_1               | 2023-06-30 10:16:30,518 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker close()
scm_1               | 2023-06-30 10:16:30,519 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6ace6ea5-447f-4e9d-beaf-9e0258503780: Stopped
scm_1               | 2023-06-30 10:16:30,520 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:16:30,523 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-f7626177-b26e-48a1-bda4-27683d55e918; layoutVersion=7; scmId=6ace6ea5-447f-4e9d-beaf-9e0258503780
scm_1               | 2023-06-30 10:16:30,528 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 50b5cf80bed9/172.19.0.10
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:16:33,556 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 50b5cf80bed9/172.19.0.10
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:16:33,586 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:16:33,717 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:16:33,962 [main] INFO reflections.Reflections: Reflections took 182 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1               | 2023-06-30 10:16:34,113 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:16:34,132 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:16:35,019 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:16:35,272 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:16:35,590 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1               | 2023-06-30 10:16:35,592 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-30 10:16:35,639 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-30 10:16:35,830 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:6ace6ea5-447f-4e9d-beaf-9e0258503780
scm_1               | 2023-06-30 10:16:35,888 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-30 10:16:35,895 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:16:35,897 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:16:35,897 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:16:35,897 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:16:35,898 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-30 10:16:35,898 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-30 10:16:35,898 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-30 10:16:35,900 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:35,900 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-30 10:16:35,901 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:16:35,910 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-30 10:16:35,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-30 10:16:35,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-30 10:16:36,087 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-30 10:16:36,089 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-30 10:16:36,090 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-30 10:16:36,090 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:16:36,090 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:16:36,093 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:16:36,096 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer: 6ace6ea5-447f-4e9d-beaf-9e0258503780: found a subdirectory /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918
scm_1               | 2023-06-30 10:16:36,101 [main] INFO server.RaftServer: 6ace6ea5-447f-4e9d-beaf-9e0258503780: addNew group-27683D55E918:[] returns group-27683D55E918:java.util.concurrent.CompletableFuture@266da047[Not completed]
scm_1               | 2023-06-30 10:16:36,120 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780: new RaftServerImpl for group-27683D55E918:[] with SCMStateMachine:uninitialized
scm_1               | 2023-06-30 10:16:36,122 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-30 10:16:36,123 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-30 10:16:36,123 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-30 10:16:36,123 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:16:36,123 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:16:36,123 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-30 10:16:36,130 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-30 10:16:36,130 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:16:36,133 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-30 10:16:36,134 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-30 10:16:36,143 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-30 10:16:36,146 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-30 10:16:36,150 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-30 10:16:36,150 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-30 10:16:36,167 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-30 10:16:36,244 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:16:36,246 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-30 10:16:36,247 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-30 10:16:36,247 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-30 10:16:36,248 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-30 10:16:36,248 [6ace6ea5-447f-4e9d-beaf-9e0258503780-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-30 10:16:36,250 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-06-30 10:16:36,251 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-30 10:16:36,251 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-06-30 10:16:36,286 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1               | 2023-06-30 10:16:36,328 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-30 10:16:36,329 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-30 10:16:36,336 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-30 10:16:36,337 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-30 10:16:36,433 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-30 10:16:36,458 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1               | 2023-06-30 10:16:36,462 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-30 10:16:36,473 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-30 10:16:36,500 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-30 10:16:36,500 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-30 10:16:36,510 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-30 10:16:36,510 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:16:36,515 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-30 10:16:36,519 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-06-30 10:16:36,526 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-30 10:16:36,526 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-06-30 10:16:36,571 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-30 10:16:36,571 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-30 10:16:36,601 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-30 10:16:36,686 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-30 10:16:36,709 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-30 10:16:36,711 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-30 10:16:36,725 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-30 10:16:36,729 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:36,731 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:16:36,782 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-06-30 10:16:37,440 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:16:37,472 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:16:37,506 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1               | 2023-06-30 10:16:37,514 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-30 10:16:37,600 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:16:37,608 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:16:37,609 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-06-30 10:16:37,610 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-30 10:16:37,652 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:16:37,664 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:16:37,665 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1               | 2023-06-30 10:16:37,666 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-30 10:16:37,777 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-30 10:16:37,778 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-30 10:16:37,778 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-30 10:16:37,783 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:16:37,788 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-30 10:16:37,793 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/in_use.lock acquired by nodename 7@50b5cf80bed9
scm_1               | 2023-06-30 10:16:37,797 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=6ace6ea5-447f-4e9d-beaf-9e0258503780} from /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/current/raft-meta
scm_1               | 2023-06-30 10:16:37,828 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: set configuration 0: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:37,830 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-30 10:16:37,853 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-30 10:16:37,853 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:37,856 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-30 10:16:37,856 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-30 10:16:37,859 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:16:37,863 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-30 10:16:37,864 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-30 10:16:37,864 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:37,879 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918
scm_1               | 2023-06-30 10:16:37,881 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:16:37,883 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:16:37,885 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:16:37,886 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-30 10:16:37,886 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-30 10:16:37,888 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-30 10:16:37,889 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-30 10:16:37,890 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-30 10:16:37,898 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-30 10:16:37,898 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:16:37,927 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-30 10:16:37,928 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-30 10:16:37,930 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-30 10:16:37,952 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: set configuration 0: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:37,952 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/current/log_inprogress_0
scm_1               | 2023-06-30 10:16:37,955 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:16:38,040 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: start as a follower, conf=0: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:38,040 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-30 10:16:38,042 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState
scm_1               | 2023-06-30 10:16:38,043 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-27683D55E918,id=6ace6ea5-447f-4e9d-beaf-9e0258503780
scm_1               | 2023-06-30 10:16:38,077 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-30 10:16:38,077 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-30 10:16:38,078 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-30 10:16:38,079 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-30 10:16:38,079 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-30 10:16:38,079 [6ace6ea5-447f-4e9d-beaf-9e0258503780-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-30 10:16:38,085 [main] INFO server.RaftServer: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start RPC server
scm_1               | 2023-06-30 10:16:38,164 [main] INFO server.GrpcService: 6ace6ea5-447f-4e9d-beaf-9e0258503780: GrpcService started, listening on 9894
scm_1               | 2023-06-30 10:16:38,171 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6ace6ea5-447f-4e9d-beaf-9e0258503780: Started
scm_1               | 2023-06-30 10:16:38,181 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-30 10:16:38,182 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-30 10:16:38,183 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-06-30 10:16:38,184 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-06-30 10:16:38,184 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-06-30 10:16:38,253 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-30 10:16:38,262 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-30 10:16:38,262 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-30 10:16:38,509 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:16:38,509 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:16:38,510 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-30 10:16:38,756 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:16:38,757 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:16:38,761 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:16:38,776 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-30 10:16:38,975 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-30 10:16:38,976 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-30 10:16:39,054 [main] INFO util.log: Logging initialized @7908ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-30 10:16:39,192 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-30 10:16:39,202 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-30 10:16:39,215 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-30 10:16:39,216 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-30 10:16:39,216 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-30 10:16:39,217 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-30 10:16:39,260 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-06-30 10:16:39,261 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-30 10:16:39,262 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-30 10:16:39,329 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-30 10:16:39,329 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-30 10:16:39,331 [main] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-06-30 10:16:39,342 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7fb84ef7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-30 10:16:39,342 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43076326{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-30 10:16:39,465 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@27bd124a{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-11207044981130066886/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-06-30 10:16:39,474 [main] INFO server.AbstractConnector: Started ServerConnector@24a5ddd7{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-30 10:16:39,475 [main] INFO server.Server: Started @8328ms
scm_1               | 2023-06-30 10:16:39,477 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-30 10:16:39,477 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-30 10:16:39,478 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-30 10:16:43,263 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO impl.FollowerState: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5221322154ns, electionTimeout:5184ms
scm_1               | 2023-06-30 10:16:43,264 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: shutdown 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState
scm_1               | 2023-06-30 10:16:43,264 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-30 10:16:43,268 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-30 10:16:43,268 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-FollowerState] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1
scm_1               | 2023-06-30 10:16:43,274 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:43,275 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-06-30 10:16:43,287 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:43,287 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.LeaderElection: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-30 10:16:43,287 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: shutdown 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1
scm_1               | 2023-06-30 10:16:43,288 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-30 10:16:43,288 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-30 10:16:43,288 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-30 10:16:43,291 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: change Leader from null to 6ace6ea5-447f-4e9d-beaf-9e0258503780 at term 2 for becomeLeader, leader elected after 7144ms
scm_1               | 2023-06-30 10:16:43,298 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-30 10:16:43,302 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:16:43,303 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:16:43,307 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-30 10:16:43,307 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-30 10:16:43,308 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-30 10:16:43,314 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:16:43,315 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-30 10:16:43,317 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO impl.RoleInfo: 6ace6ea5-447f-4e9d-beaf-9e0258503780: start 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderStateImpl
scm_1               | 2023-06-30 10:16:43,323 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-30 10:16:43,342 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/current/log_inprogress_0 to /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/current/log_0-0
scm_1               | 2023-06-30 10:16:43,353 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-LeaderElection1] INFO server.RaftServer$Division: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918: set configuration 1: peers:[6ace6ea5-447f-4e9d-beaf-9e0258503780|rpc:50b5cf80bed9:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:16:43,366 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f7626177-b26e-48a1-bda4-27683d55e918/current/log_inprogress_1
scm_1               | 2023-06-30 10:16:43,371 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-30 10:16:43,371 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-30 10:16:43,379 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:43,379 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-30 10:16:43,380 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:16:43,381 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:16:43,384 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-30 10:16:43,427 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:16:43,616 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:33712 / 172.19.0.5:33712: output error
scm_1               | 2023-06-30 10:16:43,617 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:32774 / 172.19.0.7:32774: output error
scm_1               | 2023-06-30 10:16:43,617 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:53388 / 172.19.0.6:53388: output error
scm_1               | 2023-06-30 10:16:43,632 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:16:43,637 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:16:43,637 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:16:44,350 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d2a04bcb-d332-412c-bff4-3560f6749473
scm_1               | 2023-06-30 10:16:44,363 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d2a04bcb-d332-412c-bff4-3560f6749473{ip: 172.19.0.5, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:16:44,372 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:16:44,381 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:16:44,396 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:16:44,397 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:16:44,398 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242 to datanode:d2a04bcb-d332-412c-bff4-3560f6749473
scm_1               | 2023-06-30 10:16:44,572 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:44,582 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242, Nodes: d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:16:44.397432Z[UTC]]
scm_1               | 2023-06-30 10:16:45,715 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a4810344-5fc9-4e68-93f9-7973f1df7364
scm_1               | 2023-06-30 10:16:45,719 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a4810344-5fc9-4e68-93f9-7973f1df7364{ip: 172.19.0.6, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:16:45,723 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:16:45,724 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:16:45,746 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=15fb554b-03cb-4515-a830-ffe6261ba92c to datanode:a4810344-5fc9-4e68-93f9-7973f1df7364
scm_1               | 2023-06-30 10:16:45,760 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:45,770 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 15fb554b-03cb-4515-a830-ffe6261ba92c, Nodes: a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:16:45.746735Z[UTC]]
scm_1               | 2023-06-30 10:16:45,831 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4898f574-1037-4fa8-9e86-7f94f4ca3111
scm_1               | 2023-06-30 10:16:45,831 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4898f574-1037-4fa8-9e86-7f94f4ca3111{ip: 172.19.0.7, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:16:45,831 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:16:45,832 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=57c9d9bb-d884-4f1a-8cfe-39dae38c5442 to datanode:4898f574-1037-4fa8-9e86-7f94f4ca3111
scm_1               | 2023-06-30 10:16:45,833 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:16:45,833 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:16:45,838 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-30 10:16:45,838 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-30 10:16:45,838 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:16:45,848 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:45,856 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 57c9d9bb-d884-4f1a-8cfe-39dae38c5442, Nodes: 4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:16:45.832384Z[UTC]]
scm_1               | 2023-06-30 10:16:45,870 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 to datanode:a4810344-5fc9-4e68-93f9-7973f1df7364
scm_1               | 2023-06-30 10:16:45,878 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 to datanode:4898f574-1037-4fa8-9e86-7f94f4ca3111
scm_1               | 2023-06-30 10:16:45,878 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 to datanode:d2a04bcb-d332-412c-bff4-3560f6749473
scm_1               | 2023-06-30 10:16:45,888 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:45,911 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: dfbf3335-fc72-48b8-a89c-d309d49f7363, Nodes: a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6)4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:16:45.870382Z[UTC]]
scm_1               | 2023-06-30 10:16:45,913 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b to datanode:d2a04bcb-d332-412c-bff4-3560f6749473
scm_1               | 2023-06-30 10:16:45,923 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b to datanode:4898f574-1037-4fa8-9e86-7f94f4ca3111
scm_1               | 2023-06-30 10:16:45,923 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b to datanode:a4810344-5fc9-4e68-93f9-7973f1df7364
scm_1               | 2023-06-30 10:16:45,933 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:45,940 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b contains same datanodes as previous pipelines: PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363 nodeIds: d2a04bcb-d332-412c-bff4-3560f6749473, 4898f574-1037-4fa8-9e86-7f94f4ca3111, a4810344-5fc9-4e68-93f9-7973f1df7364
scm_1               | 2023-06-30 10:16:45,941 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 5a93fe8d-013f-4021-82fd-75d1bc6db22b, Nodes: d2a04bcb-d332-412c-bff4-3560f6749473(xcompat_datanode_2.xcompat_default/172.19.0.5)4898f574-1037-4fa8-9e86-7f94f4ca3111(xcompat_datanode_1.xcompat_default/172.19.0.7)a4810344-5fc9-4e68-93f9-7973f1df7364(xcompat_datanode_3.xcompat_default/172.19.0.6), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:16:45.913624Z[UTC]]
scm_1               | 2023-06-30 10:16:45,943 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-30 10:16:45,947 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-30 10:16:49,177 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:49,191 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=15fb554b-03cb-4515-a830-ffe6261ba92c
scm_1               | 2023-06-30 10:16:49,208 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:49,479 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:49,480 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=57c9d9bb-d884-4f1a-8cfe-39dae38c5442
scm_1               | 2023-06-30 10:16:49,481 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:49,858 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:50,066 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:54,198 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:54,653 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:54,942 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:55,761 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:56,141 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:16:56,148 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=dfbf3335-fc72-48b8-a89c-d309d49f7363
scm_1               | 2023-06-30 10:16:56,151 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:16:56,151 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:16:56,151 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-30 10:16:56,151 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-30 10:16:56,151 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-30 10:16:56,153 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-30 10:16:56,156 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-30 10:16:56,165 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-30 10:16:56,191 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-30 10:16:56,194 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-06-30 10:17:00,864 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=5a93fe8d-013f-4021-82fd-75d1bc6db22b
scm_1               | 2023-06-30 10:17:13,065 [IPC Server handler 72 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-30 10:17:13,114 [6ace6ea5-447f-4e9d-beaf-9e0258503780@group-27683D55E918-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-30 10:17:13,118 [IPC Server handler 72 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-30 10:17:17,241 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=85d6fa2d-f152-4ad0-8cd5-7cdb92b9d242
scm_1               | 2023-06-30 10:17:46,950 [IPC Server handler 89 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:17:57,057 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:18:45,948 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-30 10:18:49,760 [IPC Server handler 6 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:18:59,307 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:19:52,293 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:20:01,954 [IPC Server handler 89 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:20:45,950 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-30 10:20:57,949 [IPC Server handler 69 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:21:06,857 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:21:36,714 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-30 10:22:03,549 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
scm_1               | 2023-06-30 10:22:12,184 [IPC Server handler 72 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.13
Attaching to xcompat_old_client_1_1_0_1, xcompat_datanode_1, xcompat_recon_1, xcompat_old_client_1_3_0_1, xcompat_new_client_1, xcompat_s3g_1, xcompat_datanode_2, xcompat_datanode_3, xcompat_scm_1, xcompat_old_client_1_0_0_1, xcompat_om_1, xcompat_old_client_1_2_1_1
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-30 10:22:38 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = d6bea76b6050/172.20.0.6
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.0.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_3          | STARTUP_MSG:   java = 11.0.3
datanode_3          | ************************************************************/
datanode_3          | 2023-06-30 10:22:38 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-30 10:22:40 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-30 10:22:40 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-30 10:22:41 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-30 10:22:41 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_3          | 2023-06-30 10:22:42 INFO  HddsDatanodeService:209 - HddsDatanodeService host:d6bea76b6050 ip:172.20.0.6
datanode_3          | 2023-06-30 10:22:42 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-30 10:22:43 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-06-30 10:22:43 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-30 10:22:43 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-30 10:22:43 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:22:43 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:22:43 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:22:43 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_3          | 2023-06-30 10:22:47 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:22:48 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-30 10:22:48 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_3          | 2023-06-30 10:22:48 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-30 10:22:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:22:48 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-30 10:22:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:22:49 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:22:49 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:22:50 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_3          | 2023-06-30 10:22:50 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:22:50 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-30 10:22:50 INFO  log:169 - Logging initialized @16487ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-30 10:22:51 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-30 10:22:51 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-30 10:22:51 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-30 10:22:51 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-30 10:22:51 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-30 10:22:51 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-30 10:22:51 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_3          | 2023-06-30 10:22:51 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_3          | 2023-06-30 10:22:51 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-30 10:22:51 INFO  session:338 - No SessionScavenger set, using defaults
datanode_3          | 2023-06-30 10:22:51 INFO  session:140 - node0 Scavenging every 600000ms
datanode_3          | 2023-06-30 10:22:51 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-30 10:22:51 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-30 10:22:52 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-2376432410742454769.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-30 10:22:52 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3          | 2023-06-30 10:22:52 INFO  Server:399 - Started @18910ms
datanode_3          | 2023-06-30 10:22:52 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_3          | 2023-06-30 10:22:52 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_3          | 2023-06-30 10:22:53 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:22:53 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3          | 2023-06-30 10:22:53 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.13:9891
datanode_3          | 2023-06-30 10:22:53 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-30 10:22:56 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:22:57 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_3          | 2023-06-30 10:22:57 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_3          | 2023-06-30 10:22:57 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717 at port 9858
datanode_3          | 2023-06-30 10:22:57 INFO  RaftServerProxy:304 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: start RPC server
datanode_3          | 2023-06-30 10:22:57 INFO  GrpcService:160 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3          | 2023-06-30 10:22:59 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_3          | 2023-06-30 10:23:19 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_3          | 2023-06-30 10:23:39 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_3          | 2023-06-30 10:23:55 INFO  Client:958 - Retrying connect to server: recon/172.20.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerProxy:89 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: addNew group-A8A2FEFFD5D0:[543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] returns group-A8A2FEFFD5D0:java.util.concurrent.CompletableFuture@38e89885[Not completed]
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:107 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: new RaftServerImpl for group-A8A2FEFFD5D0:[543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:103 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0: ConfigurationManager, init=-1: [543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0 does not exist. Creating ...
datanode_3          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0/in_use.lock acquired by nodename 7@d6bea76b6050
datanode_3          | 2023-06-30 10:24:00 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0 has been successfully formatted.
datanode_3          | 2023-06-30 10:24:00 INFO  ContainerStateMachine:225 - group-A8A2FEFFD5D0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:180 - new 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:196 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0: start as a follower, conf=-1: [543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:185 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:24:00 INFO  RoleInfo:143 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: start FollowerState
datanode_3          | 2023-06-30 10:24:00 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8A2FEFFD5D0,id=543fcfa0-c1eb-4d0c-8002-86ebb7aa5717
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0
datanode_3          | 2023-06-30 10:24:00 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: -275799979837601170
datanode_3          |   leastSigBits: -8281652821741283888
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerProxy:89 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: addNew group-F8460033A865:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] returns group-F8460033A865:java.util.concurrent.CompletableFuture@2103ea6f[Not completed]
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:107 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: new RaftServerImpl for group-F8460033A865:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-30 10:22:38 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = dfb021f3b1b7/172.20.0.10
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.0.0
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:103 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865: ConfigurationManager, init=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865 does not exist. Creating ...
datanode_3          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865/in_use.lock acquired by nodename 7@d6bea76b6050
datanode_3          | 2023-06-30 10:24:00 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865 has been successfully formatted.
datanode_3          | 2023-06-30 10:24:00 INFO  ContainerStateMachine:225 - group-F8460033A865: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:180 - new 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:196 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865: start as a follower, conf=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_3          | 2023-06-30 10:24:00 INFO  RaftServerImpl:185 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:24:00 INFO  RoleInfo:143 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: start FollowerState
datanode_3          | 2023-06-30 10:24:00 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F8460033A865,id=543fcfa0-c1eb-4d0c-8002-86ebb7aa5717
datanode_3          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865
datanode_3          | 2023-06-30 10:24:01 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "6ffe4e7e-001d-412a-bb99-f8460033a865"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: 8069973885369270570
datanode_3          |   leastSigBits: -4928635337645512603
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-06-30 10:24:05 INFO  FollowerState:108 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-FollowerState: change to CANDIDATE, lastRpcTime:5026ms, electionTimeout:5025ms
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:121 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: shutdown FollowerState
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: start LeaderElection
datanode_3          | 2023-06-30 10:24:05 INFO  LeaderElection:209 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-LeaderElection1: begin an election at term 1 for -1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_3          | 2023-06-30 10:24:05 INFO  FollowerState:108 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0-FollowerState: change to CANDIDATE, lastRpcTime:5217ms, electionTimeout:5187ms
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:121 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: shutdown FollowerState
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: start LeaderElection
datanode_3          | 2023-06-30 10:24:05 INFO  LeaderElection:209 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0-LeaderElection2: begin an election at term 1 for -1: [543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:134 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: shutdown LeaderElection
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:24:05 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-A8A2FEFFD5D0 with new leaderId: 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:255 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0: change Leader from null to 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717 at term 1 for becomeLeader, leader elected after 5381ms
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:24:05 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865: changes role from CANDIDATE to FOLLOWER at term 1 for appendEntries
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:134 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: shutdown LeaderElection
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: start FollowerState
datanode_3          | 2023-06-30 10:24:05 INFO  LeaderElection:61 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-LeaderElection1: Election REJECTED; received 2 response(s) [543fcfa0-c1eb-4d0c-8002-86ebb7aa5717<-f07392d7-ab98-42eb-9cf0-8171dd2a74d9#0:FAIL-t1, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717<-8a812bbc-98e3-4715-97de-06c864241bf3#0:FAIL-t1] and 0 exception(s); 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865:t1, leader=null, voted=543fcfa0-c1eb-4d0c-8002-86ebb7aa5717, raftlog=543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_3          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717: start LeaderState
datanode_3          | 2023-06-30 10:24:05 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-F8460033A865 with new leaderId: f07392d7-ab98-42eb-9cf0-8171dd2a74d9
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:255 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865: change Leader from null to f07392d7-ab98-42eb-9cf0-8171dd2a74d9 at term 1 for appendEntries, leader elected after 5265ms
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:356 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865: set configuration 0: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null at 0
datanode_3          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:397 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:397 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:24:05 INFO  RaftServerImpl:356 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0: set configuration 0: [543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null at 0
datanode_3          | 2023-06-30 10:24:06 INFO  SegmentedRaftLogWorker:596 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-A8A2FEFFD5D0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0/current/log_inprogress_0
datanode_3          | 2023-06-30 10:24:06 INFO  SegmentedRaftLogWorker:596 - 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717@group-F8460033A865-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865/current/log_inprogress_0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_1          | STARTUP_MSG:   java = 11.0.3
datanode_1          | ************************************************************/
datanode_1          | 2023-06-30 10:22:38 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-30 10:22:40 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-30 10:22:41 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-30 10:22:42 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-30 10:22:42 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_1          | 2023-06-30 10:22:43 INFO  HddsDatanodeService:209 - HddsDatanodeService host:dfb021f3b1b7 ip:172.20.0.10
datanode_1          | 2023-06-30 10:22:44 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-30 10:22:44 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-06-30 10:22:44 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-30 10:22:44 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-30 10:22:44 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:22:44 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:22:44 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:22:44 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_1          | 2023-06-30 10:22:48 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:22:48 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-30 10:22:48 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_1          | 2023-06-30 10:22:48 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-30 10:22:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:22:48 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-30 10:22:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:22:49 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:22:49 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:22:50 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_1          | 2023-06-30 10:22:50 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:22:50 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-30 10:22:50 INFO  log:169 - Logging initialized @16100ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-30 10:22:51 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-30 10:22:51 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-30 10:22:51 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-30 10:22:51 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-30 10:22:51 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-30 10:22:51 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-30 10:22:51 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_1          | 2023-06-30 10:22:51 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_1          | 2023-06-30 10:22:51 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-30 10:22:51 INFO  session:338 - No SessionScavenger set, using defaults
datanode_1          | 2023-06-30 10:22:51 INFO  session:140 - node0 Scavenging every 660000ms
datanode_1          | 2023-06-30 10:22:51 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-30 10:22:51 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-30 10:22:52 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-9020830152474412950.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-30 10:22:52 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1          | 2023-06-30 10:22:53 INFO  Server:399 - Started @18423ms
datanode_1          | 2023-06-30 10:22:53 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_1          | 2023-06-30 10:22:53 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_1          | 2023-06-30 10:22:53 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:22:53 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_1          | 2023-06-30 10:22:53 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.13:9891
datanode_1          | 2023-06-30 10:22:53 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-30 10:22:56 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:22:57 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_1          | 2023-06-30 10:22:57 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_1          | 2023-06-30 10:22:57 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis f07392d7-ab98-42eb-9cf0-8171dd2a74d9 at port 9858
datanode_1          | 2023-06-30 10:22:57 INFO  RaftServerProxy:304 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: start RPC server
datanode_1          | 2023-06-30 10:22:57 INFO  GrpcService:160 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1          | 2023-06-30 10:22:59 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_1          | 2023-06-30 10:23:19 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_1          | 2023-06-30 10:23:39 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_1          | 2023-06-30 10:23:55 INFO  Client:958 - Retrying connect to server: recon/172.20.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerProxy:89 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: addNew group-0746B4AE1AC8:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858] returns group-0746B4AE1AC8:java.util.concurrent.CompletableFuture@2f1883b1[Not completed]
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:107 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: new RaftServerImpl for group-0746B4AE1AC8:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:103 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8: ConfigurationManager, init=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/f98618a2-4152-4f1c-9505-0746b4ae1ac8 does not exist. Creating ...
datanode_1          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/f98618a2-4152-4f1c-9505-0746b4ae1ac8/in_use.lock acquired by nodename 6@dfb021f3b1b7
datanode_1          | 2023-06-30 10:24:00 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/f98618a2-4152-4f1c-9505-0746b4ae1ac8 has been successfully formatted.
datanode_1          | 2023-06-30 10:24:00 INFO  ContainerStateMachine:225 - group-0746B4AE1AC8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:180 - new f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/f98618a2-4152-4f1c-9505-0746b4ae1ac8
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:196 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8: start as a follower, conf=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858], old=null
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:185 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:24:00 INFO  RoleInfo:143 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: start FollowerState
datanode_1          | 2023-06-30 10:24:00 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0746B4AE1AC8,id=f07392d7-ab98-42eb-9cf0-8171dd2a74d9
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8
datanode_1          | 2023-06-30 10:24:00 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "f98618a2-4152-4f1c-9505-0746b4ae1ac8"
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-30 10:22:38 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 2fc7b4319876/172.20.0.8
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.0.0
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: -466658426226585828
datanode_1          |   leastSigBits: -7708747186914321720
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerProxy:89 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: addNew group-F8460033A865:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] returns group-F8460033A865:java.util.concurrent.CompletableFuture@1980aab5[Not completed]
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:107 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: new RaftServerImpl for group-F8460033A865:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_2          | STARTUP_MSG:   java = 11.0.3
datanode_2          | ************************************************************/
datanode_2          | 2023-06-30 10:22:38 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-30 10:22:39 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-30 10:22:40 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-30 10:22:41 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-30 10:22:41 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_2          | 2023-06-30 10:22:41 INFO  HddsDatanodeService:209 - HddsDatanodeService host:2fc7b4319876 ip:172.20.0.8
datanode_2          | 2023-06-30 10:22:42 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-30 10:22:42 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-06-30 10:22:42 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-30 10:22:42 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-30 10:22:42 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:22:43 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:22:43 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:22:43 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_2          | 2023-06-30 10:22:47 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:22:47 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-30 10:22:47 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_2          | 2023-06-30 10:22:47 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-30 10:22:47 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:22:48 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-30 10:22:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:22:48 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:22:48 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:22:49 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_2          | 2023-06-30 10:22:49 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:22:49 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-30 10:22:49 INFO  log:169 - Logging initialized @15502ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-30 10:22:50 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-30 10:22:50 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-30 10:22:50 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-30 10:22:50 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-30 10:22:50 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-30 10:22:50 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-30 10:22:51 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_2          | 2023-06-30 10:22:51 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_2          | 2023-06-30 10:22:51 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-30 10:22:51 INFO  session:338 - No SessionScavenger set, using defaults
datanode_2          | 2023-06-30 10:22:51 INFO  session:140 - node0 Scavenging every 660000ms
datanode_2          | 2023-06-30 10:22:51 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-30 10:22:51 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-30 10:22:53 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-7741147529188222364.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-30 10:22:53 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2          | 2023-06-30 10:22:53 INFO  Server:399 - Started @18765ms
datanode_2          | 2023-06-30 10:22:53 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2          | 2023-06-30 10:22:53 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2          | 2023-06-30 10:22:53 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:22:53 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2          | 2023-06-30 10:22:53 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.13:9891
datanode_2          | 2023-06-30 10:22:54 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-30 10:22:56 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 2fc7b4319876/172.20.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.8:56260 remote=scm/172.20.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:103 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865: ConfigurationManager, init=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865 does not exist. Creating ...
datanode_1          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865/in_use.lock acquired by nodename 6@dfb021f3b1b7
datanode_1          | 2023-06-30 10:24:00 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865 has been successfully formatted.
datanode_1          | 2023-06-30 10:24:00 INFO  ContainerStateMachine:225 - group-F8460033A865: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:180 - new f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:196 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865: start as a follower, conf=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_1          | 2023-06-30 10:24:00 INFO  RaftServerImpl:185 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:24:00 INFO  RoleInfo:143 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: start FollowerState
datanode_1          | 2023-06-30 10:24:00 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F8460033A865,id=f07392d7-ab98-42eb-9cf0-8171dd2a74d9
datanode_1          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865
datanode_1          | 2023-06-30 10:24:01 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "6ffe4e7e-001d-412a-bb99-f8460033a865"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: 8069973885369270570
datanode_1          |   leastSigBits: -4928635337645512603
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-06-30 10:24:05 INFO  FollowerState:108 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8-FollowerState: change to CANDIDATE, lastRpcTime:5194ms, electionTimeout:5193ms
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:121 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: shutdown FollowerState
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: start LeaderElection
datanode_1          | 2023-06-30 10:24:05 INFO  LeaderElection:209 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8-LeaderElection1: begin an election at term 1 for -1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858], old=null
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:134 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: shutdown LeaderElection
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-30 10:24:05 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-0746B4AE1AC8 with new leaderId: f07392d7-ab98-42eb-9cf0-8171dd2a74d9
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:255 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8: change Leader from null to f07392d7-ab98-42eb-9cf0-8171dd2a74d9 at term 1 for becomeLeader, leader elected after 5355ms
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_2          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.8:56260 remote=scm/172.20.0.4:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_2          | 2023-06-30 10:22:59 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_2          | 2023-06-30 10:23:19 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_2          | 2023-06-30 10:23:39 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_2          | 2023-06-30 10:23:55 INFO  Client:958 - Retrying connect to server: recon/172.20.0.13:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_2          | 2023-06-30 10:23:55 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_2          | 2023-06-30 10:23:55 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_2          | 2023-06-30 10:23:55 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 8a812bbc-98e3-4715-97de-06c864241bf3 at port 9858
datanode_2          | 2023-06-30 10:23:55 INFO  RaftServerProxy:304 - 8a812bbc-98e3-4715-97de-06c864241bf3: start RPC server
datanode_2          | 2023-06-30 10:23:55 INFO  GrpcService:160 - 8a812bbc-98e3-4715-97de-06c864241bf3: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerProxy:89 - 8a812bbc-98e3-4715-97de-06c864241bf3: addNew group-0E698F85E75B:[8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858] returns group-0E698F85E75B:java.util.concurrent.CompletableFuture@637074b6[Not completed]
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:107 - 8a812bbc-98e3-4715-97de-06c864241bf3: new RaftServerImpl for group-0E698F85E75B:[8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:103 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B: ConfigurationManager, init=-1: [8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/3151ed07-a70f-4fd0-a420-0e698f85e75b does not exist. Creating ...
datanode_2          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/3151ed07-a70f-4fd0-a420-0e698f85e75b/in_use.lock acquired by nodename 6@2fc7b4319876
datanode_2          | 2023-06-30 10:24:00 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/3151ed07-a70f-4fd0-a420-0e698f85e75b has been successfully formatted.
datanode_2          | 2023-06-30 10:24:00 INFO  ContainerStateMachine:225 - group-0E698F85E75B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:180 - new 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/3151ed07-a70f-4fd0-a420-0e698f85e75b
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:24:05 INFO  FollowerState:108 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-FollowerState: change to CANDIDATE, lastRpcTime:5099ms, electionTimeout:5084ms
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:121 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: shutdown FollowerState
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: start LeaderElection
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: start LeaderState
datanode_1          | 2023-06-30 10:24:05 INFO  LeaderElection:209 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-LeaderElection2: begin an election at term 1 for -1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_1          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:397 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:24:05 INFO  LeaderElection:61 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-LeaderElection2: Election PASSED; received 1 response(s) [f07392d7-ab98-42eb-9cf0-8171dd2a74d9<-8a812bbc-98e3-4715-97de-06c864241bf3#0:OK-t1] and 0 exception(s); f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865:t1, leader=null, voted=f07392d7-ab98-42eb-9cf0-8171dd2a74d9, raftlog=f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:134 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: shutdown LeaderElection
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-30 10:24:05 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-F8460033A865 with new leaderId: f07392d7-ab98-42eb-9cf0-8171dd2a74d9
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:255 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865: change Leader from null to f07392d7-ab98-42eb-9cf0-8171dd2a74d9 at term 1 for becomeLeader, leader elected after 5210ms
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:356 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8: set configuration 0: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858], old=null at 0
datanode_1          | 2023-06-30 10:24:05 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis_grpc.log_appender.f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9: start LeaderState
datanode_1          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:397 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:356 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865: set configuration 0: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null at 0
datanode_1          | 2023-06-30 10:24:05 INFO  RaftServerImpl:843 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-   LEADER: Withhold vote from candidate 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717 with term 1. State: leader=f07392d7-ab98-42eb-9cf0-8171dd2a74d9, term=1, lastRpcElapsed=null
datanode_1          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:596 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-F8460033A865-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865/current/log_inprogress_0
datanode_1          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:596 - f07392d7-ab98-42eb-9cf0-8171dd2a74d9@group-0746B4AE1AC8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f98618a2-4152-4f1c-9505-0746b4ae1ac8/current/log_inprogress_0
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:196 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B: start as a follower, conf=-1: [8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858], old=null
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:185 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:24:00 INFO  RoleInfo:143 - 8a812bbc-98e3-4715-97de-06c864241bf3: start FollowerState
datanode_2          | 2023-06-30 10:24:00 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0E698F85E75B,id=8a812bbc-98e3-4715-97de-06c864241bf3
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B
datanode_2          | 2023-06-30 10:24:00 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "3151ed07-a70f-4fd0-a420-0e698f85e75b"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: 3553882198095384528
datanode_2          |   leastSigBits: -6620275605692356773
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerProxy:89 - 8a812bbc-98e3-4715-97de-06c864241bf3: addNew group-F8460033A865:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] returns group-F8460033A865:java.util.concurrent.CompletableFuture@ef30c36[Not completed]
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:107 - 8a812bbc-98e3-4715-97de-06c864241bf3: new RaftServerImpl for group-F8460033A865:[f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:103 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865: ConfigurationManager, init=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865 does not exist. Creating ...
datanode_2          | 2023-06-30 10:24:00 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865/in_use.lock acquired by nodename 6@2fc7b4319876
datanode_2          | 2023-06-30 10:24:00 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865 has been successfully formatted.
datanode_2          | 2023-06-30 10:24:00 INFO  ContainerStateMachine:225 - group-F8460033A865: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:180 - new 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:24:00 INFO  SegmentedRaftLogWorker:129 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:196 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865: start as a follower, conf=-1: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null
datanode_2          | 2023-06-30 10:24:00 INFO  RaftServerImpl:185 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:24:00 INFO  RoleInfo:143 - 8a812bbc-98e3-4715-97de-06c864241bf3: start FollowerState
datanode_2          | 2023-06-30 10:24:00 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F8460033A865,id=8a812bbc-98e3-4715-97de-06c864241bf3
datanode_2          | 2023-06-30 10:24:00 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865
datanode_2          | 2023-06-30 10:24:01 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "6ffe4e7e-001d-412a-bb99-f8460033a865"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: 8069973885369270570
datanode_2          |   leastSigBits: -4928635337645512603
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:f07392d7-ab98-42eb-9cf0-8171dd2a74d9
datanode_2          | 2023-06-30 10:24:05 INFO  RoleInfo:121 - 8a812bbc-98e3-4715-97de-06c864241bf3: shutdown FollowerState
datanode_2          | 2023-06-30 10:24:05 INFO  FollowerState:117 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - 8a812bbc-98e3-4715-97de-06c864241bf3: start FollowerState
datanode_2          | 2023-06-30 10:24:05 INFO  FollowerState:108 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B-FollowerState: change to CANDIDATE, lastRpcTime:5022ms, electionTimeout:5006ms
datanode_2          | 2023-06-30 10:24:05 INFO  RoleInfo:121 - 8a812bbc-98e3-4715-97de-06c864241bf3: shutdown FollowerState
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - 8a812bbc-98e3-4715-97de-06c864241bf3: start LeaderElection
datanode_2          | 2023-06-30 10:24:05 INFO  LeaderElection:209 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B-LeaderElection1: begin an election at term 1 for -1: [8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858], old=null
datanode_2          | 2023-06-30 10:24:05 INFO  RoleInfo:134 - 8a812bbc-98e3-4715-97de-06c864241bf3: shutdown LeaderElection
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerImpl:185 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-30 10:24:05 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-0E698F85E75B with new leaderId: 8a812bbc-98e3-4715-97de-06c864241bf3
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerImpl:255 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B: change Leader from null to 8a812bbc-98e3-4715-97de-06c864241bf3 at term 1 for becomeLeader, leader elected after 5217ms
datanode_2          | 2023-06-30 10:24:05 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-F8460033A865 with new leaderId: f07392d7-ab98-42eb-9cf0-8171dd2a74d9
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerImpl:255 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865: change Leader from null to f07392d7-ab98-42eb-9cf0-8171dd2a74d9 at term 1 for appendEntries, leader elected after 4963ms
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerImpl:356 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865: set configuration 0: [f07392d7-ab98-42eb-9cf0-8171dd2a74d9:172.20.0.10:9858, 8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858, 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717:172.20.0.6:9858], old=null at 0
datanode_2          | 2023-06-30 10:24:05 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:397 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:24:05 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:24:05 INFO  RoleInfo:143 - 8a812bbc-98e3-4715-97de-06c864241bf3: start LeaderState
datanode_2          | 2023-06-30 10:24:05 INFO  SegmentedRaftLogWorker:397 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:24:06 INFO  RaftServerImpl:356 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B: set configuration 0: [8a812bbc-98e3-4715-97de-06c864241bf3:172.20.0.8:9858], old=null at 0
datanode_2          | 2023-06-30 10:24:06 INFO  SegmentedRaftLogWorker:596 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-0E698F85E75B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3151ed07-a70f-4fd0-a420-0e698f85e75b/current/log_inprogress_0
datanode_2          | 2023-06-30 10:24:06 INFO  SegmentedRaftLogWorker:596 - 8a812bbc-98e3-4715-97de-06c864241bf3@group-F8460033A865-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6ffe4e7e-001d-412a-bb99-f8460033a865/current/log_inprogress_0
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:22:38 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = e33f657aa53a/172.20.0.3
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-06-30 10:22:38 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:22:43 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:22:43 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.3:9862
om_1                | 2023-06-30 10:22:43 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:22:43 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:22:44 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-06-30 10:22:46 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:47 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:48 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:49 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:50 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:51 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:52 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:53 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:54 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:55 INFO  Client:958 - Retrying connect to server: scm/172.20.0.4:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:22:55 INFO  RetriableTask:62 - Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-ebeb0048-8153-4fbb-85e2-6ed1e8bc00e3;layoutVersion=0
om_1                | 2023-06-30 10:23:00 INFO  OzoneManagerStarter:124 - SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at e33f657aa53a/172.20.0.3
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:23:01 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = e33f657aa53a/172.20.0.3
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-06-30 10:23:01 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:23:02 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:23:02 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.3:9862
om_1                | 2023-06-30 10:23:02 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:23:02 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:23:02 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:23:03 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-06-30 10:23:03 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:23:03 INFO  OzoneManager:3574 - Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-30 10:23:03 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-30 10:23:03 INFO  Server:1219 - Starting Socket Reader #1 for port 9862
om_1                | 2023-06-30 10:23:03 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-30 10:23:04 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-30 10:23:04 INFO  MetricsSystemImpl:191 - OzoneManager metrics system started
om_1                | 2023-06-30 10:23:04 INFO  OzoneManager:1114 - OzoneManager RPC server is listening at om/172.20.0.3:9862
om_1                | 2023-06-30 10:23:04 INFO  BaseHttpServer:207 - Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-30 10:23:04 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-30 10:23:04 INFO  log:169 - Logging initialized @3921ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-30 10:23:04 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-30 10:23:04 INFO  HttpRequestLog:86 - Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-30 10:23:04 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-30 10:23:04 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-30 10:23:04 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-30 10:23:04 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-30 10:23:04 INFO  HttpServer2:1237 - Jetty bound to port 9874
om_1                | 2023-06-30 10:23:04 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
om_1                | 2023-06-30 10:23:04 INFO  session:333 - DefaultSessionIdManager workerName=node0
om_1                | 2023-06-30 10:23:04 INFO  session:338 - No SessionScavenger set, using defaults
om_1                | 2023-06-30 10:23:04 INFO  session:140 - node0 Scavenging every 660000ms
om_1                | 2023-06-30 10:23:04 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@2100d047{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-30 10:23:04 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@47be0f9b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-30 10:23:04 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@1d247525{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_0_0_jar-_-any-14824920314450782446.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-30 10:23:04 INFO  AbstractConnector:330 - Started ServerConnector@f4a3a8d{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om_1                | 2023-06-30 10:23:04 INFO  Server:399 - Started @4333ms
om_1                | 2023-06-30 10:23:04 INFO  MetricsSinkAdapter:204 - Sink prometheus started
om_1                | 2023-06-30 10:23:04 INFO  MetricsSystemImpl:301 - Registered sink prometheus
om_1                | 2023-06-30 10:23:04 INFO  BaseHttpServer:327 - HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-30 10:23:04 INFO  Server:1460 - IPC Server Responder: starting
om_1                | 2023-06-30 10:23:04 INFO  Server:1298 - IPC Server listener on 9862: starting
om_1                | 2023-06-30 10:23:04 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
om_1                | 2023-06-30 10:23:58 INFO  OMDBCheckpointServlet:101 - Received request to obtain OM DB checkpoint snapshot
om_1                | 2023-06-30 10:23:58 INFO  RDBCheckpointManager:86 - Created checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1688120638803 in 12 milliseconds
om_1                | 2023-06-30 10:23:58 INFO  OMDBCheckpointServlet:144 - Time taken to write the checkpoint to response output stream: 32 milliseconds
om_1                | 2023-06-30 10:23:58 INFO  RocksDBCheckpoint:78 - Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1688120638803
om_1                | 2023-06-30 10:24:10 INFO  OMVolumeCreateRequest:195 - created volume:vol1 for user:hadoop
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-30 10:22:37 INFO  ReconServer:112 - STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = cfa2b71a0b83/172.20.0.13
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.0.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
recon_1             | STARTUP_MSG:   java = 11.0.3
recon_1             | ************************************************************/
recon_1             | 2023-06-30 10:22:37 INFO  ReconServer:90 - registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-30 10:22:40 INFO  ReconRestServletModule:75 - rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-06-30 10:22:42 INFO  ReconServer:93 - Initializing Recon server...
recon_1             | 2023-06-30 10:22:44 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:22:48 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:22:50 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:22:51 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:22:51 INFO  ReconServer:101 - Creating Recon Schema.
recon_1             | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
recon_1             | 2023-06-30 10:22:54 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
recon_1             | 2023-06-30 10:22:54 INFO  BaseHttpServer:207 - Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-30 10:22:54 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-30 10:22:54 INFO  log:169 - Logging initialized @20373ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-30 10:22:54 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-30 10:22:54 WARN  HttpRequestLog:103 - Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-30 10:22:54 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-30 10:22:54 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-30 10:22:54 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-30 10:22:54 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-30 10:22:54 INFO  ReconTaskControllerImpl:79 - Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-30 10:22:55 INFO  ReconTaskControllerImpl:79 - Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-30 10:22:55 INFO  OmUtils:550 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-30 10:22:55 INFO  OmUtils:569 - No OzoneManager ServiceID configured.
recon_1             | 2023-06-30 10:22:55 INFO  deprecation:1395 - No unit for recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-06-30 10:22:55 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:22:56 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:22:56 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@5aea8994
recon_1             | 2023-06-30 10:22:56 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
recon_1             | 2023-06-30 10:22:56 WARN  DBStoreBuilder:277 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:22:56 INFO  SCMNodeManager:116 - Entering startup safe mode.
recon_1             | 2023-06-30 10:22:56 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:22:56 INFO  ReconNodeManager:100 - Loaded 0 nodes from node DB.
recon_1             | 2023-06-30 10:22:56 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-30 10:22:56 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-30 10:22:56 INFO  Server:1219 - Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-30 10:22:56 INFO  SCMPipelineManager:161 - No pipeline exists in current db
recon_1             | 2023-06-30 10:22:56 INFO  ReconServer:109 - Recon server initialized successfully!
recon_1             | 2023-06-30 10:22:56 INFO  ReconServer:134 - Starting Recon server
recon_1             | 2023-06-30 10:22:56 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-30 10:22:56 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-30 10:22:56 INFO  MetricsSystemImpl:191 - Recon metrics system started
recon_1             | 2023-06-30 10:22:56 INFO  HttpServer2:1237 - Jetty bound to port 9888
recon_1             | 2023-06-30 10:22:56 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
recon_1             | 2023-06-30 10:22:56 INFO  session:333 - DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-30 10:22:56 INFO  session:338 - No SessionScavenger set, using defaults
recon_1             | 2023-06-30 10:22:56 INFO  session:140 - node0 Scavenging every 660000ms
recon_1             | 2023-06-30 10:22:56 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@723877dd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-30 10:22:56 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@6d229b1c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-30 10:22:58 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@554d040d{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_0_0_jar-_-any-638134563246553161.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/recon}
recon_1             | 2023-06-30 10:22:58 INFO  AbstractConnector:330 - Started ServerConnector@62765aec{HTTP/1.1,[http/1.1]}{0.0.0.0:9888}
recon_1             | 2023-06-30 10:22:58 INFO  Server:399 - Started @24752ms
recon_1             | 2023-06-30 10:22:58 INFO  MetricsSinkAdapter:204 - Sink prometheus started
recon_1             | 2023-06-30 10:22:58 INFO  MetricsSystemImpl:301 - Registered sink prometheus
recon_1             | 2023-06-30 10:22:58 INFO  BaseHttpServer:327 - HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-30 10:22:58 INFO  OzoneManagerServiceProviderImpl:198 - Starting Ozone Manager Service Provider.
recon_1             | 2023-06-30 10:22:58 INFO  OzoneManagerServiceProviderImpl:176 - Registered OmDeltaRequest task 
recon_1             | 2023-06-30 10:22:58 INFO  OzoneManagerServiceProviderImpl:186 - Registered OmSnapshotRequest task 
recon_1             | 2023-06-30 10:22:58 INFO  ReconOmMetadataManagerImpl:65 - Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-30 10:22:58 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:22:58 INFO  ReconTaskControllerImpl:221 - Starting Recon Task Controller.
recon_1             | 2023-06-30 10:22:58 INFO  ReconStorageContainerManagerFacade:206 - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:22:58 INFO  ReconStorageContainerManagerFacade:256 - Obtained 0 pipelines from SCM.
recon_1             | 2023-06-30 10:22:58 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:22:58 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:22:58 INFO  Server:1460 - IPC Server Responder: starting
recon_1             | 2023-06-30 10:22:58 INFO  Server:1298 - IPC Server listener on 9891: starting
recon_1             | 2023-06-30 10:22:58 INFO  ReconScmTask:46 - Registered ContainerHealthTask task 
recon_1             | 2023-06-30 10:22:58 INFO  ReconScmTask:56 - Starting ContainerHealthTask Thread.
recon_1             | 2023-06-30 10:22:58 INFO  ReconScmTask:46 - Registered PipelineSyncTask task 
recon_1             | 2023-06-30 10:22:58 INFO  ReconScmTask:56 - Starting PipelineSyncTask Thread.
recon_1             | 2023-06-30 10:22:58 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:22:58 INFO  PipelineSyncTask:61 - Pipeline sync Thread took 15 milliseconds.
recon_1             | 2023-06-30 10:22:58 INFO  ContainerHealthTask:77 - Container Health task thread took 62 milliseconds to process 0 existing database records.
recon_1             | 2023-06-30 10:22:58 INFO  ContainerHealthTask:86 - Container Health task thread took 12 milliseconds for processing 0 containers.
recon_1             | 2023-06-30 10:23:57 INFO  NetworkTopology:111 - Added a new node: /default-rack/f07392d7-ab98-42eb-9cf0-8171dd2a74d9
recon_1             | 2023-06-30 10:23:57 INFO  SCMNodeManager:273 - Registered Data node : f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:23:57 INFO  ReconNodeManager:116 - Adding new node f07392d7-ab98-42eb-9cf0-8171dd2a74d9 to Node DB.
recon_1             | 2023-06-30 10:23:57 INFO  NetworkTopology:111 - Added a new node: /default-rack/543fcfa0-c1eb-4d0c-8002-86ebb7aa5717
recon_1             | 2023-06-30 10:23:57 INFO  SCMNodeManager:273 - Registered Data node : 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:23:57 INFO  ReconNodeManager:116 - Adding new node 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717 to Node DB.
recon_1             | 2023-06-30 10:23:57 INFO  NetworkTopology:111 - Added a new node: /default-rack/8a812bbc-98e3-4715-97de-06c864241bf3
recon_1             | 2023-06-30 10:23:57 INFO  SCMNodeManager:273 - Registered Data node : 8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:23:57 INFO  ReconNodeManager:116 - Adding new node 8a812bbc-98e3-4715-97de-06c864241bf3 to Node DB.
recon_1             | 2023-06-30 10:23:58 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-06-30 10:23:58 INFO  OzoneManagerServiceProviderImpl:409 - Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-30 10:23:58 INFO  OzoneManagerServiceProviderImpl:316 - Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688120638685
recon_1             | 2023-06-30 10:23:59 INFO  ReconOmMetadataManagerImpl:91 - Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688120638685.
recon_1             | 2023-06-30 10:23:59 INFO  OzoneManagerServiceProviderImpl:421 - Calling reprocess on Recon tasks.
recon_1             | 2023-06-30 10:23:59 INFO  ContainerKeyMapperTask:73 - Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:23:59 INFO  ContainerDBServiceProviderImpl:117 - Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1688120639108
recon_1             | 2023-06-30 10:23:59 INFO  ContainerDBServiceProviderImpl:122 - Cleaning up old Recon Container DB at /data/metadata/recon/recon-container-key.db_1688120563373.
recon_1             | 2023-06-30 10:23:59 INFO  ContainerKeyMapperTask:89 - Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:23:59 INFO  ContainerKeyMapperTask:92 - It took me 0.223 seconds to process 0 keys.
recon_1             | 2023-06-30 10:23:59 INFO  FileSizeCountTask:102 - Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=f98618a2-4152-4f1c-9505-0746b4ae1ac8. Trying to get from SCM.
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: f98618a2-4152-4f1c-9505-0746b4ae1ac8, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:f07392d7-ab98-42eb-9cf0-8171dd2a74d9, CreationTimestamp2023-06-30T10:23:57.162Z] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:24:00 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: f98618a2-4152-4f1c-9505-0746b4ae1ac8, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:f07392d7-ab98-42eb-9cf0-8171dd2a74d9, CreationTimestamp2023-06-30T10:23:57.162Z]
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0. Trying to get from SCM.
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0, Nodes: 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.232Z] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:24:00 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0, Nodes: 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.232Z]
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:83 - Pipeline ONE PipelineID=fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0 reported by 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:24:00 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0, Nodes: 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:543fcfa0-c1eb-4d0c-8002-86ebb7aa5717, CreationTimestamp2023-06-30T10:23:57.232Z] moved to OPEN state
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865. Trying to get from SCM.
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 6ffe4e7e-001d-412a-bb99-f8460033a865, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.420Z] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:24:00 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 6ffe4e7e-001d-412a-bb99-f8460033a865, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.420Z]
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 reported by f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 reported by 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=3151ed07-a70f-4fd0-a420-0e698f85e75b. Trying to get from SCM.
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 3151ed07-a70f-4fd0-a420-0e698f85e75b, Nodes: 8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:8a812bbc-98e3-4715-97de-06c864241bf3, CreationTimestamp2023-06-30T10:23:57.406Z] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:24:00 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 3151ed07-a70f-4fd0-a420-0e698f85e75b, Nodes: 8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:8a812bbc-98e3-4715-97de-06c864241bf3, CreationTimestamp2023-06-30T10:23:57.406Z]
recon_1             | 2023-06-30 10:24:00 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 reported by 8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:24:05 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 reported by f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:24:05 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 reported by f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-30 10:24:05 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 6ffe4e7e-001d-412a-bb99-f8460033a865, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:f07392d7-ab98-42eb-9cf0-8171dd2a74d9, CreationTimestamp2023-06-30T10:23:57.420Z] moved to OPEN state
recon_1             | 2023-06-30 10:24:12 INFO  ReconContainerManager:89 - New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-30 10:24:12 INFO  ReconContainerManager:157 - Successfully added container #1 to Recon.
recon_1             | 2023-06-30 10:24:20 INFO  ReconContainerManager:89 - New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-30 10:24:20 INFO  ReconContainerManager:157 - Successfully added container #2 to Recon.
recon_1             | 2023-06-30 10:24:59 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-06-30 10:24:59 INFO  OzoneManagerServiceProviderImpl:384 - Obtaining delta updates from Ozone Manager
recon_1             | 2023-06-30 10:24:59 INFO  OzoneManagerServiceProviderImpl:350 - Number of updates received from OM : 14
recon_1             | 2023-06-30 10:24:59 INFO  ContainerKeyMapperTask:151 - ContainerKeyMapperTask successfully processed 8 OM DB update event(s).
recon_1             | 2023-06-30 10:24:59 INFO  FileSizeCountTask:159 - Completed a 'process' run of FileSizeCountTask.
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:22:39 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = e062f575ea14/172.20.0.4
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.0.0
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-30 10:22:39 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
s3g_1               | 2023-06-30 10:22:39 INFO  BaseHttpServer:207 - Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-30 10:22:39 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-30 10:22:39 INFO  log:169 - Logging initialized @6024ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-30 10:22:40 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-30 10:22:40 INFO  HttpRequestLog:86 - Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-30 10:22:40 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-30 10:22:40 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-30 10:22:40 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-30 10:22:40 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-30 10:22:40 INFO  Gateway:112 - STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 5b703bd8c125/172.20.0.7
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.0.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.10.3.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
s3g_1               | STARTUP_MSG:   java = 11.0.3
s3g_1               | ************************************************************/
s3g_1               | 2023-06-30 10:22:40 INFO  Gateway:90 - registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-30 10:22:40 INFO  Gateway:68 - Starting Ozone S3 gateway
s3g_1               | 2023-06-30 10:22:41 INFO  HttpServer2:1237 - Jetty bound to port 9878
s3g_1               | 2023-06-30 10:22:41 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
s3g_1               | 2023-06-30 10:22:41 INFO  session:333 - DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-30 10:22:41 INFO  session:338 - No SessionScavenger set, using defaults
s3g_1               | 2023-06-30 10:22:41 INFO  session:140 - node0 Scavenging every 660000ms
s3g_1               | 2023-06-30 10:22:41 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7dc19a70{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-30 10:22:41 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@23941fb4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/static,AVAILABLE}
s3g_1               | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 30, 2023 10:22:54 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-30 10:22:54 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@93501be{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_0_0_jar-_-any-7481443215637014421.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-30 10:22:54 INFO  AbstractConnector:330 - Started ServerConnector@81d9a72{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
s3g_1               | 2023-06-30 10:22:54 INFO  Server:399 - Started @21251ms
s3g_1               | 2023-06-30 10:22:54 INFO  BaseHttpServer:327 - HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:22:39 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:22:40 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:22:40 INFO  StorageContainerManager:644 - SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-ebeb0048-8153-4fbb-85e2-6ed1e8bc00e3;layoutVersion=0
scm_1               | 2023-06-30 10:22:40 INFO  StorageContainerManagerStarter:124 - SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at e062f575ea14/172.20.0.4
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:22:51 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = e062f575ea14/172.20.0.4
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.0.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:22:51 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:22:51 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:22:52 WARN  DBStoreBuilder:277 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:22:52 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@7b4c50bc
scm_1               | 2023-06-30 10:22:52 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
scm_1               | 2023-06-30 10:22:53 INFO  SCMNodeManager:116 - Entering startup safe mode.
scm_1               | 2023-06-30 10:22:53 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-30 10:22:53 INFO  SCMPipelineManager:161 - No pipeline exists in current db
scm_1               | 2023-06-30 10:22:53 INFO  HealthyPipelineSafeModeRule:89 - Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:22:53 INFO  OneReplicaPipelineSafeModeRule:79 - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:22:54 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
scm_1               | 2023-06-30 10:22:55 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:22:55 INFO  Server:1219 - Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-30 10:22:55 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:22:55 INFO  Server:1219 - Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-30 10:22:55 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:22:55 INFO  Server:1219 - Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-30 10:22:55 INFO  BaseHttpServer:207 - Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-30 10:22:55 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-30 10:22:55 INFO  log:169 - Logging initialized @12495ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-30 10:22:55 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-30 10:22:55 INFO  HttpRequestLog:86 - Http request log for http.requests.scm is not defined
scm_1               | 2023-06-30 10:22:55 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-30 10:22:55 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-30 10:22:55 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-30 10:22:55 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-30 10:22:55 INFO  StorageContainerManager:784 - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:22:56 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-30 10:22:56 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-30 10:22:56 INFO  MetricsSystemImpl:191 - StorageContainerManager metrics system started
scm_1               | 2023-06-30 10:22:56 INFO  SCMClientProtocolServer:156 - RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:22:56 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-30 10:22:56 INFO  Server:1298 - IPC Server listener on 9860: starting
scm_1               | 2023-06-30 10:22:56 INFO  StorageContainerManager:796 - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:22:56 INFO  SCMBlockProtocolServer:149 - RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:22:56 INFO  Server:1298 - IPC Server listener on 9863: starting
scm_1               | 2023-06-30 10:22:56 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-30 10:22:56 INFO  StorageContainerManager:802 - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:22:56 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:22:56 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-30 10:22:56 INFO  Server:1298 - IPC Server listener on 9861: starting
scm_1               | 2023-06-30 10:22:57 INFO  HttpServer2:1237 - Jetty bound to port 9876
scm_1               | 2023-06-30 10:22:57 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
scm_1               | 2023-06-30 10:22:57 INFO  session:333 - DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-30 10:22:57 INFO  session:338 - No SessionScavenger set, using defaults
scm_1               | 2023-06-30 10:22:57 INFO  session:140 - node0 Scavenging every 600000ms
scm_1               | 2023-06-30 10:22:57 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@66ba7e45{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-30 10:22:57 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7573e12f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-30 10:22:57 WARN  Server:1670 - IPC Server handler 1 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.8:56260: output error
scm_1               | 2023-06-30 10:22:57 INFO  Server:2928 - IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.AsynchronousCloseException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
scm_1               | 2023-06-30 10:22:57 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@e9ef5b6{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_0_0_jar-_-any-1590588919863757954.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/scm}
scm_1               | 2023-06-30 10:22:57 INFO  AbstractConnector:330 - Started ServerConnector@5f80fa43{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1               | 2023-06-30 10:22:57 INFO  Server:399 - Started @14867ms
scm_1               | 2023-06-30 10:22:57 INFO  MetricsSinkAdapter:204 - Sink prometheus started
scm_1               | 2023-06-30 10:22:57 INFO  MetricsSystemImpl:301 - Registered sink prometheus
scm_1               | 2023-06-30 10:22:57 INFO  BaseHttpServer:327 - HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-30 10:22:57 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
scm_1               | 2023-06-30 10:23:57 INFO  NetworkTopology:111 - Added a new node: /default-rack/f07392d7-ab98-42eb-9cf0-8171dd2a74d9
scm_1               | 2023-06-30 10:23:57 INFO  SCMNodeManager:273 - Registered Data node : f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-30 10:23:57 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=f98618a2-4152-4f1c-9505-0746b4ae1ac8 to datanode:f07392d7-ab98-42eb-9cf0-8171dd2a74d9
scm_1               | 2023-06-30 10:23:57 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: f98618a2-4152-4f1c-9505-0746b4ae1ac8, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.162146Z]
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:71 - SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:23:57 INFO  NetworkTopology:111 - Added a new node: /default-rack/543fcfa0-c1eb-4d0c-8002-86ebb7aa5717
scm_1               | 2023-06-30 10:23:57 INFO  SCMNodeManager:273 - Registered Data node : 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:71 - SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:23:57 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0 to datanode:543fcfa0-c1eb-4d0c-8002-86ebb7aa5717
scm_1               | 2023-06-30 10:23:57 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0, Nodes: 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.232671Z]
scm_1               | 2023-06-30 10:23:57 INFO  NetworkTopology:111 - Added a new node: /default-rack/8a812bbc-98e3-4715-97de-06c864241bf3
scm_1               | 2023-06-30 10:23:57 INFO  SCMNodeManager:273 - Registered Data node : 8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:71 - SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:214 - DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:242 - All SCM safe mode pre check rules have passed
scm_1               | 2023-06-30 10:23:57 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:23:57 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=3151ed07-a70f-4fd0-a420-0e698f85e75b to datanode:8a812bbc-98e3-4715-97de-06c864241bf3
scm_1               | 2023-06-30 10:23:57 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 3151ed07-a70f-4fd0-a420-0e698f85e75b, Nodes: 8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.406404Z]
scm_1               | 2023-06-30 10:23:57 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 to datanode:f07392d7-ab98-42eb-9cf0-8171dd2a74d9
scm_1               | 2023-06-30 10:23:57 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 to datanode:8a812bbc-98e3-4715-97de-06c864241bf3
scm_1               | 2023-06-30 10:23:57 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=6ffe4e7e-001d-412a-bb99-f8460033a865 to datanode:543fcfa0-c1eb-4d0c-8002-86ebb7aa5717
scm_1               | 2023-06-30 10:23:57 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 6ffe4e7e-001d-412a-bb99-f8460033a865, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:23:57.420417Z]
scm_1               | 2023-06-30 10:24:00 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: f98618a2-4152-4f1c-9505-0746b4ae1ac8, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:f07392d7-ab98-42eb-9cf0-8171dd2a74d9, CreationTimestamp2023-06-30T10:23:57.162146Z] moved to OPEN state
scm_1               | 2023-06-30 10:24:00 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:24:00 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:24:00 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: fc2c2961-2cb8-4a6e-8d11-a8a2feffd5d0, Nodes: 543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:543fcfa0-c1eb-4d0c-8002-86ebb7aa5717, CreationTimestamp2023-06-30T10:23:57.232671Z] moved to OPEN state
scm_1               | 2023-06-30 10:24:00 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:24:00 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:24:00 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 3151ed07-a70f-4fd0-a420-0e698f85e75b, Nodes: 8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:8a812bbc-98e3-4715-97de-06c864241bf3, CreationTimestamp2023-06-30T10:23:57.406404Z] moved to OPEN state
scm_1               | 2023-06-30 10:24:00 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:24:00 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:24:05 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 6ffe4e7e-001d-412a-bb99-f8460033a865, Nodes: f07392d7-ab98-42eb-9cf0-8171dd2a74d9{ip: 172.20.0.10, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}8a812bbc-98e3-4715-97de-06c864241bf3{ip: 172.20.0.8, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}543fcfa0-c1eb-4d0c-8002-86ebb7aa5717{ip: 172.20.0.6, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:f07392d7-ab98-42eb-9cf0-8171dd2a74d9, CreationTimestamp2023-06-30T10:23:57.420417Z] moved to OPEN state
scm_1               | 2023-06-30 10:24:05 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:24:05 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:24:05 INFO  SCMSafeModeManager:214 - HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:24:05 INFO  SCMSafeModeManager:228 - ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-30 10:24:05 INFO  SCMSafeModeManager:257 - SCM exiting safe mode.
scm_1               | 2023-06-30 10:24:37 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.12
scm_1               | 2023-06-30 10:24:45 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.12
scm_1               | 2023-06-30 10:25:31 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.12
scm_1               | 2023-06-30 10:25:40 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.12
Attaching to xcompat_old_client_1_0_0_1, xcompat_old_client_1_3_0_1, xcompat_new_client_1, xcompat_recon_1, xcompat_om_1, xcompat_datanode_2, xcompat_datanode_1, xcompat_s3g_1, xcompat_old_client_1_1_0_1, xcompat_datanode_3, xcompat_old_client_1_2_1_1, xcompat_scm_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-30 10:25:57,749 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 2898081d72f3/172.21.0.8
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.1.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_1          | STARTUP_MSG:   java = 11.0.10
datanode_1          | ************************************************************/
datanode_1          | 2023-06-30 10:25:57,782 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-30 10:25:59,160 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-30 10:25:59,656 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-30 10:26:00,387 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-30 10:26:00,388 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-30 10:26:00,798 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2898081d72f3 ip:172.21.0.8
datanode_1          | 2023-06-30 10:26:01,819 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-30 10:26:01,829 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-06-30 10:26:01,833 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-30 10:26:01,851 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-30 10:26:01,994 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:26:02,216 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:26:02,216 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:26:02,217 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-30 10:26:08,569 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:26:08,925 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-30 10:26:09,585 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-30 10:26:09,620 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-30 10:26:09,620 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-30 10:26:09,648 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-30 10:26:09,649 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:26:09,650 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-30 10:26:09,651 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:26:11,099 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-30 10:26:11,120 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:26:11,135 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:26:11,252 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:26:11,296 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:26:11,829 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:26:11,938 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-30 10:26:12,073 [main] INFO util.log: Logging initialized @19015ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-30 10:26:12,707 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-30 10:26:12,718 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-30 10:26:12,770 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-30 10:26:12,797 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-30 10:26:12,799 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-30 10:26:12,799 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-30 10:26:13,023 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-30 10:26:13,024 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_1          | 2023-06-30 10:26:13,150 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-30 10:26:13,155 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-30 10:26:13,156 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-06-30 10:26:13,337 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-30 10:26:13,339 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-30 10:26:14,833 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-5981771180629465437/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-30 10:26:14,900 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-30 10:26:14,910 [main] INFO server.Server: Started @21853ms
datanode_1          | 2023-06-30 10:26:14,912 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-30 10:26:14,912 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-30 10:26:14,917 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:26:15,111 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@64b9fa9b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-30 10:26:15,499 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.11:9891
datanode_1          | 2023-06-30 10:26:15,643 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-30 10:26:18,305 [EndpointStateMachine task thread for recon/172.21.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.11:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-30 10:25:57,747 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 6454cb31ecca/172.21.0.7
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.1.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_2          | STARTUP_MSG:   java = 11.0.10
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-30 10:25:56,829 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = d6d2ba86d4d4/172.21.0.4
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.1.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_3          | STARTUP_MSG:   java = 11.0.10
datanode_2          | ************************************************************/
datanode_2          | 2023-06-30 10:25:57,778 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-30 10:25:59,214 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-30 10:25:59,701 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-30 10:26:00,510 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-30 10:26:00,510 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-30 10:26:00,958 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:6454cb31ecca ip:172.21.0.7
datanode_2          | 2023-06-30 10:26:02,060 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-30 10:26:02,061 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-06-30 10:26:02,069 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-30 10:26:02,083 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-30 10:26:02,166 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:26:02,415 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:26:02,415 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:26:02,416 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-30 10:26:09,206 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:26:09,480 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-30 10:26:10,035 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-30 10:26:10,035 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-30 10:26:10,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-30 10:26:10,059 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-30 10:26:10,076 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:26:10,077 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-30 10:26:10,106 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:26:11,605 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-30 10:26:11,606 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:26:11,614 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:26:11,772 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:26:11,800 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:26:12,326 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:26:12,482 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-30 10:26:12,621 [main] INFO util.log: Logging initialized @19755ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-30 10:26:13,260 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-30 10:26:13,284 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-30 10:26:13,339 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-30 10:26:13,360 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-30 10:26:13,361 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-30 10:26:13,361 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-30 10:26:13,585 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-30 10:26:13,592 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_2          | 2023-06-30 10:26:13,729 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-30 10:26:13,729 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-30 10:26:13,741 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-30 10:26:13,778 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2a334bac{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-30 10:26:13,779 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3ddeaa5f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-30 10:26:15,375 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@54e12f4c{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-8747543104936152369/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-30 10:26:15,408 [main] INFO server.AbstractConnector: Started ServerConnector@53ed80d3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-30 10:26:15,408 [main] INFO server.Server: Started @22543ms
datanode_2          | 2023-06-30 10:26:15,417 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-30 10:26:15,417 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-30 10:26:15,420 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:26:15,560 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2de273f0] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-30 10:26:16,058 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.11:9891
datanode_2          | 2023-06-30 10:26:16,230 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-30 10:26:19,417 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-30 10:26:19,419 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-30 10:26:19,709 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_2          | 2023-06-30 10:26:19,815 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.RaftServer: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start RPC server
datanode_2          | 2023-06-30 10:26:19,831 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: GrpcService started, listening on 9856
datanode_2          | 2023-06-30 10:26:19,832 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: GrpcService started, listening on 9857
datanode_2          | 2023-06-30 10:26:19,837 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: GrpcService started, listening on 9858
datanode_2          | 2023-06-30 10:26:19,861 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 is started using port 9858 for RATIS
datanode_2          | 2023-06-30 10:26:19,861 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-30 10:26:19,861 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-30 10:26:19,868 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@4494a55] INFO util.JvmPauseMonitor: JvmPauseMonitor-2da73d14-1164-4b4c-94db-9a0f32cfe9e9: Started
datanode_2          | 2023-06-30 10:26:24,678 [Command processor thread] INFO server.RaftServer: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: addNew group-A76B8F3523A3:[2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] returns group-A76B8F3523A3:java.util.concurrent.CompletableFuture@5deddca5[Not completed]
datanode_2          | 2023-06-30 10:26:24,823 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: new RaftServerImpl for group-A76B8F3523A3:[2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:26:24,860 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:26:24,868 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:26:24,869 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:26:24,869 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:26:24,869 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:26:24,879 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:26:24,880 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:26:24,890 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_2          | 2023-06-30 10:26:24,909 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3: ConfigurationManager, init=-1: [2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:26:24,909 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:26:24,914 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:26:24,944 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7e104799-667a-4e9c-87ea-a76b8f3523a3 does not exist. Creating ...
datanode_2          | 2023-06-30 10:26:24,993 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7e104799-667a-4e9c-87ea-a76b8f3523a3/in_use.lock acquired by nodename 7@6454cb31ecca
datanode_2          | 2023-06-30 10:26:25,029 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7e104799-667a-4e9c-87ea-a76b8f3523a3 has been successfully formatted.
datanode_2          | 2023-06-30 10:26:25,078 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-A76B8F3523A3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:26:25,116 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:26:25,134 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:26:25,187 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:26:25,187 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:26:25,194 [grpc-default-executor-0] INFO server.RaftServer: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: addNew group-6056A7998D63:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1] returns group-6056A7998D63:java.util.concurrent.CompletableFuture@7e6f9425[Not completed]
datanode_2          | 2023-06-30 10:26:25,193 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3
datanode_2          | 2023-06-30 10:26:25,210 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,239 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:26:25,248 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:26:25,261 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7e104799-667a-4e9c-87ea-a76b8f3523a3
datanode_2          | 2023-06-30 10:26:25,268 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:26:25,268 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:26:25,269 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | ************************************************************/
datanode_3          | 2023-06-30 10:25:56,877 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-30 10:25:58,309 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-30 10:25:58,793 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-30 10:25:59,514 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-30 10:25:59,514 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-30 10:26:00,015 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d6d2ba86d4d4 ip:172.21.0.4
datanode_3          | 2023-06-30 10:26:01,038 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-30 10:26:01,071 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-06-30 10:26:01,078 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-30 10:26:01,083 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-30 10:26:01,235 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:26:01,448 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:26:01,450 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:26:01,450 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-30 10:26:07,773 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:26:08,184 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-30 10:26:08,716 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-30 10:26:08,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-30 10:26:08,764 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-30 10:26:08,772 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-30 10:26:08,773 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:26:08,774 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-30 10:26:08,776 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:26:10,020 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-30 10:26:10,032 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:26:10,033 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:26:10,258 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:26:10,306 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:26:10,912 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:26:11,036 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-30 10:26:11,174 [main] INFO util.log: Logging initialized @19339ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-30 10:26:11,688 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-30 10:26:11,700 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-30 10:26:11,728 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-30 10:26:11,743 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-30 10:26:11,743 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-30 10:26:11,745 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-30 10:26:11,959 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-30 10:26:11,972 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_3          | 2023-06-30 10:26:12,212 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-30 10:26:12,212 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-30 10:26:12,223 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-30 10:26:12,304 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43bdaa1b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-30 10:26:12,308 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@ea52184{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-30 10:26:13,976 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2db33feb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-2746547756103048036/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-30 10:26:14,022 [main] INFO server.AbstractConnector: Started ServerConnector@608b1fd2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-30 10:26:14,031 [main] INFO server.Server: Started @22188ms
datanode_3          | 2023-06-30 10:26:14,035 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-30 10:26:14,035 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-30 10:26:14,038 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:26:14,164 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2c566d27] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-30 10:26:14,614 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.11:9891
datanode_3          | 2023-06-30 10:26:14,918 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-30 10:26:17,292 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:26:17,295 [EndpointStateMachine task thread for recon/172.21.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.11:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:26:19,406 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-30 10:26:19,407 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-30 10:26:19,676 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_1          | 2023-06-30 10:26:19,779 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.RaftServer: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start RPC server
datanode_1          | 2023-06-30 10:26:19,789 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: GrpcService started, listening on 9856
datanode_1          | 2023-06-30 10:26:19,792 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: GrpcService started, listening on 9857
datanode_1          | 2023-06-30 10:26:19,793 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: GrpcService started, listening on 9858
datanode_1          | 2023-06-30 10:26:19,814 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9b7b92d4-f702-4dff-b9d3-d35288d2b16e is started using port 9858 for RATIS
datanode_1          | 2023-06-30 10:26:19,814 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9b7b92d4-f702-4dff-b9d3-d35288d2b16e is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-30 10:26:19,814 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9b7b92d4-f702-4dff-b9d3-d35288d2b16e is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-30 10:26:19,817 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@76415746] INFO util.JvmPauseMonitor: JvmPauseMonitor-9b7b92d4-f702-4dff-b9d3-d35288d2b16e: Started
datanode_1          | 2023-06-30 10:26:24,155 [Command processor thread] INFO server.RaftServer: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: addNew group-EA9B23822EA7:[9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:1] returns group-EA9B23822EA7:java.util.concurrent.CompletableFuture@6f622571[Not completed]
datanode_1          | 2023-06-30 10:26:24,213 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: new RaftServerImpl for group-EA9B23822EA7:[9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:26:24,231 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:26:24,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:26:24,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:26:24,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:26:24,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:26:24,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:26:24,233 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:26:24,236 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7: ConfigurationManager, init=-1: [9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:26:24,243 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:26:24,256 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:26:24,257 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/add455bf-80a2-4b1b-aa7d-ea9b23822ea7 does not exist. Creating ...
datanode_1          | 2023-06-30 10:26:24,266 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/add455bf-80a2-4b1b-aa7d-ea9b23822ea7/in_use.lock acquired by nodename 7@2898081d72f3
datanode_1          | 2023-06-30 10:26:24,276 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/add455bf-80a2-4b1b-aa7d-ea9b23822ea7 has been successfully formatted.
datanode_1          | 2023-06-30 10:26:24,288 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-EA9B23822EA7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:26:24,296 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:26:24,329 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:26:24,353 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:26:24,361 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:26:24,373 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7
datanode_1          | 2023-06-30 10:26:24,396 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:24,422 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:26:24,423 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:26:24,464 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/add455bf-80a2-4b1b-aa7d-ea9b23822ea7
datanode_1          | 2023-06-30 10:26:24,468 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:26:24,469 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:26:24,474 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:24,474 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:26:24,475 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:26:24,484 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:26:24,487 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:26:24,488 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:26:24,512 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:24,514 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:26:24,538 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:26:24,551 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:26:24,595 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:26:24,596 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:26:24,596 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:26:24,597 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:26:24,598 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:26:24,603 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:26:24,677 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7
datanode_1          | 2023-06-30 10:26:24,681 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7
datanode_1          | 2023-06-30 10:26:24,703 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7: start as a follower, conf=-1: [9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:26:24,711 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:26:24,712 [pool-19-thread-1] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState
datanode_1          | 2023-06-30 10:26:24,740 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EA9B23822EA7,id=9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_1          | 2023-06-30 10:26:24,741 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7
datanode_1          | 2023-06-30 10:26:24,776 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=add455bf-80a2-4b1b-aa7d-ea9b23822ea7.
datanode_1          | 2023-06-30 10:26:24,776 [Command processor thread] INFO server.RaftServer: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: addNew group-6056A7998D63:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] returns group-6056A7998D63:java.util.concurrent.CompletableFuture@197ec0c3[Not completed]
datanode_1          | 2023-06-30 10:26:24,779 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: new RaftServerImpl for group-6056A7998D63:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:26:24,787 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:26:24,787 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:26:24,787 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:26:24,787 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:26:24,789 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:26:24,790 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:26:18,296 [EndpointStateMachine task thread for recon/172.21.0.11:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.11:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:26:19,323 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-30 10:26:19,324 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-30 10:26:19,741 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_3          | 2023-06-30 10:26:19,819 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.RaftServer: 736e472b-e89f-413d-b6e5-8d6dd4191048: start RPC server
datanode_3          | 2023-06-30 10:26:19,824 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 736e472b-e89f-413d-b6e5-8d6dd4191048: GrpcService started, listening on 9856
datanode_3          | 2023-06-30 10:26:19,830 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 736e472b-e89f-413d-b6e5-8d6dd4191048: GrpcService started, listening on 9857
datanode_3          | 2023-06-30 10:26:19,831 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO server.GrpcService: 736e472b-e89f-413d-b6e5-8d6dd4191048: GrpcService started, listening on 9858
datanode_3          | 2023-06-30 10:26:19,850 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 736e472b-e89f-413d-b6e5-8d6dd4191048 is started using port 9858 for RATIS
datanode_3          | 2023-06-30 10:26:19,850 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 736e472b-e89f-413d-b6e5-8d6dd4191048 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-30 10:26:19,850 [EndpointStateMachine task thread for scm/172.21.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 736e472b-e89f-413d-b6e5-8d6dd4191048 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-30 10:26:19,856 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@6610365f] INFO util.JvmPauseMonitor: JvmPauseMonitor-736e472b-e89f-413d-b6e5-8d6dd4191048: Started
datanode_3          | 2023-06-30 10:26:23,198 [Command processor thread] INFO server.RaftServer: 736e472b-e89f-413d-b6e5-8d6dd4191048: addNew group-4C2186C9621B:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1] returns group-4C2186C9621B:java.util.concurrent.CompletableFuture@5851c8da[Not completed]
datanode_3          | 2023-06-30 10:26:23,223 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048: new RaftServerImpl for group-4C2186C9621B:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:26:23,231 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:26:23,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:26:23,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:26:23,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:26:23,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:26:23,232 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:26:23,233 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:26:23,241 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B: ConfigurationManager, init=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:26:23,247 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:26:23,250 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:26:23,254 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/f9784336-76ea-441a-a276-4c2186c9621b does not exist. Creating ...
datanode_3          | 2023-06-30 10:26:23,264 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f9784336-76ea-441a-a276-4c2186c9621b/in_use.lock acquired by nodename 7@d6d2ba86d4d4
datanode_3          | 2023-06-30 10:26:23,274 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/f9784336-76ea-441a-a276-4c2186c9621b has been successfully formatted.
datanode_3          | 2023-06-30 10:26:23,311 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-4C2186C9621B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:26:23,316 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:26:23,321 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:26:23,334 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:26:23,334 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:26:23,341 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B
datanode_3          | 2023-06-30 10:26:23,358 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:23,366 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:26:23,368 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:26:23,378 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f9784336-76ea-441a-a276-4c2186c9621b
datanode_3          | 2023-06-30 10:26:23,379 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-30 10:26:23,379 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:26:23,380 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:23,380 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:26:23,382 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:26:23,383 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:26:23,386 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:26:24,790 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:26:24,790 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: ConfigurationManager, init=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:26:24,790 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:26:24,790 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:26:24,791 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 does not exist. Creating ...
datanode_1          | 2023-06-30 10:26:24,797 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63/in_use.lock acquired by nodename 7@2898081d72f3
datanode_1          | 2023-06-30 10:26:24,799 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 has been successfully formatted.
datanode_1          | 2023-06-30 10:26:24,799 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-6056A7998D63: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:26:24,800 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:26:24,800 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:26:24,800 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:26:24,800 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:26:24,800 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63
datanode_1          | 2023-06-30 10:26:24,801 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:24,801 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:26:24,801 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:26:24,801 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63
datanode_1          | 2023-06-30 10:26:24,801 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:26:24,806 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:26:24,810 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:24,810 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:26:24,810 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:26:24,810 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:26:24,811 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:26:24,811 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:26:24,811 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:24,812 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:26:24,812 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:26:24,813 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:26:24,826 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:26:24,827 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:26:24,827 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:26:24,827 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:26:24,827 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:26:24,827 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:26:24,827 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63
datanode_1          | 2023-06-30 10:26:24,828 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63
datanode_1          | 2023-06-30 10:26:24,829 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: start as a follower, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:26:24,829 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:26:24,829 [pool-19-thread-1] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState
datanode_1          | 2023-06-30 10:26:24,829 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6056A7998D63,id=9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_1          | 2023-06-30 10:26:24,829 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63
datanode_1          | 2023-06-30 10:26:25,071 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D1580A4F53E3->2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_1          | 2023-06-30 10:26:26,240 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_1          | 2023-06-30 10:26:26,361 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-99CAAF4FF0B3->736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_1          | 2023-06-30 10:26:26,416 [grpc-default-executor-0] INFO server.RaftServer: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: addNew group-A6BE02E7C387:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0] returns group-A6BE02E7C387:java.util.concurrent.CompletableFuture@3338d91[Not completed]
datanode_1          | 2023-06-30 10:26:26,423 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: new RaftServerImpl for group-A6BE02E7C387:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:26:26,423 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:26:26,424 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:26:26,424 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:26:26,425 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:26:26,425 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:26:26,425 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:26:26,425 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:26:26,434 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: ConfigurationManager, init=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:26:26,434 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:26:25,270 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:26:25,271 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:26:25,278 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:26:25,278 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:26:25,279 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:26:26,435 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:26:26,435 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387 does not exist. Creating ...
datanode_1          | 2023-06-30 10:26:26,440 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387/in_use.lock acquired by nodename 7@2898081d72f3
datanode_1          | 2023-06-30 10:26:26,441 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387 has been successfully formatted.
datanode_1          | 2023-06-30 10:26:26,448 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-A6BE02E7C387: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:26:26,448 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:26:26,448 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:26:26,448 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:26:26,448 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:26:26,449 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387
datanode_1          | 2023-06-30 10:26:26,449 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:26,449 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:26:26,451 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:26:26,452 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387
datanode_1          | 2023-06-30 10:26:26,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:26:26,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:26:26,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:26,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:26:26,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:26:26,485 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:26:26,486 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:26:26,486 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:26:26,487 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:26:26,489 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:26:26,490 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:26:26,490 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:26:26,496 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:26:26,497 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:26:26,497 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:26:26,497 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:26:26,497 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:26:26,497 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:26:26,498 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387
datanode_1          | 2023-06-30 10:26:26,498 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387
datanode_1          | 2023-06-30 10:26:26,500 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: start as a follower, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:26:26,503 [pool-19-thread-1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:26:26,504 [pool-19-thread-1] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:26,517 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A6BE02E7C387,id=9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_1          | 2023-06-30 10:26:26,517 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387
datanode_1          | 2023-06-30 10:26:26,689 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63.
datanode_1          | 2023-06-30 10:26:28,866 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: receive requestVote(ELECTION, 736e472b-e89f-413d-b6e5-8d6dd4191048, group-6056A7998D63, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:26:28,867 [grpc-default-executor-0] INFO impl.VoteContext: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FOLLOWER: accept ELECTION from 736e472b-e89f-413d-b6e5-8d6dd4191048: our priority 0 <= candidate's priority 0
datanode_1          | 2023-06-30 10:26:28,868 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_1          | 2023-06-30 10:26:28,868 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState
datanode_3          | 2023-06-30 10:26:23,386 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:26:23,398 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:23,402 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:26:23,415 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:26:23,415 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:26:23,418 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:26:23,422 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:26:23,423 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:26:23,426 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:26:23,427 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:26:23,427 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:26:23,468 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B
datanode_3          | 2023-06-30 10:26:23,472 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B
datanode_3          | 2023-06-30 10:26:23,490 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B: start as a follower, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:26:23,491 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:26:23,493 [pool-19-thread-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState
datanode_3          | 2023-06-30 10:26:23,506 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4C2186C9621B,id=736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_3          | 2023-06-30 10:26:23,506 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B
datanode_3          | 2023-06-30 10:26:23,543 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=f9784336-76ea-441a-a276-4c2186c9621b.
datanode_3          | 2023-06-30 10:26:23,543 [Command processor thread] INFO server.RaftServer: 736e472b-e89f-413d-b6e5-8d6dd4191048: addNew group-6056A7998D63:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] returns group-6056A7998D63:java.util.concurrent.CompletableFuture@27428ff7[Not completed]
datanode_3          | 2023-06-30 10:26:23,551 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048: new RaftServerImpl for group-6056A7998D63:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: ConfigurationManager, init=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:26:23,552 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:26:23,553 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:26:23,553 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 does not exist. Creating ...
datanode_3          | 2023-06-30 10:26:23,560 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63/in_use.lock acquired by nodename 7@d6d2ba86d4d4
datanode_3          | 2023-06-30 10:26:23,561 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 has been successfully formatted.
datanode_3          | 2023-06-30 10:26:23,561 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-6056A7998D63: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:26:23,562 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:26:23,562 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:26:23,562 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:26:23,562 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:26:23,562 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63
datanode_3          | 2023-06-30 10:26:23,562 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:23,562 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:26:23,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:26:23,564 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:23,564 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:26:23,564 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:26:23,564 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:26:23,598 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:26:23,598 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:26:23,598 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:26:23,599 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:26:23,599 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:26:23,599 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:26:23,600 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63
datanode_3          | 2023-06-30 10:26:23,612 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63
datanode_3          | 2023-06-30 10:26:23,615 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: start as a follower, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:26:23,618 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:26:23,618 [pool-19-thread-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState
datanode_3          | 2023-06-30 10:26:23,620 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6056A7998D63,id=736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_3          | 2023-06-30 10:26:23,620 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63
datanode_3          | 2023-06-30 10:26:23,722 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-53F05DD1A843->2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_3          | 2023-06-30 10:26:26,003 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FB97A42B51A2->9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_3          | 2023-06-30 10:26:26,532 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_3          | 2023-06-30 10:26:26,584 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63.
datanode_3          | 2023-06-30 10:26:26,584 [Command processor thread] INFO server.RaftServer: 736e472b-e89f-413d-b6e5-8d6dd4191048: addNew group-A6BE02E7C387:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-A6BE02E7C387:java.util.concurrent.CompletableFuture@3437a0d8[Not completed]
datanode_3          | 2023-06-30 10:26:26,585 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048: new RaftServerImpl for group-A6BE02E7C387:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:26:26,588 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:26:26,588 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:26:26,588 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:26:26,588 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:26:26,590 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:26:26,595 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:26:26,596 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:26:26,597 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: ConfigurationManager, init=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:26:26,598 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:26:26,598 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:26:26,604 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387 does not exist. Creating ...
datanode_3          | 2023-06-30 10:26:26,612 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387/in_use.lock acquired by nodename 7@d6d2ba86d4d4
datanode_3          | 2023-06-30 10:26:26,617 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387 has been successfully formatted.
datanode_3          | 2023-06-30 10:26:26,628 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-A6BE02E7C387: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:26:26,628 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:26:26,628 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:26:26,628 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:26:26,629 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:26:26,629 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387
datanode_3          | 2023-06-30 10:26:26,629 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:26,629 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:26:26,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:26:26,635 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387
datanode_3          | 2023-06-30 10:26:26,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-30 10:26:26,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:26:26,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:26,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:26:26,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:26:26,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:26:26,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:26:26,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:26:26,637 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:26:26,637 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:26:26,637 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:26:26,638 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:26:26,638 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:26:26,638 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:26:26,649 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:26:26,649 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:26:26,650 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:26:26,650 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:26:26,650 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387
datanode_3          | 2023-06-30 10:26:26,654 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387
datanode_3          | 2023-06-30 10:26:26,655 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: start as a follower, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:26:26,655 [pool-19-thread-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:26:26,655 [pool-19-thread-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:26,661 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A6BE02E7C387,id=736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_3          | 2023-06-30 10:26:26,661 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387
datanode_3          | 2023-06-30 10:26:26,663 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-313CC001FDAF->9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_3          | 2023-06-30 10:26:26,743 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-ADB77A1C58E3->2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_3          | 2023-06-30 10:26:26,807 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387.
datanode_2          | 2023-06-30 10:26:25,293 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,299 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:26:25,335 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:26:25,351 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:26:25,364 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:26:25,368 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:26:25,370 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:26:25,374 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:26:25,375 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:26:25,379 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:26:25,420 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3
datanode_2          | 2023-06-30 10:26:25,423 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3
datanode_2          | 2023-06-30 10:26:25,480 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: new RaftServerImpl for group-6056A7998D63:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:26:25,481 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:26:25,481 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:26:25,481 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:26:25,481 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:26:25,482 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:26:25,482 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:26:25,483 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:26:25,487 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: ConfigurationManager, init=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:26:25,488 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:26:25,488 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:26:25,488 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 does not exist. Creating ...
datanode_2          | 2023-06-30 10:26:25,490 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63/in_use.lock acquired by nodename 7@6454cb31ecca
datanode_2          | 2023-06-30 10:26:25,492 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 has been successfully formatted.
datanode_2          | 2023-06-30 10:26:25,508 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-6056A7998D63: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:26:25,511 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:26:25,513 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:26:25,514 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:26:25,515 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:26:25,515 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63
datanode_2          | 2023-06-30 10:26:25,516 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,521 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:26:25,521 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:26:25,524 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63
datanode_2          | 2023-06-30 10:26:25,525 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:26:25,525 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:26:25,525 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,525 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:26:25,525 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:26:25,526 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:26:25,526 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:26:25,526 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:26:25,536 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,537 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:26:25,537 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:26:25,537 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:26:25,541 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:26:25,543 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:26:25,543 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:26:25,544 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:26:25,544 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:26:25,544 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:26:25,545 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63
datanode_2          | 2023-06-30 10:26:25,545 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63
datanode_2          | 2023-06-30 10:26:25,552 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3: start as a follower, conf=-1: [2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:26:25,554 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:26:25,555 [pool-19-thread-1] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState
datanode_2          | 2023-06-30 10:26:25,578 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A76B8F3523A3,id=2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_2          | 2023-06-30 10:26:25,579 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3
datanode_2          | 2023-06-30 10:26:25,616 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: start as a follower, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-30 10:26:25,617 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:26:25,618 [pool-19-thread-1] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState
datanode_2          | 2023-06-30 10:26:25,617 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=7e104799-667a-4e9c-87ea-a76b8f3523a3.
datanode_2          | 2023-06-30 10:26:25,635 [Command processor thread] INFO server.RaftServer: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: addNew group-A6BE02E7C387:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-A6BE02E7C387:java.util.concurrent.CompletableFuture@627d0943[Not completed]
datanode_2          | 2023-06-30 10:26:25,635 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6056A7998D63,id=2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_2          | 2023-06-30 10:26:25,635 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63
datanode_2          | 2023-06-30 10:26:25,638 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: new RaftServerImpl for group-A6BE02E7C387:[736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:26:25,640 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:26:25,640 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:26:25,640 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:26:25,641 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:26:25,641 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:26:25,641 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:26:25,641 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:26:25,642 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: ConfigurationManager, init=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:26:25,642 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:26:25,642 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:26:25,644 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387 does not exist. Creating ...
datanode_2          | 2023-06-30 10:26:25,650 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387/in_use.lock acquired by nodename 7@6454cb31ecca
datanode_1          | 2023-06-30 10:26:28,869 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-30 10:26:28,870 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState
datanode_1          | 2023-06-30 10:26:28,886 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63 replies to ELECTION vote request: 736e472b-e89f-413d-b6e5-8d6dd4191048<-9b7b92d4-f702-4dff-b9d3-d35288d2b16e#0:OK-t1. Peer's state: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63:t1, leader=null, voted=736e472b-e89f-413d-b6e5-8d6dd4191048, raftlog=9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:26:29,827 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5115137782ns, electionTimeout:5111ms
datanode_1          | 2023-06-30 10:26:29,828 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState
datanode_1          | 2023-06-30 10:26:29,828 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:26:29,830 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:26:29,830 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1
datanode_1          | 2023-06-30 10:26:29,837 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:26:29,838 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-30 10:26:29,838 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1
datanode_1          | 2023-06-30 10:26:29,838 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-30 10:26:29,838 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EA9B23822EA7 with new leaderId: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_1          | 2023-06-30 10:26:29,839 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7: change Leader from null to 9b7b92d4-f702-4dff-b9d3-d35288d2b16e at term 1 for becomeLeader, leader elected after 5542ms
datanode_1          | 2023-06-30 10:26:29,846 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:26:29,848 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7
datanode_1          | 2023-06-30 10:26:29,849 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:26:29,849 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-30 10:26:29,853 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:26:29,853 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:26:29,854 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:26:29,858 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderStateImpl
datanode_1          | 2023-06-30 10:26:29,868 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:26:29,891 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-LeaderElection1] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7: set configuration 0: [9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-30 10:26:29,923 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-EA9B23822EA7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/add455bf-80a2-4b1b-aa7d-ea9b23822ea7/current/log_inprogress_0
datanode_1          | 2023-06-30 10:26:30,916 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: receive requestVote(ELECTION, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9, group-A6BE02E7C387, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:26:30,916 [grpc-default-executor-0] INFO impl.VoteContext: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FOLLOWER: accept ELECTION from 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: our priority 0 <= candidate's priority 0
datanode_1          | 2023-06-30 10:26:30,916 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_1          | 2023-06-30 10:26:30,916 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:30,917 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:30,917 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-30 10:26:30,920 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387 replies to ELECTION vote request: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9<-9b7b92d4-f702-4dff-b9d3-d35288d2b16e#0:OK-t1. Peer's state: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387:t1, leader=null, voted=2da73d14-1164-4b4c-94db-9a0f32cfe9e9, raftlog=9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:26:34,052 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: receive requestVote(ELECTION, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9, group-6056A7998D63, 2, (t:0, i:0))
datanode_1          | 2023-06-30 10:26:34,052 [grpc-default-executor-0] INFO impl.VoteContext: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FOLLOWER: accept ELECTION from 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:26:34,052 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_1          | 2023-06-30 10:26:34,053 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState
datanode_1          | 2023-06-30 10:26:34,053 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-30 10:26:34,053 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-FollowerState
datanode_1          | 2023-06-30 10:26:34,055 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63 replies to ELECTION vote request: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9<-9b7b92d4-f702-4dff-b9d3-d35288d2b16e#0:OK-t2. Peer's state: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63:t2, leader=null, voted=2da73d14-1164-4b4c-94db-9a0f32cfe9e9, raftlog=9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:26:34,099 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6056A7998D63 with new leaderId: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_1          | 2023-06-30 10:26:34,099 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: change Leader from null to 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 at term 2 for appendEntries, leader elected after 9298ms
datanode_1          | 2023-06-30 10:26:34,174 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63: set configuration 0: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-30 10:26:34,174 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:26:34,176 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-6056A7998D63-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63/current/log_inprogress_0
datanode_1          | 2023-06-30 10:26:36,003 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5083350644ns, electionTimeout:5083ms
datanode_1          | 2023-06-30 10:26:36,004 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:36,004 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-30 10:26:36,004 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:26:36,004 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2
datanode_2          | 2023-06-30 10:26:25,656 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387 has been successfully formatted.
datanode_3          | 2023-06-30 10:26:28,719 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5226392948ns, electionTimeout:5192ms
datanode_3          | 2023-06-30 10:26:28,720 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState
datanode_3          | 2023-06-30 10:26:28,720 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:26:28,722 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:26:28,722 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-FollowerState] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1
datanode_3          | 2023-06-30 10:26:28,740 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:26:28,741 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-30 10:26:28,741 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1
datanode_3          | 2023-06-30 10:26:28,741 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:26:28,742 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4C2186C9621B with new leaderId: 736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_3          | 2023-06-30 10:26:28,742 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B: change Leader from null to 736e472b-e89f-413d-b6e5-8d6dd4191048 at term 1 for becomeLeader, leader elected after 5426ms
datanode_3          | 2023-06-30 10:26:28,747 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:26:28,749 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B
datanode_3          | 2023-06-30 10:26:28,756 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5138219846ns, electionTimeout:5131ms
datanode_3          | 2023-06-30 10:26:28,760 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState
datanode_3          | 2023-06-30 10:26:28,760 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:26:28,760 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:26:28,760 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:26:28,761 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2
datanode_3          | 2023-06-30 10:26:28,761 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-30 10:26:28,765 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:26:28,765 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:26:28,765 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:26:28,780 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:26:28,809 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderStateImpl
datanode_3          | 2023-06-30 10:26:28,891 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:26:28,897 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2] INFO impl.LeaderElection:   Response 0: 736e472b-e89f-413d-b6e5-8d6dd4191048<-2da73d14-1164-4b4c-94db-9a0f32cfe9e9#0:FAIL-t1
datanode_3          | 2023-06-30 10:26:28,897 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-30 10:26:28,898 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-06-30 10:26:28,898 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2
datanode_3          | 2023-06-30 10:26:28,900 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-LeaderElection2] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState
datanode_3          | 2023-06-30 10:26:28,914 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:26:25,657 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-A6BE02E7C387: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:26:25,668 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:26:25,669 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:26:25,670 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:26:25,670 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:26:25,671 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387
datanode_2          | 2023-06-30 10:26:25,671 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,685 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:26:25,686 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:26:25,686 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387
datanode_2          | 2023-06-30 10:26:25,686 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:26:25,686 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:26:25,691 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,691 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:26:25,692 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:26:25,692 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:26:25,692 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:26:25,692 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:26:25,693 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:26:25,703 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:26:25,704 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:26:25,704 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:26:25,710 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:26:25,715 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:26:25,715 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:26:25,715 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:26:25,715 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:26:25,715 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:26:25,716 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387
datanode_2          | 2023-06-30 10:26:25,717 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387
datanode_2          | 2023-06-30 10:26:25,718 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: start as a follower, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:26:25,723 [pool-19-thread-1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:26:25,723 [pool-19-thread-1] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:25,724 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A6BE02E7C387,id=2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_2          | 2023-06-30 10:26:25,724 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387
datanode_2          | 2023-06-30 10:26:25,946 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-16AB234FB174->9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_2          | 2023-06-30 10:26:26,590 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D85809731B03->736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_2          | 2023-06-30 10:26:26,757 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387.
datanode_2          | 2023-06-30 10:26:28,858 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: receive requestVote(ELECTION, 736e472b-e89f-413d-b6e5-8d6dd4191048, group-6056A7998D63, 1, (t:0, i:0))
datanode_2          | 2023-06-30 10:26:28,859 [grpc-default-executor-0] INFO impl.VoteContext: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FOLLOWER: reject ELECTION from 736e472b-e89f-413d-b6e5-8d6dd4191048: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-30 10:26:28,860 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_2          | 2023-06-30 10:26:28,860 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState
datanode_2          | 2023-06-30 10:26:28,861 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:26:28,865 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState
datanode_2          | 2023-06-30 10:26:28,875 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63 replies to ELECTION vote request: 736e472b-e89f-413d-b6e5-8d6dd4191048<-2da73d14-1164-4b4c-94db-9a0f32cfe9e9#0:FAIL-t1. Peer's state: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63:t1, leader=null, voted=null, raftlog=2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-30 10:26:30,651 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5096306174ns, electionTimeout:5068ms
datanode_2          | 2023-06-30 10:26:30,652 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState
datanode_2          | 2023-06-30 10:26:30,652 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:26:30,654 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:26:30,654 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-FollowerState] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1
datanode_2          | 2023-06-30 10:26:30,660 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:26:30,661 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-30 10:26:30,661 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1
datanode_2          | 2023-06-30 10:26:30,667 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-30 10:26:30,668 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A76B8F3523A3 with new leaderId: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_2          | 2023-06-30 10:26:30,672 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3: change Leader from null to 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 at term 1 for becomeLeader, leader elected after 5578ms
datanode_2          | 2023-06-30 10:26:30,731 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:26:30,733 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3
datanode_2          | 2023-06-30 10:26:30,734 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:26:30,735 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-30 10:26:30,739 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:26:30,743 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:26:30,744 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:26:30,749 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderStateImpl
datanode_2          | 2023-06-30 10:26:30,770 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:26:30,804 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-LeaderElection1] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3: set configuration 0: [2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-30 10:26:30,866 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A76B8F3523A3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7e104799-667a-4e9c-87ea-a76b8f3523a3/current/log_inprogress_0
datanode_2          | 2023-06-30 10:26:30,878 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5154669079ns, electionTimeout:5153ms
datanode_2          | 2023-06-30 10:26:30,878 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:30,878 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:26:30,879 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:26:30,879 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2
datanode_1          | 2023-06-30 10:26:36,017 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:26:36,079 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:26:36,080 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection:   Response 0: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t2
datanode_1          | 2023-06-30 10:26:36,080 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-30 10:26:36,080 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-30 10:26:36,080 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2
datanode_1          | 2023-06-30 10:26:36,080 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection2] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:41,156 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5075474768ns, electionTimeout:5070ms
datanode_1          | 2023-06-30 10:26:41,156 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:41,156 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_1          | 2023-06-30 10:26:41,156 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:26:41,157 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3
datanode_1          | 2023-06-30 10:26:41,162 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:26:41,187 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:26:41,187 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection:   Response 0: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t3
datanode_1          | 2023-06-30 10:26:41,188 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-30 10:26:41,188 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_1          | 2023-06-30 10:26:41,194 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3
datanode_1          | 2023-06-30 10:26:41,194 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection3] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:46,323 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5129291381ns, electionTimeout:5125ms
datanode_1          | 2023-06-30 10:26:46,324 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:46,324 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_1          | 2023-06-30 10:26:46,324 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:26:46,324 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4
datanode_1          | 2023-06-30 10:26:46,327 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4 ELECTION round 0: submit vote requests at term 4 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:26:46,351 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:26:46,354 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO impl.LeaderElection:   Response 0: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t4
datanode_1          | 2023-06-30 10:26:46,354 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO impl.LeaderElection:   Response 1: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-2da73d14-1164-4b4c-94db-9a0f32cfe9e9#0:OK-t4
datanode_1          | 2023-06-30 10:26:46,354 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO impl.LeaderElection: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-30 10:26:46,359 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
datanode_1          | 2023-06-30 10:26:46,360 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4
datanode_1          | 2023-06-30 10:26:46,360 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-LeaderElection4] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:51,413 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: receive requestVote(ELECTION, 736e472b-e89f-413d-b6e5-8d6dd4191048, group-A6BE02E7C387, 5, (t:0, i:0))
datanode_1          | 2023-06-30 10:26:51,414 [grpc-default-executor-0] INFO impl.VoteContext: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FOLLOWER: accept ELECTION from 736e472b-e89f-413d-b6e5-8d6dd4191048: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:26:51,414 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_1          | 2023-06-30 10:26:51,414 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: shutdown 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:51,414 [grpc-default-executor-0] INFO impl.RoleInfo: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: start 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState
datanode_1          | 2023-06-30 10:26:51,414 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-30 10:26:51,417 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387 replies to ELECTION vote request: 736e472b-e89f-413d-b6e5-8d6dd4191048<-9b7b92d4-f702-4dff-b9d3-d35288d2b16e#0:OK-t5. Peer's state: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387:t5, leader=null, voted=736e472b-e89f-413d-b6e5-8d6dd4191048, raftlog=9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:26:51,488 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A6BE02E7C387 with new leaderId: 736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_1          | 2023-06-30 10:26:51,489 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: change Leader from null to 736e472b-e89f-413d-b6e5-8d6dd4191048 at term 5 for appendEntries, leader elected after 25040ms
datanode_1          | 2023-06-30 10:26:51,512 [grpc-default-executor-0] INFO server.RaftServer$Division: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387: set configuration 0: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:26:51,513 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:26:51,514 [9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e@group-A6BE02E7C387-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387/current/log_inprogress_0
datanode_2          | 2023-06-30 10:26:30,889 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:26:30,953 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-06-30 10:26:30,954 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection:   Response 0: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t1
datanode_2          | 2023-06-30 10:26:30,954 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection:   Response 1: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9<-9b7b92d4-f702-4dff-b9d3-d35288d2b16e#0:OK-t1
datanode_2          | 2023-06-30 10:26:30,954 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-30 10:26:30,954 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_2          | 2023-06-30 10:26:30,954 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2
datanode_2          | 2023-06-30 10:26:30,954 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-LeaderElection2] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:33,986 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5121185327ns, electionTimeout:5105ms
datanode_2          | 2023-06-30 10:26:33,986 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState
datanode_2          | 2023-06-30 10:26:33,987 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_2          | 2023-06-30 10:26:33,987 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:26:33,987 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-FollowerState] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3
datanode_2          | 2023-06-30 10:26:33,993 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-30 10:26:34,009 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-30 10:26:34,009 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO impl.LeaderElection:   Response 0: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:OK-t2
datanode_2          | 2023-06-30 10:26:34,009 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO impl.LeaderElection: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3 ELECTION round 0: result PASSED
datanode_2          | 2023-06-30 10:26:34,009 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3
datanode_2          | 2023-06-30 10:26:34,009 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_2          | 2023-06-30 10:26:34,010 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6056A7998D63 with new leaderId: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_2          | 2023-06-30 10:26:34,010 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: change Leader from null to 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 at term 2 for becomeLeader, leader elected after 8498ms
datanode_2          | 2023-06-30 10:26:34,015 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:26:34,016 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63
datanode_2          | 2023-06-30 10:26:34,016 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:26:34,016 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-30 10:26:34,017 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:26:34,017 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:26:34,017 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:26:34,043 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-30 10:26:34,043 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:26:34,043 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:26:28,965 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-LeaderElection1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B: set configuration 0: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-30 10:26:29,065 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-4C2186C9621B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f9784336-76ea-441a-a276-4c2186c9621b/current/log_inprogress_0
datanode_3          | 2023-06-30 10:26:30,936 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: receive requestVote(ELECTION, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9, group-A6BE02E7C387, 1, (t:0, i:0))
datanode_3          | 2023-06-30 10:26:30,937 [grpc-default-executor-1] INFO impl.VoteContext: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FOLLOWER: reject ELECTION from 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-30 10:26:30,937 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_3          | 2023-06-30 10:26:30,937 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:30,937 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:30,938 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-30 10:26:30,942 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387 replies to ELECTION vote request: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t1. Peer's state: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387:t1, leader=null, voted=null, raftlog=736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:26:33,996 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: receive requestVote(ELECTION, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9, group-6056A7998D63, 2, (t:0, i:0))
datanode_3          | 2023-06-30 10:26:33,996 [grpc-default-executor-1] INFO impl.VoteContext: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FOLLOWER: accept ELECTION from 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-30 10:26:33,997 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_3          | 2023-06-30 10:26:33,997 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState
datanode_3          | 2023-06-30 10:26:33,997 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState
datanode_3          | 2023-06-30 10:26:33,997 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-30 10:26:34,000 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63 replies to ELECTION vote request: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:OK-t2. Peer's state: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63:t2, leader=null, voted=2da73d14-1164-4b4c-94db-9a0f32cfe9e9, raftlog=736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:26:34,129 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6056A7998D63 with new leaderId: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9
datanode_3          | 2023-06-30 10:26:34,130 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: change Leader from null to 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 at term 2 for appendEntries, leader elected after 10567ms
datanode_3          | 2023-06-30 10:26:34,182 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63: set configuration 0: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-30 10:26:34,187 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:26:34,188 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-6056A7998D63-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63/current/log_inprogress_0
datanode_3          | 2023-06-30 10:26:36,061 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: receive requestVote(ELECTION, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e, group-A6BE02E7C387, 2, (t:0, i:0))
datanode_2          | 2023-06-30 10:26:34,049 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-30 10:26:34,052 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:26:34,052 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:26:34,053 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63
datanode_2          | 2023-06-30 10:26:34,061 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-30 10:26:34,061 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:26:34,061 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-30 10:26:34,061 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-30 10:26:34,066 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:26:34,066 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:26:34,068 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderStateImpl
datanode_2          | 2023-06-30 10:26:34,069 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:26:34,077 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-LeaderElection3] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63: set configuration 0: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:0, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-30 10:26:34,077 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-6056A7998D63-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b6a3f6b8-74cf-43ce-9e9a-6056a7998d63/current/log_inprogress_0
datanode_2          | 2023-06-30 10:26:36,066 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: receive requestVote(ELECTION, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e, group-A6BE02E7C387, 2, (t:0, i:0))
datanode_2          | 2023-06-30 10:26:36,066 [grpc-default-executor-0] INFO impl.VoteContext: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FOLLOWER: accept ELECTION from 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-30 10:26:36,066 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_2          | 2023-06-30 10:26:36,066 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:36,066 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:26:36,067 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:36,076 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387 replies to ELECTION vote request: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-2da73d14-1164-4b4c-94db-9a0f32cfe9e9#0:OK-t2. Peer's state: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387:t2, leader=null, voted=9b7b92d4-f702-4dff-b9d3-d35288d2b16e, raftlog=2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:26:41,196 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: receive requestVote(ELECTION, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e, group-A6BE02E7C387, 3, (t:0, i:0))
datanode_2          | 2023-06-30 10:26:41,197 [grpc-default-executor-0] INFO impl.VoteContext: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FOLLOWER: accept ELECTION from 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-30 10:26:41,197 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_2          | 2023-06-30 10:26:41,197 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:41,198 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:41,198 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:26:41,208 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387 replies to ELECTION vote request: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-2da73d14-1164-4b4c-94db-9a0f32cfe9e9#0:OK-t3. Peer's state: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387:t3, leader=null, voted=9b7b92d4-f702-4dff-b9d3-d35288d2b16e, raftlog=2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:26:46,332 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: receive requestVote(ELECTION, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e, group-A6BE02E7C387, 4, (t:0, i:0))
datanode_2          | 2023-06-30 10:26:46,332 [grpc-default-executor-0] INFO impl.VoteContext: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FOLLOWER: accept ELECTION from 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-30 10:26:46,333 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_2          | 2023-06-30 10:26:46,333 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:46,333 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:26:46,334 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:46,335 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387 replies to ELECTION vote request: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-2da73d14-1164-4b4c-94db-9a0f32cfe9e9#0:OK-t4. Peer's state: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387:t4, leader=null, voted=9b7b92d4-f702-4dff-b9d3-d35288d2b16e, raftlog=2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:26:51,420 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: receive requestVote(ELECTION, 736e472b-e89f-413d-b6e5-8d6dd4191048, group-A6BE02E7C387, 5, (t:0, i:0))
datanode_2          | 2023-06-30 10:26:51,420 [grpc-default-executor-0] INFO impl.VoteContext: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FOLLOWER: accept ELECTION from 736e472b-e89f-413d-b6e5-8d6dd4191048: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-30 10:26:51,420 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_2          | 2023-06-30 10:26:51,420 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: shutdown 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:51,420 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:26:51,424 [grpc-default-executor-0] INFO impl.RoleInfo: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9: start 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-FollowerState
datanode_2          | 2023-06-30 10:26:51,432 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387 replies to ELECTION vote request: 736e472b-e89f-413d-b6e5-8d6dd4191048<-2da73d14-1164-4b4c-94db-9a0f32cfe9e9#0:OK-t5. Peer's state: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387:t5, leader=null, voted=736e472b-e89f-413d-b6e5-8d6dd4191048, raftlog=2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:26:51,505 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A6BE02E7C387 with new leaderId: 736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_2          | 2023-06-30 10:26:51,507 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: change Leader from null to 736e472b-e89f-413d-b6e5-8d6dd4191048 at term 5 for appendEntries, leader elected after 25836ms
datanode_2          | 2023-06-30 10:26:51,536 [grpc-default-executor-0] INFO server.RaftServer$Division: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387: set configuration 0: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-30 10:26:51,537 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:26:51,538 [2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9@group-A6BE02E7C387-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387/current/log_inprogress_0
datanode_3          | 2023-06-30 10:26:36,061 [grpc-default-executor-1] INFO impl.VoteContext: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FOLLOWER: reject ELECTION from 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-30 10:26:36,061 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_3          | 2023-06-30 10:26:36,061 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:36,061 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-30 10:26:36,071 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:36,073 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387 replies to ELECTION vote request: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t2. Peer's state: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387:t2, leader=null, voted=null, raftlog=736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:26:41,166 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: receive requestVote(ELECTION, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e, group-A6BE02E7C387, 3, (t:0, i:0))
datanode_3          | 2023-06-30 10:26:41,166 [grpc-default-executor-1] INFO impl.VoteContext: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FOLLOWER: reject ELECTION from 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-30 10:26:41,166 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_3          | 2023-06-30 10:26:41,166 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:41,167 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-30 10:26:41,167 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:41,170 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387 replies to ELECTION vote request: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t3. Peer's state: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387:t3, leader=null, voted=null, raftlog=736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:26:46,345 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: receive requestVote(ELECTION, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e, group-A6BE02E7C387, 4, (t:0, i:0))
datanode_3          | 2023-06-30 10:26:46,345 [grpc-default-executor-1] INFO impl.VoteContext: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FOLLOWER: reject ELECTION from 9b7b92d4-f702-4dff-b9d3-d35288d2b16e: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-30 10:26:46,346 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
datanode_3          | 2023-06-30 10:26:46,346 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:46,346 [grpc-default-executor-1] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:46,346 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-30 10:26:46,348 [grpc-default-executor-1] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387 replies to ELECTION vote request: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e<-736e472b-e89f-413d-b6e5-8d6dd4191048#0:FAIL-t4. Peer's state: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387:t4, leader=null, voted=null, raftlog=736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:26:51,406 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO impl.FollowerState: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5059749104ns, electionTimeout:5058ms
datanode_3          | 2023-06-30 10:26:51,406 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState
datanode_3          | 2023-06-30 10:26:51,406 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
datanode_3          | 2023-06-30 10:26:51,406 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:26:51,406 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-FollowerState] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3
datanode_3          | 2023-06-30 10:26:51,409 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3 ELECTION round 0: submit vote requests at term 5 for -1: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:26:51,426 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:26:51,427 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection:   Response 0: 736e472b-e89f-413d-b6e5-8d6dd4191048<-9b7b92d4-f702-4dff-b9d3-d35288d2b16e#0:OK-t5
datanode_3          | 2023-06-30 10:26:51,427 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO impl.LeaderElection: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3 ELECTION round 0: result PASSED
datanode_3          | 2023-06-30 10:26:51,427 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: shutdown 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3
datanode_3          | 2023-06-30 10:26:51,427 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: changes role from CANDIDATE to LEADER at term 5 for changeToLeader
datanode_3          | 2023-06-30 10:26:51,428 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A6BE02E7C387 with new leaderId: 736e472b-e89f-413d-b6e5-8d6dd4191048
datanode_3          | 2023-06-30 10:26:51,429 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: change Leader from null to 736e472b-e89f-413d-b6e5-8d6dd4191048 at term 5 for becomeLeader, leader elected after 24799ms
datanode_3          | 2023-06-30 10:26:51,430 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:26:51,444 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387
datanode_3          | 2023-06-30 10:26:51,445 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:26:51,445 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-30 10:26:51,445 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:26:51,445 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:26:51,445 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:26:51,449 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:26:51,449 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:26:51,450 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:26:51,452 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-30 10:26:51,461 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:26:51,461 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:26:51,462 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387
datanode_3          | 2023-06-30 10:26:51,467 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:26:51,467 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:26:51,468 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:26:51,468 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-30 10:26:51,468 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:26:51,468 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:26:51,469 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO impl.RoleInfo: 736e472b-e89f-413d-b6e5-8d6dd4191048: start 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderStateImpl
datanode_3          | 2023-06-30 10:26:51,470 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:26:51,474 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/54933c8b-7f5b-4e75-ad0d-a6be02e7c387/current/log_inprogress_0
datanode_3          | 2023-06-30 10:26:51,475 [736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387-LeaderElection3] INFO server.RaftServer$Division: 736e472b-e89f-413d-b6e5-8d6dd4191048@group-A6BE02E7C387: set configuration 0: [736e472b-e89f-413d-b6e5-8d6dd4191048|rpc:172.21.0.4:9856|admin:172.21.0.4:9857|client:172.21.0.4:9858|dataStream:|priority:1, 9b7b92d4-f702-4dff-b9d3-d35288d2b16e|rpc:172.21.0.8:9856|admin:172.21.0.8:9857|client:172.21.0.8:9858|dataStream:|priority:0, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:25:56,812 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 1e2bd9029990/172.21.0.2
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-30 10:25:56,704 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 1d82289f999f/172.21.0.11
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.1.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.1.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
recon_1             | STARTUP_MSG:   java = 11.0.10
recon_1             | ************************************************************/
recon_1             | 2023-06-30 10:25:56,736 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-30 10:25:59,736 [main] INFO recon.ReconRestServletModule: rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-06-30 10:26:01,162 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-30 10:26:02,465 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:26:07,748 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:26:09,508 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:26:09,616 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:26:09,617 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-30 10:26:14,295 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-30 10:26:14,421 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-30 10:26:14,502 [main] INFO util.log: Logging initialized @21314ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-30 10:26:15,028 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-30 10:26:15,054 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-30 10:26:15,144 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-30 10:26:15,160 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-30 10:26:15,160 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-30 10:26:15,160 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-30 10:26:15,667 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-30 10:26:16,363 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-30 10:26:16,384 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-30 10:26:16,483 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-30 10:26:16,489 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-30 10:26:16,702 [main] INFO Configuration.deprecation: No unit for ozone.recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-06-30 10:26:17,072 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:26:17,232 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:26:17,280 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
recon_1             | 2023-06-30 10:26:17,282 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-30 10:26:17,386 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:26:17,489 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-30 10:26:17,501 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-30 10:26:17,503 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-30 10:26:17,543 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-30 10:26:17,575 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-30 10:26:17,638 [Listener at 0.0.0.0/9891] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
recon_1             | 2023-06-30 10:26:17,705 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-30 10:26:17,710 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-30 10:26:17,880 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-30 10:26:17,894 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-30 10:26:17,899 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-30 10:26:18,200 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-30 10:26:18,201 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
recon_1             | 2023-06-30 10:26:18,250 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-30 10:26:18,250 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-30 10:26:18,251 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-30 10:26:18,286 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4978777f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-30 10:26:18,287 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@75b38c36{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-30 10:26:20,829 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6e355249{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_1_0_jar-_-any-9969899544558042093/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/recon}
recon_1             | 2023-06-30 10:26:20,837 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@76ff68c5{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-30 10:26:20,838 [Listener at 0.0.0.0/9891] INFO server.Server: Started @27650ms
recon_1             | 2023-06-30 10:26:20,840 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-30 10:26:20,840 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-30 10:26:20,841 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-30 10:26:20,841 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-30 10:26:20,850 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-30 10:26:20,853 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-30 10:26:20,853 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-30 10:26:20,854 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:26:20,854 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-30 10:26:20,855 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:26:21,039 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 1 pipelines from SCM.
recon_1             | 2023-06-30 10:26:21,039 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:26:21,040 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=f9784336-76ea-441a-a276-4c2186c9621b from SCM.
recon_1             | 2023-06-30 10:26:21,050 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: f9784336-76ea-441a-a276-4c2186c9621b, Nodes: 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:20.326Z]
recon_1             | 2023-06-30 10:26:21,060 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:26:21,066 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-30 10:26:21,072 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-30 10:26:21,131 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-30 10:26:21,131 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-30 10:26:21,135 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-30 10:26:21,135 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-30 10:26:21,204 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 1 pipelines in house.
recon_1             | 2023-06-30 10:26:21,210 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=add455bf-80a2-4b1b-aa7d-ea9b23822ea7 from SCM.
recon_1             | 2023-06-30 10:26:21,211 [PipelineSyncTask] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: add455bf-80a2-4b1b-aa7d-ea9b23822ea7, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.158Z]
recon_1             | 2023-06-30 10:26:21,225 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 47 milliseconds.
recon_1             | 2023-06-30 10:26:21,294 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 158 milliseconds to process 0 existing database records.
recon_1             | 2023-06-30 10:26:21,317 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 17 milliseconds for processing 0 containers.
recon_1             | 2023-06-30 10:26:21,602 [IPC Server handler 4 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2da73d14-1164-4b4c-94db-9a0f32cfe9e9
recon_1             | 2023-06-30 10:26:21,607 [IPC Server handler 4 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:25:57,833 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 786a97a3d542/172.21.0.9
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-30 10:25:58,123 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-30 10:25:58,135 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-30 10:25:58,215 [main] INFO util.log: Logging initialized @6037ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-30 10:25:58,804 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-30 10:25:58,903 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-30 10:25:58,918 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-30 10:25:58,921 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-30 10:25:58,927 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-30 10:25:58,928 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-30 10:25:59,335 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 730ea96ab81f/172.21.0.5
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:25:56,864 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:25:57,301 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:25:57,529 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-6c170b35-112e-4f25-9e3a-dcbe7f0f13e7;layoutVersion=0
scm_1               | 2023-06-30 10:25:57,640 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 1e2bd9029990/172.21.0.2
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:26:08,377 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 1e2bd9029990/172.21.0.2
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:26:08,450 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:26:09,413 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:26:11,201 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:26:12,121 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
scm_1               | 2023-06-30 10:26:12,164 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-30 10:26:12,976 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-30 10:26:14,081 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-30 10:26:14,178 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-30 10:26:14,189 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1               | 2023-06-30 10:26:14,277 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-30 10:26:14,637 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:26:14,655 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:26:17,022 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:26:17,070 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-30 10:26:17,158 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:26:17,269 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-30 10:26:17,333 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:26:17,341 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-30 10:26:17,384 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:26:17,596 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-30 10:26:17,655 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-30 10:26:17,655 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-30 10:26:18,146 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:26:18,157 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:26:18,172 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-30 10:26:18,268 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:26:18,269 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:26:18,371 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-30 10:26:18,544 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:26:18,545 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:26:18,544 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:26:18,550 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-30 10:26:18,549 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:26:18,717 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1d540566] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-30 10:26:18,746 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-30 10:26:18,746 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-30 10:26:18,841 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @19722ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-30 10:26:19,374 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-30 10:26:19,407 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-30 10:26:19,425 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-30 10:26:19,436 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-30 10:26:19,443 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-30 10:26:19,443 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-30 10:26:19,746 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-06-30 10:25:57,892 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:26:04,187 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:26:04,607 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.9:9862
om_1                | 2023-06-30 10:26:04,608 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:26:04,608 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:26:04,667 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:26:06,884 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:07,903 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:08,905 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:09,906 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:10,906 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:11,907 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:12,908 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:13,909 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:14,909 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:15,910 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.2:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-30 10:26:15,917 [main] INFO utils.RetriableTask: Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-6c170b35-112e-4f25-9e3a-dcbe7f0f13e7;layoutVersion=0
om_1                | 2023-06-30 10:26:21,075 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 786a97a3d542/172.21.0.9
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:26:22,078 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 786a97a3d542/172.21.0.9
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-06-30 10:26:22,084 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:26:24,229 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:26:24,386 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.9:9862
om_1                | 2023-06-30 10:26:24,387 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:26:24,387 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:26:24,402 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:26:24,434 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:26:27,248 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:26:27,438 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-30 10:26:27,443 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-30 10:26:27,697 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-30 10:26:27,703 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:26:27,707 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-30 10:26:27,721 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-30 10:26:27,769 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:26:27,838 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872
om_1                | 2023-06-30 10:26:27,851 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-30 10:26:27,879 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-30 10:26:28,059 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-06-30 10:26:28,061 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:26:28,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-06-30 10:26:28,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:26:28,064 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:26:28,064 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-30 10:26:28,066 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:26:28,075 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-30 10:26:28,076 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
recon_1             | 2023-06-30 10:26:21,609 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 2da73d14-1164-4b4c-94db-9a0f32cfe9e9 to Node DB.
recon_1             | 2023-06-30 10:26:22,155 [IPC Server handler 1 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/736e472b-e89f-413d-b6e5-8d6dd4191048
recon_1             | 2023-06-30 10:26:22,156 [IPC Server handler 1 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:22,159 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 736e472b-e89f-413d-b6e5-8d6dd4191048 to Node DB.
recon_1             | 2023-06-30 10:26:23,136 [IPC Server handler 99 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9b7b92d4-f702-4dff-b9d3-d35288d2b16e
recon_1             | 2023-06-30 10:26:23,136 [IPC Server handler 99 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:23,137 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 9b7b92d4-f702-4dff-b9d3-d35288d2b16e to Node DB.
recon_1             | 2023-06-30 10:26:23,311 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=f9784336-76ea-441a-a276-4c2186c9621b reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:23,312 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: f9784336-76ea-441a-a276-4c2186c9621b, Nodes: 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:736e472b-e89f-413d-b6e5-8d6dd4191048, CreationTimestamp2023-06-30T10:26:20.326Z] moved to OPEN state
recon_1             | 2023-06-30 10:26:23,599 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63. Trying to get from SCM.
recon_1             | 2023-06-30 10:26:23,611 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b6a3f6b8-74cf-43ce-9e9a-6056a7998d63, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.622Z] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:26:23,611 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: b6a3f6b8-74cf-43ce-9e9a-6056a7998d63, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.622Z]
recon_1             | 2023-06-30 10:26:23,620 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:24,344 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=add455bf-80a2-4b1b-aa7d-ea9b23822ea7 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:24,345 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: add455bf-80a2-4b1b-aa7d-ea9b23822ea7, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:9b7b92d4-f702-4dff-b9d3-d35288d2b16e, CreationTimestamp2023-06-30T10:26:21.158Z] moved to OPEN state
recon_1             | 2023-06-30 10:26:24,840 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:25,127 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=7e104799-667a-4e9c-87ea-a76b8f3523a3. Trying to get from SCM.
recon_1             | 2023-06-30 10:26:25,129 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 7e104799-667a-4e9c-87ea-a76b8f3523a3, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.599Z] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:26:25,129 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7e104799-667a-4e9c-87ea-a76b8f3523a3, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.599Z]
recon_1             | 2023-06-30 10:26:25,130 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=7e104799-667a-4e9c-87ea-a76b8f3523a3 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:25,130 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 7e104799-667a-4e9c-87ea-a76b8f3523a3, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:2da73d14-1164-4b4c-94db-9a0f32cfe9e9, CreationTimestamp2023-06-30T10:26:21.599Z] moved to OPEN state
recon_1             | 2023-06-30 10:26:25,502 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:25,670 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387. Trying to get from SCM.
recon_1             | 2023-06-30 10:26:25,679 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 54933c8b-7f5b-4e75-ad0d-a6be02e7c387, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.624Z] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:26:25,679 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 54933c8b-7f5b-4e75-ad0d-a6be02e7c387, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.624Z]
recon_1             | 2023-06-30 10:26:25,680 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:25,686 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:26,488 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:26,488 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:26,647 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:26,648 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:28,744 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:28,745 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:29,841 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:29,841 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:30,674 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:30,674 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:34,028 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:34,028 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:34,028 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: b6a3f6b8-74cf-43ce-9e9a-6056a7998d63, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:2da73d14-1164-4b4c-94db-9a0f32cfe9e9, CreationTimestamp2023-06-30T10:26:21.622Z] moved to OPEN state
recon_1             | 2023-06-30 10:26:42,034 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:42,035 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-30 10:26:42,070 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-30 10:26:42,659 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:42,793 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:50,488 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:50,489 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-30 10:26:50,497 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-30 10:26:51,437 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 reported by 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:26:51,438 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 54933c8b-7f5b-4e75-ad0d-a6be02e7c387, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:736e472b-e89f-413d-b6e5-8d6dd4191048, CreationTimestamp2023-06-30T10:26:21.624Z] moved to OPEN state
recon_1             | 2023-06-30 10:26:58,533 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-30 10:26:58,553 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-30 10:27:20,874 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-30 10:27:20,874 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-30 10:27:21,250 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688120840875
recon_1             | 2023-06-30 10:27:21,253 [pool-14-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-30 10:27:21,254 [pool-14-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-30 10:27:21,280 [pool-14-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688120840875.
recon_1             | 2023-06-30 10:27:21,306 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-30 10:27:21,482 [pool-15-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-30 10:27:21,482 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:27:21,507 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1688120841484
recon_1             | 2023-06-30 10:27:21,507 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Cleaning up old Recon Container key DB at /data/metadata/recon/recon-container-key.db_1688120761495.
recon_1             | 2023-06-30 10:27:21,614 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:27:21,615 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.131 seconds to process 4 keys.
recon_1             | 2023-06-30 10:27:21,641 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-30 10:27:21,668 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
om_1                | 2023-06-30 10:26:28,384 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-30 10:26:28,385 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:26:28,386 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:26:28,394 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:26:28,399 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@7102ac3e[Not completed]
om_1                | 2023-06-30 10:26:28,400 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-30 10:26:28,422 [pool-17-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-30 10:26:28,426 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-30 10:26:28,426 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-30 10:26:28,427 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-30 10:26:28,427 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-30 10:26:28,430 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:26:28,430 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:26:28,430 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-30 10:26:28,431 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-30 10:26:28,436 [pool-17-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-30 10:26:28,436 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-30 10:26:28,437 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:26:28,440 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-30 10:26:28,444 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-30 10:26:28,458 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@786a97a3d542
om_1                | 2023-06-30 10:26:28,508 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-30 10:26:28,510 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-30 10:26:28,512 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-30 10:26:28,525 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-30 10:26:28,528 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:26:28,536 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om1@group-C5BA1605619E
om_1                | 2023-06-30 10:26:28,553 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:26:28,560 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-30 10:26:28,560 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-30 10:26:28,564 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-30 10:26:28,564 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:26:28,564 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-30 10:26:28,566 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:26:28,566 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-30 10:26:28,566 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-30 10:26:28,567 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-30 10:26:28,567 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-30 10:26:28,569 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-30 10:26:28,587 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-30 10:26:28,589 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-30 10:26:28,598 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-30 10:26:28,599 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:26:28,599 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:26:28,611 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-30 10:26:28,611 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-30 10:26:28,611 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-30 10:26:28,612 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-30 10:26:28,612 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-30 10:26:28,614 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-30 10:26:28,615 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-30 10:26:28,615 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
s3g_1               | STARTUP_MSG:   java = 11.0.10
s3g_1               | ************************************************************/
s3g_1               | 2023-06-30 10:25:59,351 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-30 10:25:59,564 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-30 10:25:59,610 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-30 10:25:59,632 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
s3g_1               | 2023-06-30 10:25:59,790 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-30 10:25:59,811 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-30 10:25:59,813 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-06-30 10:25:59,924 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@19b843ba{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-30 10:25:59,941 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b0d80ed{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 30, 2023 10:26:14 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-30 10:26:14,613 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7fe8c7db{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_1_0_jar-_-any-2950968560974150510/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-30 10:26:14,703 [main] INFO server.AbstractConnector: Started ServerConnector@51c693d{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-30 10:26:14,703 [main] INFO server.Server: Started @22525ms
s3g_1               | 2023-06-30 10:26:14,709 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | 2023-06-30 10:26:19,752 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm_1               | 2023-06-30 10:26:19,999 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-30 10:26:20,022 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-30 10:26:20,041 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-06-30 10:26:20,095 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d5c04f9{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-30 10:26:20,095 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61cd1c71{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-30 10:26:20,260 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/736e472b-e89f-413d-b6e5-8d6dd4191048
scm_1               | 2023-06-30 10:26:20,261 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:26:20,275 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:26:20,282 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:26:20,320 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:20,332 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f9784336-76ea-441a-a276-4c2186c9621b to datanode:736e472b-e89f-413d-b6e5-8d6dd4191048
scm_1               | 2023-06-30 10:26:20,351 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: f9784336-76ea-441a-a276-4c2186c9621b, Nodes: 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:20.326519Z]
scm_1               | 2023-06-30 10:26:20,695 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@37b52340{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_1_0_jar-_-any-9661755415063072475/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/scm}
scm_1               | 2023-06-30 10:26:20,712 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@6981f8f3{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-30 10:26:20,713 [Listener at 0.0.0.0/9860] INFO server.Server: Started @21601ms
scm_1               | 2023-06-30 10:26:20,721 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-30 10:26:20,721 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-30 10:26:20,722 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-30 10:26:21,155 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9b7b92d4-f702-4dff-b9d3-d35288d2b16e
scm_1               | 2023-06-30 10:26:21,156 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:26:21,157 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:21,157 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:26:21,157 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:26:21,158 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=add455bf-80a2-4b1b-aa7d-ea9b23822ea7 to datanode:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
scm_1               | 2023-06-30 10:26:21,158 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: add455bf-80a2-4b1b-aa7d-ea9b23822ea7, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.158252Z]
scm_1               | 2023-06-30 10:26:21,597 [IPC Server handler 42 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2da73d14-1164-4b4c-94db-9a0f32cfe9e9
scm_1               | 2023-06-30 10:26:21,598 [IPC Server handler 42 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:26:21,599 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:26:21,602 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:21,599 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7e104799-667a-4e9c-87ea-a76b8f3523a3 to datanode:2da73d14-1164-4b4c-94db-9a0f32cfe9e9
scm_1               | 2023-06-30 10:26:21,603 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7e104799-667a-4e9c-87ea-a76b8f3523a3, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.599305Z]
scm_1               | 2023-06-30 10:26:21,599 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:26:21,604 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
om_1                | 2023-06-30 10:26:28,649 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.21.0.9:9862
om_1                | 2023-06-30 10:26:28,649 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-30 10:26:28,650 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om1@group-C5BA1605619E
om_1                | 2023-06-30 10:26:28,653 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om1@group-C5BA1605619E
om_1                | 2023-06-30 10:26:28,659 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-30 10:26:28,660 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-30 10:26:28,661 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:26:28,662 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-30 10:26:28,663 [Listener at om/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om1@group-C5BA1605619E
om_1                | 2023-06-30 10:26:28,669 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-30 10:26:28,705 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-30 10:26:28,710 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$354/0x000000084048fc40@c1050f2] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-30 10:26:28,783 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-30 10:26:28,783 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-30 10:26:28,807 [Listener at om/9862] INFO util.log: Logging initialized @7528ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-30 10:26:28,985 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-30 10:26:28,994 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-30 10:26:29,009 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-30 10:26:29,010 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-30 10:26:29,011 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-30 10:26:29,011 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-30 10:26:29,087 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-30 10:26:29,088 [Listener at om/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
om_1                | 2023-06-30 10:26:29,111 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-30 10:26:29,111 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-30 10:26:29,112 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-30 10:26:29,124 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3c25cfe1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-30 10:26:29,124 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a083b96{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-30 10:26:29,317 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@740a0d5e{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_1_0_jar-_-any-5502869462173225331/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-30 10:26:29,322 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7c96c85{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-30 10:26:29,322 [Listener at om/9862] INFO server.Server: Started @8044ms
om_1                | 2023-06-30 10:26:29,331 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-30 10:26:29,331 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-30 10:26:29,333 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-30 10:26:29,349 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-06-30 10:26:29,333 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-30 10:26:29,334 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-30 10:26:29,358 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5c313224] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-30 10:26:33,763 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5101923531ns, electionTimeout:5085ms
om_1                | 2023-06-30 10:26:33,764 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:26:33,764 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-30 10:26:33,766 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-30 10:26:33,766 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:26:33,770 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-30 10:26:33,770 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-30 10:26:33,771 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:26:33,778 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-30 10:26:33,778 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5268ms
om_1                | 2023-06-30 10:26:33,784 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-30 10:26:33,786 [om1@group-C5BA1605619E-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.om1@group-C5BA1605619E
scm_1               | 2023-06-30 10:26:21,604 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-30 10:26:21,622 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 to datanode:2da73d14-1164-4b4c-94db-9a0f32cfe9e9
scm_1               | 2023-06-30 10:26:21,623 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 to datanode:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
scm_1               | 2023-06-30 10:26:21,623 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 to datanode:736e472b-e89f-413d-b6e5-8d6dd4191048
scm_1               | 2023-06-30 10:26:21,623 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: b6a3f6b8-74cf-43ce-9e9a-6056a7998d63, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.622589Z]
scm_1               | 2023-06-30 10:26:21,624 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 to datanode:9b7b92d4-f702-4dff-b9d3-d35288d2b16e
scm_1               | 2023-06-30 10:26:21,625 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 to datanode:736e472b-e89f-413d-b6e5-8d6dd4191048
scm_1               | 2023-06-30 10:26:21,629 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 to datanode:2da73d14-1164-4b4c-94db-9a0f32cfe9e9
scm_1               | 2023-06-30 10:26:21,630 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 54933c8b-7f5b-4e75-ad0d-a6be02e7c387, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:26:21.624466Z]
scm_1               | 2023-06-30 10:26:21,636 [RatisPipelineUtilsThread] INFO pipeline.SCMPipelineManager: Pipeline: PipelineID=54933c8b-7f5b-4e75-ad0d-a6be02e7c387 contains same datanodes as previous pipelines: PipelineID=b6a3f6b8-74cf-43ce-9e9a-6056a7998d63 nodeIds: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e, 736e472b-e89f-413d-b6e5-8d6dd4191048, 2da73d14-1164-4b4c-94db-9a0f32cfe9e9
scm_1               | 2023-06-30 10:26:23,322 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:23,322 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: f9784336-76ea-441a-a276-4c2186c9621b, Nodes: 736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:736e472b-e89f-413d-b6e5-8d6dd4191048, CreationTimestamp2023-06-30T10:26:20.326519Z] moved to OPEN state
scm_1               | 2023-06-30 10:26:23,341 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:23,615 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:23,616 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:24,354 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:24,364 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: add455bf-80a2-4b1b-aa7d-ea9b23822ea7, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:9b7b92d4-f702-4dff-b9d3-d35288d2b16e, CreationTimestamp2023-06-30T10:26:21.158252Z] moved to OPEN state
scm_1               | 2023-06-30 10:26:24,365 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:24,827 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:24,830 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:25,153 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:25,154 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 7e104799-667a-4e9c-87ea-a76b8f3523a3, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:2da73d14-1164-4b4c-94db-9a0f32cfe9e9, CreationTimestamp2023-06-30T10:26:21.599305Z] moved to OPEN state
scm_1               | 2023-06-30 10:26:25,155 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:25,503 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:25,505 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:25,669 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:25,683 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:26,486 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:26,486 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:26,650 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:26,651 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:28,751 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:28,751 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:29,842 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:29,842 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:30,678 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:30,678 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:34,020 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: b6a3f6b8-74cf-43ce-9e9a-6056a7998d63, Nodes: 2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:2da73d14-1164-4b4c-94db-9a0f32cfe9e9, CreationTimestamp2023-06-30T10:26:21.622589Z] moved to OPEN state
scm_1               | 2023-06-30 10:26:34,021 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:34,021 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:26:34,021 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:26:34,024 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:26:34,024 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-30 10:26:34,024 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-30 10:26:51,436 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 54933c8b-7f5b-4e75-ad0d-a6be02e7c387, Nodes: 9b7b92d4-f702-4dff-b9d3-d35288d2b16e{ip: 172.21.0.8, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}736e472b-e89f-413d-b6e5-8d6dd4191048{ip: 172.21.0.4, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2da73d14-1164-4b4c-94db-9a0f32cfe9e9{ip: 172.21.0.7, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:736e472b-e89f-413d-b6e5-8d6dd4191048, CreationTimestamp2023-06-30T10:26:21.624466Z] moved to OPEN state
scm_1               | 2023-06-30 10:27:07,399 [IPC Server handler 30 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.10
scm_1               | 2023-06-30 10:27:15,832 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.10
scm_1               | 2023-06-30 10:28:04,201 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.10
scm_1               | 2023-06-30 10:28:12,220 [IPC Server handler 20 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.10
om_1                | 2023-06-30 10:26:33,787 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:26:33,788 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:26:33,791 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-30 10:26:33,791 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-30 10:26:33,792 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-30 10:26:33,796 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-30 10:26:33,834 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-30 10:26:33,848 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-06-30 10:26:33,932 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-30 10:26:40,176 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-30 10:27:21,120 [qtp628402659-39] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-30 10:27:21,138 [qtp628402659-39] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688120841121 in 17 milliseconds
om_1                | 2023-06-30 10:27:21,177 [qtp628402659-39] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 38 milliseconds
om_1                | 2023-06-30 10:27:21,177 [qtp628402659-39] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688120841121
Attaching to xcompat_old_client_1_3_0_1, xcompat_new_client_1, xcompat_om_1, xcompat_datanode_2, xcompat_s3g_1, xcompat_old_client_1_1_0_1, xcompat_datanode_1, xcompat_old_client_1_0_0_1, xcompat_datanode_3, xcompat_recon_1, xcompat_old_client_1_2_1_1, xcompat_scm_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-30 10:28:31,515 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 91cbcf595cdb/172.22.0.5
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.2.1
datanode_1          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_1          | STARTUP_MSG:   java = 11.0.13
datanode_1          | ************************************************************/
datanode_1          | 2023-06-30 10:28:31,554 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-30 10:28:32,757 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-30 10:28:33,176 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-30 10:28:33,679 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-30 10:28:33,680 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-30 10:28:34,328 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:91cbcf595cdb ip:172.22.0.5
datanode_1          | 2023-06-30 10:28:35,477 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_1          | 2023-06-30 10:28:36,131 [main] INFO reflections.Reflections: Reflections took 489 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_1          | 2023-06-30 10:28:37,366 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-30 10:28:37,375 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-30 10:28:37,402 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-30 10:28:37,402 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-30 10:28:37,494 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:28:37,596 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:28:37,600 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-30 10:28:37,621 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-30 10:28:37,623 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-30 10:28:37,624 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-30 10:28:37,752 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:28:37,755 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-30 10:28:43,847 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:28:44,227 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-30 10:28:45,017 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-30 10:28:45,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-30 10:28:45,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-30 10:28:45,051 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-30 10:28:45,052 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:28:45,066 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-30 10:28:45,074 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:28:46,648 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-30 10:28:46,663 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:28:46,663 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:28:46,722 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:28:47,400 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:28:47,563 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-30 10:28:47,698 [main] INFO util.log: Logging initialized @21474ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-30 10:28:48,228 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-30 10:28:48,241 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-30 10:28:48,255 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-30 10:28:48,264 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-30 10:28:48,264 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-30 10:28:48,271 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-30 10:28:48,556 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-30 10:28:48,562 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_1          | 2023-06-30 10:28:48,792 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-30 10:28:48,793 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-30 10:28:48,796 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-30 10:28:48,867 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e3df614{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-30 10:28:48,880 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b357eb6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-30 10:28:50,394 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@71560f51{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-7871478119932945484/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-30 10:28:50,418 [main] INFO server.AbstractConnector: Started ServerConnector@1c240cf2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-30 10:28:50,422 [main] INFO server.Server: Started @24198ms
datanode_1          | 2023-06-30 10:28:50,447 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-30 10:28:50,447 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-30 10:28:50,449 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:28:50,473 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-30 10:28:50,714 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@13b34c31] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-30 10:28:50,949 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.8:9891
datanode_1          | 2023-06-30 10:28:51,079 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-30 10:28:53,857 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:28:54,857 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:28:55,858 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:28:56,992 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-30 10:28:56,993 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-30 10:28:57,260 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_1          | 2023-06-30 10:28:57,346 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.RaftServer: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start RPC server
datanode_1          | 2023-06-30 10:28:57,348 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: ce67b879-7fb6-4366-9a6c-9701a544e0b4: GrpcService started, listening on 9856
datanode_1          | 2023-06-30 10:28:57,349 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: ce67b879-7fb6-4366-9a6c-9701a544e0b4: GrpcService started, listening on 9857
datanode_1          | 2023-06-30 10:28:57,352 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: ce67b879-7fb6-4366-9a6c-9701a544e0b4: GrpcService started, listening on 9858
datanode_1          | 2023-06-30 10:28:57,362 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ce67b879-7fb6-4366-9a6c-9701a544e0b4 is started using port 9858 for RATIS
datanode_1          | 2023-06-30 10:28:57,362 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ce67b879-7fb6-4366-9a6c-9701a544e0b4 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-30 10:28:57,363 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ce67b879-7fb6-4366-9a6c-9701a544e0b4 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-30 10:28:57,388 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@574973c4] INFO util.JvmPauseMonitor: JvmPauseMonitor-ce67b879-7fb6-4366-9a6c-9701a544e0b4: Started
datanode_1          | 2023-06-30 10:28:57,888 [EndpointStateMachine task thread for recon/172.22.0.8:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 91cbcf595cdb/172.22.0.5 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.5:52830 remote=recon/172.22.0.8:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.5:52830 remote=recon/172.22.0.8:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_1          | 2023-06-30 10:29:01,740 [Command processor thread] INFO server.RaftServer: ce67b879-7fb6-4366-9a6c-9701a544e0b4: addNew group-3A602F9C4518:[ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] returns group-3A602F9C4518:java.util.concurrent.CompletableFuture@3c8c3463[Not completed]
datanode_1          | 2023-06-30 10:29:01,794 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4: new RaftServerImpl for group-3A602F9C4518:[ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:29:01,804 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:29:01,804 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:29:01,804 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:29:01,805 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:29:01,805 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:29:01,805 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:29:01,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:29:01,810 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518: ConfigurationManager, init=-1: [ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:29:01,827 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:29:01,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:29:01,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:29:01,841 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b4127be5-6df7-4633-a5e6-3a602f9c4518 does not exist. Creating ...
datanode_1          | 2023-06-30 10:29:01,856 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b4127be5-6df7-4633-a5e6-3a602f9c4518/in_use.lock acquired by nodename 7@91cbcf595cdb
datanode_1          | 2023-06-30 10:29:01,902 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b4127be5-6df7-4633-a5e6-3a602f9c4518 has been successfully formatted.
datanode_1          | 2023-06-30 10:29:01,920 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-3A602F9C4518: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:29:01,920 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-30 10:29:01,922 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:29:01,950 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:29:01,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:29:01,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:29:02,034 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:02,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:29:02,116 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:29:02,180 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b4127be5-6df7-4633-a5e6-3a602f9c4518
datanode_1          | 2023-06-30 10:29:02,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:29:02,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:29:02,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:02,213 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:29:02,219 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:29:02,225 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:29:02,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:29:02,228 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:29:02,267 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:02,283 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:29:02,319 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:29:02,319 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:29:02,322 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:29:02,345 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:29:02,345 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:29:02,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:29:02,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:29:02,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:29:02,623 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518: start as a follower, conf=-1: [ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:29:02,624 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:29:02,631 [pool-22-thread-1] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-30 10:28:28,856 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 45b87710fe06/172.22.0.2
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.2.1
datanode_3          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_3          | STARTUP_MSG:   java = 11.0.13
datanode_3          | ************************************************************/
datanode_3          | 2023-06-30 10:28:28,895 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-30 10:28:30,361 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-30 10:28:30,782 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-30 10:28:31,576 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-30 10:28:31,576 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-30 10:28:32,422 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:45b87710fe06 ip:172.22.0.2
datanode_3          | 2023-06-30 10:28:33,837 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_3          | 2023-06-30 10:28:34,565 [main] INFO reflections.Reflections: Reflections took 576 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_3          | 2023-06-30 10:28:35,862 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-30 10:28:35,885 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-30 10:28:35,897 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-30 10:28:35,904 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-30 10:28:36,049 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:28:36,142 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:28:36,150 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-30 10:28:36,157 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-30 10:28:36,158 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-30 10:28:36,189 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-30 10:28:36,319 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:28:36,320 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-30 10:28:42,850 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:28:43,163 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-30 10:28:43,894 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-30 10:28:43,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-30 10:28:43,948 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-30 10:28:43,948 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-30 10:28:43,956 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:28:43,960 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-30 10:28:43,968 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:28:45,334 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-30 10:28:45,336 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:28:45,339 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:28:45,392 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:28:45,993 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:28:46,134 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-30 10:28:46,284 [main] INFO util.log: Logging initialized @22105ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-30 10:28:46,793 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-30 10:28:46,801 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-30 10:28:46,848 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-30 10:28:46,896 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-30 10:28:46,896 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-30 10:28:46,899 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-30 10:28:47,199 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-30 10:28:47,228 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_3          | 2023-06-30 10:28:47,477 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-30 10:28:47,483 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-30 10:28:47,486 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-06-30 10:28:47,576 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f13811b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-30 10:28:47,580 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@629a9f26{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-30 10:28:49,515 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4315c28c{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-11497928434746088900/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-30 10:29:02,666 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3A602F9C4518,id=ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_1          | 2023-06-30 10:29:02,734 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b4127be5-6df7-4633-a5e6-3a602f9c4518
datanode_1          | 2023-06-30 10:29:02,738 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b4127be5-6df7-4633-a5e6-3a602f9c4518.
datanode_1          | 2023-06-30 10:29:02,739 [Command processor thread] INFO server.RaftServer: ce67b879-7fb6-4366-9a6c-9701a544e0b4: addNew group-F4999DDB4D74:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] returns group-F4999DDB4D74:java.util.concurrent.CompletableFuture@6b6fb8c2[Not completed]
datanode_1          | 2023-06-30 10:29:02,759 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4: new RaftServerImpl for group-F4999DDB4D74:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:29:02,759 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:29:02,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:29:02,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:29:02,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:29:02,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:29:02,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:29:02,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:29:02,771 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: ConfigurationManager, init=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:29:02,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:29:02,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:29:02,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:29:02,774 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74 does not exist. Creating ...
datanode_1          | 2023-06-30 10:29:02,784 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74/in_use.lock acquired by nodename 7@91cbcf595cdb
datanode_1          | 2023-06-30 10:29:02,786 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74 has been successfully formatted.
datanode_1          | 2023-06-30 10:29:02,788 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-F4999DDB4D74: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:29:02,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:29:02,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:29:02,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:29:02,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:29:02,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:02,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:29:02,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:29:02,817 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74
datanode_1          | 2023-06-30 10:29:02,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:29:02,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:29:02,824 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:02,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:29:02,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:29:02,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:29:02,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:29:02,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:29:02,840 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:02,840 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:29:02,843 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:29:02,844 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:29:02,844 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:29:02,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:29:02,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:29:02,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:29:02,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:29:02,846 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:29:02,847 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: start as a follower, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:29:02,849 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:29:02,849 [pool-22-thread-1] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:02,866 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F4999DDB4D74,id=ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_1          | 2023-06-30 10:29:02,875 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74
datanode_1          | 2023-06-30 10:29:04,562 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74.
datanode_1          | 2023-06-30 10:29:04,563 [Command processor thread] INFO server.RaftServer: ce67b879-7fb6-4366-9a6c-9701a544e0b4: addNew group-2EC72CD08CCB:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0] returns group-2EC72CD08CCB:java.util.concurrent.CompletableFuture@14f42e77[Not completed]
datanode_1          | 2023-06-30 10:29:04,564 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4: new RaftServerImpl for group-2EC72CD08CCB:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:29:04,565 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:29:04,567 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:29:04,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:29:04,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:29:04,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:29:04,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:29:04,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:29:04,570 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB: ConfigurationManager, init=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:29:04,574 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:29:04,574 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:29:04,574 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:29:04,575 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb does not exist. Creating ...
datanode_1          | 2023-06-30 10:29:04,576 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb/in_use.lock acquired by nodename 7@91cbcf595cdb
datanode_1          | 2023-06-30 10:29:04,578 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb has been successfully formatted.
datanode_1          | 2023-06-30 10:29:04,579 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2EC72CD08CCB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:29:04,579 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:29:04,579 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:29:04,579 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:29:04,580 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:29:04,599 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:04,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:29:04,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:29:04,610 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb
datanode_1          | 2023-06-30 10:29:04,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:29:04,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:29:04,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:04,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:29:04,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:29:04,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:29:04,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-30 10:28:31,445 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 56172cac5b85/172.22.0.10
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.2.1
datanode_1          | 2023-06-30 10:29:04,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:29:04,613 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:29:04,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:29:04,620 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:29:04,620 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:29:04,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:29:04,661 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:29:04,661 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:29:04,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:29:04,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:29:04,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:29:04,663 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB: start as a follower, conf=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null
datanode_1          | 2023-06-30 10:29:04,663 [pool-22-thread-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:29:04,663 [pool-22-thread-1] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-FollowerState
datanode_1          | 2023-06-30 10:29:04,668 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2EC72CD08CCB,id=ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_1          | 2023-06-30 10:29:04,681 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb
datanode_1          | 2023-06-30 10:29:04,848 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb.
datanode_1          | 2023-06-30 10:29:06,577 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:29:06,578 [grpc-default-executor-0] INFO impl.VoteContext: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FOLLOWER: reject ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-30 10:29:06,579 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:325d3290-29d2-46d6-89ed-747878c1d37e
datanode_1          | 2023-06-30 10:29:06,579 [grpc-default-executor-0] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:06,579 [grpc-default-executor-0] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:06,579 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-30 10:29:06,589 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t1. Peer's state: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74:t1, leader=null, voted=null, raftlog=ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:29:07,736 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState] INFO impl.FollowerState: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5104612659ns, electionTimeout:5072ms
datanode_1          | 2023-06-30 10:29:07,736 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState
datanode_1          | 2023-06-30 10:29:07,736 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:29:07,739 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:29:07,739 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-FollowerState] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1
datanode_1          | 2023-06-30 10:29:07,742 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:29:07,743 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-30 10:29:07,743 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1
datanode_3          | 2023-06-30 10:28:49,547 [main] INFO server.AbstractConnector: Started ServerConnector@47b33e07{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-30 10:28:49,549 [main] INFO server.Server: Started @25370ms
datanode_3          | 2023-06-30 10:28:49,551 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-30 10:28:49,555 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-30 10:28:49,557 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:28:49,595 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-30 10:28:49,807 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37a93e87] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-30 10:28:50,156 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.8:9891
datanode_3          | 2023-06-30 10:28:50,455 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-30 10:28:52,950 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:28:52,955 [EndpointStateMachine task thread for recon/172.22.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.8:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:28:53,950 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:28:54,951 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:28:56,983 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-30 10:28:56,984 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-30 10:28:57,312 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 325d3290-29d2-46d6-89ed-747878c1d37e
datanode_3          | 2023-06-30 10:28:57,366 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.RaftServer: 325d3290-29d2-46d6-89ed-747878c1d37e: start RPC server
datanode_3          | 2023-06-30 10:28:57,373 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 325d3290-29d2-46d6-89ed-747878c1d37e: GrpcService started, listening on 9856
datanode_3          | 2023-06-30 10:28:57,374 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 325d3290-29d2-46d6-89ed-747878c1d37e: GrpcService started, listening on 9857
datanode_3          | 2023-06-30 10:28:57,384 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 325d3290-29d2-46d6-89ed-747878c1d37e: GrpcService started, listening on 9858
datanode_3          | 2023-06-30 10:28:57,412 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 325d3290-29d2-46d6-89ed-747878c1d37e is started using port 9858 for RATIS
datanode_3          | 2023-06-30 10:28:57,412 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 325d3290-29d2-46d6-89ed-747878c1d37e is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-30 10:28:57,412 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 325d3290-29d2-46d6-89ed-747878c1d37e is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-30 10:28:57,414 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$310/0x00000008404be840@18582c30] INFO util.JvmPauseMonitor: JvmPauseMonitor-325d3290-29d2-46d6-89ed-747878c1d37e: Started
datanode_3          | 2023-06-30 10:28:57,974 [EndpointStateMachine task thread for recon/172.22.0.8:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 45b87710fe06/172.22.0.2 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.2:55280 remote=recon/172.22.0.8:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.2:55280 remote=recon/172.22.0.8:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 2023-06-30 10:29:07,743 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-30 10:29:07,743 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3A602F9C4518 with new leaderId: ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_1          | 2023-06-30 10:29:07,743 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518: change Leader from null to ce67b879-7fb6-4366-9a6c-9701a544e0b4 at term 1 for becomeLeader, leader elected after 5821ms
datanode_1          | 2023-06-30 10:29:07,744 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-30 10:29:07,753 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:29:07,756 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:29:07,757 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-30 10:29:07,760 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:29:07,760 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:29:07,761 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:29:07,764 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:29:07,765 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-30 10:29:07,766 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderStateImpl
datanode_1          | 2023-06-30 10:29:07,776 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:29:07,786 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-LeaderElection1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518: set configuration 0: [ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-30 10:29:07,851 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-3A602F9C4518-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b4127be5-6df7-4633-a5e6-3a602f9c4518/current/log_inprogress_0
datanode_1          | 2023-06-30 10:29:09,639 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-2EC72CD08CCB, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:29:09,639 [grpc-default-executor-0] INFO impl.VoteContext: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-FOLLOWER: accept ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:29:09,640 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:325d3290-29d2-46d6-89ed-747878c1d37e
datanode_1          | 2023-06-30 10:29:09,640 [grpc-default-executor-0] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-FollowerState
datanode_1          | 2023-06-30 10:29:09,640 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-FollowerState] INFO impl.FollowerState: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-30 10:29:09,646 [grpc-default-executor-0] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-FollowerState
datanode_1          | 2023-06-30 10:29:09,651 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:OK-t1. Peer's state: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB:t1, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-SegmentedRaftLog:OPENED:c-1, conf=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null
datanode_1          | 2023-06-30 10:29:09,768 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2EC72CD08CCB with new leaderId: 325d3290-29d2-46d6-89ed-747878c1d37e
datanode_1          | 2023-06-30 10:29:09,770 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB: change Leader from null to 325d3290-29d2-46d6-89ed-747878c1d37e at term 1 for appendEntries, leader elected after 5188ms
datanode_1          | 2023-06-30 10:29:09,801 [grpc-default-executor-0] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB: set configuration 0: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-30 10:29:09,802 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:29:09,803 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-2EC72CD08CCB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb/current/log_inprogress_0
datanode_2          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_2          | STARTUP_MSG:   java = 11.0.13
datanode_2          | ************************************************************/
datanode_2          | 2023-06-30 10:28:31,522 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-30 10:28:32,791 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-30 10:28:33,226 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-30 10:28:33,918 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-30 10:28:33,918 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-30 10:28:34,704 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:56172cac5b85 ip:172.22.0.10
datanode_2          | 2023-06-30 10:28:35,844 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_2          | 2023-06-30 10:28:36,403 [main] INFO reflections.Reflections: Reflections took 475 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_2          | 2023-06-30 10:28:37,506 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-30 10:28:37,567 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-30 10:28:37,577 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-30 10:28:37,578 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-30 10:28:37,679 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:28:37,768 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:28:37,772 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-30 10:28:37,779 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-30 10:28:37,780 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-30 10:28:37,787 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-30 10:28:37,927 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:28:37,928 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-30 10:28:44,153 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:28:44,512 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-30 10:28:45,234 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-30 10:28:45,242 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-30 10:28:45,260 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-30 10:28:45,260 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-30 10:28:45,261 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:28:45,262 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-30 10:28:45,263 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:28:46,672 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-30 10:28:46,673 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:28:46,674 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:28:46,747 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:28:47,689 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:28:47,787 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-30 10:28:47,943 [main] INFO util.log: Logging initialized @21263ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-30 10:28:48,517 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-30 10:28:48,556 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-30 10:28:48,578 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-30 10:28:48,582 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-30 10:28:48,582 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-30 10:28:48,599 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-30 10:28:48,874 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-30 10:28:48,887 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_2          | 2023-06-30 10:28:49,174 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-30 10:28:49,174 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-30 10:28:49,229 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-30 10:28:49,320 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2921199d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-30 10:28:49,320 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23ad71bf{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-30 10:28:50,758 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@51a16adf{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-2873662955974588710/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-30 10:28:50,821 [main] INFO server.AbstractConnector: Started ServerConnector@4aeb0e2b{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-30 10:28:50,821 [main] INFO server.Server: Started @24141ms
datanode_2          | 2023-06-30 10:28:50,823 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-30 10:28:50,823 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-30 10:28:50,824 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:28:50,847 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-30 10:28:51,089 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@53f623aa] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-30 10:28:51,258 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.8:9891
datanode_2          | 2023-06-30 10:28:51,412 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-30 10:28:54,257 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:28:55,258 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:28:57,044 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-30 10:28:57,063 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-30 10:28:57,277 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4b3dfc39-1d9a-4911-af0e-236ac377f4d0
datanode_2          | 2023-06-30 10:28:57,358 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.RaftServer: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start RPC server
datanode_2          | 2023-06-30 10:28:57,384 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: GrpcService started, listening on 9856
datanode_2          | 2023-06-30 10:28:57,385 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: GrpcService started, listening on 9857
datanode_2          | 2023-06-30 10:28:57,385 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO server.GrpcService: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: GrpcService started, listening on 9858
datanode_2          | 2023-06-30 10:28:57,410 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4b3dfc39-1d9a-4911-af0e-236ac377f4d0 is started using port 9858 for RATIS
datanode_2          | 2023-06-30 10:28:57,412 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4b3dfc39-1d9a-4911-af0e-236ac377f4d0 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-30 10:28:57,412 [EndpointStateMachine task thread for scm/172.22.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4b3dfc39-1d9a-4911-af0e-236ac377f4d0 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-30 10:28:57,425 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@6d19410c] INFO util.JvmPauseMonitor: JvmPauseMonitor-4b3dfc39-1d9a-4911-af0e-236ac377f4d0: Started
datanode_2          | 2023-06-30 10:29:02,098 [Command processor thread] INFO server.RaftServer: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: addNew group-0ADF25B4817A:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1] returns group-0ADF25B4817A:java.util.concurrent.CompletableFuture@5873af60[Not completed]
datanode_2          | 2023-06-30 10:29:02,213 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: new RaftServerImpl for group-0ADF25B4817A:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:29:02,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:29:02,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:29:02,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:29:02,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:29:02,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:29:02,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:29:02,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:29:02,256 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A: ConfigurationManager, init=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:29:02,259 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:29:02,262 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:29:02,263 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:29:02,265 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/85863d74-2f4f-4f0b-9bb5-0adf25b4817a does not exist. Creating ...
datanode_2          | 2023-06-30 10:29:02,287 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/85863d74-2f4f-4f0b-9bb5-0adf25b4817a/in_use.lock acquired by nodename 7@56172cac5b85
datanode_2          | 2023-06-30 10:29:02,329 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/85863d74-2f4f-4f0b-9bb5-0adf25b4817a has been successfully formatted.
datanode_2          | 2023-06-30 10:29:02,335 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-30 10:29:02,337 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0ADF25B4817A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:29:02,349 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:29:02,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:29:02,409 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:29:02,409 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:29:02,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:02,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:29:11,658 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5078904897ns, electionTimeout:5077ms
datanode_1          | 2023-06-30 10:29:11,659 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:11,659 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-30 10:29:11,659 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:29:11,659 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2
datanode_1          | 2023-06-30 10:29:11,661 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:29:11,802 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:29:11,803 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection:   Response 0: ce67b879-7fb6-4366-9a6c-9701a544e0b4<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:FAIL-t2
datanode_1          | 2023-06-30 10:29:11,803 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection:   Response 1: ce67b879-7fb6-4366-9a6c-9701a544e0b4<-325d3290-29d2-46d6-89ed-747878c1d37e#0:FAIL-t2
datanode_1          | 2023-06-30 10:29:11,803 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-30 10:29:11,803 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-30 10:29:11,803 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2
datanode_1          | 2023-06-30 10:29:11,803 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection2] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:11,819 [grpc-default-executor-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 2, (t:0, i:0))
datanode_1          | 2023-06-30 10:29:11,820 [grpc-default-executor-1] INFO impl.VoteContext: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FOLLOWER: reject ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: already has voted for ce67b879-7fb6-4366-9a6c-9701a544e0b4 at current term 2
datanode_1          | 2023-06-30 10:29:11,820 [grpc-default-executor-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t2. Peer's state: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74:t2, leader=null, voted=ce67b879-7fb6-4366-9a6c-9701a544e0b4, raftlog=ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:29:16,536 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-30 10:29:16,877 [grpc-default-executor-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 3, (t:0, i:0))
datanode_1          | 2023-06-30 10:29:16,881 [grpc-default-executor-1] INFO impl.VoteContext: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FOLLOWER: reject ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-30 10:29:16,881 [grpc-default-executor-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:325d3290-29d2-46d6-89ed-747878c1d37e
datanode_1          | 2023-06-30 10:29:16,881 [grpc-default-executor-1] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:16,881 [grpc-default-executor-1] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:16,881 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-30 10:29:16,893 [grpc-default-executor-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t3. Peer's state: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74:t3, leader=null, voted=null, raftlog=ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:02,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:29:02,563 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/85863d74-2f4f-4f0b-9bb5-0adf25b4817a
datanode_2          | 2023-06-30 10:29:02,563 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:29:02,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:29:02,579 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:02,580 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:29:02,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:29:02,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:29:02,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:29:02,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:29:02,615 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:02,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:29:02,643 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:29:02,643 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:29:02,655 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:29:02,660 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:29:02,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:29:02,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:29:02,674 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:29:02,678 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:29:02,730 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A: start as a follower, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:02,731 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:29:02,744 [pool-22-thread-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState
datanode_2          | 2023-06-30 10:29:02,756 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0ADF25B4817A,id=4b3dfc39-1d9a-4911-af0e-236ac377f4d0
datanode_2          | 2023-06-30 10:29:02,891 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=85863d74-2f4f-4f0b-9bb5-0adf25b4817a
datanode_2          | 2023-06-30 10:29:02,895 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=85863d74-2f4f-4f0b-9bb5-0adf25b4817a.
datanode_2          | 2023-06-30 10:29:02,896 [Command processor thread] INFO server.RaftServer: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: addNew group-F4999DDB4D74:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] returns group-F4999DDB4D74:java.util.concurrent.CompletableFuture@708b702b[Not completed]
datanode_2          | 2023-06-30 10:29:02,916 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: new RaftServerImpl for group-F4999DDB4D74:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:29:02,919 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:29:02,921 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:29:02,922 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:29:02,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:29:02,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:29:02,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:29:02,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:29:02,935 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: ConfigurationManager, init=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:29:02,935 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:29:02,936 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:29:02,936 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:29:02,936 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74 does not exist. Creating ...
datanode_2          | 2023-06-30 10:29:02,947 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74/in_use.lock acquired by nodename 7@56172cac5b85
datanode_2          | 2023-06-30 10:29:02,949 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74 has been successfully formatted.
datanode_2          | 2023-06-30 10:29:02,952 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-30 10:29:02,952 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-F4999DDB4D74: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:29:02,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:29:02,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:29:02,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:29:02,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:29:02,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:02,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:29:02,972 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:29:02,973 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74
datanode_2          | 2023-06-30 10:29:02,974 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:29:02,974 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:29:02,974 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:02,975 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:29:02,975 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:29:02,976 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:29:02,976 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:29:02,976 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:29:02,977 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:02,978 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:29:02,991 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:29:02,992 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:29:03,004 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:29:03,004 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:29:03,004 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:29:03,004 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:29:03,004 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:29:03,005 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:29:03,006 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: start as a follower, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:03,011 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:29:03,012 [pool-22-thread-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:03,014 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F4999DDB4D74,id=4b3dfc39-1d9a-4911-af0e-236ac377f4d0
datanode_2          | 2023-06-30 10:29:03,024 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74
datanode_2          | 2023-06-30 10:29:04,605 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74.
datanode_2          | 2023-06-30 10:29:04,607 [Command processor thread] INFO server.RaftServer: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: addNew group-2EC72CD08CCB:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0] returns group-2EC72CD08CCB:java.util.concurrent.CompletableFuture@bf522a1[Not completed]
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: new RaftServerImpl for group-2EC72CD08CCB:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:29:04,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_3          | 2023-06-30 10:29:00,828 [Command processor thread] INFO server.RaftServer: 325d3290-29d2-46d6-89ed-747878c1d37e: addNew group-D3EE97F68D01:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1] returns group-D3EE97F68D01:java.util.concurrent.CompletableFuture@7ff18fd6[Not completed]
datanode_3          | 2023-06-30 10:29:00,877 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e: new RaftServerImpl for group-D3EE97F68D01:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:29:00,881 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:29:00,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:29:00,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:29:00,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:29:00,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:29:00,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:29:00,886 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:29:00,894 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01: ConfigurationManager, init=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:29:00,895 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:29:00,898 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:29:00,901 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:29:00,902 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8715b917-c279-448e-9669-d3ee97f68d01 does not exist. Creating ...
datanode_3          | 2023-06-30 10:29:00,911 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8715b917-c279-448e-9669-d3ee97f68d01/in_use.lock acquired by nodename 6@45b87710fe06
datanode_3          | 2023-06-30 10:29:00,935 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8715b917-c279-448e-9669-d3ee97f68d01 has been successfully formatted.
datanode_3          | 2023-06-30 10:29:00,940 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-D3EE97F68D01: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:29:00,941 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:29:00,941 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-30 10:29:00,943 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:29:00,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:29:00,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:29:01,036 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:01,076 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:29:01,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:29:01,132 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8715b917-c279-448e-9669-d3ee97f68d01
datanode_3          | 2023-06-30 10:29:01,134 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-30 10:29:01,135 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:29:01,152 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:01,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:29:01,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:29:01,155 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:29:01,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:29:01,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:29:01,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:01,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:29:01,209 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:29:01,209 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:29:01,225 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:29:01,231 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:29:01,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:29:01,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:29:01,240 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:29:01,245 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:29:04,609 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB: ConfigurationManager, init=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:29:04,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:29:04,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:29:04,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:29:04,609 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb does not exist. Creating ...
datanode_2          | 2023-06-30 10:29:04,611 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb/in_use.lock acquired by nodename 7@56172cac5b85
datanode_2          | 2023-06-30 10:29:04,613 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb has been successfully formatted.
datanode_2          | 2023-06-30 10:29:04,613 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-30 10:29:04,613 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2EC72CD08CCB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:29:04,619 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:29:04,619 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:29:04,619 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:29:04,619 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:29:04,619 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:04,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:29:04,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:29:04,620 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb
datanode_2          | 2023-06-30 10:29:04,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-30 10:29:04,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:29:04,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:04,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:29:04,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:29:04,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:29:04,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:29:04,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:29:04,632 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:29:04,633 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:29:04,637 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:29:04,637 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:29:04,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:29:04,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:29:04,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:29:04,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:29:04,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:29:04,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:29:04,642 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB: start as a follower, conf=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:29:04,643 [pool-22-thread-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:29:04,644 [pool-22-thread-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-FollowerState
datanode_2          | 2023-06-30 10:29:04,677 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2EC72CD08CCB,id=4b3dfc39-1d9a-4911-af0e-236ac377f4d0
datanode_2          | 2023-06-30 10:29:04,696 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb
datanode_2          | 2023-06-30 10:29:04,839 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb.
datanode_2          | 2023-06-30 10:29:06,623 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 1, (t:0, i:0))
datanode_2          | 2023-06-30 10:29:06,624 [grpc-default-executor-1] INFO impl.VoteContext: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FOLLOWER: accept ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-30 10:29:01,337 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01: start as a follower, conf=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:01,348 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:29:01,349 [pool-22-thread-1] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState
datanode_3          | 2023-06-30 10:29:01,363 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D3EE97F68D01,id=325d3290-29d2-46d6-89ed-747878c1d37e
datanode_3          | 2023-06-30 10:29:01,406 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8715b917-c279-448e-9669-d3ee97f68d01
datanode_3          | 2023-06-30 10:29:01,407 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=8715b917-c279-448e-9669-d3ee97f68d01.
datanode_3          | 2023-06-30 10:29:01,408 [Command processor thread] INFO server.RaftServer: 325d3290-29d2-46d6-89ed-747878c1d37e: addNew group-F4999DDB4D74:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] returns group-F4999DDB4D74:java.util.concurrent.CompletableFuture@94e3ac5[Not completed]
datanode_3          | 2023-06-30 10:29:01,410 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e: new RaftServerImpl for group-F4999DDB4D74:[4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:29:01,415 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: ConfigurationManager, init=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:29:01,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:29:01,417 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:29:01,417 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74 does not exist. Creating ...
datanode_3          | 2023-06-30 10:29:01,420 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74/in_use.lock acquired by nodename 6@45b87710fe06
datanode_3          | 2023-06-30 10:29:01,427 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74 has been successfully formatted.
datanode_3          | 2023-06-30 10:29:01,428 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-30 10:29:01,429 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-F4999DDB4D74: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:29:01,430 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:29:01,430 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:29:01,430 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:29:01,431 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:29:01,431 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:01,432 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:29:01,432 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:29:01,432 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74
datanode_3          | 2023-06-30 10:29:01,432 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-30 10:29:01,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:29:01,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:01,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:29:01,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:29:01,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:29:01,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:29:01,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:29:01,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:01,435 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:29:01,436 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:29:06,625 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:325d3290-29d2-46d6-89ed-747878c1d37e
datanode_2          | 2023-06-30 10:29:06,625 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: shutdown 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:06,625 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:06,626 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:29:06,642 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t1. Peer's state: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74:t1, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:07,800 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState] INFO impl.FollowerState: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5056579806ns, electionTimeout:5054ms
datanode_2          | 2023-06-30 10:29:07,801 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: shutdown 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState
datanode_2          | 2023-06-30 10:29:07,801 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:29:07,803 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:29:07,803 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-FollowerState] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1
datanode_2          | 2023-06-30 10:29:07,808 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO impl.LeaderElection: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:07,809 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO impl.LeaderElection: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-30 10:29:07,809 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: shutdown 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1
datanode_2          | 2023-06-30 10:29:07,810 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-30 10:29:07,810 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0ADF25B4817A with new leaderId: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0
datanode_2          | 2023-06-30 10:29:07,811 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A: change Leader from null to 4b3dfc39-1d9a-4911-af0e-236ac377f4d0 at term 1 for becomeLeader, leader elected after 5461ms
datanode_2          | 2023-06-30 10:29:07,823 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:29:07,826 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:29:07,826 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-30 10:29:07,830 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:29:07,830 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:29:07,831 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:29:07,834 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:29:07,838 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-30 10:29:07,840 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderStateImpl
datanode_2          | 2023-06-30 10:29:07,849 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:29:07,869 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-LeaderElection1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A: set configuration 0: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-30 10:29:07,914 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-0ADF25B4817A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/85863d74-2f4f-4f0b-9bb5-0adf25b4817a/current/log_inprogress_0
datanode_2          | 2023-06-30 10:29:09,642 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-2EC72CD08CCB, 1, (t:0, i:0))
datanode_2          | 2023-06-30 10:29:09,642 [grpc-default-executor-1] INFO impl.VoteContext: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-FOLLOWER: accept ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-30 10:29:09,642 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:325d3290-29d2-46d6-89ed-747878c1d37e
datanode_2          | 2023-06-30 10:29:09,642 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: shutdown 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-FollowerState
datanode_2          | 2023-06-30 10:29:09,642 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-FollowerState] INFO impl.FollowerState: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:29:09,644 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-FollowerState
datanode_2          | 2023-06-30 10:29:09,651 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t1. Peer's state: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB:t1, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-SegmentedRaftLog:OPENED:c-1, conf=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null
datanode_2          | 2023-06-30 10:29:09,775 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2EC72CD08CCB with new leaderId: 325d3290-29d2-46d6-89ed-747878c1d37e
datanode_2          | 2023-06-30 10:29:09,781 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB: change Leader from null to 325d3290-29d2-46d6-89ed-747878c1d37e at term 1 for appendEntries, leader elected after 5155ms
datanode_2          | 2023-06-30 10:29:09,818 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB: set configuration 0: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-30 10:29:09,819 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:29:09,820 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-2EC72CD08CCB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb/current/log_inprogress_0
datanode_2          | 2023-06-30 10:29:11,739 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 2, (t:0, i:0))
datanode_2          | 2023-06-30 10:29:11,739 [grpc-default-executor-1] INFO impl.VoteContext: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FOLLOWER: accept ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-30 10:29:11,739 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:325d3290-29d2-46d6-89ed-747878c1d37e
datanode_2          | 2023-06-30 10:29:11,740 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: shutdown 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:11,740 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:29:11,740 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:11,753 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t2. Peer's state: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74:t2, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:11,784 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: receive requestVote(ELECTION, ce67b879-7fb6-4366-9a6c-9701a544e0b4, group-F4999DDB4D74, 2, (t:0, i:0))
datanode_2          | 2023-06-30 10:29:11,786 [grpc-default-executor-1] INFO impl.VoteContext: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FOLLOWER: reject ELECTION from ce67b879-7fb6-4366-9a6c-9701a544e0b4: already has voted for 325d3290-29d2-46d6-89ed-747878c1d37e at current term 2
datanode_2          | 2023-06-30 10:29:11,786 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74 replies to ELECTION vote request: ce67b879-7fb6-4366-9a6c-9701a544e0b4<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:FAIL-t2. Peer's state: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74:t2, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:16,506 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-30 10:29:16,875 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 3, (t:0, i:0))
datanode_2          | 2023-06-30 10:29:16,875 [grpc-default-executor-1] INFO impl.VoteContext: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FOLLOWER: accept ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-30 10:29:16,878 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:325d3290-29d2-46d6-89ed-747878c1d37e
datanode_2          | 2023-06-30 10:29:16,878 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: shutdown 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:16,878 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:29:16,879 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:16,885 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t3. Peer's state: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74:t3, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:22,047 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: receive requestVote(ELECTION, ce67b879-7fb6-4366-9a6c-9701a544e0b4, group-F4999DDB4D74, 4, (t:0, i:0))
datanode_2          | 2023-06-30 10:29:22,048 [grpc-default-executor-1] INFO impl.VoteContext: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FOLLOWER: accept ELECTION from ce67b879-7fb6-4366-9a6c-9701a544e0b4: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-30 10:29:22,048 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_2          | 2023-06-30 10:29:22,048 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: shutdown 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:22,048 [grpc-default-executor-1] INFO impl.RoleInfo: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0: start 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState
datanode_2          | 2023-06-30 10:29:22,048 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-30 10:29:22,060 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74 replies to ELECTION vote request: ce67b879-7fb6-4366-9a6c-9701a544e0b4<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t4. Peer's state: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74:t4, leader=null, voted=ce67b879-7fb6-4366-9a6c-9701a544e0b4, raftlog=4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:22,087 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 4, (t:0, i:0))
datanode_2          | 2023-06-30 10:29:22,088 [grpc-default-executor-1] INFO impl.VoteContext: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-FOLLOWER: reject ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: already has voted for ce67b879-7fb6-4366-9a6c-9701a544e0b4 at current term 4
datanode_2          | 2023-06-30 10:29:22,088 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:FAIL-t4. Peer's state: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74:t4, leader=null, voted=ce67b879-7fb6-4366-9a6c-9701a544e0b4, raftlog=4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_2          | 2023-06-30 10:29:22,183 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F4999DDB4D74 with new leaderId: ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_2          | 2023-06-30 10:29:22,183 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: change Leader from null to ce67b879-7fb6-4366-9a6c-9701a544e0b4 at term 4 for appendEntries, leader elected after 19221ms
datanode_2          | 2023-06-30 10:29:22,208 [grpc-default-executor-1] INFO server.RaftServer$Division: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74: set configuration 0: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-30 10:29:22,208 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:29:22,210 [4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0@group-F4999DDB4D74-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74/current/log_inprogress_0
datanode_2          | 2023-06-30 10:29:32,479 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-30 10:29:01,436 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:29:01,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:29:01,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:29:01,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:29:01,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:29:01,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:29:01,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:29:01,444 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: start as a follower, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:01,444 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:29:01,444 [pool-22-thread-1] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:01,444 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F4999DDB4D74,id=325d3290-29d2-46d6-89ed-747878c1d37e
datanode_3          | 2023-06-30 10:29:01,456 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74
datanode_3          | 2023-06-30 10:29:04,433 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74.
datanode_3          | 2023-06-30 10:29:04,434 [Command processor thread] INFO server.RaftServer: 325d3290-29d2-46d6-89ed-747878c1d37e: addNew group-2EC72CD08CCB:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0] returns group-2EC72CD08CCB:java.util.concurrent.CompletableFuture@28d18e8b[Not completed]
datanode_3          | 2023-06-30 10:29:04,435 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e: new RaftServerImpl for group-2EC72CD08CCB:[325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:29:04,435 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:29:04,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:29:04,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:29:04,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:29:04,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:29:04,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:29:04,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:29:04,451 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB: ConfigurationManager, init=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:29:04,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:29:04,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:29:04,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:29:04,452 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb does not exist. Creating ...
datanode_3          | 2023-06-30 10:29:04,456 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb/in_use.lock acquired by nodename 6@45b87710fe06
datanode_3          | 2023-06-30 10:29:04,463 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb has been successfully formatted.
datanode_3          | 2023-06-30 10:29:04,464 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-30 10:29:04,465 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2EC72CD08CCB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:29:04,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:29:04,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:29:04,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:29:04,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:29:04,477 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:04,477 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:29:04,478 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:29:04,478 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb
datanode_3          | 2023-06-30 10:29:04,478 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-30 10:29:22,037 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5155921151ns, electionTimeout:5137ms
datanode_1          | 2023-06-30 10:29:22,037 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState
datanode_1          | 2023-06-30 10:29:22,038 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_1          | 2023-06-30 10:29:22,038 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:29:22,038 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3
datanode_1          | 2023-06-30 10:29:22,043 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for -1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_1          | 2023-06-30 10:29:22,065 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:29:22,065 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO impl.LeaderElection:   Response 0: ce67b879-7fb6-4366-9a6c-9701a544e0b4<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t4
datanode_1          | 2023-06-30 10:29:22,065 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO impl.LeaderElection: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3 ELECTION round 0: result PASSED
datanode_1          | 2023-06-30 10:29:22,066 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: shutdown ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3
datanode_1          | 2023-06-30 10:29:22,066 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
datanode_1          | 2023-06-30 10:29:22,066 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F4999DDB4D74 with new leaderId: ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_1          | 2023-06-30 10:29:22,066 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-30 10:29:22,080 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: change Leader from null to ce67b879-7fb6-4366-9a6c-9701a544e0b4 at term 4 for becomeLeader, leader elected after 19277ms
datanode_1          | 2023-06-30 10:29:22,081 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:29:22,081 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:29:22,081 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-30 10:29:22,081 [grpc-default-executor-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: receive requestVote(ELECTION, 325d3290-29d2-46d6-89ed-747878c1d37e, group-F4999DDB4D74, 4, (t:0, i:0))
datanode_1          | 2023-06-30 10:29:22,082 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:29:22,082 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:29:22,082 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:29:22,082 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:29:22,082 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-30 10:29:22,116 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:29:22,116 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:29:22,116 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:29:22,119 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:29:22,120 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:29:22,121 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:29:22,126 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:29:22,131 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:29:22,131 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:29:22,131 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:29:22,132 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:29:22,132 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:29:22,134 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO impl.RoleInfo: ce67b879-7fb6-4366-9a6c-9701a544e0b4: start ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderStateImpl
datanode_1          | 2023-06-30 10:29:22,135 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:29:22,136 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74/current/log_inprogress_0
datanode_1          | 2023-06-30 10:29:22,145 [ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LeaderElection3] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74: set configuration 0: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-30 10:29:22,146 [grpc-default-executor-1] INFO impl.VoteContext: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-LEADER: reject ELECTION from 325d3290-29d2-46d6-89ed-747878c1d37e: already has voted for ce67b879-7fb6-4366-9a6c-9701a544e0b4 at current term 4
datanode_1          | 2023-06-30 10:29:22,146 [grpc-default-executor-1] INFO server.RaftServer$Division: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74 replies to ELECTION vote request: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t4. Peer's state: ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74:t4, leader=ce67b879-7fb6-4366-9a6c-9701a544e0b4, voted=ce67b879-7fb6-4366-9a6c-9701a544e0b4, raftlog=ce67b879-7fb6-4366-9a6c-9701a544e0b4@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=0: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-30 10:29:24,436 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-30 10:29:04,478 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:29:04,479 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:04,479 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:29:04,479 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:29:04,479 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:29:04,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:29:04,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:29:04,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:29:04,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:29:04,481 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:29:04,481 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:29:04,496 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:29:04,497 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:29:04,497 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:29:04,497 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:29:04,497 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:29:04,498 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:29:04,498 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB: start as a follower, conf=-1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:29:04,499 [pool-22-thread-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:29:04,499 [pool-22-thread-1] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState
datanode_3          | 2023-06-30 10:29:04,522 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2EC72CD08CCB,id=325d3290-29d2-46d6-89ed-747878c1d37e
datanode_3          | 2023-06-30 10:29:04,524 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb
datanode_3          | 2023-06-30 10:29:04,773 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb.
datanode_3          | 2023-06-30 10:29:06,479 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState] INFO impl.FollowerState: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5130022727ns, electionTimeout:5128ms
datanode_3          | 2023-06-30 10:29:06,479 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState
datanode_3          | 2023-06-30 10:29:06,479 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:29:06,486 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:29:06,486 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1
datanode_3          | 2023-06-30 10:29:06,496 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:06,497 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-30 10:29:06,497 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1
datanode_3          | 2023-06-30 10:29:06,497 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:29:06,498 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D3EE97F68D01 with new leaderId: 325d3290-29d2-46d6-89ed-747878c1d37e
datanode_3          | 2023-06-30 10:29:06,498 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01: change Leader from null to 325d3290-29d2-46d6-89ed-747878c1d37e at term 1 for becomeLeader, leader elected after 5556ms
datanode_3          | 2023-06-30 10:29:06,498 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-30 10:29:06,507 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5063083709ns, electionTimeout:5059ms
datanode_3          | 2023-06-30 10:29:06,509 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:06,509 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:29:06,510 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:29:06,510 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2
datanode_3          | 2023-06-30 10:29:06,510 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:29:06,524 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:06,527 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:29:06,527 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-30 10:29:06,554 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:29:06,554 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:29:06,555 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:29:06,614 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:29:06,620 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-30 10:29:06,627 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:29:06,638 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection:   Response 0: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t1
datanode_3          | 2023-06-30 10:29:06,638 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-30 10:29:06,639 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-06-30 10:29:06,655 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2
datanode_3          | 2023-06-30 10:29:06,655 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection2] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:06,676 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderStateImpl
datanode_3          | 2023-06-30 10:29:06,706 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:29:06,760 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-LeaderElection1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01: set configuration 0: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-30 10:29:06,901 [325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-D3EE97F68D01-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8715b917-c279-448e-9669-d3ee97f68d01/current/log_inprogress_0
datanode_3          | 2023-06-30 10:29:09,625 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState] INFO impl.FollowerState: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5125748143ns, electionTimeout:5085ms
datanode_3          | 2023-06-30 10:29:09,625 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState
datanode_3          | 2023-06-30 10:29:09,625 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:29:09,625 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:29:09,625 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3
datanode_3          | 2023-06-30 10:29:09,630 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:0], old=null
datanode_3          | 2023-06-30 10:29:09,655 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:29:09,655 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO impl.LeaderElection:   Response 0: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t1
datanode_3          | 2023-06-30 10:29:09,655 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3 ELECTION round 0: result PASSED
datanode_3          | 2023-06-30 10:29:09,655 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3
datanode_3          | 2023-06-30 10:29:09,656 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:29:09,656 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2EC72CD08CCB with new leaderId: 325d3290-29d2-46d6-89ed-747878c1d37e
datanode_3          | 2023-06-30 10:29:09,656 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB: change Leader from null to 325d3290-29d2-46d6-89ed-747878c1d37e at term 1 for becomeLeader, leader elected after 5179ms
datanode_3          | 2023-06-30 10:29:09,656 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:29:09,656 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:29:09,656 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-30 10:29:09,657 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:29:09,657 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:29:09,657 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:29:09,657 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:29:09,657 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-30 10:29:09,688 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:29:09,688 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:29:09,689 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:29:09,700 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-30 10:29:09,700 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:29:09,700 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:29:09,709 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:29:09,710 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:29:09,710 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:29:09,712 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-30 10:29:09,712 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:29:09,712 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:29:09,714 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderStateImpl
datanode_3          | 2023-06-30 10:29:09,715 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:29:09,720 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/290885af-9807-43c1-9509-2ec72cd08ccb/current/log_inprogress_0
datanode_3          | 2023-06-30 10:29:09,736 [325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB-LeaderElection3] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-2EC72CD08CCB: set configuration 0: [325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:1, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-06-30 10:29:11,722 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5066503765ns, electionTimeout:5030ms
datanode_3          | 2023-06-30 10:29:11,722 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:11,722 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-30 10:29:11,722 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:29:11,722 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4
datanode_3          | 2023-06-30 10:29:11,734 [grpc-default-executor-0] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: receive requestVote(ELECTION, ce67b879-7fb6-4366-9a6c-9701a544e0b4, group-F4999DDB4D74, 2, (t:0, i:0))
datanode_3          | 2023-06-30 10:29:11,736 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:11,744 [grpc-default-executor-0] INFO impl.VoteContext: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-CANDIDATE: reject ELECTION from ce67b879-7fb6-4366-9a6c-9701a544e0b4: already has voted for 325d3290-29d2-46d6-89ed-747878c1d37e at current term 2
datanode_3          | 2023-06-30 10:29:11,747 [grpc-default-executor-0] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74 replies to ELECTION vote request: ce67b879-7fb6-4366-9a6c-9701a544e0b4<-325d3290-29d2-46d6-89ed-747878c1d37e#0:FAIL-t2. Peer's state: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74:t2, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:11,824 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:29:11,824 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO impl.LeaderElection:   Response 0: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t2
datanode_3          | 2023-06-30 10:29:11,824 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO impl.LeaderElection:   Response 1: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t2
datanode_3          | 2023-06-30 10:29:11,824 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-30 10:29:11,825 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3          | 2023-06-30 10:29:11,825 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4
datanode_3          | 2023-06-30 10:29:11,825 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection4] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:16,553 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-30 10:29:16,868 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5042747786ns, electionTimeout:5031ms
datanode_3          | 2023-06-30 10:29:16,868 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:16,868 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_3          | 2023-06-30 10:29:16,868 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:29:16,868 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5
datanode_3          | 2023-06-30 10:29:16,870 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5 ELECTION round 0: submit vote requests at term 3 for -1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:16,897 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:29:16,897 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO impl.LeaderElection:   Response 0: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:OK-t3
datanode_3          | 2023-06-30 10:29:16,897 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO impl.LeaderElection:   Response 1: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t3
datanode_3          | 2023-06-30 10:29:16,898 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-30 10:29:16,898 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_3          | 2023-06-30 10:29:16,898 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5
datanode_3          | 2023-06-30 10:29:16,898 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection5] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:22,054 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.FollowerState: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5156247574ns, electionTimeout:5149ms
datanode_3          | 2023-06-30 10:29:22,055 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:22,055 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_3          | 2023-06-30 10:29:22,055 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:29:22,055 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:28:31,344 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 8bba71da4f8e/172.22.0.9
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-06-30 10:28:31,366 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:28:37,501 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:28:37,692 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.9:9862
om_1                | 2023-06-30 10:28:37,692 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:28:37,692 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:28:37,713 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:28:41,170 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8bba71da4f8e/172.22.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:28:43,176 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8bba71da4f8e/172.22.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:28:45,178 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8bba71da4f8e/172.22.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:28:47,179 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8bba71da4f8e/172.22.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:28:49,183 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8bba71da4f8e/172.22.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:28:51,193 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8bba71da4f8e/172.22.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:28:53,194 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8bba71da4f8e/172.22.0.9 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.3:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-15e146e7-1f66-4dec-a25f-085b6849b4be;layoutVersion=0
om_1                | 2023-06-30 10:28:57,143 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 8bba71da4f8e/172.22.0.9
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:28:59,207 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 8bba71da4f8e/172.22.0.9
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-06-30 10:28:59,213 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:29:00,597 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:29:00,655 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.9:9862
om_1                | 2023-06-30 10:29:00,660 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:29:00,660 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:29:00,681 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:29:00,736 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om_1                | 2023-06-30 10:29:01,460 [main] INFO reflections.Reflections: Reflections took 640 ms to scan 1 urls, producing 95 keys and 258 values [using 2 cores]
om_1                | 2023-06-30 10:29:01,489 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:29:04,235 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:29:04,712 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-30 10:29:04,713 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-30 10:29:05,022 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-30 10:29:05,056 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:29:05,056 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-30 10:29:05,100 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-30 10:29:05,116 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:29:05,163 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-30 10:29:05,175 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-30 10:29:05,207 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-30 10:29:05,316 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-06-30 10:29:05,317 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:29:05,320 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-06-30 10:29:05,320 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:29:05,321 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:29:05,322 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-30 10:29:05,323 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:29:05,325 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-30 10:29:05,325 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-30 10:29:05,659 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-30 10:29:05,660 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:29:05,662 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:29:05,677 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:29:05,703 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@478b0739[Not completed]
om_1                | 2023-06-30 10:29:05,704 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-30 10:29:05,726 [pool-23-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-30 10:29:05,730 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-30 10:29:05,731 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-30 10:29:05,732 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-30 10:29:05,737 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:29:05,738 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:29:05,738 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-30 10:29:05,739 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-30 10:29:05,742 [pool-23-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-30 10:29:05,756 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:29:05,746 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-30 10:29:05,765 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-30 10:29:05,766 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-30 10:29:05,768 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-30 10:29:05,795 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-30 10:29:05,801 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@8bba71da4f8e
om_1                | 2023-06-30 10:29:05,880 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-30 10:29:05,882 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-30 10:29:05,895 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-30 10:29:05,923 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-30 10:29:05,940 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:29:05,988 [Listener at om/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om_1                | 2023-06-30 10:29:05,997 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:29:06,034 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-30 10:29:06,035 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-30 10:29:06,039 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-30 10:29:06,039 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:29:06,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-30 10:29:06,043 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:29:06,051 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-30 10:29:06,052 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-30 10:29:06,053 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-30 10:29:06,059 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-30 10:29:06,059 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-30 10:28:30,608 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 9938c1ad2401/172.22.0.8
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.2.1
datanode_3          | 2023-06-30 10:29:22,061 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6 ELECTION round 0: submit vote requests at term 4 for -1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:22,096 [grpc-default-executor-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: receive requestVote(ELECTION, ce67b879-7fb6-4366-9a6c-9701a544e0b4, group-F4999DDB4D74, 4, (t:0, i:0))
datanode_3          | 2023-06-30 10:29:22,096 [grpc-default-executor-1] INFO impl.VoteContext: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-CANDIDATE: reject ELECTION from ce67b879-7fb6-4366-9a6c-9701a544e0b4: already has voted for 325d3290-29d2-46d6-89ed-747878c1d37e at current term 4
datanode_3          | 2023-06-30 10:29:22,097 [grpc-default-executor-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74 replies to ELECTION vote request: ce67b879-7fb6-4366-9a6c-9701a544e0b4<-325d3290-29d2-46d6-89ed-747878c1d37e#0:FAIL-t4. Peer's state: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74:t4, leader=null, voted=325d3290-29d2-46d6-89ed-747878c1d37e, raftlog=325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLog:OPENED:c-1, conf=-1: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|priority:1], old=null
datanode_3          | 2023-06-30 10:29:22,148 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:29:22,149 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO impl.LeaderElection:   Response 0: 325d3290-29d2-46d6-89ed-747878c1d37e<-4b3dfc39-1d9a-4911-af0e-236ac377f4d0#0:FAIL-t4
datanode_3          | 2023-06-30 10:29:22,149 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO impl.LeaderElection:   Response 1: 325d3290-29d2-46d6-89ed-747878c1d37e<-ce67b879-7fb6-4366-9a6c-9701a544e0b4#0:FAIL-t4
datanode_3          | 2023-06-30 10:29:22,149 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO impl.LeaderElection: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-30 10:29:22,149 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
datanode_3          | 2023-06-30 10:29:22,149 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: shutdown 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6
datanode_3          | 2023-06-30 10:29:22,149 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-LeaderElection6] INFO impl.RoleInfo: 325d3290-29d2-46d6-89ed-747878c1d37e: start 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-FollowerState
datanode_3          | 2023-06-30 10:29:22,193 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F4999DDB4D74 with new leaderId: ce67b879-7fb6-4366-9a6c-9701a544e0b4
datanode_3          | 2023-06-30 10:29:22,193 [grpc-default-executor-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: change Leader from null to ce67b879-7fb6-4366-9a6c-9701a544e0b4 at term 4 for appendEntries, leader elected after 20763ms
datanode_3          | 2023-06-30 10:29:22,216 [grpc-default-executor-1] INFO server.RaftServer$Division: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74: set configuration 0: [4b3dfc39-1d9a-4911-af0e-236ac377f4d0|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, 325d3290-29d2-46d6-89ed-747878c1d37e|rpc:172.22.0.2:9856|admin:172.22.0.2:9857|client:172.22.0.2:9858|dataStream:|priority:0, ce67b879-7fb6-4366-9a6c-9701a544e0b4|rpc:172.22.0.5:9856|admin:172.22.0.5:9857|client:172.22.0.5:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-30 10:29:22,220 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:29:22,221 [325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 325d3290-29d2-46d6-89ed-747878c1d37e@group-F4999DDB4D74-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b781839b-d726-4c54-b13e-f4999ddb4d74/current/log_inprogress_0
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:28:32,659 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 5b8c13f9f2d7/172.22.0.3
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.2.1
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-30 10:28:32,073 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-30 10:28:32,087 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-30 10:28:32,251 [main] INFO util.log: Logging initialized @5678ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-30 10:28:32,827 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-30 10:28:32,975 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-30 10:28:32,994 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-30 10:28:33,001 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-30 10:28:33,004 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-30 10:28:33,007 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-30 10:28:33,266 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = e51b1d658b12/172.22.0.12
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.2.1
s3g_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
s3g_1               | STARTUP_MSG:   java = 11.0.13
s3g_1               | ************************************************************/
s3g_1               | 2023-06-30 10:28:33,313 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-30 10:28:33,502 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-30 10:28:33,571 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-30 10:28:33,576 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
s3g_1               | 2023-06-30 10:28:33,804 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-30 10:28:33,805 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-30 10:28:33,813 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-06-30 10:28:33,912 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f284218{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-30 10:28:33,920 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@723e88f9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 30, 2023 10:28:49 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-30 10:28:50,160 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5fb3111a{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_2_1_jar-_-any-18080206770769824435/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/s3gateway}
s3g_1               | 2023-06-30 10:28:50,218 [main] INFO server.AbstractConnector: Started ServerConnector@5495333e{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-30 10:28:50,236 [main] INFO server.Server: Started @23662ms
s3g_1               | 2023-06-30 10:28:50,241 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:28:32,749 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:28:33,222 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:28:33,579 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:28:33,598 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:28:34,361 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-15e146e7-1f66-4dec-a25f-085b6849b4be; layoutVersion=2; scmId=8c3238a5-36e0-4e8e-a8c1-f0029c28aebf
scm_1               | 2023-06-30 10:28:34,645 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 5b8c13f9f2d7/172.22.0.3
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:28:46,788 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 5b8c13f9f2d7/172.22.0.3
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.2.1
om_1                | 2023-06-30 10:29:06,091 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-30 10:29:06,092 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-30 10:29:06,113 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:29:06,114 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:29:06,125 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-30 10:29:06,128 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-30 10:29:06,132 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-30 10:29:06,140 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-30 10:29:06,144 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-30 10:29:06,144 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-30 10:29:06,232 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-30 10:29:06,267 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-30 10:29:06,268 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-30 10:29:06,358 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.22.0.9:9862
om_1                | 2023-06-30 10:29:06,358 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-30 10:29:06,364 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-30 10:29:06,380 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-30 10:29:06,382 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:29:06,388 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-30 10:29:06,403 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-30 10:29:06,508 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-30 10:29:06,520 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-30 10:29:06,532 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$370/0x00000008404df040@6ee88e21] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-30 10:29:06,597 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-30 10:29:06,598 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-30 10:29:06,733 [Listener at om/9862] INFO util.log: Logging initialized @9079ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-30 10:29:06,967 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-30 10:29:06,973 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-30 10:29:06,978 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-30 10:29:06,979 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-30 10:29:06,979 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-30 10:29:06,979 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-30 10:29:07,030 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-30 10:29:07,031 [Listener at om/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
om_1                | 2023-06-30 10:29:07,073 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-30 10:29:07,073 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-30 10:29:07,074 [Listener at om/9862] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-06-30 10:29:07,088 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@67b61834{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-30 10:29:07,089 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@de77146{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-30 10:29:07,368 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@54ccb3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_1_jar-_-any-13741091708317094093/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/ozoneManager}
om_1                | 2023-06-30 10:29:07,373 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@5a2ae1ab{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-30 10:29:07,373 [Listener at om/9862] INFO server.Server: Started @9719ms
om_1                | 2023-06-30 10:29:07,375 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-30 10:29:07,375 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-30 10:29:07,377 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-30 10:29:07,382 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-30 10:29:07,396 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-06-30 10:29:07,410 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-30 10:29:07,418 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6a8a551e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-30 10:29:11,539 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5157002980ns, electionTimeout:5150ms
om_1                | 2023-06-30 10:29:11,540 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:28:46,855 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:28:47,476 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:28:47,499 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:28:48,126 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:28:48,373 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm_1               | 2023-06-30 10:28:51,080 [main] INFO reflections.Reflections: Reflections took 1045 ms to scan 3 urls, producing 103 keys and 211 values 
scm_1               | 2023-06-30 10:28:52,480 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:28:52,716 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:28:53,020 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
scm_1               | 2023-06-30 10:28:53,024 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-30 10:28:53,168 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-30 10:28:53,215 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-30 10:28:53,215 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-30 10:28:53,218 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-30 10:28:53,225 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-30 10:28:53,289 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-30 10:28:53,325 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-30 10:28:53,349 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-30 10:28:53,401 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-30 10:28:53,405 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-30 10:28:53,413 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:28:53,452 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-30 10:28:53,476 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-30 10:28:53,503 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-30 10:28:53,515 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-30 10:28:53,528 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 4 milliseconds for processing 0 containers.
scm_1               | 2023-06-30 10:28:53,538 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-30 10:28:53,541 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:28:53,546 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:28:54,842 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:28:54,918 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-30 10:28:54,960 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:28:54,961 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
recon_1             | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.2.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.33.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.33.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
recon_1             | STARTUP_MSG:   java = 11.0.13
recon_1             | ************************************************************/
recon_1             | 2023-06-30 10:28:30,650 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-30 10:28:34,164 [main] INFO reflections.Reflections: Reflections took 174 ms to scan 1 urls, producing 13 keys and 35 values 
recon_1             | 2023-06-30 10:28:36,164 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-30 10:28:37,156 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:28:42,380 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:28:43,521 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:28:43,638 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:28:43,662 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-30 10:28:47,605 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-30 10:28:47,756 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-30 10:28:47,801 [main] INFO util.log: Logging initialized @21314ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-30 10:28:48,461 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-30 10:28:48,512 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-30 10:28:48,573 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-30 10:28:48,589 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-30 10:28:48,594 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-30 10:28:48,594 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-30 10:28:49,347 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-30 10:28:50,299 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-30 10:28:50,323 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-30 10:28:50,364 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-30 10:28:50,447 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-30 10:28:50,455 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-30 10:28:51,550 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:28:51,866 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:28:51,911 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
recon_1             | 2023-06-30 10:28:51,914 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om_1                | 2023-06-30 10:29:11,542 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-30 10:29:11,545 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-30 10:29:11,545 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:29:11,550 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-30 10:29:11,550 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-30 10:29:11,551 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:29:11,551 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-30 10:29:11,552 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5669ms
om_1                | 2023-06-30 10:29:11,558 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-30 10:29:11,565 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:29:11,566 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:29:11,570 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-30 10:29:11,570 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-30 10:29:11,570 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-30 10:29:11,588 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:29:11,596 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-30 10:29:11,603 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-30 10:29:11,638 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-30 10:29:11,755 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-06-30 10:29:11,924 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-30 10:29:12,041 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | ]
om_1                | 2023-06-30 10:29:14,878 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-30 10:29:56,714 [qtp1482986993-43] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-30 10:29:56,736 [qtp1482986993-43] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688120996716 in 20 milliseconds
om_1                | 2023-06-30 10:29:56,781 [qtp1482986993-43] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 43 milliseconds
om_1                | 2023-06-30 10:29:56,781 [qtp1482986993-43] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688120996716
scm_1               | 2023-06-30 10:28:54,979 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:28:54,980 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-30 10:28:55,204 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-30 10:28:55,225 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          0.1
scm_1               | Max Datanodes to Involve per Iteration(ratio)      0.2
scm_1               | Max Size to Move per Iteration                     32212254720B
scm_1               | 
scm_1               | 2023-06-30 10:28:55,247 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-30 10:28:55,247 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-30 10:28:55,250 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:28:55,452 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-30 10:28:55,479 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-30 10:28:55,479 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-30 10:28:55,975 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:28:55,980 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:28:55,988 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-30 10:28:56,241 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:28:56,260 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:28:56,262 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:28:56,262 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-30 10:28:56,504 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:28:56,504 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:28:56,505 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:28:56,505 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-30 10:28:56,752 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3596b249] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-30 10:28:56,799 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-30 10:28:56,800 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-30 10:28:56,830 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @20164ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-30 10:28:57,239 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-30 10:28:57,284 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-30 10:28:57,306 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-30 10:28:57,326 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-30 10:28:57,332 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-30 10:28:57,333 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-30 10:28:57,479 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-30 10:28:57,482 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
scm_1               | 2023-06-30 10:28:57,601 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-30 10:28:57,601 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-30 10:28:57,608 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-30 10:28:57,637 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@25ffd826{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-30 10:28:57,645 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@39dec536{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-30 10:28:57,941 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/325d3290-29d2-46d6-89ed-747878c1d37e
scm_1               | 2023-06-30 10:28:57,950 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:28:57,984 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:28:58,004 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:28:58,013 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:28:58,026 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:28:58,050 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8715b917-c279-448e-9669-d3ee97f68d01 to datanode:325d3290-29d2-46d6-89ed-747878c1d37e
recon_1             | 2023-06-30 10:28:52,044 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:28:52,265 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
recon_1             | 2023-06-30 10:28:52,338 [main] INFO reflections.Reflections: Reflections took 68 ms to scan 3 urls, producing 103 keys and 211 values 
recon_1             | 2023-06-30 10:28:52,398 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-30 10:28:52,430 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-30 10:28:52,440 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-30 10:28:52,471 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-30 10:28:52,519 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-30 10:28:52,570 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-30 10:28:52,609 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-30 10:28:52,662 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-30 10:28:52,663 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-30 10:28:52,795 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-30 10:28:52,818 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-30 10:28:52,818 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-30 10:28:53,163 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-30 10:28:53,172 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
recon_1             | 2023-06-30 10:28:53,281 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-30 10:28:53,281 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-30 10:28:53,288 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-06-30 10:28:53,305 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6da4feeb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-30 10:28:53,316 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@498a612d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-30 10:28:56,406 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@40239b34{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_2_1_jar-_-any-1608309060013425497/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/recon}
recon_1             | 2023-06-30 10:28:56,425 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1e98b788{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-30 10:28:56,427 [Listener at 0.0.0.0/9891] INFO server.Server: Started @29940ms
recon_1             | 2023-06-30 10:28:56,435 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-30 10:28:56,435 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-30 10:28:56,436 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-30 10:28:56,436 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-30 10:28:56,455 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-30 10:28:56,466 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-30 10:28:56,466 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-30 10:28:56,470 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:28:56,470 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-30 10:28:56,484 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:28:57,541 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-30 10:28:57,544 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:28:57,544 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:28:57,551 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-30 10:28:57,567 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-30 10:28:57,751 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-30 10:28:57,751 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-30 10:28:57,791 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-30 10:28:57,791 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-30 10:28:57,842 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:28:57,869 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 110 milliseconds.
recon_1             | 2023-06-30 10:28:58,044 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 247 milliseconds to process 0 existing database records.
recon_1             | 2023-06-30 10:28:58,074 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 30 milliseconds for processing 0 containers.
recon_1             | 2023-06-30 10:28:58,286 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.2:55280: output error
recon_1             | 2023-06-30 10:28:58,286 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.5:52830: output error
recon_1             | 2023-06-30 10:28:58,302 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
scm_1               | 2023-06-30 10:28:58,156 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8715b917-c279-448e-9669-d3ee97f68d01, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:58.049Z[UTC]].
scm_1               | 2023-06-30 10:28:58,460 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@594131f2{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_1_jar-_-any-17030460574906722332/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/scm}
scm_1               | 2023-06-30 10:28:58,473 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@1983b48a{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-30 10:28:58,474 [Listener at 0.0.0.0/9860] INFO server.Server: Started @21808ms
scm_1               | 2023-06-30 10:28:58,476 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-30 10:28:58,476 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-30 10:28:58,480 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-30 10:28:58,727 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ce67b879-7fb6-4366-9a6c-9701a544e0b4
scm_1               | 2023-06-30 10:28:58,728 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:28:58,728 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:28:58,730 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b4127be5-6df7-4633-a5e6-3a602f9c4518 to datanode:ce67b879-7fb6-4366-9a6c-9701a544e0b4
scm_1               | 2023-06-30 10:28:58,730 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b4127be5-6df7-4633-a5e6-3a602f9c4518, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:58.730Z[UTC]].
scm_1               | 2023-06-30 10:28:58,744 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:28:59,113 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4b3dfc39-1d9a-4911-af0e-236ac377f4d0
scm_1               | 2023-06-30 10:28:59,114 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:28:59,114 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:28:59,115 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:28:59,116 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:28:59,116 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-30 10:28:59,126 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=85863d74-2f4f-4f0b-9bb5-0adf25b4817a to datanode:4b3dfc39-1d9a-4911-af0e-236ac377f4d0
scm_1               | 2023-06-30 10:28:59,126 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 85863d74-2f4f-4f0b-9bb5-0adf25b4817a, Nodes: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.126Z[UTC]].
scm_1               | 2023-06-30 10:28:59,116 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-30 10:28:59,128 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-30 10:28:59,128 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:28:59,155 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 to datanode:ce67b879-7fb6-4366-9a6c-9701a544e0b4
scm_1               | 2023-06-30 10:28:59,158 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 to datanode:4b3dfc39-1d9a-4911-af0e-236ac377f4d0
scm_1               | 2023-06-30 10:28:59,159 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 to datanode:325d3290-29d2-46d6-89ed-747878c1d37e
scm_1               | 2023-06-30 10:28:59,160 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b781839b-d726-4c54-b13e-f4999ddb4d74, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.155Z[UTC]].
scm_1               | 2023-06-30 10:28:59,162 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb to datanode:325d3290-29d2-46d6-89ed-747878c1d37e
scm_1               | 2023-06-30 10:28:59,163 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb to datanode:ce67b879-7fb6-4366-9a6c-9701a544e0b4
scm_1               | 2023-06-30 10:28:59,163 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb to datanode:4b3dfc39-1d9a-4911-af0e-236ac377f4d0
scm_1               | 2023-06-30 10:28:59,164 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 290885af-9807-43c1-9509-2ec72cd08ccb, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.162Z[UTC]].
scm_1               | 2023-06-30 10:28:59,167 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb contains same datanodes as previous pipelines: PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 nodeIds: 325d3290-29d2-46d6-89ed-747878c1d37e, ce67b879-7fb6-4366-9a6c-9701a544e0b4, 4b3dfc39-1d9a-4911-af0e-236ac377f4d0
scm_1               | 2023-06-30 10:29:00,976 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8715b917-c279-448e-9669-d3ee97f68d01, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:325d3290-29d2-46d6-89ed-747878c1d37e, CreationTimestamp2023-06-30T10:28:58.049Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:29:00,997 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:01,441 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:01,988 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b4127be5-6df7-4633-a5e6-3a602f9c4518, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:ce67b879-7fb6-4366-9a6c-9701a544e0b4, CreationTimestamp2023-06-30T10:28:58.730Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:29:01,990 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:02,401 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 85863d74-2f4f-4f0b-9bb5-0adf25b4817a, Nodes: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4b3dfc39-1d9a-4911-af0e-236ac377f4d0, CreationTimestamp2023-06-30T10:28:59.126Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:29:02,405 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:02,825 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:02,957 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:04,526 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:04,601 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:04,632 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:06,507 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:07,747 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:07,816 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:09,670 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 290885af-9807-43c1-9509-2ec72cd08ccb, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:325d3290-29d2-46d6-89ed-747878c1d37e, CreationTimestamp2023-06-30T10:28:59.162Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:29:09,671 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:29:09,671 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:29:09,672 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-30 10:29:09,672 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-30 10:29:09,672 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-30 10:29:09,672 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-30 10:29:09,672 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-30 10:29:09,672 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO container.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-30 10:29:14,970 [IPC Server handler 15 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-30 10:29:14,986 [IPC Server handler 15 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-30 10:29:14,986 [IPC Server handler 15 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-30 10:29:22,072 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b781839b-d726-4c54-b13e-f4999ddb4d74, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:ce67b879-7fb6-4366-9a6c-9701a544e0b4, CreationTimestamp2023-06-30T10:28:59.155Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:29:41,442 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.11
scm_1               | 2023-06-30 10:29:49,497 [IPC Server handler 96 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.11
scm_1               | 2023-06-30 10:30:40,790 [IPC Server handler 12 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.11
scm_1               | 2023-06-30 10:30:48,852 [IPC Server handler 13 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.11
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-30 10:28:58,302 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-30 10:28:58,745 [IPC Server handler 92 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ce67b879-7fb6-4366-9a6c-9701a544e0b4
recon_1             | 2023-06-30 10:28:58,748 [IPC Server handler 92 on default port 9891] INFO node.SCMNodeManager: Registered Data node : ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:28:58,754 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node ce67b879-7fb6-4366-9a6c-9701a544e0b4 to Node DB.
recon_1             | 2023-06-30 10:28:59,100 [IPC Server handler 4 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4b3dfc39-1d9a-4911-af0e-236ac377f4d0
recon_1             | 2023-06-30 10:28:59,101 [IPC Server handler 4 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:28:59,103 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 4b3dfc39-1d9a-4911-af0e-236ac377f4d0 to Node DB.
recon_1             | 2023-06-30 10:28:59,810 [IPC Server handler 4 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/325d3290-29d2-46d6-89ed-747878c1d37e
recon_1             | 2023-06-30 10:28:59,810 [IPC Server handler 4 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:28:59,811 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 325d3290-29d2-46d6-89ed-747878c1d37e to Node DB.
recon_1             | 2023-06-30 10:29:00,715 [IPC Server handler 79 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-30 10:29:00,969 [IPC Server handler 2 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-30 10:29:00,971 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=8715b917-c279-448e-9669-d3ee97f68d01. Trying to get from SCM.
recon_1             | 2023-06-30 10:29:01,002 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 8715b917-c279-448e-9669-d3ee97f68d01, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:325d3290-29d2-46d6-89ed-747878c1d37e, CreationTimestamp2023-06-30T10:28:58.049Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:29:01,074 [IPC Server handler 0 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-30 10:29:01,106 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8715b917-c279-448e-9669-d3ee97f68d01, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:325d3290-29d2-46d6-89ed-747878c1d37e, CreationTimestamp2023-06-30T10:28:58.049Z[UTC]].
recon_1             | 2023-06-30 10:29:01,154 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 148036.038us
recon_1             | 2023-06-30 10:29:01,432 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-30 10:29:01,434 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74. Trying to get from SCM.
recon_1             | 2023-06-30 10:29:01,437 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b781839b-d726-4c54-b13e-f4999ddb4d74, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.155Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:29:01,439 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b781839b-d726-4c54-b13e-f4999ddb4d74, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.155Z[UTC]].
recon_1             | 2023-06-30 10:29:01,444 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 6770.451us
recon_1             | 2023-06-30 10:29:01,445 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:01,943 [IPC Server handler 2 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-30 10:29:01,944 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b4127be5-6df7-4633-a5e6-3a602f9c4518. Trying to get from SCM.
recon_1             | 2023-06-30 10:29:01,946 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b4127be5-6df7-4633-a5e6-3a602f9c4518, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:58.730Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:29:01,947 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b4127be5-6df7-4633-a5e6-3a602f9c4518, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:58.730Z[UTC]].
recon_1             | 2023-06-30 10:29:01,948 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 1506.89us
recon_1             | 2023-06-30 10:29:01,948 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=b4127be5-6df7-4633-a5e6-3a602f9c4518 reported by ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:01,948 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b4127be5-6df7-4633-a5e6-3a602f9c4518, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:ce67b879-7fb6-4366-9a6c-9701a544e0b4, CreationTimestamp2023-06-30T10:28:58.730Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:29:01,949 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 691.795us
recon_1             | 2023-06-30 10:29:02,369 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-30 10:29:02,370 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=85863d74-2f4f-4f0b-9bb5-0adf25b4817a. Trying to get from SCM.
recon_1             | 2023-06-30 10:29:02,372 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 85863d74-2f4f-4f0b-9bb5-0adf25b4817a, Nodes: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.126Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:29:02,373 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 85863d74-2f4f-4f0b-9bb5-0adf25b4817a, Nodes: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.126Z[UTC]].
recon_1             | 2023-06-30 10:29:02,373 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 547.096us
recon_1             | 2023-06-30 10:29:02,373 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=85863d74-2f4f-4f0b-9bb5-0adf25b4817a reported by 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:02,373 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 85863d74-2f4f-4f0b-9bb5-0adf25b4817a, Nodes: 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4b3dfc39-1d9a-4911-af0e-236ac377f4d0, CreationTimestamp2023-06-30T10:28:59.126Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:29:02,374 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 328.898us
recon_1             | 2023-06-30 10:29:02,815 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:02,956 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:04,477 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:04,478 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb. Trying to get from SCM.
recon_1             | 2023-06-30 10:29:04,490 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 290885af-9807-43c1-9509-2ec72cd08ccb, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.162Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:29:04,491 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 290885af-9807-43c1-9509-2ec72cd08ccb, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:28:59.162Z[UTC]].
recon_1             | 2023-06-30 10:29:04,492 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 1101.392us
recon_1             | 2023-06-30 10:29:04,493 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb reported by 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:04,595 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:04,596 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb reported by ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:04,622 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:04,627 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb reported by 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:06,502 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:06,502 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb reported by 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:07,748 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:07,748 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb reported by ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:07,814 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:07,814 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb reported by 4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:09,666 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:09,666 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=290885af-9807-43c1-9509-2ec72cd08ccb reported by 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:09,667 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 290885af-9807-43c1-9509-2ec72cd08ccb, Nodes: 325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:325d3290-29d2-46d6-89ed-747878c1d37e, CreationTimestamp2023-06-30T10:28:59.162Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:29:09,667 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 553.996us
recon_1             | 2023-06-30 10:29:16,537 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-30 10:29:16,595 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 15436.485us
recon_1             | 2023-06-30 10:29:16,607 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-30 10:29:22,069 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b781839b-d726-4c54-b13e-f4999ddb4d74 reported by ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:29:22,069 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b781839b-d726-4c54-b13e-f4999ddb4d74, Nodes: ce67b879-7fb6-4366-9a6c-9701a544e0b4{ip: 172.22.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}4b3dfc39-1d9a-4911-af0e-236ac377f4d0{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}325d3290-29d2-46d6-89ed-747878c1d37e{ip: 172.22.0.2, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:ce67b879-7fb6-4366-9a6c-9701a544e0b4, CreationTimestamp2023-06-30T10:28:59.155Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:29:22,070 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 555.496us
recon_1             | 2023-06-30 10:29:24,443 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-30 10:29:24,446 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 335.998us
recon_1             | 2023-06-30 10:29:24,447 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-30 10:29:32,450 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-30 10:29:32,457 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@74a06d32, cost 316.898us
recon_1             | 2023-06-30 10:29:32,458 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-30 10:29:56,474 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-30 10:29:56,475 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-30 10:29:56,844 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688120996475
recon_1             | 2023-06-30 10:29:56,859 [pool-16-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-30 10:29:56,860 [pool-16-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-30 10:29:56,911 [pool-16-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688120996475.
recon_1             | 2023-06-30 10:29:56,936 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-30 10:29:56,937 [pool-17-thread-1] INFO tasks.NSSummaryTask: Completed a reprocess run of NSSummaryTask
recon_1             | 2023-06-30 10:29:57,144 [pool-17-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-30 10:29:57,144 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:29:57,281 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:29:57,282 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.137 seconds to process 4 keys.
recon_1             | 2023-06-30 10:29:57,307 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-30 10:29:57,345 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
Attaching to xcompat_datanode_2, xcompat_old_client_1_3_0_1, xcompat_datanode_1, xcompat_datanode_3, xcompat_scm_1, xcompat_om_1, xcompat_old_client_1_1_0_1, xcompat_new_client_1, xcompat_old_client_1_0_0_1, xcompat_recon_1, xcompat_old_client_1_2_1_1, xcompat_s3g_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-30 10:31:13,089 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 93407cadbf5e/172.23.0.10
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.3.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_1          | STARTUP_MSG:   java = 11.0.14.1
datanode_1          | ************************************************************/
datanode_1          | 2023-06-30 10:31:13,113 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-30 10:31:13,422 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-30 10:31:13,863 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-30 10:31:14,592 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-30 10:31:14,592 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-30 10:31:15,213 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:93407cadbf5e ip:172.23.0.10
datanode_1          | 2023-06-30 10:31:16,376 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1          | 2023-06-30 10:31:17,162 [main] INFO reflections.Reflections: Reflections took 604 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_1          | 2023-06-30 10:31:17,733 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-30 10:31:18,477 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-30 10:31:18,513 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-30 10:31:18,515 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-30 10:31:18,524 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-30 10:31:18,622 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:31:18,686 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:31:18,713 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-30 10:31:18,728 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-30 10:31:18,728 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-30 10:31:18,742 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-30 10:31:18,856 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:31:18,857 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-30 10:31:24,749 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-30 10:31:25,173 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:31:25,525 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-30 10:31:26,001 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-30 10:31:26,011 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-30 10:31:26,025 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-30 10:31:26,026 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-30 10:31:26,026 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-30 10:31:26,029 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-30 10:31:26,030 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-30 10:31:26,035 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:31:26,076 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-30 10:31:26,077 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:31:26,152 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-30 10:31:26,179 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-30 10:31:26,187 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-30 10:31:27,855 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-30 10:31:27,865 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-30 10:31:27,869 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-30 10:31:27,869 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:31:27,878 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:31:27,880 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:31:27,983 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-30 10:31:28,715 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:31:28,780 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-30 10:31:28,932 [main] INFO util.log: Logging initialized @22560ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-30 10:31:29,569 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-30 10:31:29,616 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-30 10:31:29,639 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-30 10:31:29,661 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-30 10:31:29,669 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-30 10:31:29,675 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-30 10:31:30,170 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-30 10:31:30,192 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1          | 2023-06-30 10:31:30,336 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-30 10:31:30,336 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-30 10:31:30,337 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-30 10:31:30,379 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63bfdbcb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-30 10:31:30,380 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1d7eb170{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-30 10:31:31,695 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4966bab1{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-16769122003012883623/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-30 10:31:31,730 [main] INFO server.AbstractConnector: Started ServerConnector@7197b07f{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-30 10:31:31,734 [main] INFO server.Server: Started @25362ms
datanode_1          | 2023-06-30 10:31:31,747 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-30 10:31:31,747 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-30 10:31:31,749 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:31:31,753 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-30 10:31:31,878 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2b2ff35] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-30 10:31:32,079 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.5:9891
datanode_1          | 2023-06-30 10:31:32,199 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-30 10:31:35,121 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:31:36,122 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:31:37,122 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:31:38,123 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:31:39,125 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:31:39,180 [EndpointStateMachine task thread for recon/172.23.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 93407cadbf5e/172.23.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:39874 remote=recon/172.23.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:39874 remote=recon/172.23.0.5:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-06-30 10:31:44,137 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 93407cadbf5e/172.23.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:52956 remote=scm/172.23.0.9:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:52956 remote=scm/172.23.0.9:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-06-30 10:31:44,433 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8/DS-b6f6d972-51ea-4d85-a1c5-b7c38a0041fe/container.db to cache
datanode_1          | 2023-06-30 10:31:44,434 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8/DS-b6f6d972-51ea-4d85-a1c5-b7c38a0041fe/container.db for volume DS-b6f6d972-51ea-4d85-a1c5-b7c38a0041fe
datanode_1          | 2023-06-30 10:31:44,435 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-30 10:31:44,438 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-30 10:31:44,548 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_1          | 2023-06-30 10:31:44,646 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.RaftServer: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start RPC server
datanode_1          | 2023-06-30 10:31:44,664 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: 0f3d230a-845e-43da-9c18-0d5fd63464e3: GrpcService started, listening on 9858
datanode_1          | 2023-06-30 10:31:44,665 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: 0f3d230a-845e-43da-9c18-0d5fd63464e3: GrpcService started, listening on 9856
datanode_1          | 2023-06-30 10:31:44,667 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: 0f3d230a-845e-43da-9c18-0d5fd63464e3: GrpcService started, listening on 9857
datanode_1          | 2023-06-30 10:31:44,688 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0f3d230a-845e-43da-9c18-0d5fd63464e3 is started using port 9858 for RATIS
datanode_1          | 2023-06-30 10:31:44,688 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0f3d230a-845e-43da-9c18-0d5fd63464e3 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-30 10:31:44,688 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0f3d230a-845e-43da-9c18-0d5fd63464e3 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-30 10:31:44,688 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0f3d230a-845e-43da-9c18-0d5fd63464e3: Started
datanode_1          | 2023-06-30 10:31:49,085 [Command processor thread] INFO server.RaftServer: 0f3d230a-845e-43da-9c18-0d5fd63464e3: addNew group-13469D7DBF2F:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-13469D7DBF2F:java.util.concurrent.CompletableFuture@4d5d1fc5[Not completed]
datanode_1          | 2023-06-30 10:31:49,190 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3: new RaftServerImpl for group-13469D7DBF2F:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:31:49,195 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:31:49,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:31:49,197 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:31:49,198 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:31:49,198 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-30 10:31:13,092 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 1737fd297348/172.23.0.11
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.3.0
datanode_1          | 2023-06-30 10:31:49,198 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:31:49,203 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F: ConfigurationManager, init=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:31:49,219 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:31:49,253 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:31:49,254 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:31:49,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:31:49,274 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:31:49,274 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:31:49,430 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:31:49,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:31:49,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:31:49,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:31:49,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:31:49,455 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f does not exist. Creating ...
datanode_1          | 2023-06-30 10:31:49,477 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f/in_use.lock acquired by nodename 7@93407cadbf5e
datanode_1          | 2023-06-30 10:31:49,512 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f has been successfully formatted.
datanode_1          | 2023-06-30 10:31:49,522 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-13469D7DBF2F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:31:49,523 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:31:49,576 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:31:49,600 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:31:49,601 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:31:49,607 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:31:49,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:49,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:31:49,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:31:49,677 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f
datanode_1          | 2023-06-30 10:31:49,678 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:31:49,678 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:31:49,679 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:49,679 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:31:49,686 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:31:49,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:31:49,692 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:31:49,693 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:31:49,729 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:49,735 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:31:49,735 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:31:49,736 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:31:49,746 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:31:49,746 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:31:49,763 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F: start as a follower, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:49,763 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:31:49,764 [pool-22-thread-1] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState
datanode_1          | 2023-06-30 10:31:49,774 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:31:49,778 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:31:49,794 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-13469D7DBF2F,id=0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_1          | 2023-06-30 10:31:49,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_2          | STARTUP_MSG:   java = 11.0.14.1
datanode_2          | ************************************************************/
datanode_2          | 2023-06-30 10:31:13,126 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-30 10:31:13,363 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-30 10:31:13,821 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-30 10:31:14,565 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-30 10:31:14,579 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-30 10:31:15,173 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1737fd297348 ip:172.23.0.11
datanode_2          | 2023-06-30 10:31:16,393 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_2          | 2023-06-30 10:31:17,140 [main] INFO reflections.Reflections: Reflections took 579 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_2          | 2023-06-30 10:31:17,613 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-30 10:31:18,237 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-30 10:31:18,291 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-30 10:31:18,293 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-30 10:31:18,318 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-30 10:31:18,410 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:31:18,511 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:31:18,512 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-30 10:31:18,534 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-30 10:31:18,535 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-30 10:31:18,535 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-30 10:31:18,720 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:31:18,721 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-30 10:31:24,519 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-30 10:31:24,739 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:31:24,938 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-30 10:31:25,220 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-30 10:31:25,228 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-30 10:31:25,236 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-30 10:31:25,236 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-30 10:31:25,239 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-30 10:31:25,243 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-30 10:31:25,244 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-30 10:31:25,248 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:31:25,249 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-30 10:31:25,251 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:31:25,284 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-30 10:31:25,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-30 10:31:25,317 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-30 10:31:27,075 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-30 10:31:27,077 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-30 10:31:27,086 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-30 10:31:27,095 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:31:27,095 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:31:27,101 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:31:27,230 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-30 10:31:27,868 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:31:28,000 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-30 10:31:28,147 [main] INFO util.log: Logging initialized @21390ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-30 10:31:28,958 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-30 10:31:29,029 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-30 10:31:29,111 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-30 10:31:29,113 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-30 10:31:29,131 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-30 10:31:49,801 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:31:49,801 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:31:49,801 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:31:49,869 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f
datanode_1          | 2023-06-30 10:31:49,870 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f.
datanode_1          | 2023-06-30 10:31:49,870 [Command processor thread] INFO server.RaftServer: 0f3d230a-845e-43da-9c18-0d5fd63464e3: addNew group-87F1FD387BB8:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-87F1FD387BB8:java.util.concurrent.CompletableFuture@2d1863c5[Not completed]
datanode_1          | 2023-06-30 10:31:49,887 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3: new RaftServerImpl for group-87F1FD387BB8:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: ConfigurationManager, init=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:31:49,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:31:49,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:31:49,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:31:49,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:31:49,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:31:49,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:31:49,890 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8 does not exist. Creating ...
datanode_1          | 2023-06-30 10:31:49,900 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8/in_use.lock acquired by nodename 7@93407cadbf5e
datanode_1          | 2023-06-30 10:31:49,903 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8 has been successfully formatted.
datanode_1          | 2023-06-30 10:31:49,919 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-87F1FD387BB8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:31:49,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:31:49,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:31:49,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:31:49,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:31:49,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:31:49,934 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:49,936 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:31:49,937 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:31:49,937 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8
datanode_1          | 2023-06-30 10:31:49,937 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:31:49,937 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:31:49,938 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:49,938 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:31:49,938 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:31:49,938 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:31:29,135 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-30 10:31:29,362 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-30 10:31:29,392 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2          | 2023-06-30 10:31:29,517 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-30 10:31:29,518 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-30 10:31:29,524 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-30 10:31:29,574 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@230a73f2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-30 10:31:29,576 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22d8f4ed{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-30 10:31:30,886 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@17c0274c{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-11418186921324044205/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-30 10:31:30,917 [main] INFO server.AbstractConnector: Started ServerConnector@510689af{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-30 10:31:30,917 [main] INFO server.Server: Started @24160ms
datanode_2          | 2023-06-30 10:31:30,927 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-30 10:31:30,927 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-30 10:31:30,928 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:31:30,946 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-30 10:31:31,173 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1ebb61ea] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-30 10:31:31,419 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.5:9891
datanode_2          | 2023-06-30 10:31:31,565 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-30 10:31:34,119 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:31:35,120 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:31:36,121 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:31:37,122 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:31:38,122 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:31:38,140 [EndpointStateMachine task thread for recon/172.23.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 1737fd297348/172.23.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:49442 remote=recon/172.23.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:49442 remote=recon/172.23.0.5:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-06-30 10:31:39,123 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:31:43,035 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-30 10:31:44,136 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 1737fd297348/172.23.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:56780 remote=scm/172.23.0.9:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:56780 remote=scm/172.23.0.9:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-06-30 10:31:44,392 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8/DS-491e2539-6664-4c64-9db7-2717a2eaac21/container.db to cache
datanode_2          | 2023-06-30 10:31:44,392 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8/DS-491e2539-6664-4c64-9db7-2717a2eaac21/container.db for volume DS-491e2539-6664-4c64-9db7-2717a2eaac21
datanode_2          | 2023-06-30 10:31:44,393 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-30 10:31:44,395 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-30 10:31:44,532 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_2          | 2023-06-30 10:31:44,555 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.RaftServer: f224272d-19aa-40de-aa6a-e64f79edbd5a: start RPC server
datanode_2          | 2023-06-30 10:31:44,560 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: f224272d-19aa-40de-aa6a-e64f79edbd5a: GrpcService started, listening on 9858
datanode_2          | 2023-06-30 10:31:44,561 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: f224272d-19aa-40de-aa6a-e64f79edbd5a: GrpcService started, listening on 9856
datanode_2          | 2023-06-30 10:31:44,561 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: f224272d-19aa-40de-aa6a-e64f79edbd5a: GrpcService started, listening on 9857
datanode_2          | 2023-06-30 10:31:44,603 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f224272d-19aa-40de-aa6a-e64f79edbd5a: Started
datanode_2          | 2023-06-30 10:31:44,603 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f224272d-19aa-40de-aa6a-e64f79edbd5a is started using port 9858 for RATIS
datanode_2          | 2023-06-30 10:31:44,603 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f224272d-19aa-40de-aa6a-e64f79edbd5a is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-30 10:31:44,603 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f224272d-19aa-40de-aa6a-e64f79edbd5a is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-30 10:31:48,138 [Command processor thread] INFO server.RaftServer: f224272d-19aa-40de-aa6a-e64f79edbd5a: addNew group-8FCF1F3E0F3B:[f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-8FCF1F3E0F3B:java.util.concurrent.CompletableFuture@f6c133c[Not completed]
datanode_2          | 2023-06-30 10:31:48,179 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a: new RaftServerImpl for group-8FCF1F3E0F3B:[f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:31:48,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:31:48,189 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:31:48,189 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:31:48,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:31:48,191 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:31:48,192 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:31:48,207 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B: ConfigurationManager, init=-1: peers:[f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:31:48,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:31:48,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:31:48,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:31:48,247 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:31:48,268 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:31:48,270 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:31:48,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:31:48,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-30 10:31:48,411 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-30 10:31:48,414 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-30 10:31:48,418 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-30 10:31:48,419 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b does not exist. Creating ...
datanode_2          | 2023-06-30 10:31:48,423 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b/in_use.lock acquired by nodename 7@1737fd297348
datanode_2          | 2023-06-30 10:31:48,432 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b has been successfully formatted.
datanode_2          | 2023-06-30 10:31:48,480 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8FCF1F3E0F3B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:31:48,487 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:31:48,542 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:31:48,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:31:48,565 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-30 10:31:48,572 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-30 10:31:48,580 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:48,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:31:48,614 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:31:48,636 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b
datanode_2          | 2023-06-30 10:31:48,636 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-30 10:31:48,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:31:48,643 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:48,646 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:31:48,646 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:31:48,647 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:31:48,648 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:31:48,649 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:31:48,673 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:48,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:31:48,681 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:31:48,685 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:31:48,705 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:31:48,705 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:31:48,711 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B: start as a follower, conf=-1: peers:[f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:48,714 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:31:48,715 [pool-22-thread-1] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState
datanode_2          | 2023-06-30 10:31:48,745 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:48,745 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:31:48,747 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8FCF1F3E0F3B,id=f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_2          | 2023-06-30 10:31:48,749 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:31:48,750 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:31:48,750 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:31:48,750 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:31:48,809 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b
datanode_2          | 2023-06-30 10:31:48,820 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b.
datanode_2          | 2023-06-30 10:31:48,820 [Command processor thread] INFO server.RaftServer: f224272d-19aa-40de-aa6a-e64f79edbd5a: addNew group-87F1FD387BB8:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-87F1FD387BB8:java.util.concurrent.CompletableFuture@3b906ec9[Not completed]
datanode_2          | 2023-06-30 10:31:48,846 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a: new RaftServerImpl for group-87F1FD387BB8:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:31:48,849 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:31:48,849 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:31:48,851 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:31:48,851 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:31:48,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:31:48,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:31:48,852 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: ConfigurationManager, init=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:31:48,853 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:31:48,853 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:31:48,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:31:48,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:31:48,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:31:48,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:31:48,855 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:31:48,879 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-30 10:31:48,879 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-30 10:31:48,881 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-30 10:31:48,881 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-30 10:31:48,881 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8 does not exist. Creating ...
datanode_2          | 2023-06-30 10:31:48,887 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8/in_use.lock acquired by nodename 7@1737fd297348
datanode_2          | 2023-06-30 10:31:48,890 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8 has been successfully formatted.
datanode_2          | 2023-06-30 10:31:48,906 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-87F1FD387BB8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:31:48,907 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:31:48,907 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:31:48,907 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:31:48,908 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-30 10:31:48,908 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-30 10:31:48,908 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:48,909 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:31:48,911 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:31:48,911 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8
datanode_2          | 2023-06-30 10:31:48,912 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-30 10:31:48,912 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:31:48,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:48,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:31:48,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:31:48,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:31:48,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:31:48,916 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:31:48,916 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:48,930 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:31:48,939 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:31:48,939 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:31:48,939 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:31:48,940 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:31:48,940 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: start as a follower, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:48,941 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:31:48,941 [pool-22-thread-1] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState
datanode_2          | 2023-06-30 10:31:48,975 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:48,980 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-87F1FD387BB8,id=f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_2          | 2023-06-30 10:31:48,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:31:48,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:31:48,986 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:31:48,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:31:48,988 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:31:49,000 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8
datanode_2          | 2023-06-30 10:31:51,851 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8.
datanode_2          | 2023-06-30 10:31:51,854 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a: new RaftServerImpl for group-0D608DC0A753:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:31:51,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:31:51,856 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:31:51,856 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:31:51,857 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:31:51,858 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:31:51,858 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-30 10:31:13,292 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = b79e8b2a708f/172.23.0.13
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.3.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_3          | STARTUP_MSG:   java = 11.0.14.1
datanode_3          | ************************************************************/
datanode_3          | 2023-06-30 10:31:13,296 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-30 10:31:13,465 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-30 10:31:14,010 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-30 10:31:14,605 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-30 10:31:14,605 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-30 10:31:15,127 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b79e8b2a708f ip:172.23.0.13
datanode_3          | 2023-06-30 10:31:16,361 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_3          | 2023-06-30 10:31:17,086 [main] INFO reflections.Reflections: Reflections took 608 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_3          | 2023-06-30 10:31:17,497 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-30 10:31:18,148 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-30 10:31:18,184 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-30 10:31:18,185 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-30 10:31:18,187 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-30 10:31:18,288 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:31:18,351 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:31:18,366 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-30 10:31:18,367 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-30 10:31:18,367 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-30 10:31:18,379 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-30 10:31:18,494 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:31:18,494 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-30 10:31:24,483 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-30 10:31:24,857 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:31:25,027 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-30 10:31:25,301 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-30 10:31:25,312 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-30 10:31:25,318 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-30 10:31:25,318 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-30 10:31:25,318 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-30 10:31:25,318 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-30 10:31:25,319 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-30 10:31:25,329 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:31:25,329 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-30 10:31:25,330 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:31:25,370 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-30 10:31:25,388 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-30 10:31:25,399 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-30 10:31:26,795 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-30 10:31:26,813 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-30 10:31:26,813 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-30 10:31:26,814 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:31:26,814 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:31:26,856 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:31:26,961 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-30 10:31:27,652 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:31:27,768 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-30 10:31:27,912 [main] INFO util.log: Logging initialized @21080ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-30 10:31:28,642 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-30 10:31:28,699 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-30 10:31:28,749 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-30 10:31:28,751 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-30 10:31:28,765 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753: ConfigurationManager, init=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:31:51,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:31:51,864 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-30 10:31:51,864 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-30 10:31:51,864 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-30 10:31:51,864 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-30 10:31:51,864 [Command processor thread] INFO server.RaftServer: f224272d-19aa-40de-aa6a-e64f79edbd5a: addNew group-0D608DC0A753:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns      null f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null NEW
datanode_2          | 2023-06-30 10:31:51,865 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753 does not exist. Creating ...
datanode_2          | 2023-06-30 10:31:51,879 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753/in_use.lock acquired by nodename 7@1737fd297348
datanode_2          | 2023-06-30 10:31:51,882 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753 has been successfully formatted.
datanode_2          | 2023-06-30 10:31:51,883 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0D608DC0A753: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:31:51,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:31:51,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:31:51,890 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:31:51,890 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-30 10:31:51,890 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-30 10:31:51,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:51,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:31:51,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:31:51,895 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753
datanode_2          | 2023-06-30 10:31:51,895 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:31:51,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-30 10:31:51,899 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:31:51,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:31:51,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:31:51,912 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:31:51,913 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:31:49,938 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:31:49,938 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:31:49,939 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:49,955 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:31:49,955 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:31:49,956 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:31:49,956 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:31:49,957 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:31:49,958 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: start as a follower, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:49,958 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:31:49,958 [pool-22-thread-1] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState
datanode_1          | 2023-06-30 10:31:49,975 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-87F1FD387BB8,id=0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_1          | 2023-06-30 10:31:49,976 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:31:49,976 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:31:49,976 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:31:49,976 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:31:49,976 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:31:49,983 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:31:50,003 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8
datanode_1          | 2023-06-30 10:31:51,810 [grpc-default-executor-0] INFO server.RaftServer: 0f3d230a-845e-43da-9c18-0d5fd63464e3: addNew group-0D608DC0A753:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER] returns group-0D608DC0A753:java.util.concurrent.CompletableFuture@535dbc63[Not completed]
datanode_1          | 2023-06-30 10:31:51,817 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3: new RaftServerImpl for group-0D608DC0A753:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753: ConfigurationManager, init=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:31:51,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:31:51,819 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:31:51,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:31:51,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:31:51,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:31:51,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:31:51,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:31:51,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:31:51,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:31:51,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:31:51,822 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753 does not exist. Creating ...
datanode_1          | 2023-06-30 10:31:51,823 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753/in_use.lock acquired by nodename 7@93407cadbf5e
datanode_1          | 2023-06-30 10:31:51,826 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753 has been successfully formatted.
datanode_1          | 2023-06-30 10:31:51,828 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0D608DC0A753: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:31:51,832 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:31:51,832 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:31:51,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:31:51,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:31:51,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:31:51,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:51,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:31:51,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:31:51,833 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:31:51,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:31:51,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:31:51,836 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:31:51,848 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:31:51,848 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:31:51,859 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:31:51,863 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753: start as a follower, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:51,865 [pool-22-thread-1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:31:51,866 [pool-22-thread-1] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState
datanode_1          | 2023-06-30 10:31:51,895 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0D608DC0A753,id=0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_1          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:31:51,896 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8.
datanode_1          | 2023-06-30 10:31:51,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:31:51,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:31:51,898 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:31:51,925 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:31:51,926 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:31:54,221 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: receive requestVote(ELECTION, f224272d-19aa-40de-aa6a-e64f79edbd5a, group-87F1FD387BB8, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:31:54,223 [grpc-default-executor-0] INFO impl.VoteContext: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FOLLOWER: reject ELECTION from f224272d-19aa-40de-aa6a-e64f79edbd5a: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-30 10:31:54,223 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_1          | 2023-06-30 10:31:54,224 [grpc-default-executor-0] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: shutdown 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState
datanode_1          | 2023-06-30 10:31:54,227 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState was interrupted
datanode_1          | 2023-06-30 10:31:54,228 [grpc-default-executor-0] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState
datanode_1          | 2023-06-30 10:31:54,232 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8 replies to ELECTION vote request: f224272d-19aa-40de-aa6a-e64f79edbd5a<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:FAIL-t1. Peer's state: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8:t1, leader=null, voted=null, raftlog=Memoized:0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:54,248 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:31:54,256 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:31:54,945 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState] INFO impl.FollowerState: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5181258897ns, electionTimeout:5166ms
datanode_1          | 2023-06-30 10:31:54,958 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: shutdown 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState
datanode_1          | 2023-06-30 10:31:54,958 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:31:54,961 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:31:54,961 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-FollowerState] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1
datanode_1          | 2023-06-30 10:31:54,979 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO impl.LeaderElection: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:54,980 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO impl.LeaderElection: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-30 10:31:54,980 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: shutdown 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1
datanode_1          | 2023-06-30 10:31:54,981 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-30 10:31:54,981 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-13469D7DBF2F with new leaderId: 0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_1          | 2023-06-30 10:31:54,994 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F: change Leader from null to 0f3d230a-845e-43da-9c18-0d5fd63464e3 at term 1 for becomeLeader, leader elected after 5717ms
datanode_1          | 2023-06-30 10:31:55,008 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:31:55,013 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:31:55,018 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-30 10:31:55,022 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:31:55,028 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:31:55,029 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:31:55,033 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:31:55,041 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-30 10:31:55,064 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderStateImpl
datanode_1          | 2023-06-30 10:31:55,096 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:31:55,164 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-LeaderElection1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F: set configuration 0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:55,233 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-13469D7DBF2F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f/current/log_inprogress_0
datanode_1          | 2023-06-30 10:31:56,848 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753: receive requestVote(ELECTION, e4c9087c-d909-423b-95c1-36204f0a06b4, group-0D608DC0A753, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:31:56,849 [grpc-default-executor-0] INFO impl.VoteContext: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FOLLOWER: accept ELECTION from e4c9087c-d909-423b-95c1-36204f0a06b4: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:31:56,849 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_1          | 2023-06-30 10:31:56,849 [grpc-default-executor-0] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: shutdown 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState
datanode_1          | 2023-06-30 10:31:56,849 [grpc-default-executor-0] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState
datanode_1          | 2023-06-30 10:31:56,849 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState] INFO impl.FollowerState: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState was interrupted
datanode_1          | 2023-06-30 10:31:56,857 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753 replies to ELECTION vote request: e4c9087c-d909-423b-95c1-36204f0a06b4<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:OK-t1. Peer's state: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753:t1, leader=null, voted=e4c9087c-d909-423b-95c1-36204f0a06b4, raftlog=Memoized:0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:56,859 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:31:56,860 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:31:56,969 [0f3d230a-845e-43da-9c18-0d5fd63464e3-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0D608DC0A753 with new leaderId: e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_1          | 2023-06-30 10:31:56,969 [0f3d230a-845e-43da-9c18-0d5fd63464e3-server-thread1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753: change Leader from null to e4c9087c-d909-423b-95c1-36204f0a06b4 at term 1 for appendEntries, leader elected after 5147ms
datanode_1          | 2023-06-30 10:31:56,970 [0f3d230a-845e-43da-9c18-0d5fd63464e3-server-thread1] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753: set configuration 0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:31:56,971 [0f3d230a-845e-43da-9c18-0d5fd63464e3-server-thread1] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:31:56,972 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-0D608DC0A753-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753/current/log_inprogress_0
datanode_1          | 2023-06-30 10:31:59,303 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: receive requestVote(ELECTION, f224272d-19aa-40de-aa6a-e64f79edbd5a, group-87F1FD387BB8, 2, (t:0, i:0))
datanode_1          | 2023-06-30 10:31:59,303 [grpc-default-executor-0] INFO impl.VoteContext: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FOLLOWER: reject ELECTION from f224272d-19aa-40de-aa6a-e64f79edbd5a: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-30 10:31:59,304 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_1          | 2023-06-30 10:31:59,304 [grpc-default-executor-0] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: shutdown 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState
datanode_1          | 2023-06-30 10:31:59,304 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState was interrupted
datanode_1          | 2023-06-30 10:31:59,305 [grpc-default-executor-0] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState
datanode_1          | 2023-06-30 10:31:59,306 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:31:59,306 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:31:59,307 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8 replies to ELECTION vote request: f224272d-19aa-40de-aa6a-e64f79edbd5a<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:FAIL-t2. Peer's state: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8:t2, leader=null, voted=null, raftlog=Memoized:0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:32:04,442 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5137235185ns, electionTimeout:5135ms
datanode_1          | 2023-06-30 10:32:04,442 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: shutdown 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState
datanode_1          | 2023-06-30 10:32:04,442 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_1          | 2023-06-30 10:32:04,443 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-30 10:32:04,443 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2
datanode_2          | 2023-06-30 10:31:51,913 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753: start as a follower, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:51,914 [pool-22-thread-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:31:51,914 [pool-22-thread-1] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState
datanode_2          | 2023-06-30 10:31:51,914 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0D608DC0A753,id=f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_2          | 2023-06-30 10:31:51,914 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:31:51,914 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:31:51,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:31:51,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:31:51,915 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:51,915 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=866602bc-edae-468e-b08c-0d608dc0a753
datanode_2          | 2023-06-30 10:31:51,925 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:31:52,163 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753.
datanode_2          | 2023-06-30 10:31:53,818 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState] INFO impl.FollowerState: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5103473220ns, electionTimeout:5072ms
datanode_2          | 2023-06-30 10:31:53,819 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState
datanode_2          | 2023-06-30 10:31:53,819 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:31:53,822 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:31:53,822 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1
datanode_2          | 2023-06-30 10:31:53,832 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:53,832 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-30 10:31:53,833 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1
datanode_2          | 2023-06-30 10:31:53,833 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-30 10:31:53,833 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8FCF1F3E0F3B with new leaderId: f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_2          | 2023-06-30 10:31:53,835 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B: change Leader from null to f224272d-19aa-40de-aa6a-e64f79edbd5a at term 1 for becomeLeader, leader elected after 5587ms
datanode_2          | 2023-06-30 10:31:53,846 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:31:53,868 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:31:53,869 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-30 10:31:53,875 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:31:53,875 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:31:53,876 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:31:53,881 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:31:53,904 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-30 10:31:53,911 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderStateImpl
datanode_2          | 2023-06-30 10:31:53,942 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:31:54,009 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-LeaderElection1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B: set configuration 0: peers:[f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:54,075 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-8FCF1F3E0F3B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b/current/log_inprogress_0
datanode_3          | 2023-06-30 10:31:28,765 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-30 10:31:29,023 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-30 10:31:29,047 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_3          | 2023-06-30 10:31:29,251 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-30 10:31:29,251 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-30 10:31:29,272 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-30 10:31:29,379 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7af0affa{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-30 10:31:29,380 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5b251fb9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-30 10:31:31,098 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7f0f84d4{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-13256514688075445991/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-30 10:31:31,131 [main] INFO server.AbstractConnector: Started ServerConnector@1f10fec6{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-30 10:31:31,131 [main] INFO server.Server: Started @24298ms
datanode_3          | 2023-06-30 10:31:31,147 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-30 10:31:31,150 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-30 10:31:31,156 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:31:31,188 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-30 10:31:31,450 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@58cbaa9e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-30 10:31:31,648 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.5:9891
datanode_3          | 2023-06-30 10:31:31,780 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-30 10:31:34,606 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:31:35,477 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-06-30 10:31:35,606 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:31:36,607 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:31:37,479 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-06-30 10:31:37,608 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:31:54,158 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5216941829ns, electionTimeout:5170ms
datanode_2          | 2023-06-30 10:31:54,158 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState
datanode_2          | 2023-06-30 10:31:54,158 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:31:54,158 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:31:54,159 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2
datanode_2          | 2023-06-30 10:31:54,160 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:54,172 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:54,172 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:31:54,177 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_2          | 2023-06-30 10:31:54,177 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_2          | 2023-06-30 10:31:54,251 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-30 10:31:54,257 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection:   Response 0: f224272d-19aa-40de-aa6a-e64f79edbd5a<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:FAIL-t1
datanode_2          | 2023-06-30 10:31:54,259 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-30 10:31:54,262 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_2          | 2023-06-30 10:31:54,262 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2
datanode_2          | 2023-06-30 10:31:54,262 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection2] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState
datanode_2          | 2023-06-30 10:31:54,278 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:54,278 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:31:56,876 [grpc-default-executor-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753: receive requestVote(ELECTION, e4c9087c-d909-423b-95c1-36204f0a06b4, group-0D608DC0A753, 1, (t:0, i:0))
datanode_2          | 2023-06-30 10:31:56,878 [grpc-default-executor-1] INFO impl.VoteContext: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FOLLOWER: accept ELECTION from e4c9087c-d909-423b-95c1-36204f0a06b4: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-30 10:31:56,878 [grpc-default-executor-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_2          | 2023-06-30 10:31:56,878 [grpc-default-executor-1] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState
datanode_2          | 2023-06-30 10:31:56,878 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState] INFO impl.FollowerState: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState was interrupted
datanode_2          | 2023-06-30 10:31:56,883 [grpc-default-executor-1] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState
datanode_2          | 2023-06-30 10:31:56,888 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:56,888 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:31:56,899 [grpc-default-executor-1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753 replies to ELECTION vote request: e4c9087c-d909-423b-95c1-36204f0a06b4<-f224272d-19aa-40de-aa6a-e64f79edbd5a#0:OK-t1. Peer's state: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753:t1, leader=null, voted=e4c9087c-d909-423b-95c1-36204f0a06b4, raftlog=Memoized:f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:56,960 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0D608DC0A753 with new leaderId: e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_2          | 2023-06-30 10:31:56,960 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread2] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753: change Leader from null to e4c9087c-d909-423b-95c1-36204f0a06b4 at term 1 for appendEntries, leader elected after 5100ms
datanode_3          | 2023-06-30 10:31:38,608 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:31:38,678 [EndpointStateMachine task thread for recon/172.23.0.5:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From b79e8b2a708f/172.23.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.13:60426 remote=recon/172.23.0.5:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.13:60426 remote=recon/172.23.0.5:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-06-30 10:31:39,609 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:31:44,390 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8/DS-664670a8-6d9d-4f7a-a504-8c74276c257d/container.db to cache
datanode_3          | 2023-06-30 10:31:44,390 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8/DS-664670a8-6d9d-4f7a-a504-8c74276c257d/container.db for volume DS-664670a8-6d9d-4f7a-a504-8c74276c257d
datanode_3          | 2023-06-30 10:31:44,391 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-30 10:31:44,396 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-30 10:31:44,587 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_3          | 2023-06-30 10:31:44,640 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.RaftServer: e4c9087c-d909-423b-95c1-36204f0a06b4: start RPC server
datanode_3          | 2023-06-30 10:31:44,683 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: e4c9087c-d909-423b-95c1-36204f0a06b4: GrpcService started, listening on 9858
datanode_3          | 2023-06-30 10:31:44,684 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: e4c9087c-d909-423b-95c1-36204f0a06b4: GrpcService started, listening on 9856
datanode_3          | 2023-06-30 10:31:44,685 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO server.GrpcService: e4c9087c-d909-423b-95c1-36204f0a06b4: GrpcService started, listening on 9857
datanode_3          | 2023-06-30 10:31:44,701 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e4c9087c-d909-423b-95c1-36204f0a06b4 is started using port 9858 for RATIS
datanode_3          | 2023-06-30 10:31:44,701 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e4c9087c-d909-423b-95c1-36204f0a06b4 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-30 10:31:44,701 [EndpointStateMachine task thread for scm/172.23.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e4c9087c-d909-423b-95c1-36204f0a06b4 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-30 10:31:44,713 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e4c9087c-d909-423b-95c1-36204f0a06b4: Started
datanode_3          | 2023-06-30 10:31:48,648 [Command processor thread] INFO server.RaftServer: e4c9087c-d909-423b-95c1-36204f0a06b4: addNew group-9FA95D538DF4:[e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-9FA95D538DF4:java.util.concurrent.CompletableFuture@7b37551[Not completed]
datanode_3          | 2023-06-30 10:31:48,702 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4: new RaftServerImpl for group-9FA95D538DF4:[e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:31:48,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:31:56,970 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753: set configuration 0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:56,971 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread1] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:31:56,979 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-0D608DC0A753-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753/current/log_inprogress_0
datanode_2          | 2023-06-30 10:31:59,295 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5032988047ns, electionTimeout:5017ms
datanode_2          | 2023-06-30 10:31:59,296 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState
datanode_2          | 2023-06-30 10:31:59,296 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_2          | 2023-06-30 10:31:59,296 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:31:59,296 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3
datanode_2          | 2023-06-30 10:31:59,299 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:31:59,315 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:59,315 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:31:59,316 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-30 10:31:59,316 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO impl.LeaderElection:   Response 0: f224272d-19aa-40de-aa6a-e64f79edbd5a<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:FAIL-t2
datanode_2          | 2023-06-30 10:31:59,316 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-30 10:31:59,323 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_2          | 2023-06-30 10:31:59,323 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3
datanode_2          | 2023-06-30 10:31:59,323 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection3] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState
datanode_2          | 2023-06-30 10:31:59,323 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:31:59,336 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:32:04,496 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5173150598ns, electionTimeout:5159ms
datanode_2          | 2023-06-30 10:32:04,497 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState
datanode_2          | 2023-06-30 10:32:04,497 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2          | 2023-06-30 10:32:04,497 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-30 10:32:04,497 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4
datanode_2          | 2023-06-30 10:32:04,505 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:32:04,507 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:32:04,517 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:31:48,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:31:48,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:31:48,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:31:48,712 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:31:48,713 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:31:48,722 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4: ConfigurationManager, init=-1: peers:[e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:31:48,724 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:31:48,743 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:31:48,758 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:31:48,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:31:48,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:31:48,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:31:48,934 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:31:48,935 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-30 10:31:48,945 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-30 10:31:48,958 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-30 10:31:48,960 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-30 10:31:48,961 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e1d2953d-5b26-42c0-9006-9fa95d538df4 does not exist. Creating ...
datanode_3          | 2023-06-30 10:31:48,980 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e1d2953d-5b26-42c0-9006-9fa95d538df4/in_use.lock acquired by nodename 6@b79e8b2a708f
datanode_3          | 2023-06-30 10:31:49,014 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e1d2953d-5b26-42c0-9006-9fa95d538df4 has been successfully formatted.
datanode_3          | 2023-06-30 10:31:49,041 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9FA95D538DF4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:31:49,046 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:31:49,092 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:31:49,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:31:49,111 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-30 10:31:49,111 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-30 10:31:49,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:49,144 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:31:49,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:31:49,164 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e1d2953d-5b26-42c0-9006-9fa95d538df4
datanode_3          | 2023-06-30 10:31:49,164 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-30 10:31:49,165 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:31:49,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:49,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:31:49,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:31:49,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:31:49,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:31:49,187 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:31:49,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:49,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:31:49,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:31:49,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:31:49,251 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:31:49,252 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:31:49,275 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4: start as a follower, conf=-1: peers:[e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:49,275 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:31:49,277 [pool-22-thread-1] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState
datanode_3          | 2023-06-30 10:31:49,299 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:31:49,300 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:31:49,306 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9FA95D538DF4,id=e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_3          | 2023-06-30 10:31:49,308 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:31:49,308 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:31:49,308 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:31:49,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:31:49,368 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e1d2953d-5b26-42c0-9006-9fa95d538df4
datanode_3          | 2023-06-30 10:31:49,369 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e1d2953d-5b26-42c0-9006-9fa95d538df4.
datanode_3          | 2023-06-30 10:31:49,378 [Command processor thread] INFO server.RaftServer: e4c9087c-d909-423b-95c1-36204f0a06b4: addNew group-87F1FD387BB8:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-87F1FD387BB8:java.util.concurrent.CompletableFuture@3416aa6c[Not completed]
datanode_3          | 2023-06-30 10:31:49,389 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4: new RaftServerImpl for group-87F1FD387BB8:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:31:49,389 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:31:49,395 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:31:49,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:31:49,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:31:49,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:31:49,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:31:49,396 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: ConfigurationManager, init=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:31:49,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:31:49,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:31:49,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:31:49,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:31:49,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:31:49,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:31:49,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:31:49,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-30 10:31:49,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-30 10:31:49,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-30 10:31:49,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-30 10:31:49,434 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8 does not exist. Creating ...
datanode_3          | 2023-06-30 10:31:49,459 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8/in_use.lock acquired by nodename 6@b79e8b2a708f
datanode_3          | 2023-06-30 10:31:49,461 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8 has been successfully formatted.
datanode_3          | 2023-06-30 10:31:49,464 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-87F1FD387BB8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:31:49,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:31:49,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:31:49,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:31:49,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-30 10:31:49,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-30 10:31:49,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:49,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:31:49,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:31:49,473 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8
datanode_3          | 2023-06-30 10:31:49,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-30 10:31:49,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:32:04,547 [grpc-default-executor-0] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: receive requestVote(ELECTION, 0f3d230a-845e-43da-9c18-0d5fd63464e3, group-87F1FD387BB8, 3, (t:0, i:0))
datanode_2          | 2023-06-30 10:32:04,548 [grpc-default-executor-0] INFO impl.VoteContext: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-CANDIDATE: reject ELECTION from 0f3d230a-845e-43da-9c18-0d5fd63464e3: already has voted for f224272d-19aa-40de-aa6a-e64f79edbd5a at current term 3
datanode_2          | 2023-06-30 10:32:04,550 [grpc-default-executor-0] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8 replies to ELECTION vote request: 0f3d230a-845e-43da-9c18-0d5fd63464e3<-f224272d-19aa-40de-aa6a-e64f79edbd5a#0:FAIL-t3. Peer's state: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8:t3, leader=null, voted=f224272d-19aa-40de-aa6a-e64f79edbd5a, raftlog=Memoized:f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:32:04,621 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-06-30 10:32:04,621 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO impl.LeaderElection:   Response 0: f224272d-19aa-40de-aa6a-e64f79edbd5a<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:FAIL-t3
datanode_2          | 2023-06-30 10:32:04,621 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO impl.LeaderElection:   Response 1: f224272d-19aa-40de-aa6a-e64f79edbd5a<-e4c9087c-d909-423b-95c1-36204f0a06b4#0:FAIL-t3
datanode_2          | 2023-06-30 10:32:04,622 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO impl.LeaderElection: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-30 10:32:04,622 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_2          | 2023-06-30 10:32:04,622 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: shutdown f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4
datanode_2          | 2023-06-30 10:32:04,622 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-LeaderElection4] INFO impl.RoleInfo: f224272d-19aa-40de-aa6a-e64f79edbd5a: start f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState
datanode_2          | 2023-06-30 10:32:04,638 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:32:04,638 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:32:04,669 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-87F1FD387BB8 with new leaderId: 0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_2          | 2023-06-30 10:32:04,669 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread1] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: change Leader from null to 0f3d230a-845e-43da-9c18-0d5fd63464e3 at term 3 for appendEntries, leader elected after 15814ms
datanode_2          | 2023-06-30 10:32:04,711 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread2] INFO server.RaftServer$Division: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8: set configuration 0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:32:04,712 [f224272d-19aa-40de-aa6a-e64f79edbd5a-server-thread2] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:32:04,714 [f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f224272d-19aa-40de-aa6a-e64f79edbd5a@group-87F1FD387BB8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8/current/log_inprogress_0
datanode_1          | 2023-06-30 10:32:04,450 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:32:04,455 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_1          | 2023-06-30 10:32:04,462 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:32:04,464 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:32:04,467 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_1          | 2023-06-30 10:32:04,493 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:32:04,493 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection:   Response 0: 0f3d230a-845e-43da-9c18-0d5fd63464e3<-e4c9087c-d909-423b-95c1-36204f0a06b4#0:OK-t3
datanode_1          | 2023-06-30 10:32:04,494 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO impl.LeaderElection: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2 ELECTION round 0: result PASSED
datanode_1          | 2023-06-30 10:32:04,495 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: shutdown 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2
datanode_1          | 2023-06-30 10:32:04,495 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode_1          | 2023-06-30 10:32:04,495 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-87F1FD387BB8 with new leaderId: 0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_1          | 2023-06-30 10:32:04,499 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: change Leader from null to 0f3d230a-845e-43da-9c18-0d5fd63464e3 at term 3 for becomeLeader, leader elected after 14607ms
datanode_1          | 2023-06-30 10:32:04,499 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:32:04,500 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:32:04,500 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-30 10:32:04,532 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: receive requestVote(ELECTION, f224272d-19aa-40de-aa6a-e64f79edbd5a, group-87F1FD387BB8, 3, (t:0, i:0))
datanode_1          | 2023-06-30 10:32:04,542 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:32:04,542 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:32:04,542 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:32:04,542 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:32:04,543 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-30 10:32:04,575 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:32:04,575 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:32:04,575 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:32:04,581 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:32:04,583 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:32:04,583 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:32:04,584 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-30 10:32:04,584 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-30 10:32:04,585 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:32:04,585 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:32:04,585 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:32:04,586 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:32:04,586 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:32:04,586 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:32:04,586 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-30 10:32:04,586 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-30 10:32:04,588 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO impl.RoleInfo: 0f3d230a-845e-43da-9c18-0d5fd63464e3: start 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderStateImpl
datanode_1          | 2023-06-30 10:32:04,590 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:32:04,592 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8/current/log_inprogress_0
datanode_1          | 2023-06-30 10:32:04,601 [0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LeaderElection2] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8: set configuration 0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:32:04,619 [grpc-default-executor-0] INFO impl.VoteContext: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-LEADER: reject ELECTION from f224272d-19aa-40de-aa6a-e64f79edbd5a: already has voted for 0f3d230a-845e-43da-9c18-0d5fd63464e3 at current term 3
datanode_1          | 2023-06-30 10:32:04,619 [grpc-default-executor-0] INFO server.RaftServer$Division: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8 replies to ELECTION vote request: f224272d-19aa-40de-aa6a-e64f79edbd5a<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:FAIL-t3. Peer's state: 0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8:t3, leader=0f3d230a-845e-43da-9c18-0d5fd63464e3, voted=0f3d230a-845e-43da-9c18-0d5fd63464e3, raftlog=Memoized:0f3d230a-845e-43da-9c18-0d5fd63464e3@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:49,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:49,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:31:49,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:31:49,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:31:49,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:31:49,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:31:49,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:49,475 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:31:49,477 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:31:49,477 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:31:49,478 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:31:49,478 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:31:49,479 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: start as a follower, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:49,479 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:31:49,480 [pool-22-thread-1] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState
datanode_3          | 2023-06-30 10:31:49,480 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-87F1FD387BB8,id=e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_3          | 2023-06-30 10:31:49,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:31:49,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:31:49,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:31:49,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:31:49,488 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:31:49,490 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8
datanode_3          | 2023-06-30 10:31:49,535 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:31:51,576 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8.
datanode_3          | 2023-06-30 10:31:51,579 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4: new RaftServerImpl for group-0D608DC0A753:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:31:51,582 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:31:51,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-30 10:31:51,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:31:51,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:31:51,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:31:51,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:31:51,583 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753: ConfigurationManager, init=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:31:51,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:31:51,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:31:51,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:31:51,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:31:51,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:31:51,586 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:31:51,587 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:31:51,587 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-30 10:31:51,587 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-30 10:31:51,588 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-30 10:31:51,588 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-30 10:31:51,588 [Command processor thread] INFO server.RaftServer: e4c9087c-d909-423b-95c1-36204f0a06b4: addNew group-0D608DC0A753:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns      null e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null NEW
datanode_3          | 2023-06-30 10:31:51,588 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753 does not exist. Creating ...
datanode_3          | 2023-06-30 10:31:51,603 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753/in_use.lock acquired by nodename 6@b79e8b2a708f
datanode_3          | 2023-06-30 10:31:51,608 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753 has been successfully formatted.
datanode_3          | 2023-06-30 10:31:51,609 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0D608DC0A753: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:31:51,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:31:51,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:31:51,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:31:51,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-30 10:31:51,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-30 10:31:51,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:51,613 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:31:51,613 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:31:51,613 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753
datanode_3          | 2023-06-30 10:31:51,613 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-30 10:31:51,615 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:31:51,618 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:51,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:31:51,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:31:51,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:31:51,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:31:51,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:31:51,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:31:51,626 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:31:51,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:31:51,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:31:51,629 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:31:51,629 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:31:51,640 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753: start as a follower, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:51,640 [pool-22-thread-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:31:51,641 [pool-22-thread-1] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState
datanode_3          | 2023-06-30 10:31:51,644 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0D608DC0A753,id=e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_3          | 2023-06-30 10:31:51,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:31:51,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:31:51,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:31:51,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:31:51,645 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:31:51,646 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=866602bc-edae-468e-b08c-0d608dc0a753
datanode_3          | 2023-06-30 10:31:51,666 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:31:52,107 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753.
datanode_3          | 2023-06-30 10:31:54,226 [grpc-default-executor-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: receive requestVote(ELECTION, f224272d-19aa-40de-aa6a-e64f79edbd5a, group-87F1FD387BB8, 1, (t:0, i:0))
datanode_3          | 2023-06-30 10:31:54,230 [grpc-default-executor-1] INFO impl.VoteContext: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FOLLOWER: accept ELECTION from f224272d-19aa-40de-aa6a-e64f79edbd5a: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-30 10:31:54,235 [grpc-default-executor-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_3          | 2023-06-30 10:31:54,238 [grpc-default-executor-1] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: shutdown e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState
datanode_3          | 2023-06-30 10:31:54,240 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState was interrupted
datanode_3          | 2023-06-30 10:31:54,243 [grpc-default-executor-1] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState
datanode_3          | 2023-06-30 10:31:54,245 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:31:54,248 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:31:54,265 [grpc-default-executor-1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8 replies to ELECTION vote request: f224272d-19aa-40de-aa6a-e64f79edbd5a<-e4c9087c-d909-423b-95c1-36204f0a06b4#0:OK-t1. Peer's state: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8:t1, leader=null, voted=f224272d-19aa-40de-aa6a-e64f79edbd5a, raftlog=Memoized:e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:54,484 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState] INFO impl.FollowerState: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5207623690ns, electionTimeout:5183ms
datanode_3          | 2023-06-30 10:31:54,485 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: shutdown e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState
datanode_3          | 2023-06-30 10:31:54,485 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:31:54,487 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:31:54,487 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-FollowerState] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1
datanode_3          | 2023-06-30 10:31:54,495 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO impl.LeaderElection: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:54,495 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO impl.LeaderElection: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-30 10:31:54,496 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: shutdown e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1
datanode_3          | 2023-06-30 10:31:54,496 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:31:54,496 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9FA95D538DF4 with new leaderId: e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_3          | 2023-06-30 10:31:54,506 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4: change Leader from null to e4c9087c-d909-423b-95c1-36204f0a06b4 at term 1 for becomeLeader, leader elected after 5723ms
datanode_3          | 2023-06-30 10:31:54,514 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:31:54,523 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:31:54,523 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-30 10:31:54,528 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:31:54,530 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:31:54,531 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:31:54,541 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:31:54,550 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-30 10:31:54,556 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderStateImpl
datanode_3          | 2023-06-30 10:31:54,588 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:31:54,669 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-LeaderElection1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4: set configuration 0: peers:[e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:54,849 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-9FA95D538DF4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e1d2953d-5b26-42c0-9006-9fa95d538df4/current/log_inprogress_0
datanode_3          | 2023-06-30 10:31:56,821 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState] INFO impl.FollowerState: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5180167788ns, electionTimeout:5154ms
datanode_3          | 2023-06-30 10:31:56,822 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: shutdown e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState
datanode_3          | 2023-06-30 10:31:56,822 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:31:56,822 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-30 10:31:56,822 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-FollowerState] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2
datanode_3          | 2023-06-30 10:31:56,829 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO impl.LeaderElection: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:56,833 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:31:56,833 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:31:56,833 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_3          | 2023-06-30 10:31:56,835 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_3          | 2023-06-30 10:31:56,861 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO impl.LeaderElection: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-30 10:31:56,861 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO impl.LeaderElection:   Response 0: e4c9087c-d909-423b-95c1-36204f0a06b4<-0f3d230a-845e-43da-9c18-0d5fd63464e3#0:OK-t1
datanode_3          | 2023-06-30 10:31:56,861 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO impl.LeaderElection: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-06-30 10:31:56,862 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: shutdown e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2
datanode_3          | 2023-06-30 10:31:56,862 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:31:56,862 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0D608DC0A753 with new leaderId: e4c9087c-d909-423b-95c1-36204f0a06b4
datanode_3          | 2023-06-30 10:31:56,862 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753: change Leader from null to e4c9087c-d909-423b-95c1-36204f0a06b4 at term 1 for becomeLeader, leader elected after 5277ms
datanode_3          | 2023-06-30 10:31:56,862 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:31:56,862 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:31:56,862 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-30 10:31:56,863 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:31:56,863 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:31:56,863 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:31:56,864 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:31:56,864 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-30 10:31:56,893 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:31:56,894 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:31:56,894 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:31:56,896 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-30 10:31:56,897 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:31:56,897 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:31:56,897 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-30 10:31:56,897 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-30 10:31:56,901 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-30 10:31:56,901 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:31:56,901 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-30 10:31:56,902 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-30 10:31:56,902 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:31:56,903 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:31:56,903 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-30 10:31:56,904 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-30 10:31:56,905 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderStateImpl
datanode_3          | 2023-06-30 10:31:56,906 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:31:56,907 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/866602bc-edae-468e-b08c-0d608dc0a753/current/log_inprogress_0
datanode_3          | 2023-06-30 10:31:56,910 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753-LeaderElection2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-0D608DC0A753: set configuration 0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:1|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:31:59,326 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: receive requestVote(ELECTION, f224272d-19aa-40de-aa6a-e64f79edbd5a, group-87F1FD387BB8, 2, (t:0, i:0))
datanode_3          | 2023-06-30 10:31:59,326 [grpc-default-executor-2] INFO impl.VoteContext: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FOLLOWER: accept ELECTION from f224272d-19aa-40de-aa6a-e64f79edbd5a: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-30 10:31:59,326 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:f224272d-19aa-40de-aa6a-e64f79edbd5a
datanode_3          | 2023-06-30 10:31:59,326 [grpc-default-executor-2] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: shutdown e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState
datanode_3          | 2023-06-30 10:31:59,326 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState was interrupted
datanode_3          | 2023-06-30 10:31:59,327 [grpc-default-executor-2] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState
datanode_3          | 2023-06-30 10:31:59,330 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:31:59,330 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:31:59,335 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8 replies to ELECTION vote request: f224272d-19aa-40de-aa6a-e64f79edbd5a<-e4c9087c-d909-423b-95c1-36204f0a06b4#0:OK-t2. Peer's state: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8:t2, leader=null, voted=f224272d-19aa-40de-aa6a-e64f79edbd5a, raftlog=Memoized:e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:32:04,475 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: receive requestVote(ELECTION, 0f3d230a-845e-43da-9c18-0d5fd63464e3, group-87F1FD387BB8, 3, (t:0, i:0))
datanode_3          | 2023-06-30 10:32:04,475 [grpc-default-executor-2] INFO impl.VoteContext: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FOLLOWER: accept ELECTION from 0f3d230a-845e-43da-9c18-0d5fd63464e3: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-30 10:32:04,475 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_3          | 2023-06-30 10:32:04,476 [grpc-default-executor-2] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: shutdown e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState
datanode_3          | 2023-06-30 10:32:04,476 [grpc-default-executor-2] INFO impl.RoleInfo: e4c9087c-d909-423b-95c1-36204f0a06b4: start e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState
datanode_3          | 2023-06-30 10:32:04,476 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO impl.FollowerState: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState was interrupted
datanode_3          | 2023-06-30 10:32:04,485 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:32:04,486 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8 replies to ELECTION vote request: 0f3d230a-845e-43da-9c18-0d5fd63464e3<-e4c9087c-d909-423b-95c1-36204f0a06b4#0:OK-t3. Peer's state: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8:t3, leader=null, voted=0f3d230a-845e-43da-9c18-0d5fd63464e3, raftlog=Memoized:e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:32:04,489 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:32:04,519 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: receive requestVote(ELECTION, f224272d-19aa-40de-aa6a-e64f79edbd5a, group-87F1FD387BB8, 3, (t:0, i:0))
datanode_3          | 2023-06-30 10:32:04,520 [grpc-default-executor-2] INFO impl.VoteContext: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-FOLLOWER: reject ELECTION from f224272d-19aa-40de-aa6a-e64f79edbd5a: already has voted for 0f3d230a-845e-43da-9c18-0d5fd63464e3 at current term 3
datanode_3          | 2023-06-30 10:32:04,520 [grpc-default-executor-2] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8 replies to ELECTION vote request: f224272d-19aa-40de-aa6a-e64f79edbd5a<-e4c9087c-d909-423b-95c1-36204f0a06b4#0:FAIL-t3. Peer's state: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8:t3, leader=null, voted=0f3d230a-845e-43da-9c18-0d5fd63464e3, raftlog=Memoized:e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:32:04,680 [e4c9087c-d909-423b-95c1-36204f0a06b4-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-87F1FD387BB8 with new leaderId: 0f3d230a-845e-43da-9c18-0d5fd63464e3
datanode_3          | 2023-06-30 10:32:04,681 [e4c9087c-d909-423b-95c1-36204f0a06b4-server-thread1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: change Leader from null to 0f3d230a-845e-43da-9c18-0d5fd63464e3 at term 3 for appendEntries, leader elected after 15283ms
datanode_3          | 2023-06-30 10:32:04,711 [e4c9087c-d909-423b-95c1-36204f0a06b4-server-thread1] INFO server.RaftServer$Division: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8: set configuration 0: peers:[0f3d230a-845e-43da-9c18-0d5fd63464e3|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:1|startupRole:FOLLOWER, e4c9087c-d909-423b-95c1-36204f0a06b4|rpc:172.23.0.13:9856|admin:172.23.0.13:9857|client:172.23.0.13:9858|dataStream:|priority:0|startupRole:FOLLOWER, f224272d-19aa-40de-aa6a-e64f79edbd5a|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:32:04,712 [e4c9087c-d909-423b-95c1-36204f0a06b4-server-thread1] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:32:04,716 [e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e4c9087c-d909-423b-95c1-36204f0a06b4@group-87F1FD387BB8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d3b26bd8-6443-4828-b6a0-87f1fd387bb8/current/log_inprogress_0
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:31:12,333 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 4127e3674380/172.23.0.8
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.3.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-06-30 10:31:12,381 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:31:16,648 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-30 10:31:18,673 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:31:19,024 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.8:9862
om_1                | 2023-06-30 10:31:19,035 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:31:19,036 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:31:19,144 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:31:19,960 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863]
om_1                | 2023-06-30 10:31:23,174 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:25,176 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:27,177 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:29,178 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:31,179 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:33,181 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:35,182 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:37,183 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4127e3674380/172.23.0.8 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:39,807 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cdc59c73-ccc5-488e-962e-cc57c14602c5 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:41,816 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cdc59c73-ccc5-488e-962e-cc57c14602c5 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-30 10:31:43,819 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cdc59c73-ccc5-488e-962e-cc57c14602c5 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8;layoutVersion=3
om_1                | 2023-06-30 10:31:45,902 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 4127e3674380/172.23.0.8
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:31:47,447 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 4127e3674380/172.23.0.8
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.3.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-06-30 10:31:47,458 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:31:49,850 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-30 10:31:51,057 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:31:51,216 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.8:9862
om_1                | 2023-06-30 10:31:51,216 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:31:51,217 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:31:51,263 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:31:51,338 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om_1                | 2023-06-30 10:31:52,282 [main] INFO reflections.Reflections: Reflections took 788 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om_1                | 2023-06-30 10:31:52,320 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:31:52,569 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863]
om_1                | 2023-06-30 10:31:52,627 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9863]
om_1                | 2023-06-30 10:31:53,500 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:31:53,768 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-30 10:31:53,768 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-30 10:31:54,150 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-30 10:31:54,285 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-30 10:31:54,324 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:31:54,324 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-30 10:31:54,345 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-30 10:31:54,353 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:31:54,412 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-30 10:31:54,432 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-30 10:31:54,516 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-30 10:31:54,834 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-30 10:31:54,838 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-30 10:31:54,838 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-30 10:31:54,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-30 10:31:54,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-30 10:31:54,842 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:31:54,842 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-30 10:31:54,844 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:31:54,845 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-30 10:31:54,848 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-30 10:31:54,864 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-30 10:31:54,873 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-30 10:31:54,874 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-30 10:31:55,227 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-30 10:31:55,230 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-30 10:31:55,230 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-30 10:31:55,231 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:31:55,231 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:31:55,244 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:31:55,254 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@245253d8[Not completed]
om_1                | 2023-06-30 10:31:55,255 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-30 10:31:55,283 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-30 10:31:55,295 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-30 10:31:55,311 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-30 10:31:55,311 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-30 10:31:55,311 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-30 10:31:55,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:31:55,313 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:31:55,314 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-30 10:31:55,324 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-30 10:31:55,325 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:31:55,333 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-30 10:31:55,334 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-30 10:31:55,361 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-30 10:31:55,388 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-30 10:31:55,390 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-30 10:31:55,518 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-30 10:31:55,519 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-30 10:31:55,525 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-30 10:31:55,526 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-30 10:31:55,526 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-30 10:31:55,862 [main] INFO reflections.Reflections: Reflections took 539 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om_1                | 2023-06-30 10:31:55,945 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-30 10:31:55,953 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-30 10:31:56,034 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-30 10:31:56,048 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-30 10:31:56,048 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-30 10:31:56,078 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.23.0.8:9862
om_1                | 2023-06-30 10:31:56,078 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-30 10:31:56,079 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-30 10:31:56,085 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@4127e3674380
om_1                | 2023-06-30 10:31:56,098 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-30 10:31:56,101 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-30 10:31:56,113 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-30 10:31:56,113 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:31:56,114 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-30 10:31:56,115 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-30 10:31:56,117 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:31:56,125 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-30 10:31:56,125 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-30 10:31:56,130 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-30 10:31:56,131 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:31:56,131 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-30 10:31:56,132 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:31:56,132 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-30 10:31:56,133 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-30 10:31:56,134 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-30 10:31:56,135 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-30 10:31:56,136 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-30 10:31:56,151 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-30 10:31:56,152 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-30 10:31:56,152 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-30 10:31:56,153 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-30 10:31:56,166 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:31:56,166 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:31:56,170 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:31:56,171 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-30 10:31:56,172 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:31:56,176 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-30 10:31:56,178 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-30 10:31:56,178 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-30 10:31:56,181 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-30 10:31:56,179 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-30 10:31:56,181 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-30 10:31:56,183 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-30 10:31:56,185 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-30 10:31:56,215 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-30 10:31:56,217 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-30 10:31:56,218 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-30 10:31:56,243 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-30 10:31:56,243 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-30 10:31:56,263 [Listener at om/9862] INFO util.log: Logging initialized @10111ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-30 10:31:56,330 [Listener at om/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-30 10:31:56,334 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-30 10:31:56,339 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-30 10:31:56,340 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-30 10:31:56,340 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-30 10:31:56,340 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-30 10:31:56,366 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-30 10:31:56,367 [Listener at om/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om_1                | 2023-06-30 10:31:56,393 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-30 10:31:56,393 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-30 10:31:56,395 [Listener at om/9862] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-06-30 10:31:56,424 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@17fbfb02{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-30 10:31:56,424 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e9bf744{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-30 10:31:56,683 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1c6e3ff9{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-15388285857559660613/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-30 10:31:56,689 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@1132baa3{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-30 10:31:56,690 [Listener at om/9862] INFO server.Server: Started @10539ms
om_1                | 2023-06-30 10:31:56,695 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-30 10:31:56,695 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-30 10:31:56,697 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-30 10:31:56,697 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-30 10:31:56,713 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-30 10:31:56,715 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-30 10:31:56,718 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@67e77f52] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-30 10:32:01,280 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5108735575ns, electionTimeout:5096ms
om_1                | 2023-06-30 10:32:01,281 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:32:01,282 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-30 10:32:01,284 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-30 10:32:01,284 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:32:01,294 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:32:01,294 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-30 10:32:01,295 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:32:01,295 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-30 10:32:01,295 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5934ms
om_1                | 2023-06-30 10:32:01,300 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-30 10:32:01,306 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:32:01,307 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:32:01,311 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-30 10:31:11,749 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 06135e846818/172.23.0.5
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.3.0
om_1                | 2023-06-30 10:32:01,315 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-30 10:32:01,316 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-30 10:32:01,349 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:32:01,350 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-30 10:32:01,356 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-30 10:32:01,376 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-30 10:32:01,438 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:32:01,491 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-30 10:32:01,580 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-30 10:32:03,460 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-30 10:32:03,497 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-06-30 10:32:20,810 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-30 10:32:20,810 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-30 10:32:20,810 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-30 10:32:36,989 [qtp2124688514-47] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-30 10:32:37,022 [qtp2124688514-47] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688121156989 in 32 milliseconds
om_1                | 2023-06-30 10:32:37,095 [qtp2124688514-47] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 72 milliseconds
om_1                | 2023-06-30 10:32:37,108 [qtp2124688514-47] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688121156989
om_1                | 2023-06-30 10:32:46,095 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1             | STARTUP_MSG:   java = 11.0.14.1
recon_1             | ************************************************************/
recon_1             | 2023-06-30 10:31:11,798 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-30 10:31:14,696 [main] INFO reflections.Reflections: Reflections took 364 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1             | 2023-06-30 10:31:17,892 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-30 10:31:19,286 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:31:24,953 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:31:26,433 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-30 10:31:26,439 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.006 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-30 10:31:26,464 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:31:26,616 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:31:26,617 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-30 10:31:28,875 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1             | 2023-06-30 10:31:30,664 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-30 10:31:30,713 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-30 10:31:30,754 [main] INFO util.log: Logging initialized @24507ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-30 10:31:31,029 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-30 10:31:31,050 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-30 10:31:31,066 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-30 10:31:31,067 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-30 10:31:31,072 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-30 10:31:31,072 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-30 10:31:31,610 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-30 10:31:31,963 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-30 10:31:31,988 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-30 10:31:32,006 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-30 10:31:32,026 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-30 10:31:32,026 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-30 10:31:32,420 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:31:32,519 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:31:32,563 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1             | 2023-06-30 10:31:32,565 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-30 10:31:32,622 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:31:32,778 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1             | 2023-06-30 10:31:32,876 [main] INFO reflections.Reflections: Reflections took 86 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1             | 2023-06-30 10:31:32,936 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-30 10:31:32,952 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-30 10:31:32,956 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-30 10:31:32,960 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-30 10:31:32,989 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-30 10:31:33,002 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-30 10:31:33,032 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-30 10:31:33,075 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-30 10:31:33,189 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-30 10:31:33,191 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-30 10:31:33,282 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-30 10:31:33,314 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-30 10:31:33,319 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-30 10:31:33,822 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-30 10:31:33,832 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1             | 2023-06-30 10:31:33,917 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-30 10:31:33,920 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-30 10:31:33,922 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-30 10:31:33,962 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1b37fbec{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-30 10:31:33,963 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@357f6391{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-30 10:31:36,740 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@39acf187{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-9257144263645169518/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1             | 2023-06-30 10:31:36,751 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1a1f79ce{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-30 10:31:36,751 [Listener at 0.0.0.0/9891] INFO server.Server: Started @30503ms
recon_1             | 2023-06-30 10:31:36,756 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-30 10:31:36,757 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-30 10:31:36,760 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-30 10:31:36,760 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-30 10:31:36,776 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-30 10:31:36,782 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-30 10:31:36,782 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-30 10:31:36,783 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-30 10:31:11,082 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-30 10:31:11,087 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-30 10:31:11,203 [main] INFO util.log: Logging initialized @6415ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-30 10:31:11,963 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-06-30 10:31:12,116 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-30 10:31:12,145 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-30 10:31:12,169 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-30 10:31:12,171 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-30 10:31:12,171 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-30 10:31:12,509 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 346bb3cb4df6/172.23.0.3
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.3.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1               | STARTUP_MSG:   java = 11.0.14.1
s3g_1               | ************************************************************/
s3g_1               | 2023-06-30 10:31:12,514 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-30 10:31:12,684 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-30 10:31:13,097 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-06-30 10:31:13,705 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-06-30 10:31:13,712 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-30 10:31:13,886 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-30 10:31:13,911 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1               | 2023-06-30 10:31:14,126 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-30 10:31:14,152 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-30 10:31:14,154 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-06-30 10:31:14,345 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@282cb7c7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-30 10:31:14,361 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@176b75f7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 30, 2023 10:31:30 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-30 10:31:30,515 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@20b829d5{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-892946235456211622/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-30 10:31:30,558 [main] INFO server.AbstractConnector: Started ServerConnector@38b27cdc{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-30 10:31:30,558 [main] INFO server.Server: Started @25771ms
s3g_1               | 2023-06-30 10:31:30,560 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-06-30 10:31:30,560 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-06-30 10:31:30,568 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 2023-06-30 10:31:36,783 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-30 10:31:36,784 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:31:39,848 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cdc59c73-ccc5-488e-962e-cc57c14602c5 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-06-30 10:31:41,851 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cdc59c73-ccc5-488e-962e-cc57c14602c5 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-06-30 10:31:43,855 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cdc59c73-ccc5-488e-962e-cc57c14602c5 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.9:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-06-30 10:31:46,007 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1             | 2023-06-30 10:31:46,008 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:31:46,008 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 from SCM.
recon_1             | 2023-06-30 10:31:46,025 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d3b26bd8-6443-4828-b6a0-87f1fd387bb8, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.923Z[UTC]].
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:31:16,298 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 0736b879488e/172.23.0.9
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.3.0
recon_1             | 2023-06-30 10:31:46,032 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=e1d2953d-5b26-42c0-9006-9fa95d538df4 from SCM.
recon_1             | 2023-06-30 10:31:46,032 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e1d2953d-5b26-42c0-9006-9fa95d538df4, Nodes: e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.508Z[UTC]].
recon_1             | 2023-06-30 10:31:46,033 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f from SCM.
recon_1             | 2023-06-30 10:31:46,033 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.904Z[UTC]].
recon_1             | 2023-06-30 10:31:46,034 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 from SCM.
recon_1             | 2023-06-30 10:31:46,034 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 866602bc-edae-468e-b08c-0d608dc0a753, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.937Z[UTC]].
recon_1             | 2023-06-30 10:31:46,035 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b from SCM.
recon_1             | 2023-06-30 10:31:46,035 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b, Nodes: f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.084Z[UTC]].
recon_1             | 2023-06-30 10:31:46,042 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-30 10:31:46,042 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:31:46,049 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-30 10:31:46,049 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-30 10:31:46,114 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-30 10:31:46,117 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-30 10:31:46,137 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-30 10:31:46,138 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-30 10:31:46,165 [IPC Server handler 94 on default port 9891] INFO ipc.Server: IPC Server handler 94 on default port 9891: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.11:49442
recon_1             | 2023-06-30 10:31:46,183 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-06-30 10:31:46,190 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 44 milliseconds.
recon_1             | 2023-06-30 10:31:46,361 [IPC Server handler 96 on default port 9891] WARN ipc.Server: IPC Server handler 96 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.10:39876: output error
recon_1             | 2023-06-30 10:31:46,363 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.10:39874: output error
recon_1             | 2023-06-30 10:31:46,367 [IPC Server handler 96 on default port 9891] INFO ipc.Server: IPC Server handler 96 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-30 10:31:46,367 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-30 10:31:46,367 [IPC Server handler 92 on default port 9891] WARN ipc.Server: IPC Server handler 92 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.13:60440: output error
recon_1             | 2023-06-30 10:31:46,368 [IPC Server handler 92 on default port 9891] INFO ipc.Server: IPC Server handler 92 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-30 10:31:46,375 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.13:60426: output error
recon_1             | 2023-06-30 10:31:46,375 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-30 10:31:46,376 [IPC Server handler 93 on default port 9891] WARN ipc.Server: IPC Server handler 93 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.11:49444: output error
recon_1             | 2023-06-30 10:31:46,378 [IPC Server handler 93 on default port 9891] INFO ipc.Server: IPC Server handler 93 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-30 10:31:47,072 [IPC Server handler 91 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f224272d-19aa-40de-aa6a-e64f79edbd5a
recon_1             | 2023-06-30 10:31:47,091 [IPC Server handler 91 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:47,135 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f224272d-19aa-40de-aa6a-e64f79edbd5a to Node DB.
recon_1             | 2023-06-30 10:31:47,485 [IPC Server handler 91 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e4c9087c-d909-423b-95c1-36204f0a06b4
recon_1             | 2023-06-30 10:31:47,485 [IPC Server handler 91 on default port 9891] INFO node.SCMNodeManager: Registered Data node : e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:47,486 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node e4c9087c-d909-423b-95c1-36204f0a06b4 to Node DB.
recon_1             | 2023-06-30 10:31:47,871 [IPC Server handler 3 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0f3d230a-845e-43da-9c18-0d5fd63464e3
recon_1             | 2023-06-30 10:31:47,879 [IPC Server handler 3 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:47,883 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 0f3d230a-845e-43da-9c18-0d5fd63464e3 to Node DB.
recon_1             | 2023-06-30 10:31:48,506 [IPC Server handler 91 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-30 10:31:48,509 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b reported by f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:48,510 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b, Nodes: f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f224272d-19aa-40de-aa6a-e64f79edbd5a, CreationTimestamp2023-06-30T10:31:45.084Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:31:48,901 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-30 10:31:48,902 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:49,118 [IPC Server handler 77 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-30 10:31:49,118 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=e1d2953d-5b26-42c0-9006-9fa95d538df4 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:49,118 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e1d2953d-5b26-42c0-9006-9fa95d538df4, Nodes: e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e4c9087c-d909-423b-95c1-36204f0a06b4, CreationTimestamp2023-06-30T10:31:45.508Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:31:49,493 [IPC Server handler 91 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-30 10:31:49,494 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:49,608 [IPC Server handler 3 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-30 10:31:49,608 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f reported by 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:49,609 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0f3d230a-845e-43da-9c18-0d5fd63464e3, CreationTimestamp2023-06-30T10:31:45.904Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:31:49,928 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-30 10:31:49,929 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:51,613 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:51,613 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:31:16,428 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:31:16,860 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:31:17,155 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:31:17,252 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:31:18,963 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-30 10:31:19,830 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:31:19,875 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:31:19,876 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:31:19,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:31:19,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-30 10:31:19,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-30 10:31:19,916 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-30 10:31:19,945 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:31:19,965 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-30 10:31:19,992 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:31:20,084 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-30 10:31:20,147 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-30 10:31:20,148 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-30 10:31:22,508 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-30 10:31:22,521 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-30 10:31:22,541 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-30 10:31:22,550 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:31:22,552 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:31:22,555 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:31:22,649 [main] INFO server.RaftServer: cdc59c73-ccc5-488e-962e-cc57c14602c5: addNew group-C536BDF907E8:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|priority:0|startupRole:FOLLOWER] returns group-C536BDF907E8:java.util.concurrent.CompletableFuture@43b4fe19[Not completed]
scm_1               | 2023-06-30 10:31:22,884 [pool-2-thread-1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5: new RaftServerImpl for group-C536BDF907E8:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-30 10:31:22,900 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-30 10:31:22,916 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-30 10:31:22,919 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-30 10:31:22,920 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:31:22,920 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:31:22,925 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-30 10:31:22,969 [pool-2-thread-1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: ConfigurationManager, init=-1: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-30 10:31:22,976 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:31:23,081 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-30 10:31:23,082 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-30 10:31:23,257 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-30 10:31:23,278 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-30 10:31:23,312 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-30 10:31:23,580 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-30 10:31:24,972 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-30 10:31:24,972 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-30 10:31:24,982 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-30 10:31:25,011 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-30 10:31:25,016 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-30 10:31:25,037 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8 does not exist. Creating ...
scm_1               | 2023-06-30 10:31:25,121 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/in_use.lock acquired by nodename 13@0736b879488e
scm_1               | 2023-06-30 10:31:25,200 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8 has been successfully formatted.
scm_1               | 2023-06-30 10:31:25,239 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-30 10:31:25,344 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-30 10:31:25,354 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:31:25,383 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-30 10:31:25,414 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-30 10:31:25,419 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:31:25,497 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-30 10:31:25,504 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-30 10:31:25,524 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8
scm_1               | 2023-06-30 10:31:25,527 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:31:25,529 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:31:25,530 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:31:25,533 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-30 10:31:25,540 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-30 10:31:25,548 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-30 10:31:25,554 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-30 10:31:25,640 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-30 10:31:25,735 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-30 10:31:25,740 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-30 10:31:25,752 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-30 10:31:25,760 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-30 10:31:25,976 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:31:25,991 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:31:26,016 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: start as a follower, conf=-1: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:26,026 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-06-30 10:31:26,031 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: start cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState
scm_1               | 2023-06-30 10:31:26,060 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-30 10:31:26,063 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-30 10:31:26,091 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C536BDF907E8,id=cdc59c73-ccc5-488e-962e-cc57c14602c5
scm_1               | 2023-06-30 10:31:26,093 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-30 10:31:26,105 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-30 10:31:26,123 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-30 10:31:26,129 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-30 10:31:26,180 [main] INFO server.RaftServer: cdc59c73-ccc5-488e-962e-cc57c14602c5: start RPC server
scm_1               | 2023-06-30 10:31:26,692 [main] INFO server.GrpcService: cdc59c73-ccc5-488e-962e-cc57c14602c5: GrpcService started, listening on 9894
scm_1               | 2023-06-30 10:31:26,702 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-cdc59c73-ccc5-488e-962e-cc57c14602c5: Started
scm_1               | 2023-06-30 10:31:31,202 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO impl.FollowerState: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5171704899ns, electionTimeout:5138ms
scm_1               | 2023-06-30 10:31:31,203 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: shutdown cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState
recon_1             | 2023-06-30 10:31:51,858 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:51,858 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 reported by 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:51,890 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:51,890 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 reported by f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:53,836 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:53,836 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 reported by f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:54,499 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:54,500 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:54,987 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:54,987 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 reported by 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:56,865 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:56,865 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 reported by e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:31:56,865 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 866602bc-edae-468e-b08c-0d608dc0a753, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e4c9087c-d909-423b-95c1-36204f0a06b4, CreationTimestamp2023-06-30T10:31:45.937Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:32:04,502 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 reported by 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:32:04,502 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d3b26bd8-6443-4828-b6a0-87f1fd387bb8, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0f3d230a-845e-43da-9c18-0d5fd63464e3, CreationTimestamp2023-06-30T10:31:45.923Z[UTC]] moved to OPEN state
recon_1             | 2023-06-30 10:32:05,437 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-30 10:32:05,484 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-30 10:32:36,784 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-30 10:32:36,784 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-30 10:32:37,163 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688121156784
recon_1             | 2023-06-30 10:32:37,170 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-30 10:32:37,171 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-30 10:32:37,235 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688121156784.
recon_1             | 2023-06-30 10:32:37,256 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-30 10:32:37,264 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-30 10:32:37,280 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-30 10:32:37,578 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-30 10:32:37,578 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:32:37,579 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-30 10:32:37,583 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-30 10:32:37,619 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:32:37,619 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.04 seconds to process 4 keys.
recon_1             | 2023-06-30 10:32:37,646 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-30 10:32:37,676 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-30 10:32:55,905 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-30 10:32:55,913 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-30 10:33:43,342 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
scm_1               | 2023-06-30 10:31:31,205 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-06-30 10:31:31,220 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-06-30 10:31:31,239 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: start cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1
scm_1               | 2023-06-30 10:31:31,257 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.LeaderElection: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:31,258 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.LeaderElection: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-06-30 10:31:31,258 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: shutdown cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1
scm_1               | 2023-06-30 10:31:31,264 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-06-30 10:31:31,268 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: change Leader from null to cdc59c73-ccc5-488e-962e-cc57c14602c5 at term 1 for becomeLeader, leader elected after 8007ms
scm_1               | 2023-06-30 10:31:31,273 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-30 10:31:31,300 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:31:31,301 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:31:31,323 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-30 10:31:31,325 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-30 10:31:31,328 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-30 10:31:31,341 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:31:31,349 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-30 10:31:31,361 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: start cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderStateImpl
scm_1               | 2023-06-30 10:31:31,445 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-30 10:31:31,617 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: set configuration 0: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:31,684 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/current/log_inprogress_0
scm_1               | 2023-06-30 10:31:32,704 [main] INFO server.RaftServer: cdc59c73-ccc5-488e-962e-cc57c14602c5: close
scm_1               | 2023-06-30 10:31:32,706 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: shutdown
scm_1               | 2023-06-30 10:31:32,707 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-C536BDF907E8,id=cdc59c73-ccc5-488e-962e-cc57c14602c5
scm_1               | 2023-06-30 10:31:32,707 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: shutdown cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderStateImpl
scm_1               | 2023-06-30 10:31:32,709 [main] INFO server.GrpcService: cdc59c73-ccc5-488e-962e-cc57c14602c5: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-30 10:31:32,721 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO impl.PendingRequests: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-30 10:31:32,724 [main] INFO server.GrpcService: cdc59c73-ccc5-488e-962e-cc57c14602c5: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-30 10:31:32,730 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO impl.StateMachineUpdater: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-30 10:31:32,730 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO impl.StateMachineUpdater: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-30 10:31:32,734 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO impl.StateMachineUpdater: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-30 10:31:32,734 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: closes. applyIndex: 0
scm_1               | 2023-06-30 10:31:32,736 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm_1               | 2023-06-30 10:31:32,738 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker close()
scm_1               | 2023-06-30 10:31:32,739 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-cdc59c73-ccc5-488e-962e-cc57c14602c5: Stopped
scm_1               | 2023-06-30 10:31:32,739 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:31:32,745 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-842b7c9d-2481-4cbe-85d2-c536bdf907e8; layoutVersion=4; scmId=cdc59c73-ccc5-488e-962e-cc57c14602c5
scm_1               | 2023-06-30 10:31:32,760 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 0736b879488e/172.23.0.9
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:31:35,336 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 0736b879488e/172.23.0.9
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.3.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:31:35,347 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:31:35,423 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:31:35,459 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:31:35,475 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:31:36,392 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:31:36,679 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:31:36,932 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm_1               | 2023-06-30 10:31:36,934 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-30 10:31:36,995 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-30 10:31:37,010 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:cdc59c73-ccc5-488e-962e-cc57c14602c5
scm_1               | 2023-06-30 10:31:37,059 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-30 10:31:37,107 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:31:37,108 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:31:37,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:31:37,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:31:37,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-30 10:31:37,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-30 10:31:37,110 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-30 10:31:37,113 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:31:37,115 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-30 10:31:37,117 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:31:37,126 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-30 10:31:37,131 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-30 10:31:37,131 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-30 10:31:37,328 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-30 10:31:37,330 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-30 10:31:37,330 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-30 10:31:37,330 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:31:37,331 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:31:37,336 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:31:37,343 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer: cdc59c73-ccc5-488e-962e-cc57c14602c5: found a subdirectory /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8
scm_1               | 2023-06-30 10:31:37,352 [main] INFO server.RaftServer: cdc59c73-ccc5-488e-962e-cc57c14602c5: addNew group-C536BDF907E8:[] returns group-C536BDF907E8:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm_1               | 2023-06-30 10:31:37,374 [pool-16-thread-1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5: new RaftServerImpl for group-C536BDF907E8:[] with SCMStateMachine:uninitialized
scm_1               | 2023-06-30 10:31:37,380 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-30 10:31:37,380 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-30 10:31:37,381 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-30 10:31:37,381 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:31:37,382 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:31:37,382 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-30 10:31:37,391 [pool-16-thread-1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-30 10:31:37,396 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:31:37,398 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-30 10:31:37,398 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-30 10:31:37,420 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-30 10:31:37,423 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-30 10:31:37,423 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-30 10:31:37,575 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-30 10:31:37,575 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-30 10:31:37,576 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-30 10:31:37,576 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-30 10:31:37,577 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-30 10:31:37,581 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-06-30 10:31:37,581 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-30 10:31:37,582 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-06-30 10:31:37,622 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1               | 2023-06-30 10:31:37,776 [main] INFO reflections.Reflections: Reflections took 131 ms to scan 3 urls, producing 112 keys and 252 values 
scm_1               | 2023-06-30 10:31:37,820 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-30 10:31:37,820 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-30 10:31:37,822 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-30 10:31:37,823 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-30 10:31:37,845 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-30 10:31:37,863 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-30 10:31:37,864 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-30 10:31:37,868 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-30 10:31:37,887 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-30 10:31:37,887 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-30 10:31:37,892 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-30 10:31:37,892 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:31:37,894 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-30 10:31:37,895 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-06-30 10:31:37,899 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-30 10:31:37,901 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-06-30 10:31:37,929 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-30 10:31:37,939 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-30 10:31:37,965 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-30 10:31:37,974 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-30 10:31:37,976 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-30 10:31:37,982 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-30 10:31:37,985 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:37,986 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:31:38,541 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:31:38,579 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:31:38,619 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-30 10:31:38,732 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:31:38,736 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:31:38,736 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-30 10:31:38,755 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:31:38,760 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:31:38,761 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-30 10:31:38,804 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-30 10:31:38,805 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        true
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-30 10:31:38,805 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-30 10:31:38,805 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-30 10:31:38,809 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:31:38,812 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-30 10:31:38,816 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/in_use.lock acquired by nodename 7@0736b879488e
scm_1               | 2023-06-30 10:31:38,823 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=cdc59c73-ccc5-488e-962e-cc57c14602c5} from /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/current/raft-meta
scm_1               | 2023-06-30 10:31:38,861 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: set configuration 0: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:38,866 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-30 10:31:38,872 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-30 10:31:38,872 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:31:38,873 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-30 10:31:38,874 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-30 10:31:38,880 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:31:38,884 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-30 10:31:38,885 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-30 10:31:38,889 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8
scm_1               | 2023-06-30 10:31:38,889 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:31:38,890 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:31:38,891 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:31:38,891 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-30 10:31:38,891 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-30 10:31:38,892 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-30 10:31:38,892 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-30 10:31:38,893 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-30 10:31:38,901 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-30 10:31:38,901 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-30 10:31:38,901 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-30 10:31:38,902 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-30 10:31:38,944 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: set configuration 0: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:38,962 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/current/log_inprogress_0
scm_1               | 2023-06-30 10:31:38,971 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm_1               | 2023-06-30 10:31:38,971 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:31:39,019 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: start as a follower, conf=0: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:39,022 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-30 10:31:39,023 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: start cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState
scm_1               | 2023-06-30 10:31:39,026 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C536BDF907E8,id=cdc59c73-ccc5-488e-962e-cc57c14602c5
scm_1               | 2023-06-30 10:31:39,026 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-30 10:31:39,029 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-30 10:31:39,030 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-30 10:31:39,030 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-30 10:31:39,031 [cdc59c73-ccc5-488e-962e-cc57c14602c5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-30 10:31:39,032 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-30 10:31:39,033 [Listener at 0.0.0.0/9860] INFO server.RaftServer: cdc59c73-ccc5-488e-962e-cc57c14602c5: start RPC server
scm_1               | 2023-06-30 10:31:39,066 [Listener at 0.0.0.0/9860] INFO server.GrpcService: cdc59c73-ccc5-488e-962e-cc57c14602c5: GrpcService started, listening on 9894
scm_1               | 2023-06-30 10:31:39,068 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-cdc59c73-ccc5-488e-962e-cc57c14602c5: Started
scm_1               | 2023-06-30 10:31:39,074 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-30 10:31:39,074 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-30 10:31:39,135 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-30 10:31:39,144 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-30 10:31:39,144 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-30 10:31:39,412 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:31:39,415 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:31:39,416 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-30 10:31:39,441 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:31:39,446 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:31:39,451 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:31:39,451 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-30 10:31:39,534 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@561f9d92] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-30 10:31:39,554 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-30 10:31:39,555 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-30 10:31:39,604 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @6232ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-30 10:31:39,716 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-30 10:31:39,730 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-30 10:31:39,740 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-30 10:31:39,742 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-30 10:31:39,742 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-30 10:31:39,742 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-30 10:31:39,818 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-30 10:31:39,824 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1               | 2023-06-30 10:31:39,867 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-30 10:31:39,868 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-30 10:31:39,869 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-30 10:31:39,878 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63124022{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-30 10:31:39,878 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@eaf8427{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-30 10:31:40,126 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7606bd03{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-10405694343862002900/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm_1               | 2023-06-30 10:31:40,163 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@701c223a{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-30 10:31:40,163 [Listener at 0.0.0.0/9860] INFO server.Server: Started @6791ms
scm_1               | 2023-06-30 10:31:40,165 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-30 10:31:40,165 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-30 10:31:40,166 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-30 10:31:44,072 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO impl.FollowerState: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5049599408ns, electionTimeout:5034ms
scm_1               | 2023-06-30 10:31:44,073 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: shutdown cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState
scm_1               | 2023-06-30 10:31:44,074 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-30 10:31:44,076 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-06-30 10:31:44,076 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-FollowerState] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: start cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1
scm_1               | 2023-06-30 10:31:44,087 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.LeaderElection: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:44,088 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.LeaderElection: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-30 10:31:44,089 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: shutdown cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1
scm_1               | 2023-06-30 10:31:44,089 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-30 10:31:44,089 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-30 10:31:44,089 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-30 10:31:44,103 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: change Leader from null to cdc59c73-ccc5-488e-962e-cc57c14602c5 at term 2 for becomeLeader, leader elected after 6669ms
scm_1               | 2023-06-30 10:31:44,112 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-30 10:31:44,116 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:31:44,117 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:31:44,121 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-30 10:31:44,121 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-30 10:31:44,121 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-30 10:31:44,125 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:31:44,126 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-30 10:31:44,128 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO impl.RoleInfo: cdc59c73-ccc5-488e-962e-cc57c14602c5: start cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderStateImpl
scm_1               | 2023-06-30 10:31:44,133 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-30 10:31:44,149 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-LeaderElection1] INFO server.RaftServer$Division: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8: set configuration 1: peers:[cdc59c73-ccc5-488e-962e-cc57c14602c5|rpc:0736b879488e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:31:44,155 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/current/log_inprogress_0 to /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/current/log_0-0
scm_1               | 2023-06-30 10:31:44,169 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/842b7c9d-2481-4cbe-85d2-c536bdf907e8/current/log_inprogress_1
scm_1               | 2023-06-30 10:31:44,172 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-30 10:31:44,173 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-30 10:31:44,174 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:44,175 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-30 10:31:44,175 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:31:44,176 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:31:44,177 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-30 10:31:44,183 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:31:44,273 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.10:52956: output error
scm_1               | 2023-06-30 10:31:44,274 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-30 10:31:44,275 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.11:56780: output error
scm_1               | 2023-06-30 10:31:44,275 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-30 10:31:45,063 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f224272d-19aa-40de-aa6a-e64f79edbd5a
scm_1               | 2023-06-30 10:31:45,065 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:31:45,069 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:31:45,080 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:31:45,080 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:31:45,085 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b to datanode:f224272d-19aa-40de-aa6a-e64f79edbd5a
scm_1               | 2023-06-30 10:31:45,086 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:31:45,160 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b, Nodes: f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.084Z[UTC]].
scm_1               | 2023-06-30 10:31:45,164 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:45,500 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e4c9087c-d909-423b-95c1-36204f0a06b4
scm_1               | 2023-06-30 10:31:45,501 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:31:45,502 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:31:45,504 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:31:45,508 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e1d2953d-5b26-42c0-9006-9fa95d538df4 to datanode:e4c9087c-d909-423b-95c1-36204f0a06b4
scm_1               | 2023-06-30 10:31:45,513 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e1d2953d-5b26-42c0-9006-9fa95d538df4, Nodes: e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.508Z[UTC]].
scm_1               | 2023-06-30 10:31:45,514 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:45,901 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0f3d230a-845e-43da-9c18-0d5fd63464e3
scm_1               | 2023-06-30 10:31:45,903 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:31:45,903 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:31:45,904 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f to datanode:0f3d230a-845e-43da-9c18-0d5fd63464e3
scm_1               | 2023-06-30 10:31:45,904 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:31:45,904 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:31:45,905 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-30 10:31:45,905 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-30 10:31:45,910 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-30 10:31:45,910 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:31:45,919 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.904Z[UTC]].
scm_1               | 2023-06-30 10:31:45,919 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:45,923 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 to datanode:0f3d230a-845e-43da-9c18-0d5fd63464e3
scm_1               | 2023-06-30 10:31:45,932 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 to datanode:f224272d-19aa-40de-aa6a-e64f79edbd5a
scm_1               | 2023-06-30 10:31:45,932 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 to datanode:e4c9087c-d909-423b-95c1-36204f0a06b4
scm_1               | 2023-06-30 10:31:45,935 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d3b26bd8-6443-4828-b6a0-87f1fd387bb8, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.923Z[UTC]].
scm_1               | 2023-06-30 10:31:45,936 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:45,937 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 to datanode:0f3d230a-845e-43da-9c18-0d5fd63464e3
scm_1               | 2023-06-30 10:31:45,939 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 to datanode:f224272d-19aa-40de-aa6a-e64f79edbd5a
scm_1               | 2023-06-30 10:31:45,939 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 to datanode:e4c9087c-d909-423b-95c1-36204f0a06b4
scm_1               | 2023-06-30 10:31:45,946 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 866602bc-edae-468e-b08c-0d608dc0a753, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:31:45.937Z[UTC]].
scm_1               | 2023-06-30 10:31:45,946 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:45,946 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=866602bc-edae-468e-b08c-0d608dc0a753 contains same datanodes as previous pipelines: PipelineID=d3b26bd8-6443-4828-b6a0-87f1fd387bb8 nodeIds: 0f3d230a-845e-43da-9c18-0d5fd63464e3, f224272d-19aa-40de-aa6a-e64f79edbd5a, e4c9087c-d909-423b-95c1-36204f0a06b4
scm_1               | 2023-06-30 10:31:48,507 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0d7a26c9-6ce1-42b9-aab9-8fcf1f3e0f3b, Nodes: f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f224272d-19aa-40de-aa6a-e64f79edbd5a, CreationTimestamp2023-06-30T10:31:45.084Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:31:48,514 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:48,538 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:48,917 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:49,106 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e1d2953d-5b26-42c0-9006-9fa95d538df4, Nodes: e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e4c9087c-d909-423b-95c1-36204f0a06b4, CreationTimestamp2023-06-30T10:31:45.508Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:31:49,110 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:49,118 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:49,531 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:49,608 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c2e9874f-c1c8-4c2a-bfd1-13469d7dbf2f, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0f3d230a-845e-43da-9c18-0d5fd63464e3, CreationTimestamp2023-06-30T10:31:45.904Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:31:49,614 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:49,619 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:49,920 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:51,616 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:51,861 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:51,892 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:53,844 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:54,501 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:54,983 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:56,870 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 866602bc-edae-468e-b08c-0d608dc0a753, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e4c9087c-d909-423b-95c1-36204f0a06b4, CreationTimestamp2023-06-30T10:31:45.937Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:31:56,870 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:56,872 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:31:56,873 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:31:56,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:31:56,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-30 10:31:56,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-30 10:31:56,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-30 10:31:56,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-30 10:31:56,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-30 10:31:56,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-30 10:31:56,879 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-30 10:31:56,881 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-30 10:32:03,550 [IPC Server handler 96 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-30 10:32:03,566 [cdc59c73-ccc5-488e-962e-cc57c14602c5@group-C536BDF907E8-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-30 10:32:03,571 [IPC Server handler 96 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-30 10:32:04,510 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d3b26bd8-6443-4828-b6a0-87f1fd387bb8, Nodes: 0f3d230a-845e-43da-9c18-0d5fd63464e3{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f224272d-19aa-40de-aa6a-e64f79edbd5a{ip: 172.23.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e4c9087c-d909-423b-95c1-36204f0a06b4{ip: 172.23.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0f3d230a-845e-43da-9c18-0d5fd63464e3, CreationTimestamp2023-06-30T10:31:45.923Z[UTC]] moved to OPEN state
scm_1               | 2023-06-30 10:32:31,174 [IPC Server handler 27 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.6
scm_1               | 2023-06-30 10:32:40,316 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.6
scm_1               | 2023-06-30 10:33:33,037 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.6
scm_1               | 2023-06-30 10:33:41,437 [IPC Server handler 8 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.6
Attaching to xcompat_recon_1, xcompat_datanode_5, xcompat_datanode_2, xcompat_datanode_1, xcompat_datanode_3, xcompat_old_client_1_2_1_1, xcompat_om_1, xcompat_old_client_1_1_0_1, xcompat_datanode_4, xcompat_new_client_1, xcompat_s3g_1, xcompat_old_client_1_3_0_1, xcompat_old_client_1_0_0_1, xcompat_scm_1
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-30 10:34:16,421 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 3b9b01d4e506/172.24.0.11
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-30 10:34:14,971 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = d7f44b020e87/172.24.0.8
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-06-30 10:34:16,522 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-30 10:34:16,872 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-30 10:34:17,638 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-30 10:34:18,767 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-30 10:34:18,772 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-30 10:34:19,646 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:3b9b01d4e506 ip:172.24.0.11
datanode_2          | 2023-06-30 10:34:20,881 [main] INFO reflections.Reflections: Reflections took 839 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_2          | 2023-06-30 10:34:24,008 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-06-30 10:34:24,380 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-30 10:34:25,774 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-30 10:34:25,957 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-30 10:34:25,992 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-30 10:34:25,992 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-30 10:34:26,246 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:34:26,339 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:34:26,346 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-30 10:34:26,395 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-30 10:34:26,400 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-30 10:34:26,401 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_2          | 2023-06-30 10:34:26,701 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-30 10:34:26,733 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-30 10:34:36,393 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-30 10:34:37,018 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-30 10:34:37,328 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-30 10:34:37,819 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-30 10:34:37,872 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-30 10:34:37,872 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-30 10:34:37,873 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-30 10:34:37,873 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-30 10:34:37,873 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-30 10:34:37,873 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-30 10:34:37,874 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:34:37,874 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-30 10:34:37,874 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:34:37,959 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-30 10:34:38,160 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-30 10:34:38,160 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-30 10:34:40,435 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-30 10:34:40,438 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-30 10:34:40,484 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-30 10:34:40,487 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:34:40,489 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:34:40,515 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:34:40,920 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-30 10:34:41,462 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-06-30 10:34:42,432 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:34:42,533 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-30 10:34:42,724 [main] INFO util.log: Logging initialized @37145ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-30 10:34:43,359 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-30 10:34:43,396 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-30 10:34:43,435 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-30 10:34:43,436 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-30 10:34:43,436 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-30 10:34:43,436 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-30 10:34:43,764 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-06-30 10:34:43,785 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-30 10:34:43,786 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-06-30 10:34:44,031 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-30 10:34:44,031 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-30 10:34:44,087 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-30 10:34:44,148 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1acfc058{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-30 10:34:44,164 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@12948e7a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-30 10:34:44,939 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@58267ba1{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1483615978567266363/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-30 10:34:44,991 [main] INFO server.AbstractConnector: Started ServerConnector@3a720ae3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-30 10:34:44,991 [main] INFO server.Server: Started @39412ms
datanode_2          | 2023-06-30 10:34:44,993 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-30 10:34:44,993 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-30 10:34:44,997 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-30 10:34:45,210 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-06-30 10:34:45,356 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-06-30 10:34:45,366 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-06-30 10:34:46,478 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-06-30 10:34:46,484 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-06-30 10:34:46,486 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-06-30 10:34:46,492 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-06-30 10:34:46,545 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-30 10:34:47,345 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_2          | 2023-06-30 10:34:47,673 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-30 10:34:49,907 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:49,909 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:50,908 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:50,910 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:51,909 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:51,910 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:52,910 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:53,910 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:54,911 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:55,912 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:56,913 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:34:56,955 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 3b9b01d4e506/172.24.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:34726 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:34726 remote=recon/172.24.0.14:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-30 10:34:57,914 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-30 10:35:02,926 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 3b9b01d4e506/172.24.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:39474 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:39474 remote=scm/172.24.0.2:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-30 10:35:04,114 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-9249362f-c32e-4e71-bad3-932e69624d37/container.db to cache
datanode_2          | 2023-06-30 10:35:04,114 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-9249362f-c32e-4e71-bad3-932e69624d37/container.db for volume DS-9249362f-c32e-4e71-bad3-932e69624d37
datanode_2          | 2023-06-30 10:35:04,172 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-30 10:35:04,183 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-06-30 10:35:04,463 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-06-30 10:35:04,463 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f5a485b8-4096-468e-a11f-083e698072ab
datanode_2          | 2023-06-30 10:35:04,607 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: f5a485b8-4096-468e-a11f-083e698072ab: start RPC server
datanode_2          | 2023-06-30 10:35:04,652 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: f5a485b8-4096-468e-a11f-083e698072ab: GrpcService started, listening on 9858
datanode_2          | 2023-06-30 10:35:04,695 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: f5a485b8-4096-468e-a11f-083e698072ab: GrpcService started, listening on 9856
datanode_2          | 2023-06-30 10:35:04,700 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: f5a485b8-4096-468e-a11f-083e698072ab: GrpcService started, listening on 9857
datanode_2          | 2023-06-30 10:35:04,721 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f5a485b8-4096-468e-a11f-083e698072ab is started using port 9858 for RATIS
datanode_2          | 2023-06-30 10:35:04,721 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f5a485b8-4096-468e-a11f-083e698072ab: Started
datanode_2          | 2023-06-30 10:35:04,736 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f5a485b8-4096-468e-a11f-083e698072ab is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-30 10:35:04,736 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f5a485b8-4096-468e-a11f-083e698072ab is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-30 10:35:04,802 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:35:09,931 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f5a485b8-4096-468e-a11f-083e698072ab: addNew group-E09D08F34C78:[f5a485b8-4096-468e-a11f-083e698072ab|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-E09D08F34C78:java.util.concurrent.CompletableFuture@79397f1d[Not completed]
datanode_2          | 2023-06-30 10:35:10,119 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab: new RaftServerImpl for group-E09D08F34C78:[f5a485b8-4096-468e-a11f-083e698072ab|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-30 10:35:10,129 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-30 10:35:10,133 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-30 10:35:10,135 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-30 10:35:10,136 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:35:10,136 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-30 10:35:10,139 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-30 10:35:10,188 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78: ConfigurationManager, init=-1: peers:[f5a485b8-4096-468e-a11f-083e698072ab|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-30 10:35:10,200 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-30 10:35:10,227 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-30 10:35:10,233 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-30 10:35:10,307 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-30 10:35:10,327 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-30 10:35:10,355 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-30 10:35:10,359 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-30 10:35:10,488 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-30 10:35:10,597 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-30 10:35:10,616 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-30 10:35:10,620 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-30 10:35:10,622 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-30 10:35:10,624 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-30 10:35:10,628 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-30 10:35:10,644 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0414c739-ceb6-4658-9323-e09d08f34c78 does not exist. Creating ...
datanode_2          | 2023-06-30 10:35:10,664 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0414c739-ceb6-4658-9323-e09d08f34c78/in_use.lock acquired by nodename 7@3b9b01d4e506
datanode_2          | 2023-06-30 10:35:10,699 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0414c739-ceb6-4658-9323-e09d08f34c78 has been successfully formatted.
datanode_2          | 2023-06-30 10:35:10,818 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO ratis.ContainerStateMachine: group-E09D08F34C78: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-30 10:35:10,826 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-30 10:35:10,893 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-30 10:35:10,893 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:35:10,894 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-30 10:35:10,896 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-30 10:35:10,917 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:35:10,963 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-30 10:35:10,966 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-30 10:35:10,966 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:35:10,977 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0414c739-ceb6-4658-9323-e09d08f34c78
datanode_2          | 2023-06-30 10:35:10,995 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-30 10:35:10,995 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:35:10,996 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-30 10:35:11,001 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-30 10:35:11,002 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-30 10:35:11,002 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-30 10:35:11,003 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-30 10:35:11,003 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-30 10:35:11,018 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-06-30 10:34:15,103 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-30 10:34:15,397 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-30 10:34:16,169 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-30 10:34:17,157 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-30 10:34:17,173 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-30 10:34:18,083 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d7f44b020e87 ip:172.24.0.8
datanode_1          | 2023-06-30 10:34:19,261 [main] INFO reflections.Reflections: Reflections took 803 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_1          | 2023-06-30 10:34:22,519 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-06-30 10:34:22,867 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-30 10:34:24,308 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-30 10:34:24,488 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-30 10:34:24,517 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-30 10:34:24,522 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-30 10:34:24,768 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:34:24,821 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:34:24,840 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-30 10:34:24,842 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-30 10:34:24,842 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-30 10:34:24,863 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-30 10:34:25,134 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-30 10:34:25,143 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-30 10:34:34,964 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-30 10:34:35,911 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-30 10:34:36,271 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-30 10:34:37,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-30 10:34:37,323 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-30 10:34:37,325 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-30 10:34:37,329 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-30 10:34:37,333 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-30 10:34:37,344 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-30 10:34:37,353 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-30 10:34:37,356 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:34:37,366 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-30 10:34:37,372 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:34:37,552 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-30 10:34:37,629 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-30 10:34:37,651 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-30 10:34:40,424 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-30 10:34:40,433 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-30 10:34:40,446 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-30 10:34:40,454 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:34:40,454 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:34:40,461 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:34:40,860 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-30 10:34:41,322 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-06-30 10:34:42,555 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:34:42,642 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-30 10:34:42,843 [main] INFO util.log: Logging initialized @39132ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-30 10:34:43,492 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-30 10:34:43,536 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-30 10:34:43,577 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-30 10:34:43,585 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-30 10:34:43,585 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-30 10:34:43,587 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-30 10:34:43,787 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-06-30 10:34:43,816 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-30 10:34:43,819 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-06-30 10:34:44,015 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-30 10:34:44,047 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-30 10:34:44,049 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-06-30 10:34:44,110 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d836c4a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-30 10:34:44,122 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3a36da5e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-30 10:34:45,023 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5e1bfe66{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-11296588514178011511/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-30 10:34:45,107 [main] INFO server.AbstractConnector: Started ServerConnector@677274e7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-30 10:34:45,108 [main] INFO server.Server: Started @41397ms
datanode_1          | 2023-06-30 10:34:45,123 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-30 10:34:45,124 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-30 10:34:45,125 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-30 10:34:45,332 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-06-30 10:34:45,500 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-06-30 10:34:45,531 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-06-30 10:34:46,782 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-06-30 10:34:46,827 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-06-30 10:34:46,837 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-06-30 10:34:46,838 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-06-30 10:34:46,874 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-30 10:34:47,544 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-30 10:34:16,341 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = b3b9072fc7aa/172.24.0.12
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-06-30 10:34:16,438 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-30 10:34:16,764 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-30 10:34:17,424 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-30 10:34:18,499 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-30 10:34:18,503 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-30 10:34:19,370 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b3b9072fc7aa ip:172.24.0.12
datanode_3          | 2023-06-30 10:34:20,479 [main] INFO reflections.Reflections: Reflections took 733 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_3          | 2023-06-30 10:34:23,717 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-06-30 10:34:24,111 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-30 10:34:25,358 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-30 10:34:25,469 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-30 10:34:25,490 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-30 10:34:25,512 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-30 10:34:25,739 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:34:25,766 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:34:25,797 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-30 10:34:25,811 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-30 10:34:25,813 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-30 10:34:25,818 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-30 10:35:11,022 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-30 10:35:11,048 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:35:11,053 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-30 10:35:11,054 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-30 10:35:11,071 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO segmented.SegmentedRaftLogWorker: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:35:11,071 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO segmented.SegmentedRaftLogWorker: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-30 10:35:11,084 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78: start as a follower, conf=-1: peers:[f5a485b8-4096-468e-a11f-083e698072ab|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:35:11,104 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-30 10:35:11,106 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO impl.RoleInfo: f5a485b8-4096-468e-a11f-083e698072ab: start f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState
datanode_2          | 2023-06-30 10:35:11,107 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-30 10:35:11,107 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-30 10:35:11,114 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E09D08F34C78,id=f5a485b8-4096-468e-a11f-083e698072ab
datanode_2          | 2023-06-30 10:35:11,115 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-30 10:35:11,116 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-30 10:35:11,116 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-30 10:35:11,117 [f5a485b8-4096-468e-a11f-083e698072ab-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-30 10:35:11,181 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=0414c739-ceb6-4658-9323-e09d08f34c78
datanode_2          | 2023-06-30 10:35:11,184 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=0414c739-ceb6-4658-9323-e09d08f34c78.
datanode_2          | 2023-06-30 10:35:16,127 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState] INFO impl.FollowerState: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5021687621ns, electionTimeout:5019ms
datanode_2          | 2023-06-30 10:35:16,128 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState] INFO impl.RoleInfo: f5a485b8-4096-468e-a11f-083e698072ab: shutdown f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState
datanode_2          | 2023-06-30 10:35:16,131 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-30 10:35:16,146 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-30 10:35:16,146 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-FollowerState] INFO impl.RoleInfo: f5a485b8-4096-468e-a11f-083e698072ab: start f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1
datanode_2          | 2023-06-30 10:35:16,173 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO impl.LeaderElection: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f5a485b8-4096-468e-a11f-083e698072ab|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:35:16,178 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO impl.LeaderElection: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-06-30 10:35:16,196 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO impl.LeaderElection: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f5a485b8-4096-468e-a11f-083e698072ab|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:35:16,196 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO impl.LeaderElection: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-30 10:35:16,202 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO impl.RoleInfo: f5a485b8-4096-468e-a11f-083e698072ab: shutdown f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1
datanode_2          | 2023-06-30 10:35:16,202 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-30 10:35:16,205 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E09D08F34C78 with new leaderId: f5a485b8-4096-468e-a11f-083e698072ab
datanode_2          | 2023-06-30 10:35:16,206 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78: change Leader from null to f5a485b8-4096-468e-a11f-083e698072ab at term 1 for becomeLeader, leader elected after 5904ms
datanode_2          | 2023-06-30 10:35:16,252 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-30 10:35:16,291 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:34:25,976 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-30 10:34:25,987 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-30 10:34:36,048 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-30 10:34:36,497 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-30 10:34:36,801 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-30 10:34:37,379 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-30 10:34:37,400 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-30 10:34:37,408 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-30 10:34:37,408 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-30 10:34:37,411 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-30 10:34:37,411 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-30 10:34:37,416 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-30 10:34:37,423 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:34:37,423 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-30 10:34:37,426 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:34:37,799 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-30 10:34:37,849 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-30 10:34:37,901 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-30 10:34:39,919 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-30 10:34:39,955 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-30 10:34:39,956 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-30 10:34:39,959 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:34:39,965 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:34:39,997 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:34:40,544 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-30 10:34:40,976 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-06-30 10:34:42,124 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:34:42,204 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-30 10:34:42,317 [main] INFO util.log: Logging initialized @36972ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-30 10:34:43,060 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-30 10:34:43,092 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-30 10:34:43,139 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-30 10:34:43,152 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-30 10:34:43,175 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-30 10:34:43,176 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-30 10:34:43,395 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-06-30 10:34:43,433 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-30 10:34:43,459 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-06-30 10:34:43,628 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-30 10:34:43,628 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-30 10:34:43,650 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-06-30 10:34:43,711 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6f9b5f01{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-30 10:34:43,718 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3180131e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-30 10:34:44,387 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5c20841d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3802230912567776176/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-30 10:34:44,457 [main] INFO server.AbstractConnector: Started ServerConnector@3f919230{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-30 10:34:44,463 [main] INFO server.Server: Started @39113ms
datanode_3          | 2023-06-30 10:34:44,478 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-30 10:34:44,479 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-30 10:34:44,491 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-30 10:34:44,682 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-06-30 10:34:44,869 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-06-30 10:34:44,882 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-06-30 10:34:46,276 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-06-30 10:34:46,289 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-06-30 10:34:46,289 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-06-30 10:34:46,308 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-06-30 10:34:46,351 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-30 10:34:46,994 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_3          | 2023-06-30 10:34:47,384 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-30 10:34:49,677 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:49,678 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:50,678 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:50,679 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:51,678 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:51,681 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:52,682 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:53,682 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:54,683 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:55,684 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:56,685 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:34:56,710 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From b3b9072fc7aa/172.24.0.12 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:37690 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 2023-06-30 10:35:16,293 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-30 10:35:16,313 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-30 10:35:16,313 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-30 10:35:16,317 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-30 10:35:16,363 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-30 10:35:16,374 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-30 10:35:16,379 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO impl.RoleInfo: f5a485b8-4096-468e-a11f-083e698072ab: start f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderStateImpl
datanode_2          | 2023-06-30 10:35:16,507 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-30 10:35:16,799 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-LeaderElection1] INFO server.RaftServer$Division: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78: set configuration 0: peers:[f5a485b8-4096-468e-a11f-083e698072ab|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-30 10:35:17,086 [f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f5a485b8-4096-468e-a11f-083e698072ab@group-E09D08F34C78-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0414c739-ceb6-4658-9323-e09d08f34c78/current/log_inprogress_0
datanode_2          | 2023-06-30 10:36:04,803 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:37:04,804 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:38:04,807 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:39:04,808 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:40:04,808 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-30 10:41:04,809 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:34:47,807 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-30 10:34:50,220 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:50,225 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:51,221 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:51,226 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:52,222 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:53,223 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:54,224 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:55,226 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:56,226 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:56,268 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From d7f44b020e87/172.24.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:51268 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:51268 remote=recon/172.24.0.14:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-30 10:34:57,227 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:34:58,228 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-30 10:35:03,234 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From d7f44b020e87/172.24.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:58318 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_4          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_4          | 2023-06-30 10:34:14,846 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_4          | /************************************************************
datanode_4          | STARTUP_MSG: Starting HddsDatanodeService
datanode_4          | STARTUP_MSG:   host = e5208497c3a5/172.24.0.7
datanode_4          | STARTUP_MSG:   args = []
datanode_4          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_5          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 2023-06-30 10:34:16,733 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5          | /************************************************************
datanode_5          | STARTUP_MSG: Starting HddsDatanodeService
datanode_5          | STARTUP_MSG:   host = 5a5d557d6cf0/172.24.0.15
datanode_5          | STARTUP_MSG:   args = []
datanode_5          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_5          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_5          | STARTUP_MSG:   java = 11.0.19
datanode_5          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_5          | ************************************************************/
datanode_5          | 2023-06-30 10:34:16,798 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5          | 2023-06-30 10:34:17,098 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_5          | 2023-06-30 10:34:17,743 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_5          | 2023-06-30 10:34:18,809 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_5          | 2023-06-30 10:34:18,810 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_5          | 2023-06-30 10:34:19,731 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5a5d557d6cf0 ip:172.24.0.15
datanode_5          | 2023-06-30 10:34:20,820 [main] INFO reflections.Reflections: Reflections took 728 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_5          | 2023-06-30 10:34:23,783 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_5          | 2023-06-30 10:34:24,151 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_5          | 2023-06-30 10:34:25,697 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_5          | 2023-06-30 10:34:25,872 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_5          | 2023-06-30 10:34:25,897 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_5          | 2023-06-30 10:34:25,916 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_5          | 2023-06-30 10:34:26,181 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_5          | 2023-06-30 10:34:26,237 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-06-30 10:34:26,241 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_5          | 2023-06-30 10:34:26,257 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_5          | 2023-06-30 10:34:26,268 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_5          | 2023-06-30 10:34:26,272 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_5          | 2023-06-30 10:34:26,417 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_5          | 2023-06-30 10:34:26,435 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_5          | 2023-06-30 10:34:36,741 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_5          | 2023-06-30 10:34:37,135 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-06-30 10:34:37,421 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_5          | 2023-06-30 10:34:38,491 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-06-30 10:34:38,494 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_5          | 2023-06-30 10:34:38,542 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-06-30 10:34:38,544 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_5          | 2023-06-30 10:34:38,552 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_5          | 2023-06-30 10:34:38,555 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_5          | 2023-06-30 10:34:38,574 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_5          | 2023-06-30 10:34:38,592 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:34:38,600 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_5          | 2023-06-30 10:34:38,605 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-30 10:34:38,760 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-30 10:34:38,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_5          | 2023-06-30 10:34:38,772 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_5          | 2023-06-30 10:34:40,750 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_5          | 2023-06-30 10:34:40,758 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_5          | 2023-06-30 10:34:40,772 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_5          | 2023-06-30 10:34:40,774 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-30 10:34:40,775 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-30 10:34:40,813 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-30 10:34:41,303 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_5          | 2023-06-30 10:34:41,727 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_5          | 2023-06-30 10:34:43,065 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_5          | 2023-06-30 10:34:43,145 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_5          | 2023-06-30 10:34:43,315 [main] INFO util.log: Logging initialized @37873ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_5          | 2023-06-30 10:34:43,808 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_5          | 2023-06-30 10:34:43,831 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_5          | 2023-06-30 10:34:43,872 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_5          | 2023-06-30 10:34:43,881 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_5          | 2023-06-30 10:34:43,889 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_5          | 2023-06-30 10:34:43,890 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_5          | 2023-06-30 10:34:44,066 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_5          | 2023-06-30 10:34:44,085 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_5          | 2023-06-30 10:34:44,092 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_5          | 2023-06-30 10:34:44,291 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_5          | 2023-06-30 10:34:44,321 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_5          | 2023-06-30 10:34:44,324 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_5          | 2023-06-30 10:34:44,428 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6859bbd4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:37690 remote=recon/172.24.0.14:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-30 10:34:57,686 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-30 10:35:02,701 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:58318 remote=scm/172.24.0.2:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-30 10:35:04,060 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-980ea5e0-e634-4bc8-971e-d9fb40392b67/container.db to cache
datanode_1          | 2023-06-30 10:35:04,063 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-980ea5e0-e634-4bc8-971e-d9fb40392b67/container.db for volume DS-980ea5e0-e634-4bc8-971e-d9fb40392b67
datanode_1          | 2023-06-30 10:35:04,115 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-30 10:35:04,154 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-06-30 10:35:04,454 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-06-30 10:35:04,454 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_1          | 2023-06-30 10:35:04,571 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: 90a80e44-64e8-4ca8-8f98-d749b124e197: start RPC server
datanode_1          | 2023-06-30 10:35:04,573 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 90a80e44-64e8-4ca8-8f98-d749b124e197: GrpcService started, listening on 9858
datanode_1          | 2023-06-30 10:35:04,588 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 90a80e44-64e8-4ca8-8f98-d749b124e197: GrpcService started, listening on 9856
datanode_1          | 2023-06-30 10:35:04,588 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 90a80e44-64e8-4ca8-8f98-d749b124e197: GrpcService started, listening on 9857
datanode_1          | 2023-06-30 10:35:04,609 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 90a80e44-64e8-4ca8-8f98-d749b124e197 is started using port 9858 for RATIS
datanode_1          | 2023-06-30 10:35:04,609 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 90a80e44-64e8-4ca8-8f98-d749b124e197 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-30 10:35:04,609 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 90a80e44-64e8-4ca8-8f98-d749b124e197 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-30 10:35:04,610 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-90a80e44-64e8-4ca8-8f98-d749b124e197: Started
datanode_1          | 2023-06-30 10:35:04,712 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:35:12,409 [grpc-default-executor-0] INFO server.RaftServer: 90a80e44-64e8-4ca8-8f98-d749b124e197: addNew group-12FECA489596:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-12FECA489596:java.util.concurrent.CompletableFuture@5152ee5b[Not completed]
datanode_1          | 2023-06-30 10:35:12,487 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197: new RaftServerImpl for group-12FECA489596:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_4          | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
datanode_4          | STARTUP_MSG:   java = 11.0.19
datanode_3          | java.net.SocketTimeoutException: Call From b3b9072fc7aa/172.24.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:33838 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:33838 remote=scm/172.24.0.2:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-30 10:35:04,112 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-60ae03b2-5c65-4ddd-bcd7-4e6b9279901e/container.db to cache
datanode_3          | 2023-06-30 10:35:04,112 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-60ae03b2-5c65-4ddd-bcd7-4e6b9279901e/container.db for volume DS-60ae03b2-5c65-4ddd-bcd7-4e6b9279901e
datanode_3          | 2023-06-30 10:35:04,146 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-30 10:35:04,155 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-06-30 10:35:04,410 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-06-30 10:35:04,411 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 9ca9d3f5-c23a-4767-9163-086f75187a71
datanode_3          | 2023-06-30 10:35:04,531 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: 9ca9d3f5-c23a-4767-9163-086f75187a71: start RPC server
datanode_3          | 2023-06-30 10:35:04,548 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 9ca9d3f5-c23a-4767-9163-086f75187a71: GrpcService started, listening on 9858
datanode_3          | 2023-06-30 10:35:04,577 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 9ca9d3f5-c23a-4767-9163-086f75187a71: GrpcService started, listening on 9856
datanode_3          | 2023-06-30 10:35:04,598 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 9ca9d3f5-c23a-4767-9163-086f75187a71: GrpcService started, listening on 9857
datanode_3          | 2023-06-30 10:35:04,618 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9ca9d3f5-c23a-4767-9163-086f75187a71 is started using port 9858 for RATIS
datanode_3          | 2023-06-30 10:35:04,619 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9ca9d3f5-c23a-4767-9163-086f75187a71 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-30 10:35:04,619 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9ca9d3f5-c23a-4767-9163-086f75187a71 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-30 10:35:04,619 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9ca9d3f5-c23a-4767-9163-086f75187a71: Started
datanode_3          | 2023-06-30 10:35:04,724 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:35:09,644 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 9ca9d3f5-c23a-4767-9163-086f75187a71: addNew group-949A9DD84BBC:[9ca9d3f5-c23a-4767-9163-086f75187a71|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-949A9DD84BBC:java.util.concurrent.CompletableFuture@58b6fa56[Not completed]
datanode_3          | 2023-06-30 10:35:09,754 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71: new RaftServerImpl for group-949A9DD84BBC:[9ca9d3f5-c23a-4767-9163-086f75187a71|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-30 10:35:09,759 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-30 10:35:09,762 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_4          | ************************************************************/
datanode_4          | 2023-06-30 10:34:14,921 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4          | 2023-06-30 10:34:15,181 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4          | 2023-06-30 10:34:15,802 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4          | 2023-06-30 10:34:16,648 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4          | 2023-06-30 10:34:16,648 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4          | 2023-06-30 10:34:17,436 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e5208497c3a5 ip:172.24.0.7
datanode_4          | 2023-06-30 10:34:18,615 [main] INFO reflections.Reflections: Reflections took 838 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_4          | 2023-06-30 10:34:21,927 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_4          | 2023-06-30 10:34:22,291 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_4          | 2023-06-30 10:34:23,886 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_4          | 2023-06-30 10:34:24,117 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_4          | 2023-06-30 10:34:24,131 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_4          | 2023-06-30 10:34:24,156 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_4          | 2023-06-30 10:34:24,467 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_4          | 2023-06-30 10:34:24,533 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-06-30 10:34:24,555 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_4          | 2023-06-30 10:34:24,564 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_4          | 2023-06-30 10:34:24,564 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_4          | 2023-06-30 10:34:24,565 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_5          | 2023-06-30 10:34:44,429 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@62376bdd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_5          | 2023-06-30 10:34:45,260 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1a1dd8eb{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6098045483520299606/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_5          | 2023-06-30 10:34:45,352 [main] INFO server.AbstractConnector: Started ServerConnector@48d14ea0{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_5          | 2023-06-30 10:34:45,353 [main] INFO server.Server: Started @39912ms
datanode_5          | 2023-06-30 10:34:45,376 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_5          | 2023-06-30 10:34:45,377 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_5          | 2023-06-30 10:34:45,380 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_5          | 2023-06-30 10:34:45,532 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_5          | 2023-06-30 10:34:45,735 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_5          | 2023-06-30 10:34:45,743 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_5          | 2023-06-30 10:34:47,134 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_5          | 2023-06-30 10:34:47,134 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_5          | 2023-06-30 10:34:47,142 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_5          | 2023-06-30 10:34:47,153 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_5          | 2023-06-30 10:34:47,186 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_5          | 2023-06-30 10:34:47,798 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_5          | 2023-06-30 10:34:47,974 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_5          | 2023-06-30 10:34:50,496 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:50,496 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:51,497 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:51,498 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:52,498 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:53,499 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:54,500 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:55,501 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:56,501 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:34:56,541 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_5          | java.net.SocketTimeoutException: Call From 5a5d557d6cf0/172.24.0.15 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:34964 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:34964 remote=recon/172.24.0.14:9891]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-06-30 10:34:57,502 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-30 10:35:02,514 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_5          | java.net.SocketTimeoutException: Call From 5a5d557d6cf0/172.24.0.15 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:57916 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:57916 remote=scm/172.24.0.2:9861]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 2023-06-30 10:35:09,766 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-30 10:35:09,766 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:35:09,767 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-30 10:35:09,767 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-30 10:35:09,800 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC: ConfigurationManager, init=-1: peers:[9ca9d3f5-c23a-4767-9163-086f75187a71|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-30 10:35:09,808 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-30 10:35:09,827 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-30 10:35:09,828 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-30 10:35:09,885 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-30 10:35:09,887 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-30 10:35:09,934 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-30 10:35:09,935 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-30 10:35:10,038 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-30 10:35:10,146 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-30 10:35:10,160 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-30 10:35:10,161 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-30 10:35:10,163 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-30 10:35:10,166 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-30 10:35:10,169 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-30 10:35:10,169 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc does not exist. Creating ...
datanode_3          | 2023-06-30 10:35:10,191 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc/in_use.lock acquired by nodename 7@b3b9072fc7aa
datanode_3          | 2023-06-30 10:35:10,216 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc has been successfully formatted.
datanode_3          | 2023-06-30 10:35:10,269 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO ratis.ContainerStateMachine: group-949A9DD84BBC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-30 10:35:10,332 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-30 10:35:10,358 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-30 10:35:10,359 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:35:10,363 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-30 10:35:10,363 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-30 10:35:10,378 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:35:10,407 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-30 10:35:10,407 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-30 10:35:10,407 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:35:10,467 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc
datanode_3          | 2023-06-30 10:35:10,467 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-30 10:35:10,488 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:35:10,489 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-30 10:35:10,489 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-30 10:35:10,489 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-30 10:35:10,492 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-30 10:35:10,492 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-30 10:35:10,504 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-30 10:35:10,551 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-30 10:35:10,551 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-30 10:35:10,591 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:35:10,605 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-30 10:35:10,606 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-30 10:35:10,632 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:35:10,632 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO segmented.SegmentedRaftLogWorker: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-30 10:35:10,636 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC: start as a follower, conf=-1: peers:[9ca9d3f5-c23a-4767-9163-086f75187a71|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:35:10,638 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-30 10:35:10,640 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO impl.RoleInfo: 9ca9d3f5-c23a-4767-9163-086f75187a71: start 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState
datanode_3          | 2023-06-30 10:35:10,647 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-30 10:35:10,647 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-30 10:35:10,662 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-949A9DD84BBC,id=9ca9d3f5-c23a-4767-9163-086f75187a71
datanode_3          | 2023-06-30 10:35:10,664 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-30 10:35:10,664 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-30 10:35:10,664 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-30 10:35:10,665 [9ca9d3f5-c23a-4767-9163-086f75187a71-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-30 10:35:10,709 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc
datanode_3          | 2023-06-30 10:35:10,714 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc.
datanode_3          | 2023-06-30 10:35:15,712 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState] INFO impl.FollowerState: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5071758336ns, electionTimeout:5059ms
datanode_3          | 2023-06-30 10:35:15,718 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState] INFO impl.RoleInfo: 9ca9d3f5-c23a-4767-9163-086f75187a71: shutdown 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState
datanode_3          | 2023-06-30 10:35:15,721 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-30 10:35:15,737 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-30 10:35:15,737 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-FollowerState] INFO impl.RoleInfo: 9ca9d3f5-c23a-4767-9163-086f75187a71: start 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1
datanode_3          | 2023-06-30 10:35:15,745 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO impl.LeaderElection: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[9ca9d3f5-c23a-4767-9163-086f75187a71|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:35:15,747 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO impl.LeaderElection: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-06-30 10:35:15,755 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO impl.LeaderElection: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[9ca9d3f5-c23a-4767-9163-086f75187a71|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:35:15,755 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO impl.LeaderElection: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-30 10:35:15,756 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO impl.RoleInfo: 9ca9d3f5-c23a-4767-9163-086f75187a71: shutdown 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1
datanode_3          | 2023-06-30 10:35:15,756 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-30 10:35:15,756 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-949A9DD84BBC with new leaderId: 9ca9d3f5-c23a-4767-9163-086f75187a71
datanode_3          | 2023-06-30 10:35:15,765 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC: change Leader from null to 9ca9d3f5-c23a-4767-9163-086f75187a71 at term 1 for becomeLeader, leader elected after 5871ms
datanode_3          | 2023-06-30 10:35:15,800 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-30 10:35:15,808 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-06-30 10:34:24,784 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_4          | 2023-06-30 10:34:24,794 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_4          | 2023-06-30 10:34:33,953 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_4          | 2023-06-30 10:34:34,782 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-06-30 10:34:35,086 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_4          | 2023-06-30 10:34:36,092 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-06-30 10:34:36,108 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_4          | 2023-06-30 10:34:36,113 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-06-30 10:34:36,116 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_4          | 2023-06-30 10:34:36,117 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_4          | 2023-06-30 10:34:36,117 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_4          | 2023-06-30 10:34:36,184 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4          | 2023-06-30 10:34:36,188 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:34:36,188 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_4          | 2023-06-30 10:34:36,189 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-30 10:34:36,250 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4          | 2023-06-30 10:34:36,279 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_4          | 2023-06-30 10:34:36,285 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_4          | 2023-06-30 10:34:38,445 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_4          | 2023-06-30 10:34:38,459 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_4          | 2023-06-30 10:34:38,483 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_4          | 2023-06-30 10:34:38,488 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-30 10:34:38,492 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-30 10:34:38,512 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-30 10:34:38,886 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_4          | 2023-06-30 10:34:39,254 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_4          | 2023-06-30 10:34:40,611 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_4          | 2023-06-30 10:34:40,688 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_4          | 2023-06-30 10:34:40,889 [main] INFO util.log: Logging initialized @37084ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_4          | 2023-06-30 10:34:41,515 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_4          | 2023-06-30 10:34:41,543 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4          | 2023-06-30 10:34:41,574 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4          | 2023-06-30 10:34:41,594 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_4          | 2023-06-30 10:34:41,594 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4          | 2023-06-30 10:34:41,597 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4          | 2023-06-30 10:34:41,808 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_4          | 2023-06-30 10:34:41,818 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_4          | 2023-06-30 10:34:41,828 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_4          | 2023-06-30 10:34:42,100 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4          | 2023-06-30 10:34:42,111 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4          | 2023-06-30 10:34:42,113 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_4          | 2023-06-30 10:34:42,202 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a225014{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_4          | 2023-06-30 10:34:42,210 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b8a063d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4          | 2023-06-30 10:34:42,972 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4def900a{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8708703774581151960/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_4          | 2023-06-30 10:34:43,024 [main] INFO server.AbstractConnector: Started ServerConnector@21251e43{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_4          | 2023-06-30 10:34:43,026 [main] INFO server.Server: Started @39219ms
datanode_4          | 2023-06-30 10:34:43,036 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4          | 2023-06-30 10:34:43,036 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4          | 2023-06-30 10:34:43,037 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_4          | 2023-06-30 10:34:43,195 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_4          | 2023-06-30 10:34:43,328 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_4          | 2023-06-30 10:34:43,338 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_4          | 2023-06-30 10:34:44,760 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_4          | 2023-06-30 10:34:44,760 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_4          | 2023-06-30 10:34:44,767 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_4          | 2023-06-30 10:34:44,848 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_4          | 2023-06-30 10:34:44,852 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_4          | 2023-06-30 10:34:45,551 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.14:9891
datanode_3          | 2023-06-30 10:35:15,808 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-30 10:35:15,818 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-30 10:35:15,818 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-30 10:35:15,820 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-30 10:35:15,849 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-30 10:35:15,858 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-30 10:35:15,884 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO impl.RoleInfo: 9ca9d3f5-c23a-4767-9163-086f75187a71: start 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderStateImpl
datanode_3          | 2023-06-30 10:35:16,014 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-30 10:35:16,280 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-LeaderElection1] INFO server.RaftServer$Division: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC: set configuration 0: peers:[9ca9d3f5-c23a-4767-9163-086f75187a71|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-30 10:35:16,608 [9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9ca9d3f5-c23a-4767-9163-086f75187a71@group-949A9DD84BBC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc/current/log_inprogress_0
datanode_3          | 2023-06-30 10:36:04,728 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:37:04,729 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:38:04,729 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:39:04,730 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:40:04,730 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-30 10:41:04,730 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-06-30 10:35:03,984 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-7d0471e6-5b90-4a79-9cbd-1117df7873f3/container.db to cache
datanode_5          | 2023-06-30 10:35:03,987 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-7d0471e6-5b90-4a79-9cbd-1117df7873f3/container.db for volume DS-7d0471e6-5b90-4a79-9cbd-1117df7873f3
datanode_5          | 2023-06-30 10:35:04,040 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_5          | 2023-06-30 10:35:04,068 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_5          | 2023-06-30 10:35:04,401 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_5          | 2023-06-30 10:35:04,402 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_5          | 2023-06-30 10:35:04,532 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: ebc02119-b8bf-46fc-962f-277901a1ccf1: start RPC server
datanode_5          | 2023-06-30 10:35:04,544 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: ebc02119-b8bf-46fc-962f-277901a1ccf1: GrpcService started, listening on 9858
datanode_5          | 2023-06-30 10:35:04,554 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: ebc02119-b8bf-46fc-962f-277901a1ccf1: GrpcService started, listening on 9856
datanode_5          | 2023-06-30 10:35:04,560 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: ebc02119-b8bf-46fc-962f-277901a1ccf1: GrpcService started, listening on 9857
datanode_5          | 2023-06-30 10:35:04,567 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ebc02119-b8bf-46fc-962f-277901a1ccf1 is started using port 9858 for RATIS
datanode_5          | 2023-06-30 10:35:04,578 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ebc02119-b8bf-46fc-962f-277901a1ccf1 is started using port 9857 for RATIS_ADMIN
datanode_5          | 2023-06-30 10:35:04,578 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ebc02119-b8bf-46fc-962f-277901a1ccf1 is started using port 9856 for RATIS_SERVER
datanode_5          | 2023-06-30 10:35:04,583 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-ebc02119-b8bf-46fc-962f-277901a1ccf1: Started
datanode_5          | 2023-06-30 10:35:04,669 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-30 10:35:08,709 [PipelineCommandHandlerThread-0] INFO server.RaftServer: ebc02119-b8bf-46fc-962f-277901a1ccf1: addNew group-12FECA489596:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-12FECA489596:java.util.concurrent.CompletableFuture@50a24027[Not completed]
datanode_5          | 2023-06-30 10:35:08,886 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1: new RaftServerImpl for group-12FECA489596:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-30 10:35:08,905 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-30 10:35:08,910 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-06-30 10:34:45,967 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4          | 2023-06-30 10:34:48,478 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:48,479 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:49,480 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:49,481 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:50,481 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:50,481 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:51,482 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:51,482 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.14:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:52,482 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:53,484 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:54,484 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:55,485 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:56,486 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:34:56,523 [EndpointStateMachine task thread for recon/172.24.0.14:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From e5208497c3a5/172.24.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:39106 remote=recon/172.24.0.14:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:39106 remote=recon/172.24.0.14:9891]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-30 10:35:12,494 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:35:12,506 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:35:12,508 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:35:12,509 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:35:12,509 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:35:12,510 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:35:12,546 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: ConfigurationManager, init=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:35:12,550 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:35:12,566 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:35:12,569 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:35:12,604 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:35:12,614 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-30 10:35:12,627 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:35:12,629 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:35:12,718 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-30 10:35:12,746 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:35:12,765 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:35:12,775 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:35:12,776 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:35:12,777 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:35:12,779 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:35:12,780 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596 does not exist. Creating ...
datanode_1          | 2023-06-30 10:35:12,793 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596/in_use.lock acquired by nodename 7@d7f44b020e87
datanode_1          | 2023-06-30 10:35:12,801 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596 has been successfully formatted.
datanode_1          | 2023-06-30 10:35:12,855 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO ratis.ContainerStateMachine: group-12FECA489596: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:35:12,857 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:35:12,880 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:35:12,882 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:12,883 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:35:12,886 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:35:12,910 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:12,963 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:35:12,966 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:35:12,967 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:13,004 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596
datanode_1          | 2023-06-30 10:35:13,005 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:35:13,006 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:35:13,007 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:13,015 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:35:13,015 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:35:13,020 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:35:13,020 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:35:13,022 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:35:13,059 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:13,063 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:13,085 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:35:13,085 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:35:13,086 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:35:13,103 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:35:13,104 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:35:13,125 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: start as a follower, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:13,125 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:35:13,127 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FollowerState
datanode_1          | 2023-06-30 10:35:13,130 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:35:13,133 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:35:13,135 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12FECA489596,id=90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_1          | 2023-06-30 10:35:13,138 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:35:13,139 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:35:13,142 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:35:13,143 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:35:15,043 [grpc-default-executor-0] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: receive requestVote(PRE_VOTE, ebc02119-b8bf-46fc-962f-277901a1ccf1, group-12FECA489596, 0, (t:0, i:0))
datanode_1          | 2023-06-30 10:35:15,052 [grpc-default-executor-0] INFO impl.VoteContext: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FOLLOWER: accept PRE_VOTE from ebc02119-b8bf-46fc-962f-277901a1ccf1: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:35:15,093 [grpc-default-executor-0] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596 replies to PRE_VOTE vote request: ebc02119-b8bf-46fc-962f-277901a1ccf1<-90a80e44-64e8-4ca8-8f98-d749b124e197#0:OK-t0. Peer's state: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596:t0, leader=null, voted=, raftlog=Memoized:90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:15,184 [grpc-default-executor-0] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: receive requestVote(ELECTION, ebc02119-b8bf-46fc-962f-277901a1ccf1, group-12FECA489596, 1, (t:0, i:0))
datanode_1          | 2023-06-30 10:35:15,185 [grpc-default-executor-0] INFO impl.VoteContext: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FOLLOWER: accept ELECTION from ebc02119-b8bf-46fc-962f-277901a1ccf1: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-30 10:35:15,186 [grpc-default-executor-0] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_1          | 2023-06-30 10:35:15,186 [grpc-default-executor-0] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: shutdown 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FollowerState
datanode_1          | 2023-06-30 10:35:15,189 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FollowerState] INFO impl.FollowerState: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FollowerState was interrupted
datanode_1          | 2023-06-30 10:35:15,190 [grpc-default-executor-0] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-FollowerState
datanode_1          | 2023-06-30 10:35:15,192 [grpc-default-executor-0] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596 replies to ELECTION vote request: ebc02119-b8bf-46fc-962f-277901a1ccf1<-90a80e44-64e8-4ca8-8f98-d749b124e197#0:OK-t1. Peer's state: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596:t1, leader=null, voted=ebc02119-b8bf-46fc-962f-277901a1ccf1, raftlog=Memoized:90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:16,934 [90a80e44-64e8-4ca8-8f98-d749b124e197-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-12FECA489596 with new leaderId: ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_1          | 2023-06-30 10:35:16,934 [90a80e44-64e8-4ca8-8f98-d749b124e197-server-thread1] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: change Leader from null to ebc02119-b8bf-46fc-962f-277901a1ccf1 at term 1 for appendEntries, leader elected after 4330ms
datanode_1          | 2023-06-30 10:35:17,116 [90a80e44-64e8-4ca8-8f98-d749b124e197-server-thread2] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596: set configuration 0: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:17,186 [90a80e44-64e8-4ca8-8f98-d749b124e197-server-thread2] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:35:17,736 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-12FECA489596-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596/current/log_inprogress_0
datanode_1          | 2023-06-30 10:35:18,042 [grpc-default-executor-0] INFO server.RaftServer: 90a80e44-64e8-4ca8-8f98-d749b124e197: addNew group-84B0A4D85512:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] returns group-84B0A4D85512:java.util.concurrent.CompletableFuture@e1d9271[Not completed]
datanode_1          | 2023-06-30 10:35:18,044 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197: new RaftServerImpl for group-84B0A4D85512:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-30 10:35:18,045 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:35:18,046 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:35:18,046 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:35:18,047 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:35:18,047 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:35:18,051 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-30 10:35:08,910 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-30 10:35:08,911 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-30 10:35:08,911 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-30 10:35:08,912 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-30 10:35:08,952 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596: ConfigurationManager, init=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-30 10:35:08,958 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-30 10:35:08,983 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-30 10:35:08,988 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-30 10:35:09,072 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-30 10:35:09,111 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-30 10:35:09,126 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-30 10:35:09,126 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-30 10:35:09,207 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-30 10:35:09,286 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-30 10:35:09,293 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-30 10:35:09,294 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-30 10:35:09,299 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-30 10:35:09,301 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-30 10:35:09,302 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-30 10:35:09,302 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596 does not exist. Creating ...
datanode_5          | 2023-06-30 10:35:09,319 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596/in_use.lock acquired by nodename 7@5a5d557d6cf0
datanode_5          | 2023-06-30 10:35:09,346 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596 has been successfully formatted.
datanode_5          | 2023-06-30 10:35:09,380 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO ratis.ContainerStateMachine: group-12FECA489596: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-30 10:35:09,391 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-30 10:35:09,411 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-30 10:35:09,411 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:09,415 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-30 10:35:09,418 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-30 10:35:09,427 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:09,452 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-30 10:35:09,458 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-30 10:35:09,458 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:09,501 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: new ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596
datanode_5          | 2023-06-30 10:35:09,501 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-30 10:35:09,501 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-30 10:35:09,502 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:09,502 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-30 10:35:09,502 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-30 10:35:09,503 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-30 10:35:09,503 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-30 10:34:57,487 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-30 10:35:02,499 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From e5208497c3a5/172.24.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:37376 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:37376 remote=scm/172.24.0.2:9861]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_4          | 2023-06-30 10:35:04,004 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-684b4457-e265-4672-8fb2-4c59da873ac9/container.db to cache
datanode_4          | 2023-06-30 10:35:04,005 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-028270cf-30a6-4025-a7a5-8f2c0855d65a/DS-684b4457-e265-4672-8fb2-4c59da873ac9/container.db for volume DS-684b4457-e265-4672-8fb2-4c59da873ac9
datanode_4          | 2023-06-30 10:35:04,037 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_4          | 2023-06-30 10:35:04,039 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_4          | 2023-06-30 10:35:04,443 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_4          | 2023-06-30 10:35:04,448 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4e17b060-329a-47a0-8048-d67d08b8e752
datanode_4          | 2023-06-30 10:35:04,532 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: 4e17b060-329a-47a0-8048-d67d08b8e752: start RPC server
datanode_4          | 2023-06-30 10:35:04,543 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 4e17b060-329a-47a0-8048-d67d08b8e752: GrpcService started, listening on 9858
datanode_4          | 2023-06-30 10:35:04,557 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 4e17b060-329a-47a0-8048-d67d08b8e752: GrpcService started, listening on 9856
datanode_4          | 2023-06-30 10:35:04,560 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 4e17b060-329a-47a0-8048-d67d08b8e752: GrpcService started, listening on 9857
datanode_4          | 2023-06-30 10:35:04,576 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4e17b060-329a-47a0-8048-d67d08b8e752 is started using port 9858 for RATIS
datanode_4          | 2023-06-30 10:35:04,576 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4e17b060-329a-47a0-8048-d67d08b8e752 is started using port 9857 for RATIS_ADMIN
datanode_4          | 2023-06-30 10:35:04,576 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4e17b060-329a-47a0-8048-d67d08b8e752 is started using port 9856 for RATIS_SERVER
datanode_4          | 2023-06-30 10:35:04,577 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4e17b060-329a-47a0-8048-d67d08b8e752: Started
datanode_4          | 2023-06-30 10:35:04,681 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:35:14,318 [grpc-default-executor-0] INFO server.RaftServer: 4e17b060-329a-47a0-8048-d67d08b8e752: addNew group-12FECA489596:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-12FECA489596:java.util.concurrent.CompletableFuture@192eb22f[Not completed]
datanode_4          | 2023-06-30 10:35:14,492 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752: new RaftServerImpl for group-12FECA489596:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-30 10:35:14,500 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-30 10:35:14,502 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-06-30 10:35:14,502 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-30 10:35:14,502 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-30 10:35:14,504 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-30 10:35:14,504 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-30 10:35:14,532 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596: ConfigurationManager, init=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-30 10:35:14,536 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-30 10:35:14,566 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-30 10:35:14,569 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-30 10:35:14,611 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-30 10:35:14,621 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-06-30 10:35:14,638 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-30 10:35:14,641 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-30 10:35:14,725 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-30 10:35:14,787 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-30 10:35:14,819 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-30 10:35:14,819 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-30 10:35:14,824 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-30 10:35:14,824 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-30 10:35:14,829 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-30 10:35:14,830 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596 does not exist. Creating ...
datanode_4          | 2023-06-30 10:35:14,870 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596/in_use.lock acquired by nodename 7@e5208497c3a5
datanode_4          | 2023-06-30 10:35:14,888 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596 has been successfully formatted.
datanode_4          | 2023-06-30 10:35:14,974 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO ratis.ContainerStateMachine: group-12FECA489596: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-30 10:35:15,006 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-30 10:35:15,078 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596: receive requestVote(PRE_VOTE, ebc02119-b8bf-46fc-962f-277901a1ccf1, group-12FECA489596, 0, (t:0, i:0))
datanode_4          | 2023-06-30 10:35:15,082 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 4e17b060-329a-47a0-8048-d67d08b8e752: Failed requestVote ebc02119-b8bf-46fc-962f-277901a1ccf1->4e17b060-329a-47a0-8048-d67d08b8e752#0: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596 is not in [RUNNING]: current state is STARTING
datanode_4          | 2023-06-30 10:35:15,163 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-30 10:35:15,163 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:18,051 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: ConfigurationManager, init=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:35:18,051 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:35:18,052 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:35:18,054 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:35:18,054 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:35:18,054 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-30 10:35:18,054 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:35:18,056 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:35:18,057 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-30 10:35:18,061 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:35:18,064 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:35:18,064 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:35:18,064 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:35:18,066 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:35:18,066 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:35:18,066 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512 does not exist. Creating ...
datanode_1          | 2023-06-30 10:35:18,073 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512/in_use.lock acquired by nodename 7@d7f44b020e87
datanode_1          | 2023-06-30 10:35:18,077 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512 has been successfully formatted.
datanode_1          | 2023-06-30 10:35:18,083 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO ratis.ContainerStateMachine: group-84B0A4D85512: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:35:18,095 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:35:18,096 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:35:18,101 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:18,105 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:35:18,105 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:35:18,105 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:18,110 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:35:18,112 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:35:18,115 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:18,116 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512
datanode_1          | 2023-06-30 10:35:18,118 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:35:18,118 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:35:18,119 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:18,119 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:35:18,120 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:35:18,122 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:35:18,122 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:35:18,124 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:35:18,129 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:18,137 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:18,672 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-90a80e44-64e8-4ca8-8f98-d749b124e197: Detected pause in JVM or host machine approximately 0.484s with 0.517s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=57ms
datanode_1          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=460ms
datanode_1          | 2023-06-30 10:35:18,680 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:35:18,682 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:35:18,683 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:35:18,683 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:35:18,684 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:35:18,685 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: start as a follower, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:18,686 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:35:18,686 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState
datanode_1          | 2023-06-30 10:35:18,689 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-84B0A4D85512,id=90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_1          | 2023-06-30 10:35:18,689 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:35:18,689 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:35:18,691 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:35:18,694 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-30 10:35:09,512 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-30 10:35:09,542 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:09,546 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:09,590 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-30 10:35:09,597 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-30 10:35:09,597 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-30 10:35:09,639 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-30 10:35:09,639 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-30 10:35:09,640 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596: start as a follower, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:09,640 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-30 10:35:09,641 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState
datanode_5          | 2023-06-30 10:35:09,663 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-30 10:35:09,666 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-30 10:35:09,666 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12FECA489596,id=ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_5          | 2023-06-30 10:35:09,668 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-30 10:35:09,668 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-30 10:35:09,668 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-30 10:35:09,669 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-30 10:35:09,734 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=550537c1-df67-480c-bde8-12feca489596
datanode_5          | 2023-06-30 10:35:14,854 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState] INFO impl.FollowerState: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5212797656ns, electionTimeout:5187ms
datanode_5          | 2023-06-30 10:35:14,854 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: shutdown ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState
datanode_5          | 2023-06-30 10:35:14,855 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-06-30 10:35:14,858 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-06-30 10:35:14,858 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-FollowerState] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1
datanode_5          | 2023-06-30 10:35:14,867 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:14,911 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-30 10:35:14,911 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-30 10:35:14,930 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 4e17b060-329a-47a0-8048-d67d08b8e752
datanode_5          | 2023-06-30 10:35:14,930 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_5          | 2023-06-30 10:35:15,157 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_5          | 2023-06-30 10:35:15,158 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection:   Response 0: ebc02119-b8bf-46fc-962f-277901a1ccf1<-90a80e44-64e8-4ca8-8f98-d749b124e197#0:OK-t0
datanode_5          | 2023-06-30 10:35:15,161 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_4          | 2023-06-30 10:35:15,166 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-30 10:35:15,166 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-30 10:35:15,202 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:15,261 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-30 10:35:15,261 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:35:18,694 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:35:18,690 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:35:22,538 [grpc-default-executor-0] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: receive requestVote(PRE_VOTE, ebc02119-b8bf-46fc-962f-277901a1ccf1, group-84B0A4D85512, 0, (t:0, i:0))
datanode_1          | 2023-06-30 10:35:22,538 [grpc-default-executor-0] INFO impl.VoteContext: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FOLLOWER: reject PRE_VOTE from ebc02119-b8bf-46fc-962f-277901a1ccf1: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-30 10:35:22,539 [grpc-default-executor-0] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512 replies to PRE_VOTE vote request: ebc02119-b8bf-46fc-962f-277901a1ccf1<-90a80e44-64e8-4ca8-8f98-d749b124e197#0:FAIL-t0. Peer's state: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512:t0, leader=null, voted=, raftlog=Memoized:90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:23,738 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState] INFO impl.FollowerState: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5052319040ns, electionTimeout:5029ms
datanode_1          | 2023-06-30 10:35:23,739 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: shutdown 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState
datanode_1          | 2023-06-30 10:35:23,739 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:35:23,742 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-30 10:35:23,742 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-FollowerState] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1
datanode_1          | 2023-06-30 10:35:23,748 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:23,751 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 4e17b060-329a-47a0-8048-d67d08b8e752
datanode_1          | 2023-06-30 10:35:23,751 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:35:23,758 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:35:23,763 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_1          | 2023-06-30 10:35:24,485 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:35:24,485 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection:   Response 0: 90a80e44-64e8-4ca8-8f98-d749b124e197<-4e17b060-329a-47a0-8048-d67d08b8e752#0:OK-t0
datanode_1          | 2023-06-30 10:35:24,485 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_1          | 2023-06-30 10:35:24,488 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:24,489 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:35:24,489 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:35:24,557 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-30 10:35:24,558 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection:   Response 0: 90a80e44-64e8-4ca8-8f98-d749b124e197<-4e17b060-329a-47a0-8048-d67d08b8e752#0:OK-t1
datanode_1          | 2023-06-30 10:35:24,558 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1 ELECTION round 0: result PASSED
datanode_1          | 2023-06-30 10:35:24,558 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: shutdown 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1
datanode_1          | 2023-06-30 10:35:24,558 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-06-30 10:35:15,172 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:15,177 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-30 10:35:15,177 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-30 10:35:15,202 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_5          | 2023-06-30 10:35:15,202 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection:   Response 0: ebc02119-b8bf-46fc-962f-277901a1ccf1<-90a80e44-64e8-4ca8-8f98-d749b124e197#0:OK-t1
datanode_5          | 2023-06-30 10:35:15,202 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1 ELECTION round 0: result PASSED
datanode_5          | 2023-06-30 10:35:15,203 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: shutdown ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1
datanode_5          | 2023-06-30 10:35:15,203 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-06-30 10:35:15,203 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-12FECA489596 with new leaderId: ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_5          | 2023-06-30 10:35:15,205 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596: change Leader from null to ebc02119-b8bf-46fc-962f-277901a1ccf1 at term 1 for becomeLeader, leader elected after 6131ms
datanode_5          | 2023-06-30 10:35:15,230 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-06-30 10:35:15,278 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-30 10:35:15,282 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5          | 2023-06-30 10:35:15,360 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-06-30 10:35:15,366 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | 2023-06-30 10:35:15,370 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-06-30 10:35:15,409 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-30 10:35:15,437 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-06-30 10:35:15,521 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-06-30 10:35:15,523 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:15,525 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-06-30 10:35:15,547 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-06-30 10:35:15,553 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-06-30 10:35:15,563 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-30 10:35:15,563 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-06-30 10:35:15,568 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-06-30 10:35:15,568 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-30 10:35:15,568 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-30 10:35:15,579 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-06-30 10:35:15,587 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:15,587 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-06-30 10:35:15,588 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-06-30 10:35:15,590 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-06-30 10:35:15,593 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-30 10:35:15,593 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-06-30 10:35:15,594 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-06-30 10:35:15,594 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-30 10:35:15,594 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-30 10:35:15,613 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderStateImpl
datanode_5          | 2023-06-30 10:35:15,729 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-30 10:35:15,855 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=550537c1-df67-480c-bde8-12feca489596.
datanode_5          | 2023-06-30 10:35:15,857 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-LeaderElection1] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596: set configuration 0: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:15,858 [PipelineCommandHandlerThread-0] INFO server.RaftServer: ebc02119-b8bf-46fc-962f-277901a1ccf1: addNew group-E71640CD79B9:[ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-E71640CD79B9:java.util.concurrent.CompletableFuture@3d7c289c[Not completed]
datanode_5          | 2023-06-30 10:35:15,866 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1: new RaftServerImpl for group-E71640CD79B9:[ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9: ConfigurationManager, init=-1: peers:[ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-30 10:35:15,871 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-30 10:35:15,874 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-30 10:35:15,874 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-30 10:35:15,875 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-30 10:35:15,875 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-30 10:35:15,876 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-30 10:35:15,876 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-30 10:35:15,877 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-30 10:35:15,882 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-30 10:35:15,883 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-30 10:35:15,883 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-30 10:35:15,883 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-30 10:35:15,883 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-30 10:35:15,883 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-30 10:35:15,883 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b55d9a00-bdf3-4025-8878-e71640cd79b9 does not exist. Creating ...
datanode_5          | 2023-06-30 10:35:15,889 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b55d9a00-bdf3-4025-8878-e71640cd79b9/in_use.lock acquired by nodename 7@5a5d557d6cf0
datanode_5          | 2023-06-30 10:35:15,907 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b55d9a00-bdf3-4025-8878-e71640cd79b9 has been successfully formatted.
datanode_5          | 2023-06-30 10:35:15,908 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO ratis.ContainerStateMachine: group-E71640CD79B9: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-30 10:35:15,908 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-30 10:35:16,048 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-30 10:35:15,262 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:15,312 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596: receive requestVote(ELECTION, ebc02119-b8bf-46fc-962f-277901a1ccf1, group-12FECA489596, 1, (t:0, i:0))
datanode_4          | 2023-06-30 10:35:15,312 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 4e17b060-329a-47a0-8048-d67d08b8e752: Failed requestVote ebc02119-b8bf-46fc-962f-277901a1ccf1->4e17b060-329a-47a0-8048-d67d08b8e752#0: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596 is not in [RUNNING]: current state is STARTING
datanode_4          | 2023-06-30 10:35:15,353 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596
datanode_4          | 2023-06-30 10:35:15,359 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-30 10:35:15,360 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-06-30 10:35:15,373 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:15,373 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-06-30 10:35:15,373 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-30 10:35:15,379 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-06-30 10:35:15,380 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-30 10:35:15,381 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-06-30 10:35:15,421 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:15,424 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:15,504 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-06-30 10:35:15,506 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-30 10:35:15,506 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-06-30 10:35:15,536 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-30 10:35:15,536 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-30 10:35:15,542 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596: start as a follower, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:15,544 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-06-30 10:35:15,547 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: start 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState
datanode_4          | 2023-06-30 10:35:15,557 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:15,560 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:15,568 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12FECA489596,id=4e17b060-329a-47a0-8048-d67d08b8e752
datanode_4          | 2023-06-30 10:35:15,572 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-30 10:35:15,573 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-30 10:35:15,573 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-06-30 10:35:15,574 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-30 10:35:17,159 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-12FECA489596 with new leaderId: ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_4          | 2023-06-30 10:35:17,177 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread1] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596: change Leader from null to ebc02119-b8bf-46fc-962f-277901a1ccf1 at term 1 for appendEntries, leader elected after 2548ms
datanode_4          | 2023-06-30 10:35:17,238 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread2] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596: set configuration 0: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:17,311 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread2] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-30 10:35:17,713 [grpc-default-executor-0] INFO server.RaftServer: 4e17b060-329a-47a0-8048-d67d08b8e752: addNew group-84B0A4D85512:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] returns group-84B0A4D85512:java.util.concurrent.CompletableFuture@139b96a7[Not completed]
datanode_4          | 2023-06-30 10:35:17,717 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752: new RaftServerImpl for group-84B0A4D85512:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-30 10:35:17,718 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-30 10:35:17,718 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-06-30 10:35:17,718 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-30 10:35:17,718 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-30 10:35:17,720 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-30 10:35:17,720 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-30 10:35:17,723 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: ConfigurationManager, init=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-30 10:35:17,724 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-30 10:35:17,724 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-30 10:35:17,724 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-30 10:35:17,726 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-30 10:35:17,726 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-06-30 10:35:17,726 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-30 10:35:17,726 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-30 10:35:17,727 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-30 10:35:17,731 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-30 10:35:17,731 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-30 10:35:17,734 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-30 10:35:17,734 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-30 10:35:17,735 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-30 10:35:17,735 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-30 10:35:17,735 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512 does not exist. Creating ...
datanode_4          | 2023-06-30 10:35:17,738 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512/in_use.lock acquired by nodename 7@e5208497c3a5
datanode_4          | 2023-06-30 10:35:17,742 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512 has been successfully formatted.
datanode_4          | 2023-06-30 10:35:17,754 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO ratis.ContainerStateMachine: group-84B0A4D85512: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-30 10:35:17,754 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-30 10:35:17,754 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-30 10:35:17,754 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:17,758 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-30 10:35:17,758 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-30 10:35:17,758 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:17,769 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-30 10:35:17,769 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-30 10:35:17,769 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:17,769 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512
datanode_4          | 2023-06-30 10:35:17,771 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-30 10:35:17,774 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-06-30 10:35:17,775 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:17,783 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-06-30 10:35:17,788 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-30 10:35:17,788 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-06-30 10:35:17,788 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-30 10:35:17,789 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-06-30 10:35:17,789 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:17,813 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:17,912 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-06-30 10:35:17,912 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-30 10:35:17,913 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-06-30 10:35:17,914 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-30 10:35:17,916 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-30 10:35:17,933 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: start as a follower, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:17,935 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-06-30 10:35:17,936 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: start 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState
datanode_4          | 2023-06-30 10:35:17,945 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-84B0A4D85512,id=4e17b060-329a-47a0-8048-d67d08b8e752
datanode_4          | 2023-06-30 10:35:17,946 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-30 10:35:17,947 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-30 10:35:17,948 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-06-30 10:35:17,948 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-30 10:35:17,952 [4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:17,961 [4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:18,023 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596/current/log_inprogress_0
datanode_4          | 2023-06-30 10:35:20,706 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:20,706 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:22,511 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: receive requestVote(PRE_VOTE, ebc02119-b8bf-46fc-962f-277901a1ccf1, group-84B0A4D85512, 0, (t:0, i:0))
datanode_4          | 2023-06-30 10:35:22,515 [grpc-default-executor-0] INFO impl.VoteContext: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FOLLOWER: accept PRE_VOTE from ebc02119-b8bf-46fc-962f-277901a1ccf1: our priority 0 <= candidate's priority 0
datanode_1          | 2023-06-30 10:35:24,558 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-84B0A4D85512 with new leaderId: 90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_1          | 2023-06-30 10:35:24,558 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: change Leader from null to 90a80e44-64e8-4ca8-8f98-d749b124e197 at term 1 for becomeLeader, leader elected after 6504ms
datanode_1          | 2023-06-30 10:35:24,601 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:35:24,647 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:35:24,649 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-30 10:35:24,665 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:35:24,673 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:35:24,674 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:35:24,725 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:35:24,726 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-30 10:35:24,811 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:35:24,811 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:24,811 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:35:24,824 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-30 10:35:24,825 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:35:24,830 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:35:24,830 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-30 10:35:24,830 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-30 10:35:24,830 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:35:24,830 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-30 10:35:24,840 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-30 10:35:24,840 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:24,840 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-30 10:35:24,840 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-30 10:35:24,842 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-30 10:35:24,842 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:35:24,842 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-30 10:35:24,847 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-30 10:35:24,847 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:35:24,847 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-30 10:35:24,854 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderStateImpl
datanode_1          | 2023-06-30 10:35:24,856 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:35:24,862 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512/current/log_inprogress_0
datanode_1          | 2023-06-30 10:35:24,926 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512-LeaderElection1] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-84B0A4D85512: set configuration 0: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:38,078 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197: new RaftServerImpl for group-7C226DD1E4C1:[90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-30 10:35:16,048 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:16,048 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-30 10:35:16,056 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-30 10:35:16,059 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:16,059 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-30 10:35:16,059 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-30 10:35:16,061 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:16,063 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: new ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b55d9a00-bdf3-4025-8878-e71640cd79b9
datanode_5          | 2023-06-30 10:35:16,063 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-30 10:35:16,064 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-30 10:35:16,074 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:16,074 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-30 10:35:16,074 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-30 10:35:16,074 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-30 10:35:16,074 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-30 10:35:16,074 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-30 10:35:16,076 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:16,078 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:16,905 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-30 10:35:16,906 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-30 10:35:16,906 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-30 10:35:16,939 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-30 10:35:16,939 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-30 10:35:16,962 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-ebc02119-b8bf-46fc-962f-277901a1ccf1: Detected pause in JVM or host machine approximately 0.804s with 0.876s GC time.
datanode_5          | GC pool 'ParNew' had collection(s): count=1 time=153ms
datanode_5          | GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=723ms
datanode_5          | 2023-06-30 10:35:17,004 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9: start as a follower, conf=-1: peers:[ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:17,005 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-30 10:35:17,006 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState
datanode_5          | 2023-06-30 10:35:17,080 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E71640CD79B9,id=ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_5          | 2023-06-30 10:35:17,081 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-30 10:35:17,125 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-30 10:35:17,126 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-30 10:35:17,126 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-30 10:35:17,125 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-30 10:35:17,131 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-30 10:35:17,139 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=b55d9a00-bdf3-4025-8878-e71640cd79b9
datanode_5          | 2023-06-30 10:35:17,139 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b55d9a00-bdf3-4025-8878-e71640cd79b9.
datanode_5          | 2023-06-30 10:35:17,149 [PipelineCommandHandlerThread-0] INFO server.RaftServer: ebc02119-b8bf-46fc-962f-277901a1ccf1: addNew group-84B0A4D85512:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] returns group-84B0A4D85512:java.util.concurrent.CompletableFuture@569568cc[Not completed]
datanode_4          | 2023-06-30 10:35:22,523 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512 replies to PRE_VOTE vote request: ebc02119-b8bf-46fc-962f-277901a1ccf1<-4e17b060-329a-47a0-8048-d67d08b8e752#0:OK-t0. Peer's state: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512:t0, leader=null, voted=, raftlog=Memoized:4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:23,154 [4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:23,155 [4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:24,364 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: receive requestVote(PRE_VOTE, 90a80e44-64e8-4ca8-8f98-d749b124e197, group-84B0A4D85512, 0, (t:0, i:0))
datanode_4          | 2023-06-30 10:35:24,365 [grpc-default-executor-0] INFO impl.VoteContext: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FOLLOWER: accept PRE_VOTE from 90a80e44-64e8-4ca8-8f98-d749b124e197: our priority 0 <= candidate's priority 1
datanode_4          | 2023-06-30 10:35:24,365 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512 replies to PRE_VOTE vote request: 90a80e44-64e8-4ca8-8f98-d749b124e197<-4e17b060-329a-47a0-8048-d67d08b8e752#0:OK-t0. Peer's state: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512:t0, leader=null, voted=, raftlog=Memoized:4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:24,495 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: receive requestVote(ELECTION, 90a80e44-64e8-4ca8-8f98-d749b124e197, group-84B0A4D85512, 1, (t:0, i:0))
datanode_4          | 2023-06-30 10:35:24,495 [grpc-default-executor-0] INFO impl.VoteContext: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FOLLOWER: accept ELECTION from 90a80e44-64e8-4ca8-8f98-d749b124e197: our priority 0 <= candidate's priority 1
datanode_4          | 2023-06-30 10:35:24,495 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_4          | 2023-06-30 10:35:24,495 [grpc-default-executor-0] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: shutdown 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState
datanode_4          | 2023-06-30 10:35:24,497 [4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState] INFO impl.FollowerState: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState was interrupted
datanode_4          | 2023-06-30 10:35:24,497 [grpc-default-executor-0] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: start 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-FollowerState
datanode_4          | 2023-06-30 10:35:24,521 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512 replies to ELECTION vote request: 90a80e44-64e8-4ca8-8f98-d749b124e197<-4e17b060-329a-47a0-8048-d67d08b8e752#0:OK-t1. Peer's state: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512:t1, leader=null, voted=90a80e44-64e8-4ca8-8f98-d749b124e197, raftlog=Memoized:4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:25,128 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-84B0A4D85512 with new leaderId: 90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_4          | 2023-06-30 10:35:25,131 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread1] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: change Leader from null to 90a80e44-64e8-4ca8-8f98-d749b124e197 at term 1 for appendEntries, leader elected after 7402ms
datanode_4          | 2023-06-30 10:35:25,135 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread1] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512: set configuration 0: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:25,136 [4e17b060-329a-47a0-8048-d67d08b8e752-server-thread1] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-30 10:35:25,146 [4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-84B0A4D85512-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512/current/log_inprogress_0
datanode_4          | 2023-06-30 10:35:25,865 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:25,865 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:30,947 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:30,947 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:36,061 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:36,061 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:38,298 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752: new RaftServerImpl for group-3E6B04C1DA80:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-30 10:35:38,298 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-30 10:35:38,298 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-06-30 10:35:38,299 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-30 10:35:38,299 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-30 10:35:38,298 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4e17b060-329a-47a0-8048-d67d08b8e752: addNew group-3E6B04C1DA80:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER] returns group-3E6B04C1DA80:java.util.concurrent.CompletableFuture@7546bcaa[Not completed]
datanode_4          | 2023-06-30 10:35:38,299 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-30 10:35:38,299 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-30 10:35:38,300 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80: ConfigurationManager, init=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-30 10:35:38,302 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-30 10:35:38,302 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-30 10:35:38,303 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-30 10:35:38,304 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-30 10:35:38,305 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-06-30 10:35:38,305 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-30 10:35:38,306 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-30 10:35:38,306 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-30 10:35:38,308 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-30 10:35:38,311 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-30 10:35:38,312 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-30 10:35:38,312 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-30 10:35:38,313 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-30 10:35:38,313 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-30 10:35:38,313 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bd0ac505-581c-4be1-b7d5-3e6b04c1da80 does not exist. Creating ...
datanode_4          | 2023-06-30 10:35:38,320 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bd0ac505-581c-4be1-b7d5-3e6b04c1da80/in_use.lock acquired by nodename 7@e5208497c3a5
datanode_4          | 2023-06-30 10:35:38,326 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bd0ac505-581c-4be1-b7d5-3e6b04c1da80 has been successfully formatted.
datanode_4          | 2023-06-30 10:35:38,330 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO ratis.ContainerStateMachine: group-3E6B04C1DA80: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-30 10:35:38,333 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-30 10:35:38,341 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-30 10:35:38,341 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:38,341 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-30 10:35:38,341 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-30 10:35:38,341 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:38,342 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-30 10:35:38,342 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-30 10:35:38,342 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:38,342 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bd0ac505-581c-4be1-b7d5-3e6b04c1da80
datanode_4          | 2023-06-30 10:35:38,342 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-30 10:35:38,342 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-06-30 10:35:38,343 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:38,343 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-30 10:35:17,152 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1: new RaftServerImpl for group-84B0A4D85512:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-30 10:35:17,158 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-30 10:35:17,159 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-30 10:35:17,160 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-30 10:35:17,161 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-30 10:35:17,161 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-30 10:35:17,161 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-30 10:35:17,161 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: ConfigurationManager, init=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-30 10:35:17,162 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-30 10:35:17,162 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-30 10:35:17,163 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-30 10:35:17,165 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-30 10:35:17,166 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-30 10:35:17,166 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-30 10:35:17,168 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-30 10:35:17,169 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-30 10:35:17,177 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-30 10:35:17,179 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-30 10:35:17,179 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-30 10:35:17,179 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-30 10:35:17,180 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-30 10:35:17,181 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-30 10:35:17,184 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512 does not exist. Creating ...
datanode_5          | 2023-06-30 10:35:17,190 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512/in_use.lock acquired by nodename 7@5a5d557d6cf0
datanode_5          | 2023-06-30 10:35:17,196 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512 has been successfully formatted.
datanode_5          | 2023-06-30 10:35:17,216 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO ratis.ContainerStateMachine: group-84B0A4D85512: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-30 10:35:17,218 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-30 10:35:17,218 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-30 10:35:17,218 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:17,219 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-30 10:35:17,219 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-30 10:35:17,219 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:17,242 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-30 10:35:17,242 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-30 10:35:17,242 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:17,242 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: new ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512
datanode_5          | 2023-06-30 10:35:17,243 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-30 10:35:38,343 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-30 10:35:38,343 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-06-30 10:35:38,344 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-30 10:35:38,344 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-06-30 10:35:38,349 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-06-30 10:35:38,356 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-30 10:35:38,455 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-06-30 10:35:38,455 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-30 10:35:38,456 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-06-30 10:35:38,456 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-30 10:35:38,458 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-30 10:35:38,460 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80: start as a follower, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:38,461 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-06-30 10:35:38,462 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: start 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState
datanode_4          | 2023-06-30 10:35:38,462 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E6B04C1DA80,id=4e17b060-329a-47a0-8048-d67d08b8e752
datanode_4          | 2023-06-30 10:35:38,463 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-30 10:35:38,468 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-30 10:35:38,468 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-06-30 10:35:38,468 [4e17b060-329a-47a0-8048-d67d08b8e752-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-30 10:35:38,471 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:38,472 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:38,482 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=bd0ac505-581c-4be1-b7d5-3e6b04c1da80
datanode_4          | 2023-06-30 10:35:38,487 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=bd0ac505-581c-4be1-b7d5-3e6b04c1da80.
datanode_4          | 2023-06-30 10:35:41,186 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:41,186 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:43,478 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState] INFO impl.FollowerState: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5016640936ns, electionTimeout:5006ms
datanode_4          | 2023-06-30 10:35:43,479 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: shutdown 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState
datanode_4          | 2023-06-30 10:35:43,480 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-06-30 10:35:43,484 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-06-30 10:35:43,484 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-FollowerState] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: start 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1
datanode_4          | 2023-06-30 10:35:43,495 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO impl.LeaderElection: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:43,495 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO impl.LeaderElection: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_4          | 2023-06-30 10:35:43,498 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO impl.LeaderElection: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:43,498 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO impl.LeaderElection: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-30 10:35:38,079 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-30 10:35:38,080 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-30 10:35:38,080 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-30 10:35:38,081 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:35:38,081 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-30 10:35:38,081 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-30 10:35:38,081 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1: ConfigurationManager, init=-1: peers:[90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-30 10:35:38,081 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-30 10:35:38,082 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 90a80e44-64e8-4ca8-8f98-d749b124e197: addNew group-7C226DD1E4C1:[90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-7C226DD1E4C1:java.util.concurrent.CompletableFuture@8514cd1[Not completed]
datanode_1          | 2023-06-30 10:35:38,082 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-30 10:35:38,082 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-30 10:35:38,082 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-30 10:35:38,082 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-30 10:35:38,083 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-30 10:35:38,083 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-30 10:35:38,083 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-30 10:35:38,084 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-30 10:35:38,085 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-30 10:35:38,085 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-30 10:35:38,085 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-30 10:35:38,086 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-30 10:35:38,086 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-30 10:35:38,087 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/318c2b0f-4198-4a14-b1fd-7c226dd1e4c1 does not exist. Creating ...
datanode_1          | 2023-06-30 10:35:38,089 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/318c2b0f-4198-4a14-b1fd-7c226dd1e4c1/in_use.lock acquired by nodename 7@d7f44b020e87
datanode_1          | 2023-06-30 10:35:38,090 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/318c2b0f-4198-4a14-b1fd-7c226dd1e4c1 has been successfully formatted.
datanode_1          | 2023-06-30 10:35:38,091 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO ratis.ContainerStateMachine: group-7C226DD1E4C1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-30 10:35:38,094 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-30 10:35:38,096 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-30 10:35:38,097 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:38,097 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-30 10:35:38,097 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-30 10:35:38,097 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:38,105 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-30 10:35:38,106 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-30 10:35:38,106 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:38,106 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/318c2b0f-4198-4a14-b1fd-7c226dd1e4c1
datanode_1          | 2023-06-30 10:35:38,106 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-30 10:35:38,107 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:35:38,107 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:38,107 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-30 10:35:38,107 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-30 10:35:38,107 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-30 10:35:38,107 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-30 10:35:38,108 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-30 10:35:38,111 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-30 10:35:38,112 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-30 10:35:38,480 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:35:38,480 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-30 10:35:38,480 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-30 10:35:38,480 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:35:38,481 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-30 10:35:38,482 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1: start as a follower, conf=-1: peers:[90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:38,482 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-30 10:35:38,497 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState
datanode_1          | 2023-06-30 10:35:38,491 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-90a80e44-64e8-4ca8-8f98-d749b124e197: Detected pause in JVM or host machine approximately 0.294s with 0.344s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=344ms
datanode_1          | 2023-06-30 10:35:38,532 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7C226DD1E4C1,id=90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_1          | 2023-06-30 10:35:38,534 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-30 10:35:38,534 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-30 10:35:38,534 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-30 10:35:38,534 [90a80e44-64e8-4ca8-8f98-d749b124e197-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-30 10:35:38,535 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-30 10:35:38,537 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-30 10:35:38,538 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=318c2b0f-4198-4a14-b1fd-7c226dd1e4c1
datanode_1          | 2023-06-30 10:35:38,551 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=318c2b0f-4198-4a14-b1fd-7c226dd1e4c1.
datanode_1          | 2023-06-30 10:35:43,706 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState] INFO impl.FollowerState: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5209809921ns, electionTimeout:5169ms
datanode_1          | 2023-06-30 10:35:43,707 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: shutdown 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState
datanode_1          | 2023-06-30 10:35:43,707 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-30 10:35:43,708 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-30 10:35:43,708 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-FollowerState] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2
datanode_1          | 2023-06-30 10:35:43,717 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:43,717 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-06-30 10:35:43,720 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:43,721 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO impl.LeaderElection: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-30 10:35:43,721 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: shutdown 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2
datanode_1          | 2023-06-30 10:35:43,721 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-30 10:35:43,721 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7C226DD1E4C1 with new leaderId: 90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_1          | 2023-06-30 10:35:43,722 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1: change Leader from null to 90a80e44-64e8-4ca8-8f98-d749b124e197 at term 1 for becomeLeader, leader elected after 5639ms
datanode_1          | 2023-06-30 10:35:43,722 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-30 10:35:43,723 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:35:43,723 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-30 10:35:43,736 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-30 10:35:43,737 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-30 10:35:43,738 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-30 10:35:43,739 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-30 10:35:43,739 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-30 10:35:43,739 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO impl.RoleInfo: 90a80e44-64e8-4ca8-8f98-d749b124e197: start 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderStateImpl
datanode_1          | 2023-06-30 10:35:43,740 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-30 10:35:43,745 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/318c2b0f-4198-4a14-b1fd-7c226dd1e4c1/current/log_inprogress_0
datanode_1          | 2023-06-30 10:35:43,754 [90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1-LeaderElection2] INFO server.RaftServer$Division: 90a80e44-64e8-4ca8-8f98-d749b124e197@group-7C226DD1E4C1: set configuration 0: peers:[90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-30 10:35:58,901 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-90a80e44-64e8-4ca8-8f98-d749b124e197: Detected pause in JVM or host machine approximately 0.374s with 0.419s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=419ms
datanode_1          | 2023-06-30 10:36:04,712 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:37:04,713 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:38:04,714 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:39:04,714 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:40:04,715 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-30 10:41:04,715 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-30 10:35:17,245 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-30 10:35:17,246 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:17,246 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-30 10:35:17,247 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-30 10:35:17,254 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-30 10:35:17,254 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-30 10:35:17,255 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-30 10:35:17,256 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-30 10:35:17,266 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-30 10:35:17,447 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-30 10:35:17,450 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-30 10:35:17,451 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-30 10:35:17,453 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-30 10:35:17,453 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-30 10:35:17,458 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: start as a follower, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:17,458 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-30 10:35:17,458 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState
datanode_5          | 2023-06-30 10:35:17,462 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-84B0A4D85512,id=ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_5          | 2023-06-30 10:35:17,462 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-30 10:35:17,462 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-30 10:35:17,462 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-30 10:35:17,462 [ebc02119-b8bf-46fc-962f-277901a1ccf1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-30 10:35:17,463 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-30 10:35:17,485 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-30 10:35:17,486 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512
datanode_5          | 2023-06-30 10:35:17,501 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-12FECA489596-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/550537c1-df67-480c-bde8-12feca489596/current/log_inprogress_0
datanode_5          | 2023-06-30 10:35:18,719 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512.
datanode_5          | 2023-06-30 10:35:19,154 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-ebc02119-b8bf-46fc-962f-277901a1ccf1: Detected pause in JVM or host machine approximately 0.189s with 0.334s GC time.
datanode_5          | GC pool 'ParNew' had collection(s): count=1 time=334ms
datanode_5          | 2023-06-30 10:35:22,327 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState] INFO impl.FollowerState: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5320980908ns, electionTimeout:5196ms
datanode_5          | 2023-06-30 10:35:22,328 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: shutdown ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState
datanode_5          | 2023-06-30 10:35:22,328 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-06-30 10:35:22,328 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-06-30 10:35:22,328 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-FollowerState] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2
datanode_5          | 2023-06-30 10:35:22,329 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:22,329 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_5          | 2023-06-30 10:35:22,332 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:22,332 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_5          | 2023-06-30 10:35:22,332 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: shutdown ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2
datanode_5          | 2023-06-30 10:35:22,332 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-06-30 10:35:22,332 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E71640CD79B9 with new leaderId: ebc02119-b8bf-46fc-962f-277901a1ccf1
datanode_5          | 2023-06-30 10:35:22,334 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9: change Leader from null to ebc02119-b8bf-46fc-962f-277901a1ccf1 at term 1 for becomeLeader, leader elected after 6457ms
datanode_5          | 2023-06-30 10:35:22,334 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-06-30 10:35:22,344 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-30 10:35:22,346 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5          | 2023-06-30 10:35:22,348 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-06-30 10:35:22,348 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | 2023-06-30 10:35:22,349 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-06-30 10:35:22,349 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-30 10:35:22,349 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-06-30 10:35:22,352 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderStateImpl
datanode_5          | 2023-06-30 10:35:22,353 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-30 10:35:22,355 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b55d9a00-bdf3-4025-8878-e71640cd79b9/current/log_inprogress_0
datanode_5          | 2023-06-30 10:35:22,363 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9-LeaderElection2] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-E71640CD79B9: set configuration 0: peers:[ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:22,488 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO impl.FollowerState: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5029916903ns, electionTimeout:5003ms
datanode_5          | 2023-06-30 10:35:22,488 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: shutdown ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState
datanode_5          | 2023-06-30 10:35:22,489 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-06-30 10:35:22,489 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-06-30 10:35:22,489 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3
datanode_5          | 2023-06-30 10:35:22,504 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:22,509 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-30 10:35:22,510 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-30 10:35:22,546 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode_5          | 2023-06-30 10:35:22,547 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO impl.LeaderElection:   Response 0: ebc02119-b8bf-46fc-962f-277901a1ccf1<-4e17b060-329a-47a0-8048-d67d08b8e752#0:OK-t0
datanode_5          | 2023-06-30 10:35:22,547 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO impl.LeaderElection:   Response 1: ebc02119-b8bf-46fc-962f-277901a1ccf1<-90a80e44-64e8-4ca8-8f98-d749b124e197#0:FAIL-t0
datanode_5          | 2023-06-30 10:35:22,547 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO impl.LeaderElection: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3 PRE_VOTE round 0: result REJECTED
datanode_5          | 2023-06-30 10:35:22,547 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_5          | 2023-06-30 10:35:22,547 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: shutdown ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3
datanode_5          | 2023-06-30 10:35:22,547 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-LeaderElection3] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState
datanode_5          | 2023-06-30 10:35:24,662 [grpc-default-executor-0] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: receive requestVote(ELECTION, 90a80e44-64e8-4ca8-8f98-d749b124e197, group-84B0A4D85512, 1, (t:0, i:0))
datanode_5          | 2023-06-30 10:35:24,698 [grpc-default-executor-0] INFO impl.VoteContext: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FOLLOWER: accept ELECTION from 90a80e44-64e8-4ca8-8f98-d749b124e197: our priority 0 <= candidate's priority 1
datanode_5          | 2023-06-30 10:35:24,698 [grpc-default-executor-0] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_5          | 2023-06-30 10:35:24,698 [grpc-default-executor-0] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: shutdown ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState
datanode_5          | 2023-06-30 10:35:24,698 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState] INFO impl.FollowerState: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState was interrupted
datanode_5          | 2023-06-30 10:35:24,698 [grpc-default-executor-0] INFO impl.RoleInfo: ebc02119-b8bf-46fc-962f-277901a1ccf1: start ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FollowerState
datanode_5          | 2023-06-30 10:35:24,731 [grpc-default-executor-2] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: receive requestVote(PRE_VOTE, 90a80e44-64e8-4ca8-8f98-d749b124e197, group-84B0A4D85512, 0, (t:0, i:0))
datanode_5          | 2023-06-30 10:35:24,757 [grpc-default-executor-0] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512 replies to ELECTION vote request: 90a80e44-64e8-4ca8-8f98-d749b124e197<-ebc02119-b8bf-46fc-962f-277901a1ccf1#0:OK-t1. Peer's state: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512:t1, leader=null, voted=90a80e44-64e8-4ca8-8f98-d749b124e197, raftlog=Memoized:ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:24,757 [grpc-default-executor-2] INFO impl.VoteContext: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-FOLLOWER: accept PRE_VOTE from 90a80e44-64e8-4ca8-8f98-d749b124e197: our priority 0 <= candidate's priority 1
datanode_5          | 2023-06-30 10:35:24,757 [grpc-default-executor-2] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512 replies to PRE_VOTE vote request: 90a80e44-64e8-4ca8-8f98-d749b124e197<-ebc02119-b8bf-46fc-962f-277901a1ccf1#0:OK-t1. Peer's state: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512:t1, leader=null, voted=90a80e44-64e8-4ca8-8f98-d749b124e197, raftlog=Memoized:ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:25,182 [ebc02119-b8bf-46fc-962f-277901a1ccf1-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-84B0A4D85512 with new leaderId: 90a80e44-64e8-4ca8-8f98-d749b124e197
datanode_5          | 2023-06-30 10:35:25,206 [ebc02119-b8bf-46fc-962f-277901a1ccf1-server-thread1] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: change Leader from null to 90a80e44-64e8-4ca8-8f98-d749b124e197 at term 1 for appendEntries, leader elected after 8017ms
datanode_5          | 2023-06-30 10:35:25,282 [ebc02119-b8bf-46fc-962f-277901a1ccf1-server-thread2] INFO server.RaftServer$Division: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512: set configuration 0: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:0|startupRole:FOLLOWER, 90a80e44-64e8-4ca8-8f98-d749b124e197|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER, ebc02119-b8bf-46fc-962f-277901a1ccf1|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-30 10:35:25,286 [ebc02119-b8bf-46fc-962f-277901a1ccf1-server-thread2] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-30 10:35:25,291 [ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ebc02119-b8bf-46fc-962f-277901a1ccf1@group-84B0A4D85512-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d0ff19ef-ebe8-48c0-a396-84b0a4d85512/current/log_inprogress_0
datanode_5          | 2023-06-30 10:36:04,682 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-30 10:37:04,683 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-30 10:38:04,683 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-30 10:39:04,684 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-30 10:40:04,684 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-30 10:41:04,684 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:35:43,498 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: shutdown 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1
datanode_4          | 2023-06-30 10:35:43,498 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_4          | 2023-06-30 10:35:43,499 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3E6B04C1DA80 with new leaderId: 4e17b060-329a-47a0-8048-d67d08b8e752
datanode_4          | 2023-06-30 10:35:43,525 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80: change Leader from null to 4e17b060-329a-47a0-8048-d67d08b8e752 at term 1 for becomeLeader, leader elected after 5194ms
datanode_4          | 2023-06-30 10:35:43,527 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4          | 2023-06-30 10:35:43,537 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-06-30 10:35:43,540 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-06-30 10:35:43,550 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_4          | 2023-06-30 10:35:43,550 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-06-30 10:35:43,550 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4          | 2023-06-30 10:35:43,571 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-06-30 10:35:43,577 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_4          | 2023-06-30 10:35:43,582 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO impl.RoleInfo: 4e17b060-329a-47a0-8048-d67d08b8e752: start 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderStateImpl
datanode_4          | 2023-06-30 10:35:43,585 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-30 10:35:43,588 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bd0ac505-581c-4be1-b7d5-3e6b04c1da80/current/log_inprogress_0
datanode_4          | 2023-06-30 10:35:43,601 [4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80-LeaderElection1] INFO server.RaftServer$Division: 4e17b060-329a-47a0-8048-d67d08b8e752@group-3E6B04C1DA80: set configuration 0: peers:[4e17b060-329a-47a0-8048-d67d08b8e752|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-30 10:35:46,274 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:46,274 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:51,339 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:51,340 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:35:56,385 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:35:56,385 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:01,483 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:01,483 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:04,681 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:36:06,544 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:06,544 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:11,721 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:11,722 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:16,780 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:16,780 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:21,964 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:21,965 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:27,129 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:27,130 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:32,237 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:32,237 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:37,433 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:37,434 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:42,444 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:42,444 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:47,616 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:47,616 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:52,816 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:52,817 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:36:58,001 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:36:58,001 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:03,035 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:03,036 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:04,682 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:37:08,216 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:08,216 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:13,244 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:13,244 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:18,250 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:18,250 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:23,354 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:23,354 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:28,518 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:28,518 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:33,596 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:33,596 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:38,754 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:38,754 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:43,901 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:43,901 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:49,034 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:49,035 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:54,104 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:54,104 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:37:59,117 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:37:59,117 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:04,317 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:04,317 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:04,687 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:38:09,464 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:09,464 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:14,485 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:14,486 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:19,569 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:19,569 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:24,715 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:24,716 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:29,883 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:29,883 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:35,024 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:35,024 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:40,060 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:40,061 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:45,175 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:45,175 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:50,206 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:50,207 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:38:55,367 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:38:55,367 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:00,411 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:00,412 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:04,688 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:39:05,545 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:05,545 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:10,636 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:10,637 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:15,669 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:15,669 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:20,681 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:20,681 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:25,730 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:25,731 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:30,864 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:30,864 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:36,048 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:36,049 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:41,124 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:41,124 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:46,162 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:46,163 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:51,199 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:51,199 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:39:56,252 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:39:56,252 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:01,337 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:01,337 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:04,688 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:40:06,526 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:06,526 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:11,685 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:11,686 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:16,727 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:16,727 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:21,862 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:21,862 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:26,965 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:26,965 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:32,092 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:32,093 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:37,122 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:37,122 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:42,148 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:42,149 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:47,253 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:47,253 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:52,417 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:52,417 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:40:57,552 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:40:57,553 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:41:02,687 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:41:02,687 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:41:04,689 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-30 10:41:07,752 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:41:07,752 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:41:12,772 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:41:12,772 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:41:17,836 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:41:17,836 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:41:22,929 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:41:22,929 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:41:27,991 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:41:27,991 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-30 10:41:33,128 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-30 10:41:33,128 [4e17b060-329a-47a0-8048-d67d08b8e752@group-12FECA489596-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:34:16,473 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 59a9ab9523a8/172.24.0.10
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om_1                | STARTUP_MSG:   java = 11.0.19
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-30 10:34:16,144 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 040bd4163ed4/172.24.0.14
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-30 10:34:16,530 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:34:23,418 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-30 10:34:26,588 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:34:26,961 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.10:9862
om_1                | 2023-06-30 10:34:26,967 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:34:26,974 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:34:27,228 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:34:28,333 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863]
om_1                | 2023-06-30 10:34:31,735 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-06-30 10:34:33,738 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-06-30 10:34:35,744 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-06-30 10:34:37,746 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-06-30 10:34:39,748 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-06-30 10:34:41,751 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
recon_1             | STARTUP_MSG:   java = 11.0.19
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
recon_1             | 2023-06-30 10:34:16,186 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-30 10:34:20,545 [main] INFO reflections.Reflections: Reflections took 561 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1             | 2023-06-30 10:34:25,415 [main] INFO reflections.Reflections: Reflections took 417 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1             | 2023-06-30 10:34:25,970 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-30 10:34:29,026 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:34:36,855 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-30 10:34:39,052 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-30 10:34:39,060 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.007 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-30 10:34:39,424 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-30 10:34:39,588 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-30 10:34:39,594 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-30 10:34:42,983 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-30 10:34:43,035 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-30 10:34:43,127 [main] INFO util.log: Logging initialized @38024ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-30 10:34:43,711 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-30 10:34:43,765 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-06-30 10:34:43,825 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-30 10:34:43,863 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-30 10:34:43,864 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-30 10:34:43,864 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-30 10:34:44,260 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-06-30 10:34:44,293 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-30 10:34:45,480 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-30 10:34:45,523 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-06-30 10:34:45,554 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-30 10:34:45,742 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-30 10:34:45,742 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-30 10:34:48,648 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:34:49,467 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:34:49,700 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-06-30 10:34:49,703 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-30 10:34:49,807 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:34:50,073 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-06-30 10:34:50,113 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-30 10:34:50,171 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-30 10:34:50,186 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-30 10:34:50,210 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-06-30 10:34:50,817 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-30 10:34:50,877 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-30 10:34:50,957 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1             | 2023-06-30 10:34:50,967 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-30 10:34:51,074 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-30 10:34:51,290 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-06-30 10:34:51,296 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-30 10:34:51,296 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-30 10:34:51,456 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-30 10:34:51,478 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-30 10:34:51,478 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-30 10:34:52,150 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-30 10:34:52,151 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-06-30 10:34:52,218 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-30 10:34:52,219 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-30 10:34:52,228 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-30 10:34:52,273 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@404566e7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-30 10:34:14,890 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-30 10:34:14,919 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-30 10:34:15,146 [main] INFO util.log: Logging initialized @11976ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-30 10:34:16,078 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-06-30 10:34:16,230 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-30 10:34:16,269 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-30 10:34:16,316 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-30 10:34:16,328 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-30 10:34:16,329 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-30 10:34:16,688 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir15842087214846837891
s3g_1               | 2023-06-30 10:34:17,381 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = a59fce3251e7/172.24.0.4
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
s3g_1               | STARTUP_MSG:   java = 11.0.19
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir15842087214846837891, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1               | ************************************************************/
s3g_1               | 2023-06-30 10:34:17,427 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-30 10:34:17,541 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-30 10:34:18,151 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-06-30 10:34:19,076 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-06-30 10:34:19,077 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-30 10:34:19,375 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-30 10:34:19,412 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-06-30 10:34:19,391 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1               | 2023-06-30 10:34:19,638 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-30 10:34:19,651 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-30 10:34:19,657 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-06-30 10:34:19,755 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e33c391{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-30 10:34:19,756 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@20312893{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1               | 2023-06-30 10:34:41,247 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@186295cc{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir15842087214846837891/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-7300276023217989931/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1               | 2023-06-30 10:34:41,307 [main] INFO server.AbstractConnector: Started ServerConnector@106faf11{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-30 10:34:41,330 [main] INFO server.Server: Started @38159ms
s3g_1               | 2023-06-30 10:34:41,332 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-06-30 10:34:41,340 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-06-30 10:34:41,341 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
om_1                | 2023-06-30 10:34:43,752 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-06-30 10:34:45,754 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-06-30 10:34:47,756 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-06-30 10:34:49,757 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-06-30 10:34:51,759 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-06-30 10:34:53,762 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-06-30 10:34:55,764 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 59a9ab9523a8/172.24.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-06-30 10:34:58,935 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9767da7b-2d7b-4e9f-b157-e24f4bffa08e is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-06-30 10:35:00,949 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9767da7b-2d7b-4e9f-b157-e24f4bffa08e is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1                | 2023-06-30 10:35:02,954 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9767da7b-2d7b-4e9f-b157-e24f4bffa08e is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 2023-06-30 10:34:52,276 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3d763ae5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-30 10:34:55,958 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1631da37{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-3616115989886177951/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-06-30 10:34:55,977 [main] INFO server.AbstractConnector: Started ServerConnector@4c5c0306{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-30 10:34:55,979 [main] INFO server.Server: Started @50874ms
recon_1             | 2023-06-30 10:34:55,981 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-30 10:34:55,981 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-30 10:34:55,983 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-30 10:34:55,995 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-30 10:34:56,012 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-30 10:34:56,017 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-30 10:34:56,017 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-30 10:34:56,018 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-30 10:34:56,018 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-30 10:34:56,019 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:34:59,105 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9767da7b-2d7b-4e9f-b157-e24f4bffa08e is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-06-30 10:35:01,112 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9767da7b-2d7b-4e9f-b157-e24f4bffa08e is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
recon_1             | 2023-06-30 10:35:04,899 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-30 10:35:04,899 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-30 10:35:04,899 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-30 10:35:04,905 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-30 10:35:04,910 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-30 10:35:04,938 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-30 10:35:05,939 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:34964 / 172.24.0.15:34964: output error
recon_1             | 2023-06-30 10:35:05,943 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:39106 / 172.24.0.7:39106: output error
recon_1             | 2023-06-30 10:35:05,943 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:55866 / 172.24.0.8:55866: output error
recon_1             | 2023-06-30 10:35:05,943 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:48966 / 172.24.0.15:48966: output error
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-028270cf-30a6-4025-a7a5-8f2c0855d65a;layoutVersion=6
om_1                | 2023-06-30 10:35:05,569 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 59a9ab9523a8/172.24.0.10
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-30 10:35:11,554 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 59a9ab9523a8/172.24.0.10
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:34:15,661 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 391d213fcf60/172.24.0.2
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 2023-06-30 10:35:05,944 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:34726 / 172.24.0.11:34726: output error
recon_1             | 2023-06-30 10:35:05,944 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:45528 / 172.24.0.7:45528: output error
recon_1             | 2023-06-30 10:35:05,951 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,953 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,953 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,953 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,954 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:34:15,818 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:34:17,309 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:34:20,363 [main] INFO reflections.Reflections: Reflections took 2254 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1               | 2023-06-30 10:34:22,023 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:34:22,208 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:34:25,051 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-30 10:34:26,426 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:34:26,600 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:34:26,600 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:34:26,608 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:34:26,608 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-30 10:34:26,608 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-30 10:34:26,609 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-30 10:34:26,734 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:34:26,748 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-30 10:34:26,748 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:34:27,009 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-30 10:34:27,041 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-30 10:34:27,084 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-30 10:34:31,197 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-30 10:34:31,279 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-30 10:34:31,314 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-30 10:34:31,319 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:34:31,319 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:34:31,363 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:34:31,532 [main] INFO server.RaftServer: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: addNew group-8F2C0855D65A:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|priority:0|startupRole:FOLLOWER] returns group-8F2C0855D65A:java.util.concurrent.CompletableFuture@dc4a691[Not completed]
scm_1               | 2023-06-30 10:34:31,856 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: new RaftServerImpl for group-8F2C0855D65A:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-30 10:34:31,880 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-30 10:34:31,895 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-30 10:34:31,895 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-30 10:34:31,896 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:34:31,899 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:34:31,903 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-30 10:34:32,036 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: ConfigurationManager, init=-1: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-30 10:34:32,039 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:34:32,181 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-30 10:34:32,191 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-30 10:34:32,511 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-30 10:34:32,551 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-30 10:34:32,598 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-30 10:34:32,659 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-30 10:34:32,944 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-30 10:34:33,041 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-30 10:34:34,875 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:34:34,933 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-30 10:34:34,964 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-30 10:34:34,967 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-30 10:34:34,991 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-30 10:34:35,000 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-30 10:34:35,016 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a does not exist. Creating ...
scm_1               | 2023-06-30 10:34:35,065 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/in_use.lock acquired by nodename 12@391d213fcf60
scm_1               | 2023-06-30 10:34:35,136 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a has been successfully formatted.
scm_1               | 2023-06-30 10:34:35,210 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-30 10:34:35,325 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-30 10:34:35,340 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,954 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,955 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:54750 / 172.24.0.12:54750: output error
recon_1             | 2023-06-30 10:35:05,955 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,957 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:49234 / 172.24.0.11:49234: output error
recon_1             | 2023-06-30 10:35:05,958 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:37690 / 172.24.0.12:37690: output error
recon_1             | 2023-06-30 10:35:05,958 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,961 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-06-30 10:34:35,341 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-30 10:34:35,361 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-30 10:34:35,399 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:34:35,471 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-30 10:34:35,477 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-30 10:34:35,483 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:34:35,511 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a
scm_1               | 2023-06-30 10:34:35,555 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:34:35,564 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:34:35,565 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:34:35,588 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-30 10:34:35,600 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-30 10:34:35,600 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-30 10:34:35,619 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-30 10:34:35,623 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-30 10:34:35,768 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-30 10:34:35,840 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:34:36,014 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-30 10:34:36,019 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-30 10:34:36,036 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-30 10:34:36,203 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:34:36,205 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:34:36,260 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: start as a follower, conf=-1: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:34:36,280 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-06-30 10:34:36,288 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState
scm_1               | 2023-06-30 10:34:36,370 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-30 10:34:36,376 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-30 10:34:36,396 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8F2C0855D65A,id=9767da7b-2d7b-4e9f-b157-e24f4bffa08e
scm_1               | 2023-06-30 10:34:36,440 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-30 10:34:36,470 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-30 10:34:36,501 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-30 10:34:36,526 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-30 10:34:36,659 [main] INFO server.RaftServer: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start RPC server
scm_1               | 2023-06-30 10:34:37,594 [main] INFO server.GrpcService: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: GrpcService started, listening on 9894
scm_1               | 2023-06-30 10:34:37,628 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9767da7b-2d7b-4e9f-b157-e24f4bffa08e: Started
scm_1               | 2023-06-30 10:34:41,492 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO impl.FollowerState: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5204087732ns, electionTimeout:5110ms
scm_1               | 2023-06-30 10:34:41,494 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: shutdown 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState
scm_1               | 2023-06-30 10:34:41,494 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-06-30 10:34:41,497 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-30 10:34:41,498 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:05,965 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:51268 / 172.24.0.8:51268: output error
recon_1             | 2023-06-30 10:35:05,965 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-30 10:35:06,624 [IPC Server handler 6 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9ca9d3f5-c23a-4767-9163-086f75187a71
recon_1             | 2023-06-30 10:35:06,661 [IPC Server handler 6 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 9ca9d3f5-c23a-4767-9163-086f75187a71{ip: 172.24.0.12, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:35:06,898 [IPC Server handler 9 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f5a485b8-4096-468e-a11f-083e698072ab
recon_1             | 2023-06-30 10:35:06,902 [IPC Server handler 9 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f5a485b8-4096-468e-a11f-083e698072ab{ip: 172.24.0.11, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:35:07,021 [IPC Server handler 80 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/90a80e44-64e8-4ca8-8f98-d749b124e197
recon_1             | 2023-06-30 10:35:07,022 [IPC Server handler 80 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 90a80e44-64e8-4ca8-8f98-d749b124e197{ip: 172.24.0.8, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:35:07,235 [IPC Server handler 6 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4e17b060-329a-47a0-8048-d67d08b8e752
recon_1             | 2023-06-30 10:35:07,235 [IPC Server handler 6 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 4e17b060-329a-47a0-8048-d67d08b8e752{ip: 172.24.0.7, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:35:07,344 [IPC Server handler 9 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ebc02119-b8bf-46fc-962f-277901a1ccf1
recon_1             | 2023-06-30 10:35:07,344 [IPC Server handler 9 on default port 9891] INFO node.SCMNodeManager: Registered Data node : ebc02119-b8bf-46fc-962f-277901a1ccf1{ip: 172.24.0.15, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-30 10:35:07,624 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 9ca9d3f5-c23a-4767-9163-086f75187a71 to Node DB.
recon_1             | 2023-06-30 10:35:07,642 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f5a485b8-4096-468e-a11f-083e698072ab to Node DB.
recon_1             | 2023-06-30 10:35:07,644 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 90a80e44-64e8-4ca8-8f98-d749b124e197 to Node DB.
recon_1             | 2023-06-30 10:35:07,648 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 4e17b060-329a-47a0-8048-d67d08b8e752 to Node DB.
recon_1             | 2023-06-30 10:35:07,648 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node ebc02119-b8bf-46fc-962f-277901a1ccf1 to Node DB.
recon_1             | 2023-06-30 10:35:09,468 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=550537c1-df67-480c-bde8-12feca489596. Trying to get from SCM.
recon_1             | 2023-06-30 10:35:09,531 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 550537c1-df67-480c-bde8-12feca489596, Nodes: ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.025Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:35:09,825 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=550537c1-df67-480c-bde8-12feca489596 reported by ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)
recon_1             | 2023-06-30 10:35:10,450 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc. Trying to get from SCM.
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
om_1                | STARTUP_MSG:   java = 11.0.19
scm_1               | 2023-06-30 10:34:41,559 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:34:41,592 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-06-30 10:34:41,628 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:34:41,632 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-06-30 10:34:41,632 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: shutdown 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1
scm_1               | 2023-06-30 10:34:41,633 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-06-30 10:34:41,643 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: change Leader from null to 9767da7b-2d7b-4e9f-b157-e24f4bffa08e at term 1 for becomeLeader, leader elected after 9139ms
scm_1               | 2023-06-30 10:34:41,683 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-30 10:34:41,723 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:34:41,724 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:34:41,837 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-30 10:34:41,956 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-30 10:34:41,958 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-30 10:34:42,050 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:34:42,101 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-30 10:34:42,291 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderStateImpl
scm_1               | 2023-06-30 10:34:42,582 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-30 10:34:43,116 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: set configuration 0: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:34:43,831 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/current/log_inprogress_0
scm_1               | 2023-06-30 10:34:45,696 [main] INFO server.RaftServer: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: close
scm_1               | 2023-06-30 10:34:45,700 [main] INFO server.GrpcService: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-30 10:34:45,703 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: shutdown
scm_1               | 2023-06-30 10:34:45,712 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8F2C0855D65A,id=9767da7b-2d7b-4e9f-b157-e24f4bffa08e
scm_1               | 2023-06-30 10:34:45,716 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: shutdown 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderStateImpl
scm_1               | 2023-06-30 10:34:45,738 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO impl.PendingRequests: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-30 10:34:45,788 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO impl.StateMachineUpdater: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-30 10:34:45,788 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO impl.StateMachineUpdater: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-30 10:34:45,791 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO impl.StateMachineUpdater: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-30 10:34:45,849 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: closes. applyIndex: 0
scm_1               | 2023-06-30 10:34:45,878 [main] INFO server.GrpcService: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-30 10:34:45,957 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker close()
scm_1               | 2023-06-30 10:34:45,959 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9767da7b-2d7b-4e9f-b157-e24f4bffa08e: Stopped
scm_1               | 2023-06-30 10:34:45,959 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:34:45,970 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-028270cf-30a6-4025-a7a5-8f2c0855d65a; layoutVersion=7; scmId=9767da7b-2d7b-4e9f-b157-e24f4bffa08e
scm_1               | 2023-06-30 10:34:46,059 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
recon_1             | 2023-06-30 10:35:10,473 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc, Nodes: 9ca9d3f5-c23a-4767-9163-086f75187a71(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.553Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:35:10,474 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc reported by 9ca9d3f5-c23a-4767-9163-086f75187a71(xcompat_datanode_3.xcompat_default/172.24.0.12)
recon_1             | 2023-06-30 10:35:10,935 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=0414c739-ceb6-4658-9323-e09d08f34c78. Trying to get from SCM.
recon_1             | 2023-06-30 10:35:10,954 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 0414c739-ceb6-4658-9323-e09d08f34c78, Nodes: f5a485b8-4096-468e-a11f-083e698072ab(xcompat_datanode_2.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.883Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:35:10,961 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=0414c739-ceb6-4658-9323-e09d08f34c78 reported by f5a485b8-4096-468e-a11f-083e698072ab(xcompat_datanode_2.xcompat_default/172.24.0.11)
recon_1             | 2023-06-30 10:35:12,952 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=550537c1-df67-480c-bde8-12feca489596 reported by 90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)
recon_1             | 2023-06-30 10:35:15,109 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=550537c1-df67-480c-bde8-12feca489596 reported by 4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7)
recon_1             | 2023-06-30 10:35:15,247 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=550537c1-df67-480c-bde8-12feca489596 reported by ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)
recon_1             | 2023-06-30 10:35:16,901 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b55d9a00-bdf3-4025-8878-e71640cd79b9. Trying to get from SCM.
recon_1             | 2023-06-30 10:35:16,941 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b55d9a00-bdf3-4025-8878-e71640cd79b9, Nodes: ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:ebc02119-b8bf-46fc-962f-277901a1ccf1, CreationTimestamp2023-06-30T10:35:06.039Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:35:16,943 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=b55d9a00-bdf3-4025-8878-e71640cd79b9 reported by ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)
recon_1             | 2023-06-30 10:35:17,218 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512. Trying to get from SCM.
recon_1             | 2023-06-30 10:35:17,226 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d0ff19ef-ebe8-48c0-a396-84b0a4d85512, Nodes: 4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7)90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.050Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:35:17,228 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 reported by ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)
recon_1             | 2023-06-30 10:35:17,766 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 reported by 4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7)
recon_1             | 2023-06-30 10:35:18,090 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 reported by 90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)
recon_1             | 2023-06-30 10:35:22,340 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 reported by ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)
recon_1             | 2023-06-30 10:35:24,628 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 reported by 90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)
recon_1             | 2023-06-30 10:35:38,471 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=bd0ac505-581c-4be1-b7d5-3e6b04c1da80. Trying to get from SCM.
recon_1             | 2023-06-30 10:35:38,497 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: bd0ac505-581c-4be1-b7d5-3e6b04c1da80, Nodes: 4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4e17b060-329a-47a0-8048-d67d08b8e752, CreationTimestamp2023-06-30T10:35:05.587Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:35:38,592 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=318c2b0f-4198-4a14-b1fd-7c226dd1e4c1. Trying to get from SCM.
recon_1             | 2023-06-30 10:35:38,598 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 318c2b0f-4198-4a14-b1fd-7c226dd1e4c1, Nodes: 90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:90a80e44-64e8-4ca8-8f98-d749b124e197, CreationTimestamp2023-06-30T10:35:06.058Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-30 10:35:56,018 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-30 10:35:56,019 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-30 10:35:57,337 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688121356019
recon_1             | 2023-06-30 10:35:57,368 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-06-30 10:35:58,391 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688121356019.
recon_1             | 2023-06-30 10:35:58,487 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_5.xcompat_default.
recon_1             | 2023-06-30 10:35:58,674 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-30 10:35:58,895 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-30 10:36:00,153 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1             | 2023-06-30 10:36:00,168 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-30 10:36:00,169 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-30 10:36:00,181 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:36:00,182 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-30 10:36:00,182 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-30 10:36:00,191 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-30 10:36:00,191 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.009 seconds to process 0 keys.
recon_1             | 2023-06-30 10:36:00,225 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-30 10:36:00,228 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-30 10:36:05,164 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-30 10:36:05,164 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-30 10:36:05,171 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-30 10:36:05,171 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-30 10:36:05,174 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1             | 2023-06-30 10:36:05,200 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1             | 2023-06-30 10:36:05,200 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-06-30 10:36:05,205 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 36 milliseconds.
recon_1             | 2023-06-30 10:36:05,218 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-06-30 10:36:05,288 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 98 milliseconds to process 0 existing database records.
recon_1             | 2023-06-30 10:36:05,318 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 30 milliseconds for processing 1 containers.
recon_1             | 2023-06-30 10:36:13,839 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_5.xcompat_default.
recon_1             | 2023-06-30 10:36:13,843 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=93722aea-ab79-4635-a891-312fc5625891 not found. Cannot add container #2
recon_1             | 2023-06-30 10:36:13,844 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-30 10:36:13,902 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_4.xcompat_default.
recon_1             | 2023-06-30 10:36:13,921 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=93722aea-ab79-4635-a891-312fc5625891 not found. Cannot add container #2
recon_1             | 2023-06-30 10:36:13,932 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-30 10:36:13,926 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-30 10:36:13,952 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconContainerManager: Pipeline PipelineID=93722aea-ab79-4635-a891-312fc5625891 not found. Cannot add container #2
recon_1             | 2023-06-30 10:36:13,955 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-30 10:36:14,470 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-30 10:36:14,475 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=93722aea-ab79-4635-a891-312fc5625891 not found. Cannot add container #2
recon_1             | 2023-06-30 10:36:14,475 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-30 10:36:14,539 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-30 10:36:14,544 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=93722aea-ab79-4635-a891-312fc5625891 not found. Cannot add container #2
recon_1             | 2023-06-30 10:36:14,544 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-30 10:37:05,235 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1             | 2023-06-30 10:37:05,259 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:37:05,260 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 24
recon_1             | 2023-06-30 10:37:39,525 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_5.xcompat_default.
recon_1             | 2023-06-30 10:37:39,554 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-30 10:37:39,561 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-30 10:37:39,567 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-30 10:38:05,280 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:38:05,280 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 20
recon_1             | 2023-06-30 10:39:05,281 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:39:05,282 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-30 10:40:05,283 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-30 10:40:05,283 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-30 10:41:05,218 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1             | 2023-06-30 10:41:05,225 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=93722aea-ab79-4635-a891-312fc5625891 from SCM.
recon_1             | 2023-06-30 10:41:05,229 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 16 milliseconds.
recon_1             | 2023-06-30 10:41:05,283 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-30 10:35:11,613 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-30 10:35:14,571 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-30 10:35:16,319 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-30 10:35:16,602 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.10:9862
om_1                | 2023-06-30 10:35:16,604 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-30 10:35:16,604 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-30 10:35:16,807 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:35:17,193 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-06-30 10:35:18,234 [main] INFO reflections.Reflections: Reflections took 710 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om_1                | 2023-06-30 10:35:18,274 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-06-30 10:35:18,411 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:35:19,405 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863]
om_1                | 2023-06-30 10:35:19,548 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863]
om_1                | 2023-06-30 10:35:20,672 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-06-30 10:35:20,724 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:35:21,029 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-06-30 10:35:21,792 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-30 10:35:21,907 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-06-30 10:35:21,927 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-30 10:35:22,012 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-06-30 10:35:22,013 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-06-30 10:35:22,315 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-30 10:35:22,566 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:35:22,572 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-30 10:35:22,622 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-30 10:35:22,637 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-30 10:35:22,692 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-30 10:35:22,740 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-30 10:35:22,830 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-30 10:35:22,853 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-30 10:35:22,855 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-30 10:35:22,855 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-30 10:35:22,856 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-30 10:35:22,856 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-30 10:35:22,856 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-30 10:35:22,856 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-30 10:35:22,858 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:35:22,858 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-30 10:35:22,859 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-30 10:35:22,869 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-30 10:35:22,871 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-30 10:35:22,871 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-30 10:35:23,370 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-30 10:35:23,376 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-30 10:35:23,377 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-30 10:35:23,377 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:35:23,378 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:35:23,383 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:35:23,431 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@74a15f4[Not completed]
om_1                | 2023-06-30 10:35:23,431 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-30 10:35:23,459 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-30 10:35:23,482 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-30 10:35:23,484 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-30 10:35:23,485 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-30 10:35:23,485 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-30 10:35:23,485 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-30 10:35:23,485 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-30 10:35:23,489 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-30 10:35:23,511 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-30 10:35:23,515 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-30 10:35:23,534 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-30 10:35:23,548 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-30 10:35:23,654 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-30 10:35:23,667 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-06-30 10:35:23,734 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-30 10:35:23,734 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-30 10:35:24,002 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-06-30 10:35:24,506 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-30 10:35:24,543 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-30 10:35:24,563 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-30 10:35:24,589 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-30 10:35:24,606 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-30 10:35:24,606 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-30 10:35:26,283 [main] INFO reflections.Reflections: Reflections took 2509 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om_1                | 2023-06-30 10:35:27,025 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-30 10:35:27,064 [main] INFO ipc.Server: Listener at om:9862
om_1                | 2023-06-30 10:35:27,068 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-30 10:35:28,243 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-30 10:35:28,268 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-30 10:35:28,268 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-30 10:35:28,340 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.24.0.10:9862
recon_1             | 2023-06-30 10:41:05,283 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-30 10:41:05,319 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
recon_1             | 2023-06-30 10:41:05,322 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 3 milliseconds for processing 2 containers.
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 391d213fcf60/172.24.0.2
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-30 10:34:52,014 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 391d213fcf60/172.24.0.2
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | 2023-06-30 10:35:28,340 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-30 10:35:28,343 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-30 10:35:28,350 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@59a9ab9523a8
om_1                | 2023-06-30 10:35:28,356 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-30 10:35:28,361 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-30 10:35:28,375 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-30 10:35:28,376 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:35:28,379 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-30 10:35:28,381 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-30 10:35:28,384 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:35:28,391 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-30 10:35:28,391 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-30 10:35:28,391 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:35:28,396 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-30 10:35:28,397 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:35:28,397 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-30 10:35:28,398 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-30 10:35:28,399 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-30 10:35:28,400 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-30 10:35:28,401 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-30 10:35:28,402 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-30 10:35:28,404 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-30 10:35:28,411 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-30 10:35:28,412 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-30 10:35:28,421 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-30 10:35:28,421 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-30 10:35:28,421 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-30 10:35:28,434 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:35:28,434 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-30 10:35:28,437 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:35:28,438 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-30 10:35:28,441 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:35:28,450 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-30 10:35:28,457 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-30 10:35:28,458 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-30 10:35:28,460 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-30 10:35:28,461 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-30 10:35:28,461 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-30 10:35:28,463 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-30 10:35:28,486 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-30 10:35:28,633 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-30 10:35:28,637 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-30 10:35:28,644 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-30 10:35:28,754 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-30 10:35:28,756 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-30 10:35:28,859 [main] INFO util.log: Logging initialized @22743ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-30 10:35:29,132 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-30 10:35:29,150 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-30 10:35:29,166 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-30 10:35:29,171 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-30 10:35:29,171 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-30 10:35:29,172 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-30 10:35:29,265 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1                | 2023-06-30 10:35:29,274 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-30 10:35:29,276 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-06-30 10:35:29,326 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-30 10:35:29,327 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-30 10:35:29,329 [main] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-06-30 10:35:29,344 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e2b679e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-30 10:35:29,345 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@470467b0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-30 10:35:29,565 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7993ec00{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-16863420720480043384/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-06-30 10:35:29,585 [main] INFO server.AbstractConnector: Started ServerConnector@4cf7e832{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-30 10:35:29,585 [main] INFO server.Server: Started @23469ms
om_1                | 2023-06-30 10:35:29,596 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-30 10:35:29,596 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-30 10:35:29,598 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-30 10:35:29,605 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-30 10:35:29,609 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-30 10:35:29,811 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-30 10:35:30,058 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-06-30 10:35:33,621 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5180383718ns, electionTimeout:5162ms
om_1                | 2023-06-30 10:35:33,622 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-30 10:35:33,622 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-30 10:35:33,625 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-06-30 10:35:33,625 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:35:33,627 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:35:33,628 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-06-30 10:35:33,630 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:35:33,630 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-30 10:35:33,630 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-30 10:35:33,630 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-30 10:35:33,633 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 9977ms
om_1                | 2023-06-30 10:35:33,639 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-30 10:35:33,642 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:35:33,643 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-30 10:35:33,647 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-30 10:35:33,647 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-30 10:35:33,647 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-30 10:35:33,652 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-30 10:35:33,654 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-30 10:35:33,655 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-30 10:35:33,709 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-30 10:35:33,733 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-30 10:35:33,820 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-30 10:35:33,921 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-30 10:35:35,841 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:volnhxwy for user:hadoop
om_1                | 2023-06-30 10:35:40,988 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: default of layout LEGACY in volume: volnhxwy
om_1                | 2023-06-30 10:35:45,890 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ratis of layout LEGACY in volume: volnhxwy
om_1                | 2023-06-30 10:35:50,357 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ecbucket of layout LEGACY in volume: volnhxwy
om_1                | 2023-06-30 10:35:56,767 [qtp2089378474-54] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-06-30 10:35:57,127 [qtp2089378474-54] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1688121357095 in 31 milliseconds
om_1                | 2023-06-30 10:35:57,146 [qtp2089378474-54] INFO db.RDBCheckpointUtils: Waited for 16 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1688121357095 availability.
om_1                | 2023-06-30 10:35:57,205 [qtp2089378474-54] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 57 milliseconds
om_1                | 2023-06-30 10:35:57,205 [qtp2089378474-54] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
om_1                | 2023-06-30 10:35:57,206 [qtp2089378474-54] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688121357095
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/134d141263bbc7b9037ac995ee74f88efd7a9b71 ; compiled by 'runner' on 2023-06-30T09:41Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-30 10:34:52,044 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-30 10:34:52,123 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:34:52,395 [main] INFO reflections.Reflections: Reflections took 222 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1               | 2023-06-30 10:34:52,552 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-30 10:34:52,562 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-30 10:34:53,706 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:34:54,037 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-30 10:34:54,486 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1               | 2023-06-30 10:34:54,487 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-30 10:34:54,627 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-30 10:34:54,872 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:9767da7b-2d7b-4e9f-b157-e24f4bffa08e
scm_1               | 2023-06-30 10:34:55,038 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-30 10:34:55,059 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:34:55,065 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:34:55,065 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-30 10:34:55,066 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-30 10:34:55,066 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-30 10:34:55,066 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-30 10:34:55,066 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-30 10:34:55,068 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:34:55,071 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-30 10:34:55,073 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:34:55,085 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-30 10:34:55,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-30 10:34:55,094 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-30 10:34:55,451 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-30 10:34:55,453 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-30 10:34:55,460 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-30 10:34:55,460 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:34:55,460 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:34:55,490 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:34:55,510 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: found a subdirectory /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a
scm_1               | 2023-06-30 10:34:55,522 [main] INFO server.RaftServer: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: addNew group-8F2C0855D65A:[] returns group-8F2C0855D65A:java.util.concurrent.CompletableFuture@266da047[Not completed]
scm_1               | 2023-06-30 10:34:55,588 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: new RaftServerImpl for group-8F2C0855D65A:[] with SCMStateMachine:uninitialized
scm_1               | 2023-06-30 10:34:55,590 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-30 10:34:55,590 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-30 10:34:55,590 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-30 10:34:55,590 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-30 10:34:55,591 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-30 10:34:55,591 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-30 10:34:55,602 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-30 10:34:55,602 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-30 10:34:55,614 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-30 10:34:55,614 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-30 10:34:55,628 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-30 10:34:55,631 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-30 10:34:55,644 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-30 10:34:55,644 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-30 10:34:55,669 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-30 10:34:55,810 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-30 10:34:55,814 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-30 10:34:55,816 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-30 10:34:55,818 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-30 10:34:55,818 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-30 10:34:55,819 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-30 10:34:55,822 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-06-30 10:34:55,822 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-30 10:34:55,826 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-06-30 10:34:55,899 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1               | 2023-06-30 10:34:55,949 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-30 10:34:55,950 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-30 10:34:55,964 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-30 10:34:55,966 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-30 10:34:56,093 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-30 10:34:56,114 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1               | 2023-06-30 10:34:56,124 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-30 10:34:56,138 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-30 10:34:56,172 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-30 10:34:56,172 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-30 10:34:56,183 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-30 10:34:56,185 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:34:56,192 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-30 10:34:56,194 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-06-30 10:34:56,204 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-30 10:34:56,205 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-06-30 10:34:56,253 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-30 10:34:56,253 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-30 10:34:56,283 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-30 10:34:56,421 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-30 10:34:56,440 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-30 10:34:56,440 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-30 10:34:56,453 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-30 10:34:56,459 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:34:56,463 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:34:56,516 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-06-30 10:34:57,256 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:34:57,288 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:34:57,317 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1               | 2023-06-30 10:34:57,319 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-30 10:34:57,386 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:34:57,392 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:34:57,392 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-06-30 10:34:57,392 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-30 10:34:57,478 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-30 10:34:57,500 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-30 10:34:57,501 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1               | 2023-06-30 10:34:57,513 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-30 10:34:57,654 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-30 10:34:57,655 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-30 10:34:57,655 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-30 10:34:57,663 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:34:57,668 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-30 10:34:57,672 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/in_use.lock acquired by nodename 6@391d213fcf60
scm_1               | 2023-06-30 10:34:57,684 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=9767da7b-2d7b-4e9f-b157-e24f4bffa08e} from /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/current/raft-meta
scm_1               | 2023-06-30 10:34:57,713 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: set configuration 0: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:34:57,716 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-30 10:34:57,724 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-30 10:34:57,724 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:34:57,725 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-30 10:34:57,726 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-30 10:34:57,728 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:34:57,736 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-30 10:34:57,736 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-30 10:34:57,737 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:34:57,741 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a
scm_1               | 2023-06-30 10:34:57,742 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:34:57,743 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:34:57,744 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-30 10:34:57,745 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-30 10:34:57,745 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-30 10:34:57,746 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-30 10:34:57,746 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-30 10:34:57,747 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-30 10:34:57,754 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-30 10:34:57,754 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-30 10:34:57,780 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-30 10:34:57,780 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-30 10:34:57,781 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-30 10:34:57,816 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: set configuration 0: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:34:57,825 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/current/log_inprogress_0
scm_1               | 2023-06-30 10:34:57,834 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-30 10:34:57,883 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: start as a follower, conf=0: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:34:57,884 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-30 10:34:57,885 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState
scm_1               | 2023-06-30 10:34:57,886 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-30 10:34:57,886 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-30 10:34:57,888 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8F2C0855D65A,id=9767da7b-2d7b-4e9f-b157-e24f4bffa08e
scm_1               | 2023-06-30 10:34:57,892 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-30 10:34:57,892 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-30 10:34:57,892 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-30 10:34:57,893 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-30 10:34:57,895 [main] INFO server.RaftServer: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start RPC server
scm_1               | 2023-06-30 10:34:57,975 [main] INFO server.GrpcService: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: GrpcService started, listening on 9894
scm_1               | 2023-06-30 10:34:57,985 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9767da7b-2d7b-4e9f-b157-e24f4bffa08e: Started
scm_1               | 2023-06-30 10:34:57,988 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-30 10:34:57,988 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-30 10:34:57,991 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-06-30 10:34:57,991 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-06-30 10:34:57,991 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-06-30 10:34:58,087 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-30 10:34:58,097 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-30 10:34:58,097 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-30 10:34:58,338 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-30 10:34:58,339 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:34:58,340 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-30 10:34:58,479 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:34:58,481 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-30 10:34:58,481 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:34:58,492 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-30 10:34:58,647 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-30 10:34:58,647 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-30 10:34:58,710 [main] INFO util.log: Logging initialized @10502ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-30 10:34:58,980 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-30 10:34:58,992 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-30 10:34:59,018 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-30 10:34:59,021 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-30 10:34:59,021 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-30 10:34:59,022 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-30 10:34:59,100 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-06-30 10:34:59,106 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-30 10:34:59,109 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-30 10:34:59,164 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-30 10:34:59,164 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-30 10:34:59,165 [main] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-30 10:34:59,175 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7fb84ef7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-30 10:34:59,176 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43076326{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-30 10:34:59,293 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@27bd124a{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-17898707674581784336/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-06-30 10:34:59,301 [main] INFO server.AbstractConnector: Started ServerConnector@24a5ddd7{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-30 10:34:59,301 [main] INFO server.Server: Started @11093ms
scm_1               | 2023-06-30 10:34:59,307 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-30 10:34:59,307 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-30 10:34:59,309 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-30 10:35:02,920 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO impl.FollowerState: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5034808993ns, electionTimeout:5032ms
scm_1               | 2023-06-30 10:35:02,921 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: shutdown 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState
scm_1               | 2023-06-30 10:35:02,921 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-30 10:35:02,924 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-30 10:35:02,927 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-FollowerState] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1
scm_1               | 2023-06-30 10:35:02,939 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:35:02,939 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-06-30 10:35:02,958 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:35:02,959 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.LeaderElection: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-30 10:35:02,959 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: shutdown 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1
scm_1               | 2023-06-30 10:35:02,961 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-30 10:35:02,961 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-30 10:35:02,961 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-30 10:35:02,969 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: change Leader from null to 9767da7b-2d7b-4e9f-b157-e24f4bffa08e at term 2 for becomeLeader, leader elected after 7333ms
scm_1               | 2023-06-30 10:35:02,981 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-30 10:35:02,990 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:35:02,990 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-30 10:35:02,995 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-30 10:35:02,995 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-30 10:35:02,996 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-30 10:35:03,001 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-30 10:35:03,008 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-30 10:35:03,010 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO impl.RoleInfo: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e: start 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderStateImpl
scm_1               | 2023-06-30 10:35:03,014 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-30 10:35:03,018 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/current/log_inprogress_0 to /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/current/log_0-0
scm_1               | 2023-06-30 10:35:03,038 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/028270cf-30a6-4025-a7a5-8f2c0855d65a/current/log_inprogress_1
scm_1               | 2023-06-30 10:35:03,044 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-LeaderElection1] INFO server.RaftServer$Division: 9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A: set configuration 1: peers:[9767da7b-2d7b-4e9f-b157-e24f4bffa08e|rpc:391d213fcf60:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-30 10:35:03,048 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-30 10:35:03,049 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-30 10:35:03,057 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:03,058 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-30 10:35:03,058 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-30 10:35:03,058 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-30 10:35:03,062 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-30 10:35:03,067 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-30 10:35:03,390 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:33838 / 172.24.0.12:33838: output error
scm_1               | 2023-06-30 10:35:03,391 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:35:03,394 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:37376 / 172.24.0.7:37376: output error
scm_1               | 2023-06-30 10:35:03,396 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:35:03,398 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:57916 / 172.24.0.15:57916: output error
scm_1               | 2023-06-30 10:35:03,398 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:35:03,399 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:39474 / 172.24.0.11:39474: output error
scm_1               | 2023-06-30 10:35:03,400 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:35:03,401 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:58318 / 172.24.0.8:58318: output error
scm_1               | 2023-06-30 10:35:03,401 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-30 10:35:05,388 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4e17b060-329a-47a0-8048-d67d08b8e752
scm_1               | 2023-06-30 10:35:05,392 [IPC Server handler 17 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/90a80e44-64e8-4ca8-8f98-d749b124e197
scm_1               | 2023-06-30 10:35:05,406 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4e17b060-329a-47a0-8048-d67d08b8e752{ip: 172.24.0.7, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:35:05,411 [IPC Server handler 17 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 90a80e44-64e8-4ca8-8f98-d749b124e197{ip: 172.24.0.8, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:35:05,425 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:35:05,435 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:35:05,456 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:35:05,557 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-30 10:35:05,568 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:35:05,581 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:35:05,588 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=bd0ac505-581c-4be1-b7d5-3e6b04c1da80 to datanode:4e17b060-329a-47a0-8048-d67d08b8e752
scm_1               | 2023-06-30 10:35:05,615 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ebc02119-b8bf-46fc-962f-277901a1ccf1
scm_1               | 2023-06-30 10:35:05,616 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : ebc02119-b8bf-46fc-962f-277901a1ccf1{ip: 172.24.0.15, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:35:05,616 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:35:05,618 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-30 10:35:05,618 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:35:05,618 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-30 10:35:05,619 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-30 10:35:05,619 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:35:06,006 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:06,021 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: bd0ac505-581c-4be1-b7d5-3e6b04c1da80, Nodes: 4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:05.587102Z[UTC]]
scm_1               | 2023-06-30 10:35:06,025 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=550537c1-df67-480c-bde8-12feca489596 to datanode:ebc02119-b8bf-46fc-962f-277901a1ccf1
scm_1               | 2023-06-30 10:35:06,025 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=550537c1-df67-480c-bde8-12feca489596 to datanode:90a80e44-64e8-4ca8-8f98-d749b124e197
scm_1               | 2023-06-30 10:35:06,025 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=550537c1-df67-480c-bde8-12feca489596 to datanode:4e17b060-329a-47a0-8048-d67d08b8e752
scm_1               | 2023-06-30 10:35:06,035 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:06,038 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 550537c1-df67-480c-bde8-12feca489596, Nodes: ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.025095Z[UTC]]
scm_1               | 2023-06-30 10:35:06,039 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b55d9a00-bdf3-4025-8878-e71640cd79b9 to datanode:ebc02119-b8bf-46fc-962f-277901a1ccf1
scm_1               | 2023-06-30 10:35:06,046 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:06,049 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: b55d9a00-bdf3-4025-8878-e71640cd79b9, Nodes: ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.039510Z[UTC]]
scm_1               | 2023-06-30 10:35:06,050 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 to datanode:4e17b060-329a-47a0-8048-d67d08b8e752
scm_1               | 2023-06-30 10:35:06,050 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 to datanode:90a80e44-64e8-4ca8-8f98-d749b124e197
scm_1               | 2023-06-30 10:35:06,050 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 to datanode:ebc02119-b8bf-46fc-962f-277901a1ccf1
scm_1               | 2023-06-30 10:35:06,054 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:06,057 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512 contains same datanodes as previous pipelines: PipelineID=550537c1-df67-480c-bde8-12feca489596 nodeIds: 4e17b060-329a-47a0-8048-d67d08b8e752, 90a80e44-64e8-4ca8-8f98-d749b124e197, ebc02119-b8bf-46fc-962f-277901a1ccf1
scm_1               | 2023-06-30 10:35:06,057 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: d0ff19ef-ebe8-48c0-a396-84b0a4d85512, Nodes: 4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7)90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.050156Z[UTC]]
scm_1               | 2023-06-30 10:35:06,058 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=318c2b0f-4198-4a14-b1fd-7c226dd1e4c1 to datanode:90a80e44-64e8-4ca8-8f98-d749b124e197
scm_1               | 2023-06-30 10:35:06,061 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:06,062 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 318c2b0f-4198-4a14-b1fd-7c226dd1e4c1, Nodes: 90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.058046Z[UTC]]
scm_1               | 2023-06-30 10:35:06,063 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-30 10:35:06,067 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-30 10:35:06,550 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9ca9d3f5-c23a-4767-9163-086f75187a71
scm_1               | 2023-06-30 10:35:06,550 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 9ca9d3f5-c23a-4767-9163-086f75187a71{ip: 172.24.0.12, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:35:06,552 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:35:06,553 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc to datanode:9ca9d3f5-c23a-4767-9163-086f75187a71
scm_1               | 2023-06-30 10:35:06,556 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:06,565 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc, Nodes: 9ca9d3f5-c23a-4767-9163-086f75187a71(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.553520Z[UTC]]
scm_1               | 2023-06-30 10:35:06,566 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
scm_1               | 2023-06-30 10:35:06,882 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f5a485b8-4096-468e-a11f-083e698072ab
scm_1               | 2023-06-30 10:35:06,882 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f5a485b8-4096-468e-a11f-083e698072ab{ip: 172.24.0.11, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-30 10:35:06,882 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-30 10:35:06,883 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0414c739-ceb6-4658-9323-e09d08f34c78 to datanode:f5a485b8-4096-468e-a11f-083e698072ab
scm_1               | 2023-06-30 10:35:06,892 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:06,892 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 0414c739-ceb6-4658-9323-e09d08f34c78, Nodes: f5a485b8-4096-468e-a11f-083e698072ab(xcompat_datanode_2.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:35:06.883899Z[UTC]]
scm_1               | 2023-06-30 10:35:06,893 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm_1               | 2023-06-30 10:35:10,544 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:10,545 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=0e1f0bc9-23e4-46e6-8ccf-949a9dd84bbc
scm_1               | 2023-06-30 10:35:10,564 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:35:10,978 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:10,992 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=0414c739-ceb6-4658-9323-e09d08f34c78
scm_1               | 2023-06-30 10:35:10,992 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:35:15,297 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-30 10:35:15,298 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=550537c1-df67-480c-bde8-12feca489596
scm_1               | 2023-06-30 10:35:15,298 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-30 10:35:15,298 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-30 10:35:15,299 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-30 10:35:15,299 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-30 10:35:15,299 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-30 10:35:15,299 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-30 10:35:15,300 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-30 10:35:15,300 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-30 10:35:15,376 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-30 10:35:15,382 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-06-30 10:35:16,945 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=b55d9a00-bdf3-4025-8878-e71640cd79b9
scm_1               | 2023-06-30 10:35:24,595 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=d0ff19ef-ebe8-48c0-a396-84b0a4d85512
scm_1               | 2023-06-30 10:35:38,460 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=bd0ac505-581c-4be1-b7d5-3e6b04c1da80
scm_1               | 2023-06-30 10:35:38,535 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=318c2b0f-4198-4a14-b1fd-7c226dd1e4c1
scm_1               | 2023-06-30 10:35:54,795 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-30 10:35:54,845 [9767da7b-2d7b-4e9f-b157-e24f4bffa08e@group-8F2C0855D65A-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-30 10:35:54,852 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-30 10:36:11,542 [IPC Server handler 13 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 93722aea-ab79-4635-a891-312fc5625891, Nodes: 9ca9d3f5-c23a-4767-9163-086f75187a71(xcompat_datanode_3.xcompat_default/172.24.0.12)90a80e44-64e8-4ca8-8f98-d749b124e197(xcompat_datanode_1.xcompat_default/172.24.0.8)ebc02119-b8bf-46fc-962f-277901a1ccf1(xcompat_datanode_5.xcompat_default/172.24.0.15)4e17b060-329a-47a0-8048-d67d08b8e752(xcompat_datanode_4.xcompat_default/172.24.0.7)f5a485b8-4096-468e-a11f-083e698072ab(xcompat_datanode_2.xcompat_default/172.24.0.11), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T10:36:11.491135Z[UTC]]
scm_1               | 2023-06-30 10:37:06,894 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm_1               | 2023-06-30 10:37:28,834 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
scm_1               | 2023-06-30 10:39:06,895 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm_1               | 2023-06-30 10:39:56,445 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-30 10:41:06,896 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
