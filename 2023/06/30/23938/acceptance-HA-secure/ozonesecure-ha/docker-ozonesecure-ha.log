Attaching to ozonesecure-ha_kms_1, ozonesecure-ha_om1_1, ozonesecure-ha_datanode2_1, ozonesecure-ha_scm1.org_1, ozonesecure-ha_httpfs_1, ozonesecure-ha_datanode3_1, ozonesecure-ha_kdc_1, ozonesecure-ha_recon_1, ozonesecure-ha_scm3.org_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_om3_1, ozonesecure-ha_s3g_1, ozonesecure-ha_om2_1, ozonesecure-ha_datanode1_1
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2023-06-30 12:21:34,802 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = b043fa92fe53/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
datanode2_1  | STARTUP_MSG:   java = 11.0.19
datanode2_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode2_1  | ************************************************************/
datanode2_1  | 2023-06-30 12:21:34,940 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-06-30 12:21:35,436 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2023-06-30 12:21:36,616 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2023-06-30 12:21:38,127 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2023-06-30 12:21:38,127 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2023-06-30 12:21:39,394 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b043fa92fe53 ip:172.25.0.103
datanode2_1  | 2023-06-30 12:21:45,524 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2023-06-30 12:21:47,230 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2023-06-30 12:21:47,231 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2023-06-30 12:21:49,210 [main] INFO reflections.Reflections: Reflections took 1379 ms to scan 2 urls, producing 107 keys and 231 values 
datanode2_1  | 2023-06-30 12:21:53,645 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2023-06-30 12:21:56,336 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode2_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode2_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode2_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode2_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode2_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode2_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode2_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode2_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode2_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode2_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode2_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode2_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode2_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode2_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode2_1  | 2023-06-30 12:21:56,363 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2023-06-30 12:21:56,364 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2023-06-30 12:21:56,394 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2023-06-30 12:22:07,288 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2023-06-30 12:22:07,619 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:b043fa92fe53
datanode2_1  | 2023-06-30 12:22:07,623 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2023-06-30 12:22:07,678 [main] ERROR utils.CertificateSignRequest: Invalid domain b043fa92fe53
datanode2_1  | 2023-06-30 12:22:07,689 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@b043fa92fe53
datanode2_1  | 2023-06-30 12:22:12,098 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 1
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode2_1  |            Start Date: Fri Jun 30 12:19:58 UTC 2023
datanode2_1  |            Final Date: Mon Aug 07 12:19:58 UTC 2028
datanode2_1  |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode2_1  |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
datanode2_1  |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
datanode2_1  |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
datanode2_1  |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
datanode2_1  |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
datanode2_1  |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
datanode2_1  |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
datanode2_1  |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
datanode2_1  |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
datanode2_1  |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
datanode2_1  |                        5a94528eee6bdc4274bc019b1d82a247fece378c
datanode2_1  |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
datanode2_1  |                        c78036d6743d01601451bd47bf51bec81c366e20
datanode2_1  |                        347a5c23f63ab77d1a66e410a5866e0f
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0x6
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2023-06-30 12:22:12,203 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 431623557457
datanode2_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode2_1  |            Start Date: Fri Jun 30 12:22:10 UTC 2023
datanode2_1  |            Final Date: Sat Jun 29 12:22:10 UTC 2024
datanode2_1  |             SubjectDN: CN=dn@b043fa92fe53,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode2_1  |            Public Key: RSA Public Key [b1:7a:a4:2e:10:61:4b:51:81:ed:89:56:19:a0:e0:99:00:b5:0e:58],[56:66:d1:a4]
datanode2_1  |         modulus: 9ff4c33b9747b8c90f9d7363f14822215394faceaf2409b0213917fae2f0e76624613c5286d47c7b48943713d7957655be0341e111056b28c5b85b13d41776d25846b9a6e03203e004115a5a69c34cf88e3e61bb81f2c82943c8e86466d0021273d96b9d1019e7fc0f1f1831e6f2ad9b0d71804fc584f04c57cf60e92034fd69a8e051b98e31ebee9f9134ff028dae81b4220166e1d54632ee23232094cdd4ec72454950df754b90f72ec0bc825e5f6132cfabc98bba9716f7c8f52895cdbf3d142178964863ff5ab5f142ff1f1653d681e75413afb7cb9b8a81ff9a2aa85d00de85daf6ee0f669fb573096a9309ab380ca077f23919ea36795af04cbe8eff21
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: 3b71d098f33858b92cb0927c1d7e6fc582d1e9e6
datanode2_1  |                        f14a6d25e20871e74702d48dd8f0103cc6bfdb6b
datanode2_1  |                        3447c98c33de2f6ee26e92b209b68b2d2881757b
datanode2_1  |                        096a2d1ab1be658c4c259717322e6a1eda71780b
datanode2_1  |                        fe2754fce5279a904a3d5ebc4bf3c0ade58ea19e
datanode2_1  |                        4828e890c6ccafabec0e6329904a33922c6f4d83
datanode2_1  |                        27280475fb83a9869e1b5c6e337719f332bb3fe3
datanode2_1  |                        c2b3f5c9ae146a72fc6f49732c8e4213e670f01c
datanode2_1  |                        ecc183cd1a06abc64fe3d2ccb3e401dd45874c0c
datanode2_1  |                        47602d316deb050980ea14055eef98f79ae996f3
datanode2_1  |                        d120ec17a4762bacc944f4d144531407d19f7b69
datanode2_1  |                        97816c43f1aab21283797edfc78f8f32a7741d83
datanode2_1  |                        7e59245ad2980495f60fbe04b317da98
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  | 
datanode2_1  |                        critical(true) KeyUsage: 0xb8
datanode2_1  |  from file: /data/metadata/dn/certs/431623557457.crt.
datanode2_1  | 2023-06-30 12:22:12,256 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode2_1  |          SerialNumber: 299862451261
datanode2_1  |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode2_1  |            Start Date: Fri Jun 30 12:19:58 UTC 2023
datanode2_1  |            Final Date: Mon Aug 07 12:19:58 UTC 2028
datanode2_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode2_1  |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
datanode2_1  |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
datanode2_1  | public exponent: 10001
datanode2_1  | 
datanode2_1  |   Signature Algorithm: SHA256WITHRSA
datanode2_1  |             Signature: bceb238776cef1f96370453157455bfacebc30a6
datanode2_1  |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
datanode2_1  |                        92b8465e76c5427a65937f03f4c7003698371efa
datanode2_1  |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
datanode2_1  |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
datanode2_1  |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
datanode2_1  |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
datanode2_1  |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
datanode2_1  |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
datanode2_1  |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
datanode2_1  |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
datanode2_1  |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
datanode2_1  |                        82b7771fe8683859200f859a8629c61a
datanode2_1  |        Extensions: 
datanode2_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode2_1  |     Tagged [7] IMPLICIT 
datanode2_1  |         DER Octet String[4] 
datanode2_1  |     Tagged [2] IMPLICIT 
datanode2_1  |         DER Octet String[8] 
datanode2_1  | 
datanode2_1  |                        critical(true) BasicConstraints: isCa(true)
datanode2_1  |                        critical(true) KeyUsage: 0xbe
datanode2_1  |  from file: /data/metadata/dn/certs/CA-299862451261.crt.
datanode2_1  | 2023-06-30 12:22:12,300 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797738 ms and interval 86400000 ms.
datanode2_1  | 2023-06-30 12:22:12,899 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2023-06-30 12:22:13,200 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode2_1  | 2023-06-30 12:22:13,855 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z).
datanode2_1  | 2023-06-30 12:22:13,865 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M5.271844S and interval of PT1M
datanode2_1  | 2023-06-30 12:22:13,988 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode2_1  | 2023-06-30 12:22:14,262 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode2_1  | 2023-06-30 12:22:15,733 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2023-06-30 12:22:15,937 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode2_1  | 2023-06-30 12:22:15,988 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2023-06-30 12:22:15,999 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2023-06-30 12:22:16,231 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  | 2023-06-30 12:22:16,271 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-30 12:22:16,291 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2023-06-30 12:22:16,310 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2023-06-30 12:22:16,311 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode2_1  | 2023-06-30 12:22:16,314 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2023-06-30 12:22:16,538 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2023-06-30 12:22:16,552 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2023-06-30 12:22:24,490 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2023-06-30 12:21:35,594 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = 56dd990c81df/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
datanode1_1  | STARTUP_MSG:   java = 11.0.19
datanode1_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode1_1  | ************************************************************/
datanode1_1  | 2023-06-30 12:21:35,758 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2023-06-30 12:21:36,208 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2023-06-30 12:21:37,260 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2023-06-30 12:21:38,703 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2023-06-30 12:21:38,704 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2023-06-30 12:21:40,463 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:56dd990c81df ip:172.25.0.102
datanode1_1  | 2023-06-30 12:21:46,197 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2023-06-30 12:21:47,675 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2023-06-30 12:21:47,675 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2023-06-30 12:21:49,652 [main] INFO reflections.Reflections: Reflections took 1326 ms to scan 2 urls, producing 107 keys and 231 values 
datanode1_1  | 2023-06-30 12:21:54,484 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2023-06-30 12:21:56,431 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode1_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode1_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode1_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode1_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode1_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode1_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode1_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode1_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode1_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode1_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode1_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode1_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode1_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode1_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode1_1  | 2023-06-30 12:21:56,456 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2023-06-30 12:21:56,472 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2023-06-30 12:21:56,474 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2023-06-30 12:22:04,047 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2023-06-30 12:22:04,350 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:56dd990c81df
datanode1_1  | 2023-06-30 12:22:04,357 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2023-06-30 12:22:04,410 [main] ERROR utils.CertificateSignRequest: Invalid domain 56dd990c81df
datanode1_1  | 2023-06-30 12:22:04,413 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@56dd990c81df
datanode1_1  | 2023-06-30 12:22:08,828 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 1
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode1_1  |            Start Date: Fri Jun 30 12:19:58 UTC 2023
datanode1_1  |            Final Date: Mon Aug 07 12:19:58 UTC 2028
datanode1_1  |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode1_1  |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
datanode1_1  |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
datanode1_1  |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
datanode1_1  |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
datanode1_1  |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
datanode1_1  |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
datanode1_1  |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
datanode1_1  |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
datanode1_1  |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
datanode1_1  |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
datanode1_1  |                        5a94528eee6bdc4274bc019b1d82a247fece378c
datanode1_1  |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
datanode1_1  |                        c78036d6743d01601451bd47bf51bec81c366e20
datanode1_1  |                        347a5c23f63ab77d1a66e410a5866e0f
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0x6
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode1_1  | 2023-06-30 12:22:08,914 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 428663643003
datanode1_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode1_1  |            Start Date: Fri Jun 30 12:22:07 UTC 2023
datanode1_1  |            Final Date: Sat Jun 29 12:22:07 UTC 2024
datanode1_1  |             SubjectDN: CN=dn@56dd990c81df,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode1_1  |            Public Key: RSA Public Key [38:da:a6:f1:ae:95:24:8c:a1:cc:b3:9c:1d:d0:53:17:7a:5b:9a:6f],[56:66:d1:a4]
datanode1_1  |         modulus: c486a036f87a54cd3c43aeb3c619daba9721aab58789f0e23f67135d8355714e77a058ba1f56f9427f4c054bd376d02993a8197b5da4b034492ae25be5050d6de4a21df353e3ecf16aefca5baf251ac335a756d43c5f3e9c8841156119dfd515da4123a348fb204166ab47360e6baf35722f5dd87306503e51d304ab8fe3687039c7050069f3bbc34f476df3ec8a16eb58b4a8a70679cb9986857a0acdda11c1363397164f1c8d2c0a5f379701c0f46268c4f9dbf5f84fbd2ca06990f899b10c5cf0c1bdf6ad1ecc7c838aad79692998f4cdff817fcd26e988fb1c9db2fc5c82bca37f1bba10972c96d6d3c9fc23d7020941fcd80f60ead948f738adbc0d6d41
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: 4e284a8a48ad9412b956bd3683e40c7257376c16
datanode1_1  |                        12b03c83344994c48ee5782689322a29cad25ef2
datanode1_1  |                        412e4785a6286ca30cfbbd1c47b7195e2fe9c8c4
datanode1_1  |                        8c5dcdec3237d05284e6a984453db8edd8481de3
datanode1_1  |                        a61b61178bf276738a99f8d604a9accc604d49fd
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
om1_1        | Waiting for the service scm3.org:9894
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-30 12:21:34,781 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
om1_1        | STARTUP_MSG:   java = 11.0.19
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2023-06-30 12:21:35,299 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = aa09f3d2afc3/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
datanode3_1  | STARTUP_MSG:   java = 11.0.19
datanode3_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode3_1  | ************************************************************/
datanode3_1  | 2023-06-30 12:21:35,415 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2023-06-30 12:21:35,996 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2023-06-30 12:21:37,082 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2023-06-30 12:21:38,577 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2023-06-30 12:21:38,579 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2023-06-30 12:21:40,197 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:aa09f3d2afc3 ip:172.25.0.104
datanode3_1  | 2023-06-30 12:21:46,466 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2023-06-30 12:21:47,475 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2023-06-30 12:21:47,475 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2023-06-30 12:21:49,333 [main] INFO reflections.Reflections: Reflections took 1464 ms to scan 2 urls, producing 107 keys and 231 values 
datanode3_1  | 2023-06-30 12:21:53,946 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
httpfs_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
httpfs_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
httpfs_1     | 2023-06-30 12:19:38,077 [main] INFO server.HttpFSServerWebServer: STARTUP_MSG: 
httpfs_1     | /************************************************************
httpfs_1     | STARTUP_MSG: Starting HttpFSServerWebServer
httpfs_1     | STARTUP_MSG:   host = httpfs/172.25.0.119
httpfs_1     | STARTUP_MSG:   args = []
httpfs_1     | STARTUP_MSG:   version = 3.3.6
httpfs_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/curator-client-4.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/json-simple-1.1.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/curator-framework-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.5.6.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/zookeeper-jute-3.5.6.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-filesystem-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar
httpfs_1     | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
httpfs_1     | STARTUP_MSG:   java = 11.0.19
httpfs_1     | ************************************************************/
httpfs_1     | 2023-06-30 12:19:38,151 [main] INFO server.HttpFSServerWebServer: registered UNIX signal handlers for [TERM, HUP, INT]
httpfs_1     | 2023-06-30 12:19:39,137 [main] INFO util.log: Logging initialized @5114ms to org.eclipse.jetty.util.log.Slf4jLog
httpfs_1     | 2023-06-30 12:19:39,883 [main] INFO http.HttpRequestLog: Http request log for http.requests.webhdfs is not defined
httpfs_1     | 2023-06-30 12:19:39,941 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
httpfs_1     | 2023-06-30 12:19:39,981 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context webhdfs
httpfs_1     | 2023-06-30 12:19:39,981 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
httpfs_1     | 2023-06-30 12:19:39,981 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
httpfs_1     | 2023-06-30 12:19:40,290 [main] INFO http.HttpServer2: Jetty bound to port 14000
httpfs_1     | 2023-06-30 12:19:40,317 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
httpfs_1     | 2023-06-30 12:19:40,442 [main] INFO server.session: DefaultSessionIdManager workerName=node0
httpfs_1     | 2023-06-30 12:19:40,442 [main] INFO server.session: No SessionScavenger set, using defaults
httpfs_1     | 2023-06-30 12:19:40,450 [main] INFO server.session: node0 Scavenging every 600000ms
httpfs_1     | 2023-06-30 12:19:40,538 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2c1b194a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
httpfs_1     | 2023-06-30 12:19:40,557 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@437da279{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
httpfs_1     | log4j:WARN No appenders could be found for logger (org.eclipse.jetty.webapp.WebAppClassLoader).
httpfs_1     | log4j:WARN Please initialize the log4j system properly.
httpfs_1     | log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
httpfs_1     | 12:19:41,263  WARN Server:474 - Log4j [/etc/hadoop/httpfs-log4j.properties] configuration file not found, using default configuration from classpath
httpfs_1     | 12:19:41,272  INFO Server:389 - ++++++++++++++++++++++++++++++++++++++++++++++++++++++
httpfs_1     | 12:19:41,272  INFO Server:390 - Server [httpfs] starting
httpfs_1     | 12:19:41,273  INFO Server:391 -   Built information:
httpfs_1     | 12:19:41,273  INFO Server:392 -     Version           : 1.4.0-SNAPSHOT
httpfs_1     | 12:19:41,273  INFO Server:394 -     Source Repository : REPO NOT AVAIL
httpfs_1     | 12:19:41,273  INFO Server:396 -     Source Revision   : REVISION NOT AVAIL
httpfs_1     | 12:19:41,275  INFO Server:398 -     Built by          : runner
httpfs_1     | 12:19:41,275  INFO Server:400 -     Built timestamp   : 2023-06-30T11:54:45+0000
httpfs_1     | 12:19:41,275  INFO Server:402 -   Runtime information:
httpfs_1     | 12:19:41,276  INFO Server:403 -     Home   dir: /opt/hadoop
httpfs_1     | 12:19:41,276  INFO Server:404 -     Config dir: /etc/hadoop
httpfs_1     | 12:19:41,276  INFO Server:405 -     Log    dir: /opt/hadoop/log
httpfs_1     | 12:19:41,276  INFO Server:406 -     Temp   dir: /opt/hadoop/temp
httpfs_1     | 12:19:41,431  INFO Server:544 - System property sets  httpfs.log.dir: /opt/hadoop/log
httpfs_1     | 12:19:41,431  INFO Server:544 - System property sets  httpfs.temp.dir: /opt/hadoop/temp
httpfs_1     | 12:19:41,431  INFO Server:544 - System property sets  httpfs.config.dir: /etc/hadoop
httpfs_1     | 12:19:41,431  INFO Server:544 - System property sets  httpfs.home.dir: /opt/hadoop
httpfs_1     | 12:19:41,511  INFO FileSystemAccessService:162 - Using FileSystemAccess JARs version [3.3.6]
httpfs_1     | 12:19:42,078  INFO UserGroupInformation:1132 - Login successful for user httpfs/httpfs@EXAMPLE.COM using keytab file httpfs.keytab. Keytab auto renewal enabled : false
httpfs_1     | 12:19:42,079  INFO FileSystemAccessService:191 - Using FileSystemAccess Kerberos authentication, principal [httpfs/httpfs@EXAMPLE.COM] keytab [/etc/security/keytabs/httpfs.keytab]
httpfs_1     | 12:19:42,196  INFO Server:413 - Services initialized
httpfs_1     | 12:19:42,196  INFO Server:423 - Server [httpfs] started!, status [NORMAL]
httpfs_1     | 12:19:42,196  INFO HttpFSServerWebApp:107 - Connects to Namenode [ofs://omservice]
httpfs_1     | 12:19:42,196  INFO HttpFSServerWebApp:126 - Initializing HttpFSServerMetrics
httpfs_1     | 12:19:42,439  INFO JvmPauseMonitor:188 - Starting JVM pause monitor
httpfs_1     | 12:19:42,726  INFO MetricsConfig:120 - Loaded properties from hadoop-metrics2.properties
httpfs_1     | 12:19:43,434  INFO MetricsSystemImpl:378 - Scheduled Metric snapshot period at 10 second(s).
httpfs_1     | 12:19:43,434  INFO MetricsSystemImpl:191 - HttpFSServer metrics system started
httpfs_1     | 12:19:43,639  INFO KerberosAuthenticationHandler:175 - Using keytab /etc/security/keytabs/httpfs.keytab, for principal HTTP/httpfs@EXAMPLE.COM
httpfs_1     | 12:19:43,717  INFO AbstractDelegationTokenSecretManager:415 - Updating the current master key for generating delegation tokens
httpfs_1     | 12:19:43,740  INFO AbstractDelegationTokenSecretManager:797 - Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
httpfs_1     | 12:19:43,741  INFO AbstractDelegationTokenSecretManager:415 - Updating the current master key for generating delegation tokens
httpfs_1     | Jun 30, 2023 12:19:43 PM com.sun.jersey.api.core.PackagesResourceConfig init
httpfs_1     | INFO: Scanning for root resource and provider classes in the packages:
httpfs_1     |   org.apache.ozone.fs.http.server
httpfs_1     |   org.apache.ozone.lib.wsrs
httpfs_1     | Jun 30, 2023 12:19:44 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
httpfs_1     | INFO: Root resource classes found:
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSServer
httpfs_1     | Jun 30, 2023 12:19:44 PM com.sun.jersey.api.core.ScanningResourceConfig logClasses
httpfs_1     | INFO: Provider classes found:
kdc_1        | Jun 30 12:19:32 kdc krb5kdc[7](info): Loaded
kdc_1        | Jun 30 12:19:32 kdc krb5kdc[7](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jun 30 12:19:32 kdc krb5kdc[7](info): setting up network...
kdc_1        | Jun 30 12:19:32 kdc krb5kdc[7](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jun 30 12:19:32 kdc krb5kdc[7](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jun 30 12:19:32 kdc krb5kdc[7](info): set up 4 sockets
kdc_1        | Jun 30 12:19:32 kdc krb5kdc[7](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jun 30 12:19:34 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688127574, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode2_1  | 2023-06-30 12:22:26,158 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode2_1  | 2023-06-30 12:22:26,300 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode2_1  | 2023-06-30 12:22:26,886 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-06-30 12:22:27,737 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2023-06-30 12:22:28,482 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-30 12:22:28,500 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2023-06-30 12:22:28,507 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-06-30 12:22:28,513 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2023-06-30 12:22:28,527 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode2_1  | 2023-06-30 12:22:28,529 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2023-06-30 12:22:28,541 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode2_1  | 2023-06-30 12:22:28,550 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 12:22:28,556 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2023-06-30 12:22:28,560 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-30 12:22:28,656 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode2_1  | 2023-06-30 12:22:28,765 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode2_1  | 2023-06-30 12:22:28,767 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-06-30 12:22:35,362 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode2_1  | 2023-06-30 12:22:35,994 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode2_1  | 2023-06-30 12:22:36,002 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode2_1  | 2023-06-30 12:22:36,006 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode2_1  | 2023-06-30 12:22:36,019 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode2_1  | 2023-06-30 12:22:36,059 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode2_1  | 2023-06-30 12:22:36,063 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode2_1  | 2023-06-30 12:22:36,141 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode2_1  | 2023-06-30 12:22:36,159 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode2_1  | 2023-06-30 12:22:36,182 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode2_1  | 2023-06-30 12:22:36,188 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode2_1  | 2023-06-30 12:22:36,682 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode2_1  | 2023-06-30 12:22:36,692 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode2_1  | 2023-06-30 12:22:36,701 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-30 12:22:36,714 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-30 12:22:36,756 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-30 12:22:36,801 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb17c884e] REGISTERED
datanode2_1  | 2023-06-30 12:22:36,841 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb17c884e] BIND: 0.0.0.0/0.0.0.0:9855
datanode2_1  | 2023-06-30 12:22:36,872 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb17c884e, L:/0.0.0.0:9855] ACTIVE
datanode2_1  | 2023-06-30 12:22:37,199 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode2_1  | 2023-06-30 12:22:38,247 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode2_1  | 2023-06-30 12:22:39,729 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | 2023-06-30 12:22:39,734 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2023-06-30 12:22:39,739 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode2_1  | 2023-06-30 12:22:40,061 [main] INFO util.log: Logging initialized @80665ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2023-06-30 12:22:41,128 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2023-06-30 12:22:41,197 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2023-06-30 12:22:41,205 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2023-06-30 12:22:41,207 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode2_1  | 2023-06-30 12:22:41,212 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode2_1  | 2023-06-30 12:22:41,234 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2023-06-30 12:22:41,592 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode2_1  | 2023-06-30 12:22:41,597 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2023-06-30 12:22:41,616 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode2_1  | 2023-06-30 12:22:41,977 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2023-06-30 12:22:41,977 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2023-06-30 12:22:42,011 [main] INFO server.session: node0 Scavenging every 660000ms
datanode2_1  | 2023-06-30 12:22:42,254 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-06-30 12:22:42,304 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3abe1c9e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2023-06-30 12:22:42,313 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@549fc0b3{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2023-06-30 12:22:43,472 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-06-30 12:22:43,585 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4483e237{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2074058650807485736/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2023-06-30 12:22:43,670 [main] INFO server.AbstractConnector: Started ServerConnector@2cfc4475{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2023-06-30 12:22:43,671 [main] INFO server.Server: Started @84275ms
datanode2_1  | 2023-06-30 12:22:43,697 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2023-06-30 12:22:43,697 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2023-06-30 12:22:43,707 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2023-06-30 12:22:43,946 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode2_1  | 2023-06-30 12:22:43,997 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode3_1  | 2023-06-30 12:21:56,749 [main] WARN client.DNCertificateClient: Certificates could not be loaded.
datanode3_1  | java.nio.file.NoSuchFileException: /data/metadata/dn/certs
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
datanode3_1  | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
datanode3_1  | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
datanode3_1  | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
datanode3_1  | 	at java.base/java.nio.file.Files.list(Files.java:3699)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
datanode3_1  | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DNCertificateClient.<init>(DNCertificateClient.java:63)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:382)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:276)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:210)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:178)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:95)
datanode3_1  | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
datanode3_1  | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
datanode3_1  | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
datanode3_1  | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
datanode3_1  | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
datanode3_1  | 	at picocli.CommandLine.execute(CommandLine.java:2078)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
datanode3_1  | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
datanode3_1  | 	at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:160)
datanode3_1  | 2023-06-30 12:21:56,773 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2023-06-30 12:21:56,787 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2023-06-30 12:21:56,794 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2023-06-30 12:22:02,510 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2023-06-30 12:22:02,822 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:aa09f3d2afc3
datanode3_1  | 2023-06-30 12:22:02,822 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2023-06-30 12:22:02,873 [main] ERROR utils.CertificateSignRequest: Invalid domain aa09f3d2afc3
datanode3_1  | 2023-06-30 12:22:02,877 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@aa09f3d2afc3
datanode3_1  | 2023-06-30 12:22:07,545 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 427123221923
datanode3_1  |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode3_1  |            Start Date: Fri Jun 30 12:22:05 UTC 2023
datanode3_1  |            Final Date: Sat Jun 29 12:22:05 UTC 2024
datanode3_1  |             SubjectDN: CN=dn@aa09f3d2afc3,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode3_1  |            Public Key: RSA Public Key [3a:70:e7:52:b1:fa:25:20:90:d9:0e:68:97:a5:49:83:d5:35:4f:68],[56:66:d1:a4]
datanode3_1  |         modulus: c8f18fe54c4be4bb851aefcaceabe0acde58dafa70dbd88467ef9cff3ac6752d38b1d36206afebda01d4c1ba56a0127b5416085fb9b0c8a70c6b642f62abe700f77bb5fabfef8ebb3ba85ca4e83a288889fd0d12b7fabf0c97f328a3f2cceefaceebec68a440b9841d27bf338c8bebaf6564cffd8cce47bef9b45b949dd8c7ea2a395664f5d8352cc52b44a789b8d5d4f1e81c23459dd8d9ecedb4ae10d368fc7c3d6036466c01ace0dd357b866d6523d3397cd6936192f9071e3a9f068a7327d14aa08d13106891b71d9d1a95e6d86164f406a04653dfa22fac2289f8a83acb75a973a7841d92c73695b4c8bf002d196c3035ff43f3e5e2bd1ce02711cbea77
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 6ca4b57ceac17c406881848ea9ad2dcc89c4c15f
datanode3_1  |                        9e4dfb93c850894b43e9fa070d09a140b988f23f
datanode3_1  |                        8df873f925da1db7d13afbd90563943dae4cb2bf
datanode3_1  |                        c1efbb1c22227918c44cb646f621875ddadf24f3
datanode3_1  |                        8c2779351ec2f93adac182ada74d28e1c8bf8d11
datanode3_1  |                        22e74832c9d9b99df74696eb869ee696bed70ef2
datanode3_1  |                        cf3b3ee957fdc5dffadd35caf50dccb97918a529
datanode3_1  |                        108e17f40aa7e41ff86c2fd9b8fee1c70651850f
datanode3_1  |                        63b2ac698ea02be0df778eed6d1ac16d31120757
datanode3_1  |                        0b62f486e27f9b6c25f5783cfcc94a6839314ce8
datanode3_1  |                        061b1c5d7b2195f0e93f05022481ea4cadb74119
datanode3_1  |                        823f8c4f164959d91f25d2f268878b4b59ff5e1f
datanode3_1  |                        e41ea8112b184d1596d134b72a4fa912
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  | 
datanode3_1  |                        critical(true) KeyUsage: 0xb8
datanode3_1  |  from file: /data/metadata/dn/certs/427123221923.crt.
datanode3_1  | 2023-06-30 12:22:07,592 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 1
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode3_1  |            Start Date: Fri Jun 30 12:19:58 UTC 2023
datanode3_1  |            Final Date: Mon Aug 07 12:19:58 UTC 2028
datanode3_1  |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode3_1  |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
datanode3_1  |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
datanode3_1  |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
datanode3_1  |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
datanode3_1  |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
datanode3_1  |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
datanode3_1  |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
datanode3_1  |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-30 12:21:34,907 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-06-30 12:21:46,130 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-30 12:21:49,406 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om1_1        | 2023-06-30 12:21:50,472 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-30 12:21:50,473 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-30 12:21:50,473 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1        | 2023-06-30 12:21:52,348 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-30 12:21:52,355 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-30 12:21:52,417 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 12:21:54,076 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-9c10c649-f59d-4933-ad5d-654b70cf4739;layoutVersion=6
om1_1        | 2023-06-30 12:21:58,431 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om1_1        | 2023-06-30 12:21:58,433 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2023-06-30 12:21:59,341 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-30 12:22:05,202 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om1_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om1_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om1_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om1_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om1_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om1_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om1_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om1_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1383)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1362)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om1_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om1_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om1_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om1_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om1_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om1_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om1_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om1_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om1_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om1_1        | 2023-06-30 12:22:05,228 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2023-06-30 12:22:05,242 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om1_1        | 2023-06-30 12:22:05,252 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2023-06-30 12:22:10,053 [main] INFO om.OzoneManager: Init response: GETCERT
om1_1        | 2023-06-30 12:22:10,383 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om1_1        | 2023-06-30 12:22:10,384 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2023-06-30 12:22:10,437 [main] ERROR utils.CertificateSignRequest: Invalid domain om1
om1_1        | 2023-06-30 12:22:10,449 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,clusterId:CID-9c10c649-f59d-4933-ad5d-654b70cf4739,subject:om1
om1_1        | 2023-06-30 12:22:13,055 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 1
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om1_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
om1_1        |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
om1_1        |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
om1_1        |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
om1_1        |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
om1_1        |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
om1_1        |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
om1_1        |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
om1_1        |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
om1_1        |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
om1_1        |                        5a94528eee6bdc4274bc019b1d82a247fece378c
om1_1        |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
om1_1        |                        c78036d6743d01601451bd47bf51bec81c366e20
om1_1        |                        347a5c23f63ab77d1a66e410a5866e0f
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-30 12:22:13,090 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 299862451261
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om1_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
om1_1        |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: bceb238776cef1f96370453157455bfacebc30a6
om1_1        |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
om1_1        |                        92b8465e76c5427a65937f03f4c7003698371efa
om1_1        |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
om1_1        |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
om1_1        |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
om1_1        |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
om1_1        |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
om1_1        |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
om1_1        |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
om1_1        |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
om1_1        |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
om1_1        |                        82b7771fe8683859200f859a8629c61a
kdc_1        | Jun 30 12:19:36 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.114: ISSUE: authtime 1688127576, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, s3g/s3g@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:19:41 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.119: ISSUE: authtime 1688127581, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, httpfs/httpfs@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:19:48 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.115: ISSUE: authtime 1688127588, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:20:03 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1688127603, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:20:08 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1688127608, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:20:14 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.115: ISSUE: authtime 1688127588, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:20:16 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1688127603, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:20:16 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688127574, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:20:29 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688127629, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:20:33 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1688127633, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:20:45 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688127629, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:20:47 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.117: ISSUE: authtime 1688127633, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:20:50 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688127650, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:20:55 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1688127655, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:20:56 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1688127655, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:21:03 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688127650, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:21:05 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1688127665, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:08 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688127668, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:20 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.118: ISSUE: authtime 1688127665, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:21:31 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688127668, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:21:41 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688127701, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:46 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1688127706, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:47 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1688127707, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:47 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1688127707, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:51 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1688127711, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:52 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1688127712, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:54 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1688127714, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:21:57 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1688127711, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:21:57 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1688127712, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:21:59 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1688127714, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:22:05 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1688127707, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:22:06 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1688127707, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:22:09 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1688127706, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:22:43 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688127701, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:22:45 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.102: ISSUE: authtime 1688127707, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 30 12:22:46 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.104: ISSUE: authtime 1688127707, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 30 12:22:48 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1688127768, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:22:49 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.103: ISSUE: authtime 1688127706, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jun 30 12:22:50 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1688127770, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:22:51 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1688127771, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:22:54 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1688127774, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jun 30 12:22:56 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.111: ISSUE: authtime 1688127768, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:22:57 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.112: ISSUE: authtime 1688127770, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:22:58 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.113: ISSUE: authtime 1688127771, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jun 30 12:23:28 kdc krb5kdc[7](info): TGS_REQ (4 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19)}) 172.25.0.116: ISSUE: authtime 1688127774, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-30 12:21:35,240 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
om2_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-30 12:21:35,352 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-06-30 12:21:47,615 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-30 12:21:52,256 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-30 12:21:52,760 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-06-30 12:21:52,762 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-30 12:21:52,764 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-30 12:21:54,970 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-30 12:21:54,973 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-30 12:21:55,017 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 12:21:56,112 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-9c10c649-f59d-4933-ad5d-654b70cf4739;layoutVersion=6
om2_1        | 2023-06-30 12:22:00,439 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om2_1        | 2023-06-30 12:22:00,439 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2023-06-30 12:22:01,136 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-06-30 12:22:07,082 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om2_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om2_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om2_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om2_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om2_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om2_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om2_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om2_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1383)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1362)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om2_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om2_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om2_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om2_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om2_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om2_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om2_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om2_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om2_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om2_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om2_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om2_1        | 2023-06-30 12:22:07,110 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2023-06-30 12:22:07,137 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om2_1        | 2023-06-30 12:22:07,143 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2023-06-30 12:22:10,376 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2023-06-30 12:22:10,882 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
om2_1        | 2023-06-30 12:22:10,886 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2023-06-30 12:22:10,934 [main] ERROR utils.CertificateSignRequest: Invalid domain om2
om2_1        | 2023-06-30 12:22:10,944 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,clusterId:CID-9c10c649-f59d-4933-ad5d-654b70cf4739,subject:om2
om2_1        | 2023-06-30 12:22:13,648 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 433509364865
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Start Date: Fri Jun 30 12:22:11 UTC 2023
om2_1        |            Final Date: Sat Jun 29 12:22:11 UTC 2024
om2_1        |             SubjectDN: CN=om2,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Public Key: RSA Public Key [5d:2e:2a:ea:fb:ba:d0:47:bd:6a:6e:87:33:d6:03:63:5f:a7:d0:40],[56:66:d1:a4]
om2_1        |         modulus: ec2c91fcdfa86a2775cb1a3a42e9f1fa9cc7813c126d44798142994b5b664db447d953d1369ba973568885275d00f7b10718623126a755d96c4a41aed35f81e5c8242b0dccface550d18a543e2bde55c071de07acad67ff72cb19e29b233fe225090698d25dff01b889afdce236021eb684cdea4615f109fc1d47252ff3d965a21ab01ad3c7ca68649b277bd4e889bc83dab6b106d246f0f9e5d19f183aff048b731c651caa626e2923f9f1d0d7792953f796c62bd02e7516bde9a86e804e03d76d14ae38e4eefb419bcec4a0851cb385074e3feb5dd65465c36a9ecf4ddecc1e5b70bff9825d6dd54eaf67d4868a739df2d2376ae609afb46137108d9cefe71
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 0ebdc84d902ca541526961c0d6e47a3125626ee9
om2_1        |                        7f250c1d782c9d378dd50c0742108264b3d7f478
om2_1        |                        3346c2bec5c1ef1f632ca77c1bd6e7edd4a3f5a4
om2_1        |                        7aaabf31510a65cd0284ac181840bed490f83f8a
om2_1        |                        6d5594aaa099fb87d7ecee664a5ca9ed99b9ae51
om2_1        |                        e288ad0d39f5f32503ac53e1af18ddb6ba729336
om2_1        |                        591d3f716492edddb328cb3bdee71eefce15f10f
om2_1        |                        054344a94f5f60377c1fd081190155e64c326e71
om2_1        |                        02be9810fe299ccf49b8026a40794301fcd9a861
om2_1        |                        e5e767c6c3ce4da12e43f5d4e4c4c1384644042e
om2_1        |                        3ca01711ffa5f2ca9e85793843747404a05d6209
om2_1        |                        4af78cb6686537cc8dd87d38ec028057a0bfcbac
om2_1        |                        dadfd528d65f29ab36f8015f2c3a1664
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
om2_1        |  from file: /data/metadata/om/certs/433509364865.crt.
om2_1        | 2023-06-30 12:22:13,874 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om2_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
om2_1        |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
om2_1        |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
om2_1        |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
om2_1        |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
om2_1        |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
om2_1        |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
om2_1        |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
om2_1        |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
om2_1        |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
om2_1        |                        5a94528eee6bdc4274bc019b1d82a247fece378c
om2_1        |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
om2_1        |                        c78036d6743d01601451bd47bf51bec81c366e20
om2_1        |                        347a5c23f63ab77d1a66e410a5866e0f
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        6cf7eb572f09a97f0f3ad34b60fffb4b68073ac3
datanode1_1  |                        68e0c4cdd65321e3d15db9339133b139e3637efe
datanode1_1  |                        b6a825ffe41b196f95ac5a10fe8435078c52ce53
datanode1_1  |                        d1a564058960ef292ed879ace935ea4ef0772aa5
datanode1_1  |                        ded0676deb6da2fe7882c78b0e00817a55919262
datanode1_1  |                        780657313aaeebf58d0afa08edb3e571dd22bdf8
datanode1_1  |                        05198bb7b953165ac342c5c65f04c58fac7fb7c4
datanode1_1  |                        803810b5349f942abab09f51f4ebde6a
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  | 
datanode1_1  |                        critical(true) KeyUsage: 0xb8
datanode1_1  |  from file: /data/metadata/dn/certs/428663643003.crt.
datanode1_1  | 2023-06-30 12:22:08,965 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode1_1  |          SerialNumber: 299862451261
datanode1_1  |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode1_1  |            Start Date: Fri Jun 30 12:19:58 UTC 2023
datanode1_1  |            Final Date: Mon Aug 07 12:19:58 UTC 2028
datanode1_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode1_1  |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
datanode1_1  |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
datanode1_1  | public exponent: 10001
datanode1_1  | 
datanode1_1  |   Signature Algorithm: SHA256WITHRSA
datanode1_1  |             Signature: bceb238776cef1f96370453157455bfacebc30a6
datanode1_1  |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
datanode1_1  |                        92b8465e76c5427a65937f03f4c7003698371efa
datanode1_1  |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
datanode1_1  |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
datanode1_1  |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
datanode1_1  |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
datanode1_1  |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
datanode1_1  |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
datanode1_1  |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
datanode1_1  |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
datanode1_1  |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
datanode1_1  |                        82b7771fe8683859200f859a8629c61a
datanode1_1  |        Extensions: 
datanode1_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode1_1  |     Tagged [7] IMPLICIT 
datanode1_1  |         DER Octet String[4] 
datanode1_1  |     Tagged [2] IMPLICIT 
datanode1_1  |         DER Octet String[8] 
datanode1_1  | 
datanode1_1  |                        critical(true) BasicConstraints: isCa(true)
datanode1_1  |                        critical(true) KeyUsage: 0xbe
datanode1_1  |  from file: /data/metadata/dn/certs/CA-299862451261.crt.
datanode1_1  | 2023-06-30 12:22:09,027 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116798024 ms and interval 86400000 ms.
datanode1_1  | 2023-06-30 12:22:09,797 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2023-06-30 12:22:10,112 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode1_1  | 2023-06-30 12:22:10,670 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z).
datanode1_1  | 2023-06-30 12:22:10,676 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M8.460872S and interval of PT1M
datanode1_1  | 2023-06-30 12:22:10,818 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode1_1  | 2023-06-30 12:22:11,061 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2023-06-30 12:22:12,634 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2023-06-30 12:22:12,833 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode1_1  | 2023-06-30 12:22:12,874 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2023-06-30 12:22:12,886 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2023-06-30 12:22:13,210 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2023-06-30 12:22:13,261 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-06-30 12:22:13,288 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode1_1  | 2023-06-30 12:22:13,319 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode1_1  | 2023-06-30 12:22:13,324 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2023-06-30 12:22:13,326 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode1_1  | 2023-06-30 12:22:13,617 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2023-06-30 12:22:13,628 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2023-06-30 12:22:20,764 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode1_1  | 2023-06-30 12:22:22,267 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode1_1  | 2023-06-30 12:22:22,857 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode1_1  | 2023-06-30 12:22:23,378 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-06-30 12:22:23,784 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2023-06-30 12:22:24,535 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-30 12:22:24,557 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2023-06-30 12:22:24,557 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-06-30 12:22:24,558 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2023-06-30 12:22:24,562 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode1_1  | 2023-06-30 12:22:24,562 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2023-06-30 12:22:24,572 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2023-06-30 12:22:24,579 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 12:22:24,585 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2023-06-30 12:22:24,598 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-30 12:22:24,722 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-06-30 12:22:24,768 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode1_1  | 2023-06-30 12:22:24,776 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-06-30 12:22:32,035 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode1_1  | 2023-06-30 12:22:32,271 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode1_1  | 2023-06-30 12:22:32,282 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode1_1  | 2023-06-30 12:22:32,287 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode1_1  | 2023-06-30 12:22:32,298 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode1_1  | 2023-06-30 12:22:32,341 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode1_1  | 2023-06-30 12:22:32,353 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode1_1  | 2023-06-30 12:22:32,434 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode1_1  | 2023-06-30 12:22:32,457 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode1_1  | 2023-06-30 12:22:32,480 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode1_1  | 2023-06-30 12:22:32,483 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode1_1  | 2023-06-30 12:22:32,968 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode1_1  | 2023-06-30 12:22:32,978 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode1_1  | 2023-06-30 12:22:32,995 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-30 12:22:32,995 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-30 12:22:33,069 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-30 12:22:33,106 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa602c8c4] REGISTERED
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-06-30 12:22:13,985 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 299862451261
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om2_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
om2_1        |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: bceb238776cef1f96370453157455bfacebc30a6
om2_1        |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
om2_1        |                        92b8465e76c5427a65937f03f4c7003698371efa
om2_1        |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
om2_1        |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
om2_1        |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
om2_1        |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
om2_1        |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
om2_1        |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
om2_1        |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
om2_1        |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
om2_1        |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
om2_1        |                        82b7771fe8683859200f859a8629c61a
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-299862451261.crt.
om2_1        | 2023-06-30 12:22:14,026 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797005 ms and interval 86400000 ms.
om2_1        | 2023-06-30 12:22:14,031 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | 2023-06-30 12:22:14,132 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-06-30 12:22:30,321 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
datanode3_1  |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
datanode3_1  |                        5a94528eee6bdc4274bc019b1d82a247fece378c
datanode3_1  |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
datanode3_1  |                        c78036d6743d01601451bd47bf51bec81c366e20
datanode3_1  |                        347a5c23f63ab77d1a66e410a5866e0f
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
datanode3_1  |                        critical(true) KeyUsage: 0x6
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
datanode3_1  | 
datanode3_1  |  from file: /data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2023-06-30 12:22:07,658 [main] INFO client.DNCertificateClient: Added certificate   [0]         Version: 3
datanode3_1  |          SerialNumber: 299862451261
datanode3_1  |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode3_1  |            Start Date: Fri Jun 30 12:19:58 UTC 2023
datanode3_1  |            Final Date: Mon Aug 07 12:19:58 UTC 2028
datanode3_1  |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
datanode3_1  |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
datanode3_1  |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
datanode3_1  | public exponent: 10001
datanode3_1  | 
datanode3_1  |   Signature Algorithm: SHA256WITHRSA
datanode3_1  |             Signature: bceb238776cef1f96370453157455bfacebc30a6
datanode3_1  |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
datanode3_1  |                        92b8465e76c5427a65937f03f4c7003698371efa
datanode3_1  |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
datanode3_1  |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
datanode3_1  |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
datanode3_1  |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
datanode3_1  |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
datanode3_1  |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
datanode3_1  |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
datanode3_1  |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
datanode3_1  |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
datanode3_1  |                        82b7771fe8683859200f859a8629c61a
datanode3_1  |        Extensions: 
datanode3_1  |                        critical(false) 2.5.29.17 value = Sequence
datanode3_1  |     Tagged [7] IMPLICIT 
datanode3_1  |         DER Octet String[4] 
datanode3_1  |     Tagged [2] IMPLICIT 
datanode3_1  |         DER Octet String[8] 
datanode3_1  | 
datanode3_1  |                        critical(true) BasicConstraints: isCa(true)
datanode3_1  |                        critical(true) KeyUsage: 0xbe
datanode3_1  |  from file: /data/metadata/dn/certs/CA-299862451261.crt.
datanode3_1  | 2023-06-30 12:22:07,717 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor for dn is started with first delay 29116797335 ms and interval 86400000 ms.
datanode3_1  | 2023-06-30 12:22:08,224 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2023-06-30 12:22:08,518 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 26, TTL PT1H
datanode3_1  | 2023-06-30 12:22:10,375 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z).
datanode3_1  | 2023-06-30 12:22:10,376 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT3M8.760063S and interval of PT1M
datanode3_1  | 2023-06-30 12:22:10,562 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode3_1  | 2023-06-30 12:22:10,787 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode3_1  | 2023-06-30 12:22:12,055 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2023-06-30 12:22:12,193 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode3_1  | 2023-06-30 12:22:12,226 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2023-06-30 12:22:12,229 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2023-06-30 12:22:12,566 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode3_1  | 2023-06-30 12:22:12,605 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-30 12:22:12,626 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2023-06-30 12:22:12,659 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2023-06-30 12:22:12,666 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2023-06-30 12:22:12,668 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2023-06-30 12:22:12,914 [Thread-9] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2023-06-30 12:22:12,919 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2023-06-30 12:22:20,945 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode3_1  | 2023-06-30 12:22:22,463 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode3_1  | 2023-06-30 12:22:22,575 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode3_1  | 2023-06-30 12:22:22,974 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-06-30 12:22:23,363 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2023-06-30 12:22:24,361 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-30 12:22:24,394 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2023-06-30 12:22:24,408 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-06-30 12:22:24,416 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2023-06-30 12:22:24,425 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode3_1  | 2023-06-30 12:22:24,425 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2023-06-30 12:22:24,440 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0xbe
om1_1        |  from file: /data/metadata/om/certs/CA-299862451261.crt.
om1_1        | 2023-06-30 12:22:13,126 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 433148233533
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Start Date: Fri Jun 30 12:22:11 UTC 2023
om1_1        |            Final Date: Sat Jun 29 12:22:11 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Public Key: RSA Public Key [f6:ea:f6:d9:03:ad:99:83:27:2b:d4:dc:59:d6:27:d5:66:ee:1e:6e],[56:66:d1:a4]
om1_1        |         modulus: b144932dc7ba95027fd3f34ef2203dba73ab7626ddd0ab4341675c985db9a763c8801a43324894ae799cc9106d311d2b7cb4e1424677bf648a4cdc5a8c2af8b3b1c3a03e79d5df44866e58ea5749bb2471a9e18fd4cb846a0a8cb3394961306247e53a19059643e0939ccd8b86ccbdab980d0fb73ea082b3b3b5ae23724615202e971fa1bb7a1404c378bb5657d5d646723dda337a320fcf8501c3e9604175e0cde6830184393c9075d4707976e94f1a5dd0ba10e81e7a1522d359e3e5f5a33e0a80e19bd857868b51713b1200ffd7caeadd303e5aabe24b749c9f85a6c3c42de9c1fc1db29331b0bb082adacff051dc3dfb9ccfe78737d2e43ab66325ce9a1b
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 6106b9f6232084b6e6427aa93c49f097edfdb3f7
om1_1        |                        6009f8c842e9915e26a0845f5c3002ae82024f80
om1_1        |                        f0e622cd8b428f8acb9fc6f5345c8c2d23fd8eed
om1_1        |                        ce9fd5f8c3c63400f6001fcb273975111642b20d
om1_1        |                        6ed1fe9017ddffeb9d5c1786aa08b64765d47fc7
om1_1        |                        5b47793ff0c459beb3c9c9d5caad54a6af107ecd
om1_1        |                        daca156668c0fccb07198f3750e9705bc187e40b
om1_1        |                        baa0f2337f85acb329eb88d03d02967038c67b09
om1_1        |                        76c9d3537570713a6ca66b263cac8be0e1a545fe
om1_1        |                        8005b54e67fc7b8259ce369fed37a2f06c97b8da
om1_1        |                        0317b3a4b86e6a31f3195eb638d37befeb15db98
om1_1        |                        839a0920bd67a6ac97873207e578eebd69437130
om1_1        |                        bb496acd7ab7ce97f3598cedb9e5ee3d
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/433148233533.crt.
om1_1        | 2023-06-30 12:22:13,199 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116797861 ms and interval 86400000 ms.
om1_1        | 2023-06-30 12:22:13,200 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | 2023-06-30 12:22:13,233 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-06-30 12:22:29,598 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
om1_1        | STARTUP_MSG:   java = 11.0.19
om1_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1        | ************************************************************/
om1_1        | 2023-06-30 12:22:29,699 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-06-30 12:22:39,641 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-06-30 12:22:43,434 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om1_1        | 2023-06-30 12:22:44,062 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-06-30 12:22:44,069 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om1: om1
om1_1        | 2023-06-30 12:22:44,073 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1        | 2023-06-30 12:22:44,184 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 12:22:44,821 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om1_1        | 2023-06-30 12:22:47,156 [main] INFO reflections.Reflections: Reflections took 1976 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om1_1        | 2023-06-30 12:22:47,390 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om1_1        | 2023-06-30 12:22:49,277 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-06-30 12:22:49,277 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-06-30 12:22:49,278 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 12:22:53,846 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-30 12:22:54,687 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-06-30 12:23:00,729 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-06-30 12:23:02,293 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 1
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om1_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om1_1        |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
om1_1        |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
om1_1        |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
om1_1        |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
om1_1        |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
om1_1        |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
om1_1        |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
om1_1        |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
om1_1        |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
om1_1        |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
om1_1        |                        5a94528eee6bdc4274bc019b1d82a247fece378c
om1_1        |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
om1_1        |                        c78036d6743d01601451bd47bf51bec81c366e20
om1_1        |                        347a5c23f63ab77d1a66e410a5866e0f
om1_1        |        Extensions: 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0x6
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-06-30 12:23:02,321 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 299862451261
om1_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om1_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om1_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
om1_1        |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: bceb238776cef1f96370453157455bfacebc30a6
om1_1        |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
om1_1        |                        92b8465e76c5427a65937f03f4c7003698371efa
om1_1        |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
om1_1        |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
om1_1        |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
om1_1        |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
om1_1        |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
om1_1        |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
om1_1        |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
om1_1        |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
om1_1        |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
om1_1        |                        82b7771fe8683859200f859a8629c61a
om1_1        |        Extensions: 
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        |     Tagged [2] IMPLICIT 
om1_1        |         DER Octet String[8] 
om1_1        | 
om1_1        |                        critical(true) BasicConstraints: isCa(true)
om1_1        |                        critical(true) KeyUsage: 0xbe
om1_1        |  from file: /data/metadata/om/certs/CA-299862451261.crt.
om1_1        | 2023-06-30 12:23:02,347 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om1_1        |          SerialNumber: 433148233533
om1_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Start Date: Fri Jun 30 12:22:11 UTC 2023
om1_1        |            Final Date: Sat Jun 29 12:22:11 UTC 2024
om1_1        |             SubjectDN: CN=om1,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om1_1        |            Public Key: RSA Public Key [f6:ea:f6:d9:03:ad:99:83:27:2b:d4:dc:59:d6:27:d5:66:ee:1e:6e],[56:66:d1:a4]
om1_1        |         modulus: b144932dc7ba95027fd3f34ef2203dba73ab7626ddd0ab4341675c985db9a763c8801a43324894ae799cc9106d311d2b7cb4e1424677bf648a4cdc5a8c2af8b3b1c3a03e79d5df44866e58ea5749bb2471a9e18fd4cb846a0a8cb3394961306247e53a19059643e0939ccd8b86ccbdab980d0fb73ea082b3b3b5ae23724615202e971fa1bb7a1404c378bb5657d5d646723dda337a320fcf8501c3e9604175e0cde6830184393c9075d4707976e94f1a5dd0ba10e81e7a1522d359e3e5f5a33e0a80e19bd857868b51713b1200ffd7caeadd303e5aabe24b749c9f85a6c3c42de9c1fc1db29331b0bb082adacff051dc3dfb9ccfe78737d2e43ab66325ce9a1b
om1_1        | public exponent: 10001
om1_1        | 
om1_1        |   Signature Algorithm: SHA256WITHRSA
om1_1        |             Signature: 6106b9f6232084b6e6427aa93c49f097edfdb3f7
om1_1        |                        6009f8c842e9915e26a0845f5c3002ae82024f80
om1_1        |                        f0e622cd8b428f8acb9fc6f5345c8c2d23fd8eed
om1_1        |                        ce9fd5f8c3c63400f6001fcb273975111642b20d
om1_1        |                        6ed1fe9017ddffeb9d5c1786aa08b64765d47fc7
om1_1        |                        5b47793ff0c459beb3c9c9d5caad54a6af107ecd
om1_1        |                        daca156668c0fccb07198f3750e9705bc187e40b
om1_1        |                        baa0f2337f85acb329eb88d03d02967038c67b09
om1_1        |                        76c9d3537570713a6ca66b263cac8be0e1a545fe
om1_1        |                        8005b54e67fc7b8259ce369fed37a2f06c97b8da
om1_1        |                        0317b3a4b86e6a31f3195eb638d37befeb15db98
om1_1        |                        839a0920bd67a6ac97873207e578eebd69437130
om1_1        |                        bb496acd7ab7ce97f3598cedb9e5ee3d
om1_1        |        Extensions: 
datanode2_1  | 2023-06-30 12:22:44,022 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode2_1  | 2023-06-30 12:22:46,355 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode2_1  | 2023-06-30 12:22:46,356 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode2_1  | 2023-06-30 12:22:46,392 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode2_1  | 2023-06-30 12:22:46,599 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode2_1  | 2023-06-30 12:22:46,599 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode2_1  | 2023-06-30 12:22:47,521 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2023-06-30 12:22:47,671 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode2_1  | 2023-06-30 12:22:50,707 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-9c10c649-f59d-4933-ad5d-654b70cf4739/DS-ee76c7a7-2f27-4b25-8af3-9700781491c9/container.db to cache
datanode2_1  | 2023-06-30 12:22:50,714 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-9c10c649-f59d-4933-ad5d-654b70cf4739/DS-ee76c7a7-2f27-4b25-8af3-9700781491c9/container.db for volume DS-ee76c7a7-2f27-4b25-8af3-9700781491c9
datanode2_1  | 2023-06-30 12:22:50,764 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2023-06-30 12:22:50,839 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode2_1  | 2023-06-30 12:22:51,244 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode2_1  | 2023-06-30 12:22:51,256 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
datanode2_1  | 2023-06-30 12:22:51,592 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.RaftServer: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: start RPC server
datanode2_1  | 2023-06-30 12:22:51,743 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: GrpcService started, listening on 9858
datanode2_1  | 2023-06-30 12:22:51,831 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: GrpcService started, listening on 9856
datanode2_1  | 2023-06-30 12:22:51,840 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: GrpcService started, listening on 9857
datanode2_1  | 2023-06-30 12:22:51,919 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a9efabc3-aa7d-48e7-8786-4f3f9c9666fc is started using port 9858 for RATIS
datanode2_1  | 2023-06-30 12:22:51,920 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a9efabc3-aa7d-48e7-8786-4f3f9c9666fc is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2023-06-30 12:22:51,920 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a9efabc3-aa7d-48e7-8786-4f3f9c9666fc is started using port 9856 for RATIS_SERVER
datanode2_1  | 2023-06-30 12:22:51,920 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a9efabc3-aa7d-48e7-8786-4f3f9c9666fc is started using port 9855 for RATIS_DATASTREAM
datanode2_1  | 2023-06-30 12:22:51,926 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: Started
datanode2_1  | 2023-06-30 12:22:52,106 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-30 12:22:52,106 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-06-30 12:22:52,165 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode2_1  | 2023-06-30 12:23:26,593 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: addNew group-0B2DA0E47784:[a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-0B2DA0E47784:java.util.concurrent.CompletableFuture@4f1b2568[Not completed]
datanode2_1  | 2023-06-30 12:23:26,859 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: new RaftServerImpl for group-0B2DA0E47784:[a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-30 12:23:26,872 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-30 12:23:26,880 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-30 12:23:26,880 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-30 12:23:26,881 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-30 12:23:26,893 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-30 12:23:26,893 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-30 12:23:26,986 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784: ConfigurationManager, init=-1: peers:[a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-30 12:23:26,993 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-30 12:23:27,055 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-30 12:23:27,059 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-30 12:23:27,225 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-30 12:22:33,139 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa602c8c4] BIND: 0.0.0.0/0.0.0.0:9855
datanode1_1  | 2023-06-30 12:22:33,185 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa602c8c4, L:/0.0.0.0:9855] ACTIVE
datanode1_1  | 2023-06-30 12:22:33,467 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode1_1  | 2023-06-30 12:22:34,407 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode1_1  | 2023-06-30 12:22:35,839 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode1_1  | 2023-06-30 12:22:35,841 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2023-06-30 12:22:35,851 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2023-06-30 12:22:36,229 [main] INFO util.log: Logging initialized @76705ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2023-06-30 12:22:37,548 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2023-06-30 12:22:37,616 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2023-06-30 12:22:37,633 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2023-06-30 12:22:37,634 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode1_1  | 2023-06-30 12:22:37,637 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2023-06-30 12:22:37,658 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2023-06-30 12:22:38,100 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode1_1  | 2023-06-30 12:22:38,109 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2023-06-30 12:22:38,125 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode1_1  | 2023-06-30 12:22:38,372 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2023-06-30 12:22:38,374 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2023-06-30 12:22:38,386 [main] INFO server.session: node0 Scavenging every 660000ms
datanode1_1  | 2023-06-30 12:22:38,552 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-30 12:22:38,596 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3d8d926f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2023-06-30 12:22:38,608 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@d884e9a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2023-06-30 12:22:39,531 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-06-30 12:22:39,692 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@42ea4bac{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-11675899592881295464/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2023-06-30 12:22:39,816 [main] INFO server.AbstractConnector: Started ServerConnector@1361b39a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2023-06-30 12:22:39,827 [main] INFO server.Server: Started @80307ms
datanode1_1  | 2023-06-30 12:22:39,846 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2023-06-30 12:22:39,850 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2023-06-30 12:22:39,858 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2023-06-30 12:22:40,189 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode1_1  | 2023-06-30 12:22:40,222 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSExceptionProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONProvider
httpfs_1     |   class org.apache.ozone.lib.wsrs.JSONMapProvider
httpfs_1     |   class org.apache.ozone.fs.http.server.HttpFSParametersProvider
httpfs_1     | Jun 30, 2023 12:19:45 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
httpfs_1     | INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
httpfs_1     | 12:19:48,662  INFO ContextHandler:921 - Started o.e.j.w.WebAppContext@2af4129d{webhdfs,/,file:///tmp/hadoop-hadoop/httpfs/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-httpfsgateway-1.4.0-SNAPSHOT.jar!/webapps/webhdfs}
httpfs_1     | 12:19:48,705  INFO AbstractConnector:333 - Started ServerConnector@173ed316{HTTP/1.1, (http/1.1)}{0.0.0.0:14000}
httpfs_1     | 12:19:48,709  INFO Server:415 - Started @14699ms
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
om2_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1        | ************************************************************/
om2_1        | 2023-06-30 12:22:30,395 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-06-30 12:22:40,458 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-06-30 12:22:43,891 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om2_1        | 2023-06-30 12:22:44,819 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-06-30 12:22:44,822 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om2: om2
om2_1        | 2023-06-30 12:22:44,890 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1        | 2023-06-30 12:22:45,082 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 12:22:45,958 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om2_1        | 2023-06-30 12:22:48,163 [main] INFO reflections.Reflections: Reflections took 1644 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om2_1        | 2023-06-30 12:22:48,445 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om2_1        | 2023-06-30 12:22:51,389 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-06-30 12:22:51,390 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-06-30 12:22:51,396 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 12:22:56,036 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-30 12:22:56,704 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-06-30 12:23:01,838 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-06-30 12:23:03,565 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 433509364865
om2_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Start Date: Fri Jun 30 12:22:11 UTC 2023
om2_1        |            Final Date: Sat Jun 29 12:22:11 UTC 2024
datanode3_1  | 2023-06-30 12:22:24,446 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 12:22:24,460 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2023-06-30 12:22:24,482 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-30 12:22:24,669 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-06-30 12:22:24,736 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode3_1  | 2023-06-30 12:22:24,747 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-06-30 12:22:32,679 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode3_1  | 2023-06-30 12:22:32,931 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode3_1  | 2023-06-30 12:22:32,939 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
datanode3_1  | 2023-06-30 12:22:32,942 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode3_1  | 2023-06-30 12:22:32,947 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode3_1  | 2023-06-30 12:22:32,973 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode3_1  | 2023-06-30 12:22:32,976 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode3_1  | 2023-06-30 12:22:33,053 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode3_1  | 2023-06-30 12:22:33,079 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode3_1  | 2023-06-30 12:22:33,116 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode3_1  | 2023-06-30 12:22:33,121 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 9855 (custom)
datanode3_1  | 2023-06-30 12:22:33,560 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode3_1  | 2023-06-30 12:22:33,577 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode3_1  | 2023-06-30 12:22:33,581 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-30 12:22:33,583 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-30 12:22:33,617 [c26c1b41-9621-4ea7-a0f2-632ca831791f-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xff431034] REGISTERED
datanode3_1  | 2023-06-30 12:22:33,629 [c26c1b41-9621-4ea7-a0f2-632ca831791f-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xff431034] BIND: 0.0.0.0/0.0.0.0:9855
datanode3_1  | 2023-06-30 12:22:33,640 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-30 12:22:33,683 [c26c1b41-9621-4ea7-a0f2-632ca831791f-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xff431034, L:/0.0.0.0:9855] ACTIVE
datanode3_1  | 2023-06-30 12:22:34,030 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode3_1  | 2023-06-30 12:22:34,775 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode3_1  | 2023-06-30 12:22:36,221 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2023-06-30 12:22:36,232 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2023-06-30 12:22:36,238 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2023-06-30 12:22:36,531 [main] INFO util.log: Logging initialized @78026ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2023-06-30 12:22:37,836 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2023-06-30 12:22:37,916 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2023-06-30 12:22:37,936 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2023-06-30 12:22:37,946 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2023-06-30 12:22:37,947 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2023-06-30 12:22:37,978 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2023-06-30 12:22:38,452 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode3_1  | 2023-06-30 12:22:38,474 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2023-06-30 12:22:38,493 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode3_1  | 2023-06-30 12:22:38,831 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2023-06-30 12:22:38,833 [main] INFO server.session: No SessionScavenger set, using defaults
datanode3_1  | 2023-06-30 12:22:38,859 [main] INFO server.session: node0 Scavenging every 660000ms
datanode3_1  | 2023-06-30 12:22:39,035 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-30 12:22:39,059 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@248e31a1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2023-06-30 12:22:39,066 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@59ea2a36{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2023-06-30 12:22:40,252 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-06-30 12:22:40,359 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@515c3dd2{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-9340703732403661120/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2023-06-30 12:22:40,439 [main] INFO server.AbstractConnector: Started ServerConnector@1eaf95d2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2023-06-30 12:22:40,442 [main] INFO server.Server: Started @81941ms
datanode3_1  | 2023-06-30 12:22:40,465 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2023-06-30 12:22:40,467 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2023-06-30 12:22:40,475 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2023-06-30 12:22:40,741 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode3_1  | 2023-06-30 12:22:40,807 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode1_1  | 2023-06-30 12:22:40,247 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode1_1  | 2023-06-30 12:22:42,200 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode1_1  | 2023-06-30 12:22:42,202 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode1_1  | 2023-06-30 12:22:42,203 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode1_1  | 2023-06-30 12:22:42,234 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode1_1  | 2023-06-30 12:22:42,322 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 2023-06-30 12:22:43,366 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode1_1  | 2023-06-30 12:22:43,439 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode1_1  | 2023-06-30 12:22:49,749 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-9c10c649-f59d-4933-ad5d-654b70cf4739/DS-4678f5ba-27b4-4a63-8672-4acf4f450e8a/container.db to cache
datanode1_1  | 2023-06-30 12:22:49,751 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-9c10c649-f59d-4933-ad5d-654b70cf4739/DS-4678f5ba-27b4-4a63-8672-4acf4f450e8a/container.db for volume DS-4678f5ba-27b4-4a63-8672-4acf4f450e8a
datanode1_1  | 2023-06-30 12:22:49,791 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2023-06-30 12:22:49,815 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode1_1  | 2023-06-30 12:22:50,044 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode1_1  | 2023-06-30 12:22:50,048 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 30c441e1-2cd9-4c36-9389-fe8b01b88a57
datanode1_1  | 2023-06-30 12:22:50,411 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: start RPC server
datanode1_1  | 2023-06-30 12:22:50,441 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: GrpcService started, listening on 9858
datanode1_1  | 2023-06-30 12:22:50,460 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: GrpcService started, listening on 9856
datanode1_1  | 2023-06-30 12:22:50,470 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: GrpcService started, listening on 9857
datanode1_1  | 2023-06-30 12:22:50,553 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 30c441e1-2cd9-4c36-9389-fe8b01b88a57 is started using port 9858 for RATIS
datanode1_1  | 2023-06-30 12:22:50,554 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 30c441e1-2cd9-4c36-9389-fe8b01b88a57 is started using port 9857 for RATIS_ADMIN
datanode1_1  | 2023-06-30 12:22:50,554 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 30c441e1-2cd9-4c36-9389-fe8b01b88a57 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2023-06-30 12:22:50,554 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 30c441e1-2cd9-4c36-9389-fe8b01b88a57 is started using port 9855 for RATIS_DATASTREAM
datanode1_1  | 2023-06-30 12:22:50,560 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-30c441e1-2cd9-4c36-9389-fe8b01b88a57: Started
datanode1_1  | 2023-06-30 12:22:50,711 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode1_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode1_1  | Caused by: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	... 1 more
datanode1_1  | 2023-06-30 12:22:50,711 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-30 12:22:50,711 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-06-30 12:22:50,907 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode1_1  | 2023-06-30 12:23:24,177 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: addNew group-31D80CAD3087:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] returns group-31D80CAD3087:java.util.concurrent.CompletableFuture@79e1f76c[Not completed]
datanode1_1  | 2023-06-30 12:23:24,490 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: new RaftServerImpl for group-31D80CAD3087:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-30 12:23:24,498 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | Waiting for the service scm3.org:9894
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-06-30 12:21:34,502 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-30 12:21:34,590 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-06-30 12:21:47,217 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-30 12:21:51,078 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-30 12:21:51,845 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-30 12:21:51,845 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
om3_1        | 2023-06-30 12:21:51,851 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-30 12:21:53,436 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-30 12:21:53,436 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-30 12:21:53,510 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 12:21:54,868 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-9c10c649-f59d-4933-ad5d-654b70cf4739;layoutVersion=6
om3_1        | 2023-06-30 12:21:58,537 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om3_1        | 2023-06-30 12:21:58,537 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2023-06-30 12:21:59,579 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-30 12:22:05,696 [main] WARN security.OMCertificateClient: Certificates could not be loaded.
om3_1        | java.nio.file.NoSuchFileException: /data/metadata/om/certs
om3_1        | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
om3_1        | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
om3_1        | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
om3_1        | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
om3_1        | 	at java.base/java.nio.file.Files.list(Files.java:3699)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
om2_1        |             SubjectDN: CN=om2,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Public Key: RSA Public Key [5d:2e:2a:ea:fb:ba:d0:47:bd:6a:6e:87:33:d6:03:63:5f:a7:d0:40],[56:66:d1:a4]
om2_1        |         modulus: ec2c91fcdfa86a2775cb1a3a42e9f1fa9cc7813c126d44798142994b5b664db447d953d1369ba973568885275d00f7b10718623126a755d96c4a41aed35f81e5c8242b0dccface550d18a543e2bde55c071de07acad67ff72cb19e29b233fe225090698d25dff01b889afdce236021eb684cdea4615f109fc1d47252ff3d965a21ab01ad3c7ca68649b277bd4e889bc83dab6b106d246f0f9e5d19f183aff048b731c651caa626e2923f9f1d0d7792953f796c62bd02e7516bde9a86e804e03d76d14ae38e4eefb419bcec4a0851cb385074e3feb5dd65465c36a9ecf4ddecc1e5b70bff9825d6dd54eaf67d4868a739df2d2376ae609afb46137108d9cefe71
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 0ebdc84d902ca541526961c0d6e47a3125626ee9
om2_1        |                        7f250c1d782c9d378dd50c0742108264b3d7f478
om2_1        |                        3346c2bec5c1ef1f632ca77c1bd6e7edd4a3f5a4
om2_1        |                        7aaabf31510a65cd0284ac181840bed490f83f8a
om2_1        |                        6d5594aaa099fb87d7ecee664a5ca9ed99b9ae51
om2_1        |                        e288ad0d39f5f32503ac53e1af18ddb6ba729336
om2_1        |                        591d3f716492edddb328cb3bdee71eefce15f10f
om2_1        |                        054344a94f5f60377c1fd081190155e64c326e71
om2_1        |                        02be9810fe299ccf49b8026a40794301fcd9a861
om2_1        |                        e5e767c6c3ce4da12e43f5d4e4c4c1384644042e
om2_1        |                        3ca01711ffa5f2ca9e85793843747404a05d6209
om2_1        |                        4af78cb6686537cc8dd87d38ec028057a0bfcbac
om2_1        |                        dadfd528d65f29ab36f8015f2c3a1664
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        | 
om2_1        |                        critical(true) KeyUsage: 0xb8
om2_1        |  from file: /data/metadata/om/certs/433509364865.crt.
om2_1        | 2023-06-30 12:23:03,616 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 1
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om2_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om2_1        |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
om2_1        |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
om2_1        |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
om2_1        |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
om2_1        |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
om2_1        |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
om2_1        |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
om2_1        |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
om2_1        |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
om2_1        |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
om2_1        |                        5a94528eee6bdc4274bc019b1d82a247fece378c
om2_1        |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
om2_1        |                        c78036d6743d01601451bd47bf51bec81c366e20
om2_1        |                        347a5c23f63ab77d1a66e410a5866e0f
om2_1        |        Extensions: 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0x6
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-06-30 12:23:03,644 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om2_1        |          SerialNumber: 299862451261
om2_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om2_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om2_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om2_1        |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
om2_1        |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
om2_1        | public exponent: 10001
om2_1        | 
om2_1        |   Signature Algorithm: SHA256WITHRSA
om2_1        |             Signature: bceb238776cef1f96370453157455bfacebc30a6
om2_1        |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
om2_1        |                        92b8465e76c5427a65937f03f4c7003698371efa
om2_1        |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
om2_1        |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
om2_1        |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
om2_1        |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
om2_1        |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
om2_1        |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
om2_1        |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
om2_1        |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
om2_1        |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
om2_1        |                        82b7771fe8683859200f859a8629c61a
om2_1        |        Extensions: 
om2_1        |                        critical(false) 2.5.29.17 value = Sequence
om2_1        |     Tagged [7] IMPLICIT 
om2_1        |         DER Octet String[4] 
om2_1        |     Tagged [2] IMPLICIT 
om2_1        |         DER Octet String[8] 
om2_1        | 
om2_1        |                        critical(true) BasicConstraints: isCa(true)
om2_1        |                        critical(true) KeyUsage: 0xbe
om2_1        |  from file: /data/metadata/om/certs/CA-299862451261.crt.
om3_1        | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
om3_1        | 	at org.apache.hadoop.ozone.security.OMCertificateClient.<init>(OMCertificateClient.java:71)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1383)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:1362)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.init(OzoneManagerStarter.java:204)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.initOm(OzoneManagerStarter.java:102)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om3_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om3_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om3_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om3_1        | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om3_1        | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om3_1        | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om3_1        | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om3_1        | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om3_1        | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om3_1        | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om3_1        | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om3_1        | 2023-06-30 12:22:05,721 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2023-06-30 12:22:05,732 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om3_1        | 2023-06-30 12:22:05,739 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2023-06-30 12:22:12,595 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2023-06-30 12:22:12,932 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2023-06-30 12:22:12,937 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2023-06-30 12:22:12,986 [main] ERROR utils.CertificateSignRequest: Invalid domain om3
om3_1        | 2023-06-30 12:22:12,989 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,clusterId:CID-9c10c649-f59d-4933-ad5d-654b70cf4739,subject:om3
om3_1        | 2023-06-30 12:22:15,423 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om3_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
om3_1        |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
om3_1        |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
om3_1        |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
om3_1        |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
om3_1        |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
om3_1        |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
om3_1        |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
om3_1        |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
om3_1        |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
om3_1        |                        5a94528eee6bdc4274bc019b1d82a247fece378c
om3_1        |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
om3_1        |                        c78036d6743d01601451bd47bf51bec81c366e20
om3_1        |                        347a5c23f63ab77d1a66e410a5866e0f
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-30 12:22:15,457 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 435562639432
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Start Date: Fri Jun 30 12:22:14 UTC 2023
om3_1        |            Final Date: Sat Jun 29 12:22:14 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Public Key: RSA Public Key [bf:24:60:84:a2:f4:39:b7:cb:b7:22:80:41:70:82:01:55:bd:73:bc],[56:66:d1:a4]
om3_1        |         modulus: df064c3ffcd1b04a1ec7fdf3a277f3c5d6ffa99c1637e0cba8564562c2e0a16027c3d48628dc7abe0e4f570bfbed36510b4ff1a123a00bff9626da8980105be3d6cc5fc3f5b632596419b5177a62645a67df9a3063d7448aa609e5948b0b7c4e81c78209d80111b6e841a8da2173f472ab6a5fb9fe3df24edfc920716ea8c7bb5ef6af1dc805b3e3b91402f25f7f4af13ada022ff1d9eeebb1f150085b727b9469d7b06cd3e65acaef5dd46810868db3a69613f01402145b83340a469a691e8a5b461c3374fc0b317a2128565d6998806156bd794e1e4d13b2be86ff0dce0cd541f798ea4ebad9bc3267560b45bcabf849e9dce6617541a38c80f3d5587a7703
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 163c7be70d98401313ece8c785b9565ed6644f74
om3_1        |                        e7ea0f62621f2b5fa9e48b2dfe87f39d82699cd7
om3_1        |                        7c8f486fa90f878a7b6d81292072777c4563ee9b
om3_1        |                        970fcd79148c39f487a2af73e025e1186ec1af47
om3_1        |                        a2c8d80bc3cccd6c2243fe16c6c9c06272ecb67e
om3_1        |                        4312b8a5082e467e8198e9159767712a895adf13
om3_1        |                        f2d13307322b058305b5a9e6454f10f960571258
om3_1        |                        5c8e3055a893cb667b1a8fb7458e8c9ffdb92a3b
om3_1        |                        1fa8c5c36d28b0b6fe46ac748cfd6ae7c3b6121c
om3_1        |                        8d63ca29b5508bcd3f657ffb6907a2e94bf11157
om3_1        |                        45d0d3156425fe634b9e3f4d9cb5218f1d878f01
om3_1        |                        9d123bfdf68304a2963e37a51382cbdf33a86d15
om3_1        |                        12b1017ac30a8f3a9e4f411d735d226d
datanode2_1  | 2023-06-30 12:23:27,252 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-30 12:23:27,278 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-30 12:23:27,279 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-30 12:23:27,482 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-30 12:23:27,641 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-30 12:23:27,704 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-30 12:23:27,727 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-30 12:23:27,730 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-30 12:23:27,823 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-30 12:23:27,823 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-30 12:23:27,824 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d5b5f96c-9e99-4dc5-8f79-0b2da0e47784 does not exist. Creating ...
datanode2_1  | 2023-06-30 12:23:27,890 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d5b5f96c-9e99-4dc5-8f79-0b2da0e47784/in_use.lock acquired by nodename 6@b043fa92fe53
datanode2_1  | 2023-06-30 12:23:27,984 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d5b5f96c-9e99-4dc5-8f79-0b2da0e47784 has been successfully formatted.
datanode2_1  | 2023-06-30 12:23:28,172 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO ratis.ContainerStateMachine: group-0B2DA0E47784: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-30 12:23:28,216 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-30 12:23:28,476 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-30 12:23:28,486 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 12:23:28,506 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-30 12:23:28,517 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-30 12:23:28,583 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 12:23:28,689 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-30 12:23:28,700 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-06-30 12:23:28,700 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 12:23:28,783 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d5b5f96c-9e99-4dc5-8f79-0b2da0e47784
datanode2_1  | 2023-06-30 12:23:28,795 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-06-30 12:23:28,797 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 12:23:28,807 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 12:23:28,810 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-06-30 12:23:28,813 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-30 12:23:28,822 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-30 12:23:28,823 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-30 12:23:28,827 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-30 12:23:29,016 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-06-30 12:23:29,022 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 12:23:29,168 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-30 12:23:29,169 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-06-30 12:23:29,170 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-30 12:23:29,201 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO segmented.SegmentedRaftLogWorker: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 12:23:29,201 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO segmented.SegmentedRaftLogWorker: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 12:23:29,210 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784: start as a follower, conf=-1: peers:[a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 12:23:29,212 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-30 12:22:40,841 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode3_1  | 2023-06-30 12:22:43,463 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [testuser, recon, om, dn]
datanode3_1  | 2023-06-30 12:22:43,464 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode3_1  | 2023-06-30 12:22:43,473 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode3_1  | 2023-06-30 12:22:43,504 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode3_1  | 2023-06-30 12:22:43,528 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode3_1  | 2023-06-30 12:22:44,295 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2023-06-30 12:22:44,385 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode3_1  | 2023-06-30 12:22:49,694 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-9c10c649-f59d-4933-ad5d-654b70cf4739/DS-2d17a23b-7c79-4b37-95f9-6380c541795d/container.db to cache
datanode3_1  | 2023-06-30 12:22:49,695 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-9c10c649-f59d-4933-ad5d-654b70cf4739/DS-2d17a23b-7c79-4b37-95f9-6380c541795d/container.db for volume DS-2d17a23b-7c79-4b37-95f9-6380c541795d
datanode3_1  | 2023-06-30 12:22:49,738 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2023-06-30 12:22:49,771 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode3_1  | 2023-06-30 12:22:49,939 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode3_1  | 2023-06-30 12:22:49,939 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c26c1b41-9621-4ea7-a0f2-632ca831791f
datanode3_1  | 2023-06-30 12:22:50,063 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode3_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode3_1  | Caused by: java.util.concurrent.TimeoutException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	... 1 more
datanode3_1  | 2023-06-30 12:22:50,188 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: c26c1b41-9621-4ea7-a0f2-632ca831791f: start RPC server
datanode3_1  | 2023-06-30 12:22:50,229 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: c26c1b41-9621-4ea7-a0f2-632ca831791f: GrpcService started, listening on 9858
datanode3_1  | 2023-06-30 12:22:50,301 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: c26c1b41-9621-4ea7-a0f2-632ca831791f: GrpcService started, listening on 9856
datanode3_1  | 2023-06-30 12:22:50,306 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: c26c1b41-9621-4ea7-a0f2-632ca831791f: GrpcService started, listening on 9857
datanode3_1  | 2023-06-30 12:22:50,329 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c26c1b41-9621-4ea7-a0f2-632ca831791f is started using port 9858 for RATIS
datanode3_1  | 2023-06-30 12:22:50,330 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c26c1b41-9621-4ea7-a0f2-632ca831791f is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2023-06-30 12:22:50,330 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c26c1b41-9621-4ea7-a0f2-632ca831791f is started using port 9856 for RATIS_SERVER
datanode3_1  | 2023-06-30 12:22:50,331 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c26c1b41-9621-4ea7-a0f2-632ca831791f is started using port 9855 for RATIS_DATASTREAM
datanode3_1  | 2023-06-30 12:22:50,343 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c26c1b41-9621-4ea7-a0f2-632ca831791f: Started
datanode3_1  | 2023-06-30 12:22:50,508 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-30 12:22:50,509 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-06-30 12:22:50,727 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode3_1  | 2023-06-30 12:23:25,744 [PipelineCommandHandlerThread-0] INFO server.RaftServer: c26c1b41-9621-4ea7-a0f2-632ca831791f: addNew group-9491141422F5:[c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] returns group-9491141422F5:java.util.concurrent.CompletableFuture@5b33c5b[Not completed]
datanode3_1  | 2023-06-30 12:23:26,004 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f: new RaftServerImpl for group-9491141422F5:[c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-30 12:23:26,049 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-30 12:23:26,060 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-30 12:23:26,065 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-30 12:23:26,066 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-30 12:23:26,074 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-30 12:23:26,076 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-30 12:23:26,220 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5: ConfigurationManager, init=-1: peers:[c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-30 12:23:26,223 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-30 12:23:26,366 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-30 12:23:26,431 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-30 12:23:26,584 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-30 12:23:26,635 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-30 12:23:26,650 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-30 12:23:26,685 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-30 12:23:26,854 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-30 12:23:27,101 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-30 12:23:27,127 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-30 12:23:27,128 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-30 12:23:27,129 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-30 12:23:27,132 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-30 12:23:27,135 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-30 12:23:27,137 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/394fb16c-ab50-4148-8258-9491141422f5 does not exist. Creating ...
datanode3_1  | 2023-06-30 12:23:27,173 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/394fb16c-ab50-4148-8258-9491141422f5/in_use.lock acquired by nodename 7@aa09f3d2afc3
datanode3_1  | 2023-06-30 12:23:27,220 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/394fb16c-ab50-4148-8258-9491141422f5 has been successfully formatted.
datanode3_1  | 2023-06-30 12:23:27,276 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO ratis.ContainerStateMachine: group-9491141422F5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-30 12:23:27,288 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-30 12:23:27,416 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-30 12:23:27,424 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 12:23:27,445 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-30 12:23:27,470 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-30 12:23:27,499 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 12:23:27,673 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-30 12:23:27,683 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-30 12:23:27,684 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 12:23:27,744 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO segmented.SegmentedRaftLogWorker: new c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/394fb16c-ab50-4148-8258-9491141422f5
datanode3_1  | 2023-06-30 12:23:27,745 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-30 12:23:27,746 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 12:23:27,765 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 12:23:27,767 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-30 12:23:27,774 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-30 12:23:27,781 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-30 12:23:27,790 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-30 12:23:27,792 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2023-06-30 12:23:03,658 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116747352 ms and interval 86400000 ms.
om2_1        | 2023-06-30 12:23:04,170 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
datanode2_1  | 2023-06-30 12:23:29,224 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: start a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState
datanode2_1  | 2023-06-30 12:23:29,233 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-30 12:23:29,238 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-30 12:23:29,273 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0B2DA0E47784,id=a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
datanode2_1  | 2023-06-30 12:23:29,280 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-06-30 12:23:29,283 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-06-30 12:23:29,286 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-06-30 12:23:29,287 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-06-30 12:23:29,368 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=d5b5f96c-9e99-4dc5-8f79-0b2da0e47784
datanode2_1  | 2023-06-30 12:23:29,380 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d5b5f96c-9e99-4dc5-8f79-0b2da0e47784.
datanode2_1  | 2023-06-30 12:23:29,387 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: addNew group-42D4F8D6F6D0:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-42D4F8D6F6D0:java.util.concurrent.CompletableFuture@7e61e162[Not completed]
datanode2_1  | 2023-06-30 12:23:29,398 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: new RaftServerImpl for group-42D4F8D6F6D0:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-06-30 12:23:29,399 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-06-30 12:23:29,400 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-06-30 12:23:29,401 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-06-30 12:23:29,401 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-06-30 12:23:29,402 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-06-30 12:23:29,402 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-06-30 12:23:29,404 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0: ConfigurationManager, init=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-06-30 12:23:29,406 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-06-30 12:23:29,407 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-06-30 12:23:29,408 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-06-30 12:23:29,409 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-06-30 12:23:29,410 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode2_1  | 2023-06-30 12:23:29,410 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-06-30 12:23:29,411 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-06-30 12:23:29,413 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode2_1  | 2023-06-30 12:23:29,417 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-06-30 12:23:29,418 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-06-30 12:23:29,419 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-06-30 12:23:29,419 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-06-30 12:23:29,419 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-06-30 12:23:29,420 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-06-30 12:23:29,421 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0 does not exist. Creating ...
om1_1        |                        critical(false) 2.5.29.17 value = Sequence
om1_1        |     Tagged [7] IMPLICIT 
om1_1        |         DER Octet String[4] 
om1_1        | 
om1_1        |                        critical(true) KeyUsage: 0xb8
om1_1        |  from file: /data/metadata/om/certs/433148233533.crt.
om1_1        | 2023-06-30 12:23:02,363 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116748648 ms and interval 86400000 ms.
om1_1        | 2023-06-30 12:23:02,849 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
datanode1_1  | 2023-06-30 12:23:24,505 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-30 12:23:24,505 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-30 12:23:24,506 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-30 12:23:24,507 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-30 12:23:24,508 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-30 12:23:24,580 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087: ConfigurationManager, init=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-30 12:23:24,581 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-30 12:23:24,622 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-30 12:23:24,631 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-30 12:23:24,728 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-06-30 12:23:24,749 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-30 12:23:24,772 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-30 12:23:24,774 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-30 12:23:24,956 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-30 12:23:25,173 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-30 12:23:25,204 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-30 12:23:25,217 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-30 12:23:25,217 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-30 12:23:25,249 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-30 12:23:25,257 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-30 12:23:25,269 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/db58cf25-deb4-4e5f-a339-31d80cad3087 does not exist. Creating ...
datanode1_1  | 2023-06-30 12:23:25,312 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/db58cf25-deb4-4e5f-a339-31d80cad3087/in_use.lock acquired by nodename 6@56dd990c81df
datanode1_1  | 2023-06-30 12:23:25,368 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/db58cf25-deb4-4e5f-a339-31d80cad3087 has been successfully formatted.
datanode1_1  | 2023-06-30 12:23:25,531 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO ratis.ContainerStateMachine: group-31D80CAD3087: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-30 12:23:25,544 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-30 12:23:25,663 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-30 12:23:25,678 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 12:23:25,682 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-30 12:23:25,683 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-30 12:23:25,689 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 12:23:25,719 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-30 12:23:25,766 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-30 12:23:25,773 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 12:23:25,881 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/db58cf25-deb4-4e5f-a339-31d80cad3087
datanode1_1  | 2023-06-30 12:23:25,884 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-30 12:23:25,896 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 12:23:25,902 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 12:23:25,935 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-30 12:23:25,936 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-30 12:23:25,937 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-30 12:23:25,940 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-30 12:23:25,941 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2023-06-30 12:23:03,146 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 12:23:04,066 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1        | 2023-06-30 12:23:06,839 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1        | 2023-06-30 12:23:06,967 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2023-06-30 12:23:06,967 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2023-06-30 12:23:07,160 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om1_1        | 2023-06-30 12:23:07,268 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-06-30 12:23:07,660 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om1_1        | 2023-06-30 12:23:07,675 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om1_1        | 2023-06-30 12:23:08,984 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om1_1        | 2023-06-30 12:23:09,386 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-30 12:23:09,386 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2023-06-30 12:23:09,455 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om1_1        | 2023-06-30 12:23:09,455 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2023-06-30 12:23:10,148 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1        | 2023-06-30 12:23:10,207 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-06-30 12:23:10,420 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1        | 2023-06-30 12:23:10,496 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2023-06-30 12:23:10,996 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-30 12:23:11,006 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-06-30 12:23:11,094 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2023-06-30 12:23:11,111 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-30 12:23:11,114 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-30 12:23:11,115 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-06-30 12:23:11,116 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-06-30 12:23:11,117 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1        | 2023-06-30 12:23:11,117 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2023-06-30 12:23:11,120 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1        | 2023-06-30 12:23:11,123 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 12:23:11,131 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2023-06-30 12:23:11,135 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-30 12:23:11,303 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-06-30 12:23:11,334 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1        | 2023-06-30 12:23:11,335 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-06-30 12:23:13,571 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1        | 2023-06-30 12:23:13,598 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1        | 2023-06-30 12:23:13,603 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1        | 2023-06-30 12:23:13,604 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-06-30 12:23:13,607 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-06-30 12:23:13,637 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-30 12:23:13,704 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@307fe491[Not completed]
om1_1        | 2023-06-30 12:23:13,706 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1        | 2023-06-30 12:23:13,770 [main] INFO om.OzoneManager: Creating RPC Server
om1_1        | 2023-06-30 12:23:13,853 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1        | 2023-06-30 12:23:13,890 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2023-06-30 12:23:13,894 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2023-06-30 12:23:13,903 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2023-06-30 12:23:13,904 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-06-30 12:23:13,910 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-06-30 12:23:13,910 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2023-06-30 12:23:14,038 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1        | 2023-06-30 12:23:14,040 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-06-30 12:23:14,083 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2023-06-30 12:23:14,084 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 2023-06-30 12:23:14,259 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
datanode1_1  | 2023-06-30 12:23:26,096 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-30 12:23:26,104 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 12:23:26,319 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-30 12:23:26,401 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-30 12:23:26,408 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-30 12:23:26,503 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO segmented.SegmentedRaftLogWorker: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-30 12:23:26,503 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO segmented.SegmentedRaftLogWorker: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-30 12:23:26,517 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087: start as a follower, conf=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 12:23:26,524 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-30 12:23:26,526 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: start 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState
datanode1_1  | 2023-06-30 12:23:26,569 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-31D80CAD3087,id=30c441e1-2cd9-4c36-9389-fe8b01b88a57
datanode1_1  | 2023-06-30 12:23:26,575 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-30 12:23:26,595 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 12:23:26,595 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-30 12:23:26,616 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-30 12:23:26,617 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-30 12:23:26,617 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-30 12:23:26,767 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=db58cf25-deb4-4e5f-a339-31d80cad3087
datanode1_1  | 2023-06-30 12:23:26,786 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=db58cf25-deb4-4e5f-a339-31d80cad3087.
datanode1_1  | 2023-06-30 12:23:26,787 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: addNew group-42D4F8D6F6D0:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-42D4F8D6F6D0:java.util.concurrent.CompletableFuture@29fd5678[Not completed]
datanode1_1  | 2023-06-30 12:23:26,833 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: new RaftServerImpl for group-42D4F8D6F6D0:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-06-30 12:23:26,833 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-06-30 12:23:26,839 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-06-30 12:23:26,839 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-06-30 12:23:26,849 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-06-30 12:23:26,850 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-06-30 12:23:26,850 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-06-30 12:23:26,855 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0: ConfigurationManager, init=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-06-30 12:23:26,855 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-06-30 12:23:26,858 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-06-30 12:23:26,858 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-06-30 12:23:26,871 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om2_1        | 2023-06-30 12:23:04,413 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 12:23:05,820 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1        | 2023-06-30 12:23:07,804 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1        | 2023-06-30 12:23:07,914 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2023-06-30 12:23:07,914 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2023-06-30 12:23:08,224 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om2_1        | 2023-06-30 12:23:08,282 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-06-30 12:23:08,542 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om2_1        | 2023-06-30 12:23:08,546 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om2_1        | 2023-06-30 12:23:09,523 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om2_1        | 2023-06-30 12:23:10,551 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-30 12:23:10,554 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2023-06-30 12:23:10,669 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om2_1        | 2023-06-30 12:23:10,672 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2023-06-30 12:23:11,732 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2023-06-30 12:23:11,846 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-06-30 12:23:12,075 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1        | 2023-06-30 12:23:12,148 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2023-06-30 12:23:12,900 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-30 12:23:12,912 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-06-30 12:23:13,055 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2023-06-30 12:23:13,108 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-30 12:23:13,115 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-30 12:23:13,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-06-30 12:23:13,123 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-06-30 12:23:13,124 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1        | 2023-06-30 12:23:13,124 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2023-06-30 12:23:13,126 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2023-06-30 12:23:13,135 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 12:23:13,136 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2023-06-30 12:23:13,144 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-06-30 12:23:13,207 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1        | 2023-06-30 12:23:13,225 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1        | 2023-06-30 12:23:13,228 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-06-30 12:23:15,399 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1        | 2023-06-30 12:23:15,426 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1        | 2023-06-30 12:23:15,430 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1        | 2023-06-30 12:23:15,434 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-30 12:23:15,435 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-30 12:23:15,497 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-30 12:23:15,581 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@291b3e68[Not completed]
om2_1        | 2023-06-30 12:23:15,583 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1        | 2023-06-30 12:23:15,621 [main] INFO om.OzoneManager: Creating RPC Server
om2_1        | 2023-06-30 12:23:15,800 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1        | 2023-06-30 12:23:15,845 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2023-06-30 12:23:15,848 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2023-06-30 12:23:15,854 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2023-06-30 12:23:15,854 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-06-30 12:23:15,856 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-06-30 12:23:15,857 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2023-06-30 12:23:15,949 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1        | 2023-06-30 12:23:15,951 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-06-30 12:23:16,030 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2023-06-30 12:23:16,036 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1        | 2023-06-30 12:23:16,223 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2023-06-30 12:23:14,336 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1        | 2023-06-30 12:23:14,377 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2023-06-30 12:23:14,378 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2023-06-30 12:23:14,742 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om1_1        | 2023-06-30 12:23:15,108 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-06-30 12:23:15,174 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-06-30 12:23:15,196 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1        | 2023-06-30 12:23:15,206 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1        | 2023-06-30 12:23:15,215 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1        | 2023-06-30 12:23:15,223 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1        | 2023-06-30 12:23:17,266 [main] INFO reflections.Reflections: Reflections took 2988 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om1_1        | 2023-06-30 12:23:18,430 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2023-06-30 12:23:18,472 [main] INFO ipc.Server: Listener at om1:9862
om1_1        | 2023-06-30 12:23:18,489 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2023-06-30 12:23:23,701 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2023-06-30 12:23:23,811 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2023-06-30 12:23:23,811 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2023-06-30 12:23:24,574 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2023-06-30 12:23:24,575 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2023-06-30 12:23:24,602 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1        | 2023-06-30 12:23:24,635 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om1
om1_1        | 2023-06-30 12:23:24,685 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1        | 2023-06-30 12:23:24,708 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2023-06-30 12:23:24,755 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2023-06-30 12:23:24,756 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 12:23:24,764 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1        | 2023-06-30 12:23:24,768 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1        | 2023-06-30 12:23:24,785 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-06-30 12:23:24,815 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2023-06-30 12:23:24,821 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2023-06-30 12:23:24,822 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 12:23:24,848 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1        | 2023-06-30 12:23:24,849 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2023-06-30 12:23:24,853 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2023-06-30 12:23:24,857 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-06-30 12:23:24,858 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2023-06-30 12:23:24,862 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1        | 2023-06-30 12:23:24,869 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1        | 2023-06-30 12:23:24,871 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1        | 2023-06-30 12:23:24,872 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2023-06-30 12:23:24,987 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2023-06-30 12:23:24,987 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-06-30 12:23:25,254 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1        | 2023-06-30 12:23:25,302 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1        | 2023-06-30 12:23:25,305 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1        | 2023-06-30 12:23:25,380 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-30 12:23:25,385 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-30 12:23:25,403 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-30 12:23:25,407 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2023-06-30 12:23:25,420 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-30 12:23:25,431 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-30 12:23:25,446 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-30 12:23:29,425 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0/in_use.lock acquired by nodename 6@b043fa92fe53
datanode2_1  | 2023-06-30 12:23:29,430 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0 has been successfully formatted.
datanode2_1  | 2023-06-30 12:23:29,438 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO ratis.ContainerStateMachine: group-42D4F8D6F6D0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-06-30 12:23:29,440 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-06-30 12:23:29,441 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-06-30 12:23:29,441 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 12:23:29,442 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-06-30 12:23:29,443 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-06-30 12:23:29,444 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 12:23:29,446 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-06-30 12:23:29,449 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-06-30 12:23:29,449 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 12:23:29,449 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0
datanode2_1  | 2023-06-30 12:23:29,449 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-06-30 12:23:29,450 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 12:23:29,450 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-06-30 12:23:29,485 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-06-30 12:23:29,486 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-06-30 12:23:29,486 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-06-30 12:23:29,486 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-06-30 12:23:29,486 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-06-30 12:23:29,489 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-06-30 12:23:29,510 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-06-30 12:23:31,191 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: Detected pause in JVM or host machine approximately 1.412s with 1.518s GC time.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=77ms
datanode2_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1441ms
datanode2_1  | 2023-06-30 12:23:31,245 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-06-30 12:23:31,258 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-06-30 12:23:31,258 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-06-30 12:23:31,267 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO segmented.SegmentedRaftLogWorker: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 12:23:31,268 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO segmented.SegmentedRaftLogWorker: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-06-30 12:23:31,287 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0: start as a follower, conf=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 12:23:31,288 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-06-30 12:23:31,297 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: start a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState
datanode2_1  | 2023-06-30 12:23:31,313 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-42D4F8D6F6D0,id=a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
datanode2_1  | 2023-06-30 12:23:31,316 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-06-30 12:23:31,316 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-06-30 12:23:31,316 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-06-30 12:23:31,316 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-06-30 12:23:31,332 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-30 12:23:31,389 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-30 12:23:31,395 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0
datanode2_1  | 2023-06-30 12:23:31,801 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-06-30 12:23:34,291 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState] INFO impl.FollowerState: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5067957326ns, electionTimeout:5033ms
datanode2_1  | 2023-06-30 12:23:34,300 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: shutdown a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState
datanode2_1  | 2023-06-30 12:23:34,318 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-06-30 12:23:34,370 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode2_1  | 2023-06-30 12:23:34,377 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-FollowerState] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: start a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1
datanode2_1  | 2023-06-30 12:23:34,397 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO impl.LeaderElection: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 12:23:34,418 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO impl.LeaderElection: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode2_1  | 2023-06-30 12:23:34,428 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO impl.LeaderElection: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 12:23:34,428 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO impl.LeaderElection: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode2_1  | 2023-06-30 12:23:34,429 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: shutdown a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1
datanode2_1  | 2023-06-30 12:23:34,431 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode2_1  | 2023-06-30 12:23:34,437 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0B2DA0E47784 with new leaderId: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
datanode2_1  | 2023-06-30 12:23:34,499 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784: change Leader from null to a9efabc3-aa7d-48e7-8786-4f3f9c9666fc at term 1 for becomeLeader, leader elected after 7212ms
datanode2_1  | 2023-06-30 12:23:34,697 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2023-06-30 12:23:34,814 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 12:23:34,827 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode2_1  | 2023-06-30 12:23:34,896 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2023-06-30 12:23:34,897 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2023-06-30 12:23:34,915 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2023-06-30 12:23:35,029 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2023-06-30 12:23:35,050 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode2_1  | 2023-06-30 12:23:35,089 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: start a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderStateImpl
datanode2_1  | 2023-06-30 12:23:35,325 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-06-30 12:23:35,869 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-LeaderElection1] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784: set configuration 0: peers:[a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 12:23:36,503 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState] INFO impl.FollowerState: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5206473065ns, electionTimeout:5114ms
datanode2_1  | 2023-06-30 12:23:36,537 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: shutdown a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState
datanode2_1  | 2023-06-30 12:23:36,540 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServer$Division: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-06-30 12:23:36,544 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode2_1  | 2023-06-30 12:23:36,544 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-FollowerState] INFO impl.RoleInfo: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: start a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-LeaderElection2
datanode2_1  | 2023-06-30 12:23:36,634 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-LeaderElection2] INFO impl.LeaderElection: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-06-30 12:23:36,957 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-0B2DA0E47784-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d5b5f96c-9e99-4dc5-8f79-0b2da0e47784/current/log_inprogress_0
datanode2_1  | 2023-06-30 12:23:37,032 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 30c441e1-2cd9-4c36-9389-fe8b01b88a57
datanode2_1  | 2023-06-30 12:23:37,036 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-06-30 12:23:37,037 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-06-30 12:23:37,055 [a9efabc3-aa7d-48e7-8786-4f3f9c9666fc@group-42D4F8D6F6D0-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for c26c1b41-9621-4ea7-a0f2-632ca831791f
datanode2_1  | 2023-06-30 12:23:37,738 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a9efabc3-aa7d-48e7-8786-4f3f9c9666fc: Detected pause in JVM or host machine approximately 0.461s with 0.545s GC time.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=545ms
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2023-06-30 12:19:41,306 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
recon_1      | STARTUP_MSG:   java = 11.0.19
recon_1      | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1      | ************************************************************/
recon_1      | 2023-06-30 12:19:41,355 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-06-30 12:19:44,473 [main] INFO reflections.Reflections: Reflections took 330 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1      | 2023-06-30 12:19:47,550 [main] INFO reflections.Reflections: Reflections took 241 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1      | 2023-06-30 12:19:47,818 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2023-06-30 12:19:48,103 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2023-06-30 12:19:48,910 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file recon.keytab. Keytab auto renewal enabled : false
recon_1      | 2023-06-30 12:19:48,916 [main] INFO recon.ReconServer: Recon login successful.
recon_1      | 2023-06-30 12:19:48,966 [main] INFO recon.ReconServer: ReconStorageConfig initialized.Initializing certificate.
recon_1      | 2023-06-30 12:19:48,970 [main] INFO recon.ReconServer: Initializing secure Recon.
recon_1      | 2023-06-30 12:19:51,325 [main] WARN security.ReconCertificateClient: Certificates could not be loaded.
recon_1      | java.nio.file.NoSuchFileException: /data/metadata/recon/certs
recon_1      | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
recon_1      | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
recon_1      | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
recon_1      | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
recon_1      | 	at java.base/java.nio.file.Files.list(Files.java:3699)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
recon_1      | 	at org.apache.hadoop.hdds.security.x509.certificate.client.CommonCertificateClient.<init>(CommonCertificateClient.java:50)
recon_1      | 	at org.apache.hadoop.ozone.recon.security.ReconCertificateClient.<init>(ReconCertificateClient.java:62)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.initializeCertificateClient(ReconServer.java:184)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:126)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:71)
recon_1      | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1953)
recon_1      | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
recon_1      | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
recon_1      | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
recon_1      | 	at picocli.CommandLine.execute(CommandLine.java:2078)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:94)
recon_1      | 2023-06-30 12:19:51,332 [main] ERROR security.ReconCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
recon_1      | 2023-06-30 12:19:51,340 [main] INFO security.ReconCertificateClient: Certificate client init case: 0
recon_1      | 2023-06-30 12:19:51,341 [main] INFO security.ReconCertificateClient: Creating keypair for client as keypair and certificate not found.
recon_1      | 2023-06-30 12:19:54,263 [main] INFO recon.ReconServer: Init response: GETCERT
recon_1      | 2023-06-30 12:19:54,269 [main] INFO security.ReconCertificateClient: Creating CSR for Recon.
recon_1      | 2023-06-30 12:19:54,441 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.115,host:recon
recon_1      | 2023-06-30 12:19:54,442 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
recon_1      | 2023-06-30 12:19:54,447 [main] ERROR utils.CertificateSignRequest: Invalid domain recon
recon_1      | 2023-06-30 12:20:01,047 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1      | 2023-06-30 12:20:03,049 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1      | 2023-06-30 12:20:05,051 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-30 12:20:07,054 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/435562639432.crt.
om3_1        | 2023-06-30 12:22:15,501 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 299862451261
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om3_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
om3_1        |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: bceb238776cef1f96370453157455bfacebc30a6
om3_1        |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
om3_1        |                        92b8465e76c5427a65937f03f4c7003698371efa
om3_1        |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
om3_1        |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
om3_1        |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
om3_1        |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
om3_1        |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
om3_1        |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
om3_1        |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
om3_1        |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
om3_1        |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
om3_1        |                        82b7771fe8683859200f859a8629c61a
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-299862451261.crt.
om3_1        | 2023-06-30 12:22:15,546 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116798492 ms and interval 86400000 ms.
om3_1        | 2023-06-30 12:22:15,546 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om3_1        | 2023-06-30 12:22:15,704 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-06-30 12:22:31,904 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | 2023-06-30 12:23:27,899 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-30 12:23:27,906 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 12:23:28,102 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-30 12:23:28,111 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-30 12:23:28,116 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-30 12:23:28,222 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO segmented.SegmentedRaftLogWorker: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-30 12:23:28,227 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO segmented.SegmentedRaftLogWorker: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-30 12:23:28,255 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5: start as a follower, conf=-1: peers:[c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 12:23:28,256 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-30 12:23:28,276 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: start c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState
datanode3_1  | 2023-06-30 12:23:28,320 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-30 12:23:28,324 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-30 12:23:28,325 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9491141422F5,id=c26c1b41-9621-4ea7-a0f2-632ca831791f
datanode3_1  | 2023-06-30 12:23:28,362 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-30 12:23:28,372 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-30 12:23:28,374 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-30 12:23:28,378 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-30 12:23:28,757 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=394fb16c-ab50-4148-8258-9491141422f5
datanode3_1  | 2023-06-30 12:23:28,773 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=394fb16c-ab50-4148-8258-9491141422f5.
datanode3_1  | 2023-06-30 12:23:28,773 [PipelineCommandHandlerThread-0] INFO server.RaftServer: c26c1b41-9621-4ea7-a0f2-632ca831791f: addNew group-42D4F8D6F6D0:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] returns group-42D4F8D6F6D0:java.util.concurrent.CompletableFuture@5a7fcbee[Not completed]
datanode3_1  | 2023-06-30 12:23:28,818 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f: new RaftServerImpl for group-42D4F8D6F6D0:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-06-30 12:23:28,820 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-06-30 12:23:28,821 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-06-30 12:23:28,825 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-06-30 12:23:28,826 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-06-30 12:23:28,827 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-06-30 12:23:28,828 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-06-30 12:23:28,834 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0: ConfigurationManager, init=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-06-30 12:23:28,835 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-06-30 12:23:28,837 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-06-30 12:23:28,843 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-06-30 12:23:28,844 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-06-30 12:23:28,847 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode3_1  | 2023-06-30 12:23:28,848 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-06-30 12:23:28,852 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-06-30 12:23:28,853 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode3_1  | 2023-06-30 12:23:28,863 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-06-30 12:23:28,885 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-06-30 12:23:28,886 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-06-30 12:23:28,887 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-06-30 12:23:28,889 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-06-30 12:23:28,890 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-06-30 12:23:28,891 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0 does not exist. Creating ...
datanode3_1  | 2023-06-30 12:23:28,903 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0/in_use.lock acquired by nodename 7@aa09f3d2afc3
datanode3_1  | 2023-06-30 12:23:28,915 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0 has been successfully formatted.
datanode3_1  | 2023-06-30 12:23:28,993 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO ratis.ContainerStateMachine: group-42D4F8D6F6D0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-06-30 12:23:28,996 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-06-30 12:23:29,000 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-06-30 12:23:29,007 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 12:23:29,010 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-06-30 12:23:29,010 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-06-30 12:23:29,015 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 12:23:29,021 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-06-30 12:23:29,028 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-06-30 12:23:29,028 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 12:23:29,029 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO segmented.SegmentedRaftLogWorker: new c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0
datanode3_1  | 2023-06-30 12:23:29,029 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-06-30 12:23:29,030 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 12:23:29,031 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-06-30 12:23:29,031 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-06-30 12:23:29,032 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-06-30 12:23:29,034 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-06-30 12:23:29,035 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-06-30 12:23:29,036 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-06-30 12:23:29,043 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-06-30 12:23:29,081 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-06-30 12:23:30,527 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-c26c1b41-9621-4ea7-a0f2-632ca831791f: Detected pause in JVM or host machine approximately 0.991s with 1.409s GC time.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=33ms
datanode3_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1376ms
datanode3_1  | 2023-06-30 12:23:30,550 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-06-30 12:23:30,556 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-06-30 12:23:30,556 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-06-30 12:23:30,559 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO segmented.SegmentedRaftLogWorker: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-30 12:23:30,567 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO segmented.SegmentedRaftLogWorker: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-06-30 12:23:30,582 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0: start as a follower, conf=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 12:23:30,587 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-06-30 12:23:30,613 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: start c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState
datanode3_1  | 2023-06-30 12:23:30,630 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-42D4F8D6F6D0,id=c26c1b41-9621-4ea7-a0f2-632ca831791f
datanode3_1  | 2023-06-30 12:23:30,632 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-06-30 12:23:30,632 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-06-30 12:23:30,633 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-06-30 12:23:30,634 [c26c1b41-9621-4ea7-a0f2-632ca831791f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-06-30 12:23:30,637 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-30 12:23:30,666 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0
datanode3_1  | 2023-06-30 12:23:30,698 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-30 12:23:31,281 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-06-30 12:23:33,476 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState] INFO impl.FollowerState: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5204844678ns, electionTimeout:5141ms
datanode3_1  | 2023-06-30 12:23:33,479 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: shutdown c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState
datanode3_1  | 2023-06-30 12:23:33,486 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-06-30 12:23:33,501 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode3_1  | 2023-06-30 12:23:33,505 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-FollowerState] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: start c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1
datanode3_1  | 2023-06-30 12:23:33,552 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO impl.LeaderElection: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 12:23:33,576 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO impl.LeaderElection: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode3_1  | 2023-06-30 12:23:33,624 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO impl.LeaderElection: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 12:23:33,625 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO impl.LeaderElection: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2023-06-30 12:23:26,874 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode1_1  | 2023-06-30 12:23:26,874 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-06-30 12:23:26,876 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-06-30 12:23:26,878 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode1_1  | 2023-06-30 12:23:26,888 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-06-30 12:23:26,889 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-06-30 12:23:26,890 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-06-30 12:23:26,891 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-06-30 12:23:26,891 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-06-30 12:23:26,893 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-06-30 12:23:26,893 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0 does not exist. Creating ...
datanode1_1  | 2023-06-30 12:23:26,903 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0/in_use.lock acquired by nodename 6@56dd990c81df
datanode1_1  | 2023-06-30 12:23:26,913 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0 has been successfully formatted.
datanode1_1  | 2023-06-30 12:23:26,957 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO ratis.ContainerStateMachine: group-42D4F8D6F6D0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-06-30 12:23:26,958 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-06-30 12:23:26,984 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-06-30 12:23:26,997 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 12:23:27,004 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-06-30 12:23:27,010 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-06-30 12:23:27,021 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 12:23:27,024 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-06-30 12:23:27,026 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-06-30 12:23:27,031 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 12:23:27,031 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b411bba0-46fb-490f-9c02-42d4f8d6f6d0
datanode1_1  | 2023-06-30 12:23:27,031 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-06-30 12:23:27,032 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 12:23:27,034 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-06-30 12:23:27,035 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-06-30 12:23:27,036 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-06-30 12:23:27,036 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-06-30 12:23:27,038 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-06-30 12:23:27,040 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-06-30 12:23:27,050 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-06-30 12:23:27,058 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-06-30 12:23:28,562 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-30c441e1-2cd9-4c36-9389-fe8b01b88a57: Detected pause in JVM or host machine approximately 1.308s with 1.490s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=119ms
datanode1_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1371ms
datanode1_1  | 2023-06-30 12:23:28,604 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-06-30 12:23:28,604 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-06-30 12:23:28,609 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-06-30 12:23:28,611 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO segmented.SegmentedRaftLogWorker: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-06-30 12:23:28,617 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO segmented.SegmentedRaftLogWorker: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2023-06-30 12:23:25,434 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1        | 2023-06-30 12:23:25,493 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2023-06-30 12:23:25,494 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2023-06-30 12:23:25,495 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2023-06-30 12:23:25,511 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2023-06-30 12:23:25,574 [main] INFO server.RaftServer: om1: start RPC server
om1_1        | 2023-06-30 12:23:26,130 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1        | 2023-06-30 12:23:26,142 [main] INFO om.OzoneManager: Starting secret key client.
om1_1        | 2023-06-30 12:23:26,586 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2023-06-30 12:23:27,472 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z).
om1_1        | 2023-06-30 12:23:27,497 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M51.639658S and interval of PT1M
om1_1        | 2023-06-30 12:23:27,553 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2023-06-30 12:23:27,557 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 433148233533
om1_1        | 2023-06-30 12:23:27,648 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2023-06-30 12:23:27,654 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2023-06-30 12:23:29,655 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1        | 2023-06-30 12:23:29,656 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om1_1        | 2023-06-30 12:23:29,656 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om1_1        | 2023-06-30 12:23:30,014 [main] INFO util.log: Logging initialized @75178ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1        | 2023-06-30 12:23:30,586 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5166551754ns, electionTimeout:5111ms
om1_1        | 2023-06-30 12:23:30,724 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1        | 2023-06-30 12:23:30,773 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1        | 2023-06-30 12:23:30,984 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1        | 2023-06-30 12:23:31,001 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1        | 2023-06-30 12:23:31,370 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-30 12:23:32,860 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-30 12:23:32,867 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
om3_1        | STARTUP_MSG:   java = 11.0.19
om2_1        | 2023-06-30 12:23:16,264 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1        | 2023-06-30 12:23:16,305 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2023-06-30 12:23:16,306 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2023-06-30 12:23:16,860 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om2_1        | 2023-06-30 12:23:17,447 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-06-30 12:23:17,518 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2023-06-30 12:23:17,519 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1        | 2023-06-30 12:23:17,530 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1        | 2023-06-30 12:23:17,539 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1        | 2023-06-30 12:23:17,553 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        | 2023-06-30 12:23:19,277 [main] INFO reflections.Reflections: Reflections took 2933 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om2_1        | 2023-06-30 12:23:20,658 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1        | 2023-06-30 12:23:20,696 [main] INFO ipc.Server: Listener at om2:9862
om2_1        | 2023-06-30 12:23:20,709 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2023-06-30 12:23:28,861 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2023-06-30 12:23:29,063 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2023-06-30 12:23:29,063 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2023-06-30 12:23:29,839 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2023-06-30 12:23:29,847 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2023-06-30 12:23:29,891 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om2_1        | 2023-06-30 12:23:29,934 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@om2
om2_1        | 2023-06-30 12:23:30,032 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1        | 2023-06-30 12:23:30,133 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2023-06-30 12:23:30,261 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2023-06-30 12:23:30,267 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 12:23:30,290 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1        | 2023-06-30 12:23:30,301 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1        | 2023-06-30 12:23:30,332 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-30 12:23:30,407 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2023-06-30 12:23:30,412 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2023-06-30 12:23:30,412 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 12:23:30,490 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1        | 2023-06-30 12:23:30,493 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2023-06-30 12:23:30,495 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2023-06-30 12:23:30,506 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-06-30 12:23:30,507 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2023-06-30 12:23:30,507 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1        | 2023-06-30 12:23:30,508 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2023-06-30 12:23:30,510 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2023-06-30 12:23:30,511 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2023-06-30 12:23:30,735 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2023-06-30 12:23:30,743 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-06-30 12:23:31,605 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1        | 2023-06-30 12:23:31,654 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1        | 2023-06-30 12:23:31,680 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2023-06-30 12:23:32,359 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-30 12:23:32,376 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2023-06-30 12:23:32,530 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-30 12:23:32,561 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2023-06-30 12:23:32,726 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-30 12:23:32,780 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-30 12:23:32,788 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-30 12:23:32,828 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1        | 2023-06-30 12:23:32,967 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2023-06-30 12:23:32,981 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2023-06-30 12:23:33,007 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2023-06-30 12:23:33,057 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1        | 2023-06-30 12:23:33,313 [main] INFO server.RaftServer: om2: start RPC server
om2_1        | 2023-06-30 12:23:33,901 [main] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2023-06-30 12:23:33,964 [main] INFO om.OzoneManager: Starting secret key client.
om2_1        | 2023-06-30 12:23:34,000 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2023-06-30 12:23:34,601 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z).
om2_1        | 2023-06-30 12:23:34,616 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M44.520666S and interval of PT1M
om2_1        | 2023-06-30 12:23:34,632 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2023-06-30 12:23:34,721 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 433509364865
om2_1        | 2023-06-30 12:23:35,009 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om2_1        | 2023-06-30 12:23:35,020 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2023-06-30 12:23:36,288 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1        | 2023-06-30 12:23:36,288 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om2_1        | 2023-06-30 12:23:36,293 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om2_1        | 2023-06-30 12:23:36,596 [main] INFO util.log: Logging initialized @80620ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1        | 2023-06-30 12:23:37,986 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5291919006ns, electionTimeout:5146ms
om2_1        | 2023-06-30 12:23:38,006 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1        | 2023-06-30 12:23:38,011 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1        | 2023-06-30 12:23:38,017 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om2_1        | 2023-06-30 12:23:38,024 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1        | 2023-06-30 12:23:38,128 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-06-30 12:23:38,742 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-06-30 12:23:38,742 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-06-30 12:23:38,757 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1        | 2023-06-30 12:23:38,760 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1        | 2023-06-30 12:23:39,435 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1        | 2023-06-30 12:23:40,095 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2023-06-30 12:23:40,219 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1        | 2023-06-30 12:23:40,220 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        | 2023-06-30 12:23:40,226 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1        | 2023-06-30 12:23:40,351 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
datanode1_1  | 2023-06-30 12:23:28,619 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0: start as a follower, conf=-1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 12:23:28,623 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-06-30 12:23:28,669 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: start 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState
datanode1_1  | 2023-06-30 12:23:28,682 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-42D4F8D6F6D0,id=30c441e1-2cd9-4c36-9389-fe8b01b88a57
datanode1_1  | 2023-06-30 12:23:28,684 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-06-30 12:23:28,684 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-06-30 12:23:28,685 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-06-30 12:23:28,685 [30c441e1-2cd9-4c36-9389-fe8b01b88a57-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-06-30 12:23:28,687 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 12:23:28,721 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0
datanode1_1  | 2023-06-30 12:23:28,742 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-30 12:23:29,082 [PipelineCommandHandlerThread-0] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-06-30 12:23:31,752 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState] INFO impl.FollowerState: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5219327191ns, electionTimeout:5124ms
datanode1_1  | 2023-06-30 12:23:31,780 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: shutdown 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState
datanode1_1  | 2023-06-30 12:23:31,834 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-30 12:23:31,965 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-30 12:23:32,009 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-FollowerState] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: start 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1
datanode1_1  | 2023-06-30 12:23:32,082 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 12:23:32,088 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode1_1  | 2023-06-30 12:23:32,099 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 12:23:32,099 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2023-06-30 12:23:32,099 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: shutdown 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1
datanode1_1  | 2023-06-30 12:23:32,114 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2023-06-30 12:23:32,119 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-31D80CAD3087 with new leaderId: 30c441e1-2cd9-4c36-9389-fe8b01b88a57
datanode1_1  | 2023-06-30 12:23:32,121 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087: change Leader from null to 30c441e1-2cd9-4c36-9389-fe8b01b88a57 at term 1 for becomeLeader, leader elected after 7393ms
datanode1_1  | 2023-06-30 12:23:32,591 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-06-30 12:23:32,745 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 12:23:32,787 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-06-30 12:23:32,940 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-06-30 12:23:32,946 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-06-30 12:23:32,968 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-06-30 12:23:33,162 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-06-30 12:23:33,245 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2023-06-30 12:23:33,296 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: start 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderStateImpl
datanode1_1  | 2023-06-30 12:23:33,479 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-06-30 12:23:33,878 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState] INFO impl.FollowerState: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5209573063ns, electionTimeout:5134ms
datanode1_1  | 2023-06-30 12:23:33,912 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: shutdown 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState
datanode1_1  | 2023-06-30 12:23:33,914 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-LeaderElection1] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087: set configuration 0: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 12:23:33,915 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServer$Division: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-06-30 12:23:33,916 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode1_1  | 2023-06-30 12:23:33,917 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-FollowerState] INFO impl.RoleInfo: 30c441e1-2cd9-4c36-9389-fe8b01b88a57: start 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2
datanode1_1  | 2023-06-30 12:23:34,017 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 12:23:34,210 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for c26c1b41-9621-4ea7-a0f2-632ca831791f
datanode1_1  | 2023-06-30 12:23:34,211 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 12:23:34,218 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-06-30 12:23:34,213 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
datanode1_1  | 2023-06-30 12:23:35,246 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-30c441e1-2cd9-4c36-9389-fe8b01b88a57: Detected pause in JVM or host machine approximately 0.628s with 0.823s GC time.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=823ms
datanode1_1  | 2023-06-30 12:23:36,200 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-31D80CAD3087-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/db58cf25-deb4-4e5f-a339-31d80cad3087/current/log_inprogress_0
datanode1_1  | 2023-06-30 12:23:39,278 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
datanode1_1  | 2023-06-30 12:23:39,278 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2 PRE_VOTE round 0: result TIMEOUT
datanode1_1  | 2023-06-30 12:23:39,278 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO impl.LeaderElection: 30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2 PRE_VOTE round 1: submit vote requests at term 0 for -1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-06-30 12:23:39,533 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-06-30 12:23:39,535 [30c441e1-2cd9-4c36-9389-fe8b01b88a57@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1        | ************************************************************/
om3_1        | 2023-06-30 12:22:32,001 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-06-30 12:22:41,800 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-06-30 12:22:45,033 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is omservice
om3_1        | 2023-06-30 12:22:45,902 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-06-30 12:22:45,902 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omservice.om3: om3
om3_1        | 2023-06-30 12:22:45,902 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1        | 2023-06-30 12:22:46,105 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 12:22:46,971 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om3_1        | 2023-06-30 12:22:49,412 [main] INFO reflections.Reflections: Reflections took 1943 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om3_1        | 2023-06-30 12:22:49,611 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om3_1        | 2023-06-30 12:22:51,936 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-06-30 12:22:51,936 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-06-30 12:22:51,936 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 12:22:56,683 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-30 12:22:57,297 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-06-30 12:23:01,742 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-06-30 12:23:03,147 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 1
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om3_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om3_1        |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
om3_1        |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
om3_1        |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
om3_1        |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
om3_1        |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
om3_1        |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
om3_1        |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
om3_1        |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
om3_1        |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
om3_1        |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
om3_1        |                        5a94528eee6bdc4274bc019b1d82a247fece378c
om3_1        |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
om3_1        |                        c78036d6743d01601451bd47bf51bec81c366e20
om3_1        |                        347a5c23f63ab77d1a66e410a5866e0f
om3_1        |        Extensions: 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0x6
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |  from file: /data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-06-30 12:23:03,181 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 435562639432
om3_1        |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Start Date: Fri Jun 30 12:22:14 UTC 2023
om3_1        |            Final Date: Sat Jun 29 12:22:14 UTC 2024
om3_1        |             SubjectDN: CN=om3,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Public Key: RSA Public Key [bf:24:60:84:a2:f4:39:b7:cb:b7:22:80:41:70:82:01:55:bd:73:bc],[56:66:d1:a4]
om3_1        |         modulus: df064c3ffcd1b04a1ec7fdf3a277f3c5d6ffa99c1637e0cba8564562c2e0a16027c3d48628dc7abe0e4f570bfbed36510b4ff1a123a00bff9626da8980105be3d6cc5fc3f5b632596419b5177a62645a67df9a3063d7448aa609e5948b0b7c4e81c78209d80111b6e841a8da2173f472ab6a5fb9fe3df24edfc920716ea8c7bb5ef6af1dc805b3e3b91402f25f7f4af13ada022ff1d9eeebb1f150085b727b9469d7b06cd3e65acaef5dd46810868db3a69613f01402145b83340a469a691e8a5b461c3374fc0b317a2128565d6998806156bd794e1e4d13b2be86ff0dce0cd541f798ea4ebad9bc3267560b45bcabf849e9dce6617541a38c80f3d5587a7703
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: 163c7be70d98401313ece8c785b9565ed6644f74
om3_1        |                        e7ea0f62621f2b5fa9e48b2dfe87f39d82699cd7
om3_1        |                        7c8f486fa90f878a7b6d81292072777c4563ee9b
om3_1        |                        970fcd79148c39f487a2af73e025e1186ec1af47
om3_1        |                        a2c8d80bc3cccd6c2243fe16c6c9c06272ecb67e
om3_1        |                        4312b8a5082e467e8198e9159767712a895adf13
om3_1        |                        f2d13307322b058305b5a9e6454f10f960571258
om3_1        |                        5c8e3055a893cb667b1a8fb7458e8c9ffdb92a3b
om3_1        |                        1fa8c5c36d28b0b6fe46ac748cfd6ae7c3b6121c
om3_1        |                        8d63ca29b5508bcd3f657ffb6907a2e94bf11157
om3_1        |                        45d0d3156425fe634b9e3f4d9cb5218f1d878f01
om3_1        |                        9d123bfdf68304a2963e37a51382cbdf33a86d15
om3_1        |                        12b1017ac30a8f3a9e4f411d735d226d
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        | 
om3_1        |                        critical(true) KeyUsage: 0xb8
om3_1        |  from file: /data/metadata/om/certs/435562639432.crt.
om3_1        | 2023-06-30 12:23:03,205 [main] INFO security.OMCertificateClient: Added certificate   [0]         Version: 3
om3_1        |          SerialNumber: 299862451261
om3_1        |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Start Date: Fri Jun 30 12:19:58 UTC 2023
om3_1        |            Final Date: Mon Aug 07 12:19:58 UTC 2028
om3_1        |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
om3_1        |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
om3_1        |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
om3_1        | public exponent: 10001
om3_1        | 
om3_1        |   Signature Algorithm: SHA256WITHRSA
om3_1        |             Signature: bceb238776cef1f96370453157455bfacebc30a6
om3_1        |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
om3_1        |                        92b8465e76c5427a65937f03f4c7003698371efa
om3_1        |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
om3_1        |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
om3_1        |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
recon_1      | 2023-06-30 12:20:09,055 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
recon_1      | 2023-06-30 12:20:11,057 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-30 12:20:15,373 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1      | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
recon_1      | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1      | , while invoking $Proxy41.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
recon_1      | 2023-06-30 12:20:17,382 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
recon_1      | 2023-06-30 12:20:19,385 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy41.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-30 12:20:22,104 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 1
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
recon_1      |            Start Date: Fri Jun 30 12:19:58 UTC 2023
recon_1      |            Final Date: Mon Aug 07 12:19:58 UTC 2028
recon_1      |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
recon_1      |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
recon_1      |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
recon_1      |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
recon_1      |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
recon_1      |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
recon_1      |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
recon_1      |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
recon_1      |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
recon_1      |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
recon_1      |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
recon_1      |                        5a94528eee6bdc4274bc019b1d82a247fece378c
recon_1      |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
recon_1      |                        c78036d6743d01601451bd47bf51bec81c366e20
recon_1      |                        347a5c23f63ab77d1a66e410a5866e0f
recon_1      |        Extensions: 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0x6
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
recon_1      |  from file: /data/metadata/recon/certs/ROOTCA-1.crt.
recon_1      | 2023-06-30 12:20:22,118 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 323029312086
recon_1      |              IssuerDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
recon_1      |            Start Date: Fri Jun 30 12:20:21 UTC 2023
recon_1      |            Final Date: Sat Jun 29 12:20:21 UTC 2024
recon_1      |             SubjectDN: CN=recon@recon,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
recon_1      |            Public Key: RSA Public Key [ce:01:c4:33:3c:08:38:12:1f:68:a0:da:c3:2c:ee:ba:df:fb:e3:a7],[56:66:d1:a4]
recon_1      |         modulus: c7076e98f4f07f7b6b8474acc13d4d2a77060ccfbb1d970364eedfce9fb7fc12463af3ed321c36764e5fcb988b3757333d0f269b56bf8686776b230fca2b85a869ce6b140270ae0a3059f9e10eef63626af7c7e07f36aa13a9f241fbee52373526458b2d22d2680e62cb0002180ccde2b32d153b64bb9b44f55ae2fb6264ca0997abad95e15b93d51a0de655516d546e6565f81506888801590d61ff783b179065b0cac19fd703e0610ecd0b7aa4aefeafc41344762cdbfd5dcb0aaa389643da0facbee0b43c145fa96a4904ef213ca1b495eb1a1c8b5889faf91aef56d90400cf7bb4675241ce37cf713ab75a220ee8ad27a0e8d10f5648a94ade1eec1acb19
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: 322037e510e067dc03425ff33b1d7cd9d9dcd9c4
recon_1      |                        4a142666878e1a3a351dc86716f063a07f308fcb
recon_1      |                        326b33d977f3207e7abd1cdbc003d644caeaddbe
recon_1      |                        e2694bbba50ab20c787a06a7389d768d7962da33
om1_1        | 2023-06-30 12:23:33,032 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1        | 2023-06-30 12:23:33,032 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1        | 2023-06-30 12:23:33,531 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om1: Detected pause in JVM or host machine approximately 0.281s without any GCs.
om1_1        | 2023-06-30 12:23:34,702 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1        | 2023-06-30 12:23:34,853 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1        | 2023-06-30 12:23:34,904 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1        | 2023-06-30 12:23:34,906 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        | 2023-06-30 12:23:34,909 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1        | 2023-06-30 12:23:34,976 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om1_1        | 2023-06-30 12:23:36,060 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om1_1        | 2023-06-30 12:23:36,211 [main] INFO http.HttpServer2: Jetty bound to port 9874
om1_1        | 2023-06-30 12:23:36,235 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om1_1        | 2023-06-30 12:23:36,828 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1        | 2023-06-30 12:23:36,828 [main] INFO server.session: No SessionScavenger set, using defaults
om1_1        | 2023-06-30 12:23:36,917 [main] INFO server.session: node0 Scavenging every 660000ms
om1_1        | 2023-06-30 12:23:37,261 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-06-30 12:23:37,368 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@226451a2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1        | 2023-06-30 12:23:37,384 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1be2c8a4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1        | 2023-06-30 12:23:37,819 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.500644767s. [buffered_nanos=2514333000, waiting_for_connection]
om1_1        | 2023-06-30 12:23:38,051 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 1 exception(s):
om1_1        | 2023-06-30 12:23:38,071 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.500644767s. [buffered_nanos=2514333000, waiting_for_connection]
om1_1        | 2023-06-30 12:23:38,074 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
om1_1        | 2023-06-30 12:23:38,101 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-06-30 12:23:38,139 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-06-30 12:23:38,140 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-06-30 12:23:39,073 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-06-30 12:23:39,624 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7c31a506{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-8458716997381350822/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1        | 2023-06-30 12:23:39,952 [main] INFO server.AbstractConnector: Started ServerConnector@533167f0{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1        | 2023-06-30 12:23:39,952 [main] INFO server.Server: Started @85116ms
om1_1        | 2023-06-30 12:23:40,084 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1        | 2023-06-30 12:23:40,086 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1        | 2023-06-30 12:23:40,101 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1        | 2023-06-30 12:23:40,139 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1        | 2023-06-30 12:23:40,211 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
datanode3_1  | 2023-06-30 12:23:33,625 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: shutdown c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1
datanode3_1  | 2023-06-30 12:23:33,627 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2023-06-30 12:23:33,628 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9491141422F5 with new leaderId: c26c1b41-9621-4ea7-a0f2-632ca831791f
datanode3_1  | 2023-06-30 12:23:33,678 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5: change Leader from null to c26c1b41-9621-4ea7-a0f2-632ca831791f at term 1 for becomeLeader, leader elected after 7047ms
datanode3_1  | 2023-06-30 12:23:33,867 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2023-06-30 12:23:33,960 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 12:23:33,979 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2023-06-30 12:23:34,062 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2023-06-30 12:23:34,075 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2023-06-30 12:23:34,091 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2023-06-30 12:23:34,239 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-06-30 12:23:34,268 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2023-06-30 12:23:34,290 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: start c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderStateImpl
datanode3_1  | 2023-06-30 12:23:34,680 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-06-30 12:23:35,642 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-LeaderElection1] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5: set configuration 0: peers:[c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 12:23:35,736 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState] INFO impl.FollowerState: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5122672121ns, electionTimeout:5034ms
datanode3_1  | 2023-06-30 12:23:35,767 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: shutdown c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState
datanode3_1  | 2023-06-30 12:23:35,768 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServer$Division: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-06-30 12:23:35,777 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode3_1  | 2023-06-30 12:23:35,783 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-FollowerState] INFO impl.RoleInfo: c26c1b41-9621-4ea7-a0f2-632ca831791f: start c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-LeaderElection2
datanode3_1  | 2023-06-30 12:23:35,824 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-LeaderElection2] INFO impl.LeaderElection: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[30c441e1-2cd9-4c36-9389-fe8b01b88a57|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9855|priority:0|startupRole:FOLLOWER, c26c1b41-9621-4ea7-a0f2-632ca831791f|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9855|priority:0|startupRole:FOLLOWER, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9855|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-06-30 12:23:36,122 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-06-30 12:23:36,122 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-06-30 12:23:36,132 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 30c441e1-2cd9-4c36-9389-fe8b01b88a57
datanode3_1  | 2023-06-30 12:23:36,197 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-42D4F8D6F6D0-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
datanode3_1  | 2023-06-30 12:23:37,622 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-c26c1b41-9621-4ea7-a0f2-632ca831791f: Detected pause in JVM or host machine approximately 0.504s with 0.659s GC time.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=659ms
datanode3_1  | 2023-06-30 12:23:38,118 [c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c26c1b41-9621-4ea7-a0f2-632ca831791f@group-9491141422F5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/394fb16c-ab50-4148-8258-9491141422f5/current/log_inprogress_0
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2023-06-30 12:19:36,761 [main] INFO security.UserGroupInformation: Login successful for user s3g/s3g@EXAMPLE.COM using keytab file s3g.keytab. Keytab auto renewal enabled : false
s3g_1        | 2023-06-30 12:19:36,776 [main] INFO s3.Gateway: S3Gateway login successful.
s3g_1        | 2023-06-30 12:19:37,669 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2023-06-30 12:19:37,669 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2023-06-30 12:19:37,669 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2023-06-30 12:19:37,986 [main] INFO util.log: Logging initialized @7895ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2023-06-30 12:19:38,727 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2023-06-30 12:19:38,788 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2023-06-30 12:19:38,803 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2023-06-30 12:19:38,803 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2023-06-30 12:19:38,804 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2023-06-30 12:19:38,833 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2023-06-30 12:19:39,053 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir14037525671399358629
s3g_1        | 2023-06-30 12:19:40,101 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
s3g_1        | STARTUP_MSG:   java = 11.0.19
s3g_1        | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir14037525671399358629, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1        | ************************************************************/
s3g_1        | 2023-06-30 12:19:40,160 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2023-06-30 12:19:40,197 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2023-06-30 12:19:40,687 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1        | 2023-06-30 12:19:41,288 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1        | 2023-06-30 12:19:41,288 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1        | 2023-06-30 12:19:41,524 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1        | 2023-06-30 12:19:41,560 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2023-06-30 12:19:41,563 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1        | 2023-06-30 12:19:41,743 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2023-06-30 12:19:41,764 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2023-06-30 12:19:41,766 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1        | 2023-06-30 12:19:41,912 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-30 12:19:41,969 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@9257031{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2023-06-30 12:19:41,996 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b4ef7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | 2023-06-30 12:19:51,474 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-06-30 12:19:52,498 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.168s with 0.195s GC time.
s3g_1        | GC pool 'ParNew' had collection(s): count=1 time=195ms
s3g_1        | 2023-06-30 12:19:55,391 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@76fc5687{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir14037525671399358629/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-10524120978835780619/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2023-06-30 12:19:55,404 [main] INFO server.AbstractConnector: Started ServerConnector@341a8659{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2023-06-30 12:19:55,405 [main] INFO server.Server: Started @25314ms
s3g_1        | 2023-06-30 12:19:55,406 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1        | 2023-06-30 12:19:55,406 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1        | 2023-06-30 12:19:55,411 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1      |                        2aad96a5fe960f5c35699d7b4ee75fe15e55e3c9
recon_1      |                        b443cca6cf8ff14594907dce81040a3eb82fbe65
recon_1      |                        0f08c8eb8d3180b65948321a05c173a8bf028426
recon_1      |                        ef756dd7721980e00b71a191f8599234b41bb7e3
recon_1      |                        562db1b8d1d8e8a067b9aa8c6b7519dfe20a49bf
recon_1      |                        e5d1803d92b4940910083c19b55228b4fd23e6a8
recon_1      |                        e7cb9ecf737ef271416d9afdce8bd21dbd31531e
recon_1      |                        68083b2d2fb51ab6a0756649a78703871d583293
recon_1      |                        18ecde1cf3bbb47ddd75928ab2262954
recon_1      |        Extensions: 
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      | 
recon_1      |                        critical(true) KeyUsage: 0xb8
recon_1      |  from file: /data/metadata/recon/certs/323029312086.crt.
recon_1      | 2023-06-30 12:20:22,134 [main] INFO security.ReconCertificateClient: Added certificate   [0]         Version: 3
recon_1      |          SerialNumber: 299862451261
recon_1      |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
recon_1      |            Start Date: Fri Jun 30 12:19:58 UTC 2023
recon_1      |            Final Date: Mon Aug 07 12:19:58 UTC 2028
recon_1      |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
recon_1      |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
recon_1      |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
recon_1      | public exponent: 10001
recon_1      | 
recon_1      |   Signature Algorithm: SHA256WITHRSA
recon_1      |             Signature: bceb238776cef1f96370453157455bfacebc30a6
recon_1      |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
recon_1      |                        92b8465e76c5427a65937f03f4c7003698371efa
recon_1      |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
recon_1      |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
recon_1      |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
recon_1      |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
recon_1      |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
recon_1      |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
recon_1      |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
recon_1      |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
recon_1      |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
recon_1      |                        82b7771fe8683859200f859a8629c61a
recon_1      |        Extensions: 
recon_1      |                        critical(false) 2.5.29.17 value = Sequence
recon_1      |     Tagged [7] IMPLICIT 
recon_1      |         DER Octet String[4] 
recon_1      |     Tagged [2] IMPLICIT 
recon_1      |         DER Octet String[8] 
recon_1      | 
recon_1      |                        critical(true) BasicConstraints: isCa(true)
recon_1      |                        critical(true) KeyUsage: 0xbe
recon_1      |  from file: /data/metadata/recon/certs/CA-299862451261.crt.
recon_1      | 2023-06-30 12:20:22,149 [main] INFO security.ReconCertificateClient: CertificateLifetimeMonitor for recon is started with first delay 29116798864 ms and interval 86400000 ms.
recon_1      | 2023-06-30 12:20:22,152 [main] INFO recon.ReconServer: Successfully stored SCM signed certificate, case:GETCERT.
recon_1      | 2023-06-30 12:20:23,057 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-30 12:20:25,291 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2023-06-30 12:20:26,026 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1      | 2023-06-30 12:20:26,035 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.008 seconds to initialized 0 records to KEY_CONTAINER table
recon_1      | 2023-06-30 12:20:26,138 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-06-30 12:20:26,243 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2023-06-30 12:20:26,244 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2023-06-30 12:20:27,991 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2023-06-30 12:20:27,992 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2023-06-30 12:20:27,992 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
recon_1      | 2023-06-30 12:20:28,049 [main] INFO util.log: Logging initialized @54527ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2023-06-30 12:20:28,757 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1      | 2023-06-30 12:20:28,804 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1      | 2023-06-30 12:20:28,812 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2023-06-30 12:20:28,813 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2023-06-30 12:20:28,813 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 2023-06-30 12:20:28,825 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2023-06-30 12:20:29,065 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1      | 2023-06-30 12:20:29,107 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1      | 2023-06-30 12:20:29,652 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2023-06-30 12:20:29,668 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1      | 2023-06-30 12:20:29,681 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1      | 2023-06-30 12:20:29,717 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1      | 2023-06-30 12:20:31,743 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-30 12:20:32,263 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-30 12:20:32,532 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2023-06-30 12:20:32,534 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2023-06-30 12:20:32,822 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-06-30 12:20:33,230 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1      | 2023-06-30 12:20:33,333 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2023-06-30 12:20:33,513 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2023-06-30 12:20:33,574 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2023-06-30 12:20:33,698 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1      | 2023-06-30 12:20:34,735 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1      | 2023-06-30 12:20:34,835 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2023-06-30 12:20:34,870 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1      | 2023-06-30 12:20:34,882 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2023-06-30 12:20:34,989 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1      | 2023-06-30 12:20:35,338 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1      | 2023-06-30 12:20:35,353 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2023-06-30 12:20:35,358 [main] INFO recon.ReconServer: Starting Recon server
recon_1      | 2023-06-30 12:20:35,583 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2023-06-30 12:20:35,630 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2023-06-30 12:20:35,638 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2023-06-30 12:20:36,417 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2023-06-30 12:20:36,425 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1      | 2023-06-30 12:20:36,611 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2023-06-30 12:20:36,611 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2023-06-30 12:20:36,620 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1      | 2023-06-30 12:20:36,670 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-30 12:20:36,673 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1a2d9177{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1      | 2023-06-30 12:20:36,674 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1a42da0a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1      | 2023-06-30 12:20:37,971 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-30 12:20:37,993 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-06-30 12:20:43,437 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@37baf62a{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-10390146046296239236/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1      | 2023-06-30 12:20:43,474 [main] INFO server.AbstractConnector: Started ServerConnector@17e03804{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1      | 2023-06-30 12:20:43,475 [main] INFO server.Server: Started @69954ms
recon_1      | 2023-06-30 12:20:43,477 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1      | 2023-06-30 12:20:43,480 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1      | 2023-06-30 12:20:43,487 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1      | 2023-06-30 12:20:43,487 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1      | 2023-06-30 12:20:43,500 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1      | 2023-06-30 12:20:43,509 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1      | 2023-06-30 12:20:43,509 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1      | 2023-06-30 12:20:43,509 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
om3_1        |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
om3_1        |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
om3_1        |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
om3_1        |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
om3_1        |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
om3_1        |                        82b7771fe8683859200f859a8629c61a
om3_1        |        Extensions: 
om3_1        |                        critical(false) 2.5.29.17 value = Sequence
om3_1        |     Tagged [7] IMPLICIT 
om3_1        |         DER Octet String[4] 
om3_1        |     Tagged [2] IMPLICIT 
om3_1        |         DER Octet String[8] 
om3_1        | 
om3_1        |                        critical(true) BasicConstraints: isCa(true)
om3_1        |                        critical(true) KeyUsage: 0xbe
om3_1        |  from file: /data/metadata/om/certs/CA-299862451261.crt.
om3_1        | 2023-06-30 12:23:03,217 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor for om is started with first delay 29116750793 ms and interval 86400000 ms.
om3_1        | 2023-06-30 12:23:03,514 [main] INFO om.OzoneManager: OM start with adminUsers: [testuser, recon, om]
om3_1        | 2023-06-30 12:23:03,760 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 12:23:04,712 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1        | 2023-06-30 12:23:06,941 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1        | 2023-06-30 12:23:07,152 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2023-06-30 12:23:07,154 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2023-06-30 12:23:07,629 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om3_1        | 2023-06-30 12:23:07,691 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-06-30 12:23:08,047 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1        | 2023-06-30 12:23:08,062 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om3_1        | 2023-06-30 12:23:09,576 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om3_1        | 2023-06-30 12:23:10,264 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-30 12:23:10,267 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1        | 2023-06-30 12:23:10,351 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om3_1        | 2023-06-30 12:23:10,352 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2023-06-30 12:23:11,234 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2023-06-30 12:23:11,302 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-06-30 12:23:11,413 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1        | 2023-06-30 12:23:11,473 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2023-06-30 12:23:11,911 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-30 12:23:11,920 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-06-30 12:23:12,000 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2023-06-30 12:23:12,058 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-30 12:23:12,082 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-30 12:23:12,085 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-06-30 12:23:12,086 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-06-30 12:23:12,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1        | 2023-06-30 12:23:12,089 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2023-06-30 12:23:12,091 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2023-06-30 12:23:12,103 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 12:23:12,105 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2023-06-30 12:23:12,114 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-30 12:23:12,170 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-06-30 12:23:12,189 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1        | 2023-06-30 12:23:12,195 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-06-30 12:23:14,099 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1        | 2023-06-30 12:23:14,125 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1        | 2023-06-30 12:23:14,129 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1        | 2023-06-30 12:23:14,129 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-30 12:23:14,132 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-30 12:23:14,171 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-30 12:19:47,970 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-30 12:19:48,073 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-06-30 12:19:49,026 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 12:19:49,956 [main] INFO reflections.Reflections: Reflections took 575 ms to scan 3 urls, producing 132 keys and 287 values 
scm1.org_1   | 2023-06-30 12:19:50,728 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-30 12:19:50,833 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-30 12:19:50,998 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-30 12:19:51,015 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-30 12:19:51,073 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2023-06-30 12:19:55,732 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-30 12:19:55,754 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm1.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm1.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm1.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm1.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm1.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:1274)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:186)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:116)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm1.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm1.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm1.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm1.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm1.org_1   | 2023-06-30 12:19:55,756 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2023-06-30 12:19:55,757 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2023-06-30 12:19:56,986 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2023-06-30 12:19:58,169 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-30 12:19:58,173 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2023-06-30 12:23:14,248 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@42c82f73[Not completed]
om3_1        | 2023-06-30 12:23:14,249 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2023-06-30 12:23:14,283 [main] INFO om.OzoneManager: Creating RPC Server
om3_1        | 2023-06-30 12:23:14,370 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1        | 2023-06-30 12:23:14,444 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2023-06-30 12:23:14,446 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2023-06-30 12:23:14,450 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2023-06-30 12:23:14,452 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-06-30 12:23:14,453 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-06-30 12:23:14,453 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2023-06-30 12:23:14,518 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1        | 2023-06-30 12:23:14,520 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-06-30 12:23:14,552 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2023-06-30 12:23:14,553 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1        | 2023-06-30 12:23:14,703 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
recon_1      | 2023-06-30 12:20:43,509 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1      | 2023-06-30 12:20:43,510 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1      | 2023-06-30 12:20:44,926 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1      | 2023-06-30 12:20:44,929 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2023-06-30 12:20:44,929 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1      | 2023-06-30 12:20:44,932 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1      | 2023-06-30 12:20:44,939 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1      | 2023-06-30 12:20:44,944 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1      | 2023-06-30 12:21:03,510 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1      | 2023-06-30 12:21:03,511 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1      | 2023-06-30 12:21:05,715 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately. Current retry count: 3.
recon_1      | 2023-06-30 12:21:05,720 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately. Current retry count: 4.
recon_1      | 2023-06-30 12:21:05,721 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1      | 2023-06-30 12:21:07,733 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately. Current retry count: 6.
recon_1      | 2023-06-30 12:21:07,735 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately. Current retry count: 7.
recon_1      | 2023-06-30 12:21:07,736 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1      | 2023-06-30 12:21:09,738 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately. Current retry count: 9.
recon_1      | 2023-06-30 12:21:09,741 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately. Current retry count: 10.
recon_1      | 2023-06-30 12:21:09,743 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1      | 2023-06-30 12:21:11,747 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately. Current retry count: 12.
recon_1      | 2023-06-30 12:21:11,748 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately. Current retry count: 13.
recon_1      | 2023-06-30 12:21:11,752 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
recon_1      | 2023-06-30 12:21:13,755 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately. Current retry count: 15.
recon_1      | 2023-06-30 12:21:13,765 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately. Current retry count: 16.
recon_1      | 2023-06-30 12:21:13,772 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
recon_1      | 2023-06-30 12:21:15,774 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately. Current retry count: 18.
recon_1      | 2023-06-30 12:21:15,776 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately. Current retry count: 19.
recon_1      | 2023-06-30 12:21:15,780 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 20.
recon_1      | 2023-06-30 12:21:17,781 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately. Current retry count: 21.
recon_1      | 2023-06-30 12:21:17,783 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately. Current retry count: 22.
recon_1      | 2023-06-30 12:21:17,784 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 23.
recon_1      | 2023-06-30 12:21:19,789 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately. Current retry count: 24.
recon_1      | 2023-06-30 12:21:19,796 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately. Current retry count: 25.
recon_1      | 2023-06-30 12:21:19,806 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 26.
recon_1      | 2023-06-30 12:21:21,808 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately. Current retry count: 27.
recon_1      | 2023-06-30 12:21:21,813 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately. Current retry count: 28.
recon_1      | 2023-06-30 12:21:21,824 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 29.
recon_1      | 2023-06-30 12:21:23,826 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately. Current retry count: 30.
recon_1      | 2023-06-30 12:21:23,828 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately. Current retry count: 31.
recon_1      | 2023-06-30 12:21:23,829 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 32.
recon_1      | 2023-06-30 12:21:25,831 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately. Current retry count: 33.
recon_1      | 2023-06-30 12:21:25,832 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately. Current retry count: 34.
recon_1      | 2023-06-30 12:21:25,833 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 35.
om3_1        | 2023-06-30 12:23:14,725 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1        | 2023-06-30 12:23:14,757 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2023-06-30 12:23:14,758 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2023-06-30 12:23:14,900 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1        | 2023-06-30 12:23:15,260 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-06-30 12:23:15,274 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-06-30 12:23:15,277 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1        | 2023-06-30 12:23:15,279 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1        | 2023-06-30 12:23:15,282 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1        | 2023-06-30 12:23:15,284 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1        | 2023-06-30 12:23:17,939 [main] INFO reflections.Reflections: Reflections took 2980 ms to scan 8 urls, producing 24 keys and 644 values [using 2 cores]
om3_1        | 2023-06-30 12:23:19,317 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2023-06-30 12:23:19,355 [main] INFO ipc.Server: Listener at om3:9862
om3_1        | 2023-06-30 12:23:19,371 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2023-06-30 12:23:25,267 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2023-06-30 12:23:25,393 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2023-06-30 12:23:25,397 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2023-06-30 12:23:26,276 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2023-06-30 12:23:26,276 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2023-06-30 12:23:26,278 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1        | 2023-06-30 12:23:26,324 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om3
om3_1        | 2023-06-30 12:23:26,440 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om3_1        | 2023-06-30 12:23:26,471 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2023-06-30 12:23:26,602 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2023-06-30 12:23:26,614 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 12:23:26,644 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1        | 2023-06-30 12:23:26,654 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1        | 2023-06-30 12:23:26,682 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-06-30 12:23:26,750 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2023-06-30 12:23:26,758 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2023-06-30 12:23:26,759 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 12:23:26,827 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1        | 2023-06-30 12:23:26,830 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1        | 2023-06-30 12:23:26,832 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2023-06-30 12:23:26,855 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-06-30 12:23:26,860 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2023-06-30 12:23:26,863 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1        | 2023-06-30 12:23:26,870 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2023-06-30 12:23:26,878 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2023-06-30 12:23:26,880 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2023-06-30 12:23:27,033 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2023-06-30 12:23:27,033 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-06-30 12:23:27,248 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1        | 2023-06-30 12:23:27,305 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1        | 2023-06-30 12:23:27,305 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2023-06-30 12:23:27,381 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-30 12:23:27,381 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2023-06-30 12:23:27,404 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-06-30 12:23:27,430 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2023-06-30 12:23:27,451 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1        | 2023-06-30 12:23:27,490 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1        | 2023-06-30 12:23:27,505 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-30 12:20:03,028 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-30 12:20:03,039 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-30 12:20:03,137 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 12:20:03,298 [main] INFO reflections.Reflections: Reflections took 120 ms to scan 3 urls, producing 132 keys and 287 values 
scm2.org_1   | 2023-06-30 12:20:03,376 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-30 12:20:03,377 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-30 12:20:03,404 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-30 12:20:03,404 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-30 12:20:03,559 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-30 12:20:03,559 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-30 12:20:03,682 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-30 12:20:07,892 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
scm2.org_1   | 2023-06-30 12:20:09,894 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
scm2.org_1   | 2023-06-30 12:20:11,896 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm2.org_1   | 2023-06-30 12:20:13,905 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
scm2.org_1   | 2023-06-30 12:20:16,184 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
scm2.org_1   | 2023-06-30 12:20:18,189 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
scm2.org_1   | 2023-06-30 12:20:20,341 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm2.org_1   | 2023-06-30 12:20:21,124 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-30 12:20:21,127 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm2.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm2.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm2.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm2.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm2.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm2.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm2.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm2.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm2.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm2.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm2.org_1   | 2023-06-30 12:20:21,128 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2023-06-30 12:20:21,129 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2023-06-30 12:20:22,819 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm2.org_1   | 2023-06-30 12:20:22,896 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2023-06-30 12:20:22,897 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2023-06-30 12:20:22,904 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:58e977b4-5318-453e-8f7b-12be220c6897,clusterId:CID-9c10c649-f59d-4933-ad5d-654b70cf4739,subject:scm-sub@scm2.org
scm2.org_1   | 2023-06-30 12:20:26,297 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2023-06-30 12:20:26,320 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-9c10c649-f59d-4933-ad5d-654b70cf4739, SCMID 58e977b4-5318-453e-8f7b-12be220c6897
scm2.org_1   | 2023-06-30 12:20:26,320 [main] INFO server.StorageContainerManager: Primary SCM Node ID ce3545fb-3d5f-455d-aab8-f276e4d7a1ce
scm2.org_1   | 2023-06-30 12:20:26,366 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-06-30 12:20:31,842 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | 2023-06-30 12:19:58,359 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739 to CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739, valid from Fri Jun 30 12:19:58 UTC 2023 to Mon Aug 07 12:19:58 UTC 2028
scm1.org_1   | 2023-06-30 12:19:58,405 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-06-30 12:19:58,405 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-06-30 12:19:58,409 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,clusterId:CID-9c10c649-f59d-4933-ad5d-654b70cf4739,subject:scm-sub@scm1.org
scm1.org_1   | 2023-06-30 12:19:58,578 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2023-06-30 12:19:58,783 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-30 12:19:58,928 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 12:19:58,929 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 12:19:58,934 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 12:19:58,935 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 12:19:58,935 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-06-30 12:19:58,935 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-30 12:19:58,937 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-06-30 12:19:58,939 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:19:58,940 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-06-30 12:19:58,940 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 12:19:58,951 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 12:19:58,954 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-30 12:19:58,955 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-30 12:19:59,354 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-30 12:19:59,356 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-30 12:19:59,357 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-30 12:19:59,357 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 12:19:59,358 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 12:19:59,371 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 12:19:59,380 [main] INFO server.RaftServer: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: addNew group-654B70CF4739:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] returns group-654B70CF4739:java.util.concurrent.CompletableFuture@41167ded[Not completed]
scm1.org_1   | 2023-06-30 12:19:59,402 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: new RaftServerImpl for group-654B70CF4739:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-30 12:19:59,405 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-30 12:19:59,406 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-30 12:19:59,406 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-30 12:19:59,407 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 12:19:59,407 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 12:19:59,407 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-30 12:19:59,418 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: ConfigurationManager, init=-1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-30 12:19:59,418 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 12:19:59,424 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-06-30 12:19:59,425 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-30 12:19:59,453 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-30 12:19:59,457 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-30 12:19:59,462 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-30 12:19:59,462 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-30 12:19:59,483 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-30 12:19:59,491 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-30 12:19:59,618 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 12:19:59,623 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-30 12:19:59,624 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-30 12:19:59,625 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-30 12:19:59,626 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-30 12:19:59,627 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1      | 2023-06-30 12:21:27,834 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately. Current retry count: 36.
recon_1      | 2023-06-30 12:21:27,838 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately. Current retry count: 37.
recon_1      | 2023-06-30 12:21:27,850 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 38.
recon_1      | 2023-06-30 12:21:29,864 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately. Current retry count: 39.
recon_1      | 2023-06-30 12:21:29,867 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately. Current retry count: 40.
recon_1      | 2023-06-30 12:21:29,868 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 41.
recon_1      | 2023-06-30 12:21:31,869 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately. Current retry count: 42.
recon_1      | 2023-06-30 12:21:31,871 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately. Current retry count: 43.
recon_1      | 2023-06-30 12:21:31,873 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 44.
recon_1      | 2023-06-30 12:21:33,875 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately. Current retry count: 45.
recon_1      | 2023-06-30 12:21:33,878 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately. Current retry count: 46.
recon_1      | 2023-06-30 12:21:33,879 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 47.
recon_1      | 2023-06-30 12:21:35,882 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover immediately. Current retry count: 48.
recon_1      | 2023-06-30 12:21:35,934 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 49 failover attempts. Trying to failover immediately. Current retry count: 49.
recon_1      | 2023-06-30 12:21:35,963 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 50 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 50.
recon_1      | 2023-06-30 12:21:37,965 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 51 failover attempts. Trying to failover immediately. Current retry count: 51.
recon_1      | 2023-06-30 12:21:37,966 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 52 failover attempts. Trying to failover immediately. Current retry count: 52.
recon_1      | 2023-06-30 12:21:37,967 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 53 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 53.
recon_1      | 2023-06-30 12:21:39,968 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 54 failover attempts. Trying to failover immediately. Current retry count: 54.
recon_1      | 2023-06-30 12:21:39,969 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 55 failover attempts. Trying to failover immediately. Current retry count: 55.
recon_1      | 2023-06-30 12:21:39,970 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 56 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 56.
recon_1      | 2023-06-30 12:21:41,972 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 57 failover attempts. Trying to failover immediately. Current retry count: 57.
recon_1      | 2023-06-30 12:21:41,973 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 58 failover attempts. Trying to failover immediately. Current retry count: 58.
recon_1      | 2023-06-30 12:21:41,975 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 59 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 59.
recon_1      | 2023-06-30 12:21:43,977 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 60 failover attempts. Trying to failover immediately. Current retry count: 60.
recon_1      | 2023-06-30 12:21:43,978 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 61 failover attempts. Trying to failover immediately. Current retry count: 61.
recon_1      | 2023-06-30 12:21:43,981 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 62 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 62.
recon_1      | 2023-06-30 12:21:45,983 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 63 failover attempts. Trying to failover immediately. Current retry count: 63.
recon_1      | 2023-06-30 12:21:45,983 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 64 failover attempts. Trying to failover immediately. Current retry count: 64.
recon_1      | 2023-06-30 12:21:45,984 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 65 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 65.
recon_1      | 2023-06-30 12:21:47,986 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 66 failover attempts. Trying to failover immediately. Current retry count: 66.
recon_1      | 2023-06-30 12:21:47,987 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 67 failover attempts. Trying to failover immediately. Current retry count: 67.
recon_1      | 2023-06-30 12:21:47,988 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 68 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 68.
recon_1      | 2023-06-30 12:21:49,990 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 69 failover attempts. Trying to failover immediately. Current retry count: 69.
recon_1      | 2023-06-30 12:21:49,991 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 70 failover attempts. Trying to failover immediately. Current retry count: 70.
recon_1      | 2023-06-30 12:21:49,991 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 71 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 71.
recon_1      | 2023-06-30 12:21:51,993 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 72 failover attempts. Trying to failover immediately. Current retry count: 72.
recon_1      | 2023-06-30 12:21:51,994 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 73 failover attempts. Trying to failover immediately. Current retry count: 73.
recon_1      | 2023-06-30 12:21:51,996 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 74 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 74.
recon_1      | 2023-06-30 12:21:53,997 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 75 failover attempts. Trying to failover immediately. Current retry count: 75.
recon_1      | 2023-06-30 12:21:53,999 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 76 failover attempts. Trying to failover immediately. Current retry count: 76.
recon_1      | 2023-06-30 12:21:54,003 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 77 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 77.
recon_1      | 2023-06-30 12:21:56,012 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 78 failover attempts. Trying to failover immediately. Current retry count: 78.
recon_1      | 2023-06-30 12:21:56,023 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 79 failover attempts. Trying to failover immediately. Current retry count: 79.
recon_1      | 2023-06-30 12:21:56,029 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 80 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 80.
recon_1      | 2023-06-30 12:21:58,031 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 81 failover attempts. Trying to failover immediately. Current retry count: 81.
recon_1      | 2023-06-30 12:21:58,032 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 82 failover attempts. Trying to failover immediately. Current retry count: 82.
recon_1      | 2023-06-30 12:21:58,035 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 83 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 83.
om3_1        | 2023-06-30 12:23:27,506 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-06-30 12:23:27,539 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2023-06-30 12:23:27,546 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2023-06-30 12:23:27,547 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2023-06-30 12:23:27,547 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2023-06-30 12:23:27,601 [main] INFO server.RaftServer: om3: start RPC server
om3_1        | 2023-06-30 12:23:28,336 [main] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2023-06-30 12:23:28,374 [main] INFO om.OzoneManager: Starting secret key client.
om3_1        | 2023-06-30 12:23:28,408 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2023-06-30 12:23:29,613 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z).
om3_1        | 2023-06-30 12:23:29,650 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT1M49.486916S and interval of PT1M
om3_1        | 2023-06-30 12:23:29,710 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2023-06-30 12:23:29,712 [main] INFO security.OzoneDelegationTokenSecretManager: Updating current master key for generating tokens. Cert id 435562639432
om3_1        | 2023-06-30 12:23:29,782 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2023-06-30 12:23:29,789 [Thread[Thread-21,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        | 2023-06-30 12:23:32,502 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om3: Detected pause in JVM or host machine approximately 0.191s without any GCs.
om3_1        | 2023-06-30 12:23:32,773 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5329941161ns, electionTimeout:5198ms
om3_1        | 2023-06-30 12:23:32,807 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1        | 2023-06-30 12:23:32,915 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1        | 2023-06-30 12:23:33,072 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1        | 2023-06-30 12:23:33,094 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1        | 2023-06-30 12:23:33,269 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1        | 2023-06-30 12:23:33,286 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om3_1        | 2023-06-30 12:23:33,286 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om3_1        | 2023-06-30 12:23:33,311 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-06-30 12:23:33,715 [main] INFO util.log: Logging initialized @76463ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1        | 2023-06-30 12:23:33,721 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-06-30 12:23:33,721 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-06-30 12:23:33,748 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om3_1        | 2023-06-30 12:23:33,759 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1        | 2023-06-30 12:23:36,400 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1        | 2023-06-30 12:23:36,691 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1        | 2023-06-30 12:23:36,806 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om3_1        | 2023-06-30 12:23:36,816 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om3_1        | 2023-06-30 12:23:36,896 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om3_1        | 2023-06-30 12:23:37,026 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om3_1        | 2023-06-30 12:23:37,765 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om3_1        | 2023-06-30 12:23:37,845 [main] INFO http.HttpServer2: Jetty bound to port 9874
om3_1        | 2023-06-30 12:23:37,861 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om3_1        | 2023-06-30 12:23:38,147 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.645901882s. [buffered_nanos=2670936703, waiting_for_connection]
om3_1        | 2023-06-30 12:23:38,149 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.659262285s. [buffered_nanos=2681144572, waiting_for_connection]
om3_1        | 2023-06-30 12:23:38,154 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
om3_1        | 2023-06-30 12:23:38,156 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.645901882s. [buffered_nanos=2670936703, waiting_for_connection]
om3_1        | 2023-06-30 12:23:38,156 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.659262285s. [buffered_nanos=2681144572, waiting_for_connection]
om3_1        | 2023-06-30 12:23:38,158 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result REJECTED
om3_1        | 2023-06-30 12:23:38,169 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
om3_1        | 2023-06-30 12:23:38,169 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
om3_1        | 2023-06-30 12:23:38,172 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1        | 2023-06-30 12:23:38,258 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1        | 2023-06-30 12:23:38,261 [main] INFO server.session: No SessionScavenger set, using defaults
om3_1        | 2023-06-30 12:23:38,279 [main] INFO server.session: node0 Scavenging every 600000ms
om3_1        | 2023-06-30 12:23:38,494 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2023-06-30 12:23:38,519 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5c10b993{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1        | 2023-06-30 12:23:38,530 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@56a11071{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2023-06-30 12:19:59,628 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739 does not exist. Creating ...
scm1.org_1   | 2023-06-30 12:19:59,642 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/in_use.lock acquired by nodename 13@scm1.org
scm1.org_1   | 2023-06-30 12:19:59,651 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739 has been successfully formatted.
scm1.org_1   | 2023-06-30 12:19:59,656 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-30 12:19:59,666 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-30 12:19:59,666 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:19:59,668 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-30 12:19:59,668 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-30 12:19:59,671 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 12:19:59,677 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-30 12:19:59,677 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-30 12:19:59,677 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:19:59,682 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   | 2023-06-30 12:19:59,683 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 12:19:59,683 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 12:19:59,684 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 12:19:59,685 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-30 12:19:59,685 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-30 12:19:59,688 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-30 12:19:59,689 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-30 12:19:59,689 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-30 12:19:59,701 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-30 12:19:59,701 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:19:59,723 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 12:19:59,723 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 12:19:59,723 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-06-30 12:19:59,731 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-30 12:19:59,731 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-30 12:19:59,733 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: start as a follower, conf=-1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:19:59,733 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2023-06-30 12:19:59,735 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState
scm1.org_1   | 2023-06-30 12:19:59,738 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-654B70CF4739,id=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce
scm1.org_1   | 2023-06-30 12:19:59,740 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-30 12:19:59,740 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-30 12:19:59,740 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-30 12:19:59,741 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-30 12:19:59,741 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-30 12:19:59,741 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-30 12:19:59,760 [main] INFO server.RaftServer: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start RPC server
scm1.org_1   | 2023-06-30 12:19:59,833 [main] INFO server.GrpcService: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-30 12:19:59,836 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: Started
recon_1      | 2023-06-30 12:22:00,036 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 84 failover attempts. Trying to failover immediately. Current retry count: 84.
recon_1      | 2023-06-30 12:22:00,038 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 85 failover attempts. Trying to failover immediately. Current retry count: 85.
recon_1      | 2023-06-30 12:22:00,051 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 86 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 86.
recon_1      | 2023-06-30 12:22:02,053 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 87 failover attempts. Trying to failover immediately. Current retry count: 87.
recon_1      | 2023-06-30 12:22:02,054 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 88 failover attempts. Trying to failover immediately. Current retry count: 88.
recon_1      | 2023-06-30 12:22:02,054 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 89 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 89.
recon_1      | 2023-06-30 12:22:04,056 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 90 failover attempts. Trying to failover immediately. Current retry count: 90.
recon_1      | 2023-06-30 12:22:04,057 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 91 failover attempts. Trying to failover immediately. Current retry count: 91.
recon_1      | 2023-06-30 12:22:04,058 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 92 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 92.
recon_1      | 2023-06-30 12:22:06,060 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 93 failover attempts. Trying to failover immediately. Current retry count: 93.
recon_1      | 2023-06-30 12:22:06,061 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 94 failover attempts. Trying to failover immediately. Current retry count: 94.
recon_1      | 2023-06-30 12:22:06,062 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 95 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 95.
recon_1      | 2023-06-30 12:22:08,064 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 96 failover attempts. Trying to failover immediately. Current retry count: 96.
recon_1      | 2023-06-30 12:22:08,065 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 97 failover attempts. Trying to failover immediately. Current retry count: 97.
recon_1      | 2023-06-30 12:22:08,066 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 98 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 98.
recon_1      | 2023-06-30 12:22:10,067 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 99 failover attempts. Trying to failover immediately. Current retry count: 99.
recon_1      | 2023-06-30 12:22:10,068 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 100 failover attempts. Trying to failover immediately. Current retry count: 100.
recon_1      | 2023-06-30 12:22:10,070 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 101 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 101.
recon_1      | 2023-06-30 12:22:12,071 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 102 failover attempts. Trying to failover immediately. Current retry count: 102.
recon_1      | 2023-06-30 12:22:12,072 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 103 failover attempts. Trying to failover immediately. Current retry count: 103.
recon_1      | 2023-06-30 12:22:12,073 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 104 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 104.
recon_1      | 2023-06-30 12:22:14,074 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 105 failover attempts. Trying to failover immediately. Current retry count: 105.
recon_1      | 2023-06-30 12:22:14,075 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 106 failover attempts. Trying to failover immediately. Current retry count: 106.
recon_1      | 2023-06-30 12:22:14,076 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 107 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 107.
recon_1      | 2023-06-30 12:22:16,090 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 108 failover attempts. Trying to failover immediately. Current retry count: 108.
recon_1      | 2023-06-30 12:22:16,099 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 109 failover attempts. Trying to failover immediately. Current retry count: 109.
recon_1      | 2023-06-30 12:22:16,120 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 110 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 110.
recon_1      | 2023-06-30 12:22:18,125 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 111 failover attempts. Trying to failover immediately. Current retry count: 111.
recon_1      | 2023-06-30 12:22:18,128 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 112 failover attempts. Trying to failover immediately. Current retry count: 112.
recon_1      | 2023-06-30 12:22:18,147 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 113 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 113.
recon_1      | 2023-06-30 12:22:20,148 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 114 failover attempts. Trying to failover immediately. Current retry count: 114.
recon_1      | 2023-06-30 12:22:20,149 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 115 failover attempts. Trying to failover immediately. Current retry count: 115.
recon_1      | 2023-06-30 12:22:20,150 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 116 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 116.
recon_1      | 2023-06-30 12:22:22,151 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 117 failover attempts. Trying to failover immediately. Current retry count: 117.
recon_1      | 2023-06-30 12:22:22,153 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 118 failover attempts. Trying to failover immediately. Current retry count: 118.
recon_1      | 2023-06-30 12:22:22,163 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 119 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 119.
recon_1      | 2023-06-30 12:22:24,165 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 120 failover attempts. Trying to failover immediately. Current retry count: 120.
recon_1      | 2023-06-30 12:22:24,165 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 121 failover attempts. Trying to failover immediately. Current retry count: 121.
recon_1      | 2023-06-30 12:22:24,166 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 122 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 122.
recon_1      | 2023-06-30 12:22:26,177 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 123 failover attempts. Trying to failover immediately. Current retry count: 123.
recon_1      | 2023-06-30 12:22:26,182 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 124 failover attempts. Trying to failover immediately. Current retry count: 124.
recon_1      | 2023-06-30 12:22:26,186 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 125 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 125.
recon_1      | 2023-06-30 12:22:28,193 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 126 failover attempts. Trying to failover immediately. Current retry count: 126.
recon_1      | 2023-06-30 12:22:28,195 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 127 failover attempts. Trying to failover immediately. Current retry count: 127.
recon_1      | 2023-06-30 12:22:28,196 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 128 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 128.
recon_1      | 2023-06-30 12:22:30,199 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 129 failover attempts. Trying to failover immediately. Current retry count: 129.
recon_1      | 2023-06-30 12:22:30,202 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 130 failover attempts. Trying to failover immediately. Current retry count: 130.
recon_1      | 2023-06-30 12:22:30,211 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 131 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 131.
recon_1      | 2023-06-30 12:22:32,221 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 132 failover attempts. Trying to failover immediately. Current retry count: 132.
recon_1      | 2023-06-30 12:22:32,223 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 133 failover attempts. Trying to failover immediately. Current retry count: 133.
recon_1      | 2023-06-30 12:22:32,224 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 134 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 134.
recon_1      | 2023-06-30 12:22:34,226 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 135 failover attempts. Trying to failover immediately. Current retry count: 135.
recon_1      | 2023-06-30 12:22:34,229 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 136 failover attempts. Trying to failover immediately. Current retry count: 136.
recon_1      | 2023-06-30 12:22:34,230 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 137 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 137.
recon_1      | 2023-06-30 12:22:36,232 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 138 failover attempts. Trying to failover immediately. Current retry count: 138.
recon_1      | 2023-06-30 12:22:36,233 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 139 failover attempts. Trying to failover immediately. Current retry count: 139.
recon_1      | 2023-06-30 12:22:36,234 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 140 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 140.
recon_1      | 2023-06-30 12:22:38,237 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 141 failover attempts. Trying to failover immediately. Current retry count: 141.
recon_1      | 2023-06-30 12:22:38,239 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 142 failover attempts. Trying to failover immediately. Current retry count: 142.
recon_1      | 2023-06-30 12:22:38,241 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 143 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 143.
recon_1      | 2023-06-30 12:22:40,242 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 144 failover attempts. Trying to failover immediately. Current retry count: 144.
recon_1      | 2023-06-30 12:22:40,244 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 145 failover attempts. Trying to failover immediately. Current retry count: 145.
recon_1      | 2023-06-30 12:22:40,244 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 146 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 146.
recon_1      | 2023-06-30 12:22:42,246 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 147 failover attempts. Trying to failover immediately. Current retry count: 147.
recon_1      | 2023-06-30 12:22:42,249 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 148 failover attempts. Trying to failover immediately. Current retry count: 148.
recon_1      | 2023-06-30 12:22:42,250 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 149 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 149.
recon_1      | 2023-06-30 12:22:44,252 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 150 failover attempts. Trying to failover immediately. Current retry count: 150.
recon_1      | 2023-06-30 12:22:44,255 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 151 failover attempts. Trying to failover immediately. Current retry count: 151.
recon_1      | 2023-06-30 12:22:44,261 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 152 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 152.
recon_1      | 2023-06-30 12:22:45,383 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:49406 / 172.25.0.102:49406
recon_1      | 2023-06-30 12:22:45,414 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 12:22:46,278 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 153 failover attempts. Trying to failover immediately. Current retry count: 153.
recon_1      | 2023-06-30 12:22:46,292 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 154 failover attempts. Trying to failover immediately. Current retry count: 154.
recon_1      | 2023-06-30 12:22:46,297 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 155 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 155.
recon_1      | 2023-06-30 12:22:46,582 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:55842 / 172.25.0.104:55842
recon_1      | 2023-06-30 12:22:46,728 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 12:22:48,299 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 156 failover attempts. Trying to failover immediately. Current retry count: 156.
recon_1      | 2023-06-30 12:22:48,399 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 157 failover attempts. Trying to failover immediately. Current retry count: 157.
recon_1      | 2023-06-30 12:22:48,434 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 158 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 158.
recon_1      | 2023-06-30 12:22:49,466 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:43838 / 172.25.0.103:43838
recon_1      | 2023-06-30 12:22:49,702 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 12:22:50,441 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 159 failover attempts. Trying to failover immediately. Current retry count: 159.
recon_1      | 2023-06-30 12:22:50,456 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 160 failover attempts. Trying to failover immediately. Current retry count: 160.
recon_1      | 2023-06-30 12:22:50,467 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 161 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 161.
recon_1      | 2023-06-30 12:22:50,592 [IPC Server handler 3 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/30c441e1-2cd9-4c36-9389-fe8b01b88a57
recon_1      | 2023-06-30 12:22:50,773 [IPC Server handler 3 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 30c441e1-2cd9-4c36-9389-fe8b01b88a57{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886], networkLocation: /default-rack, certSerialId: 428663643003, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-30 12:22:51,349 [IPC Server handler 5 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c26c1b41-9621-4ea7-a0f2-632ca831791f
recon_1      | 2023-06-30 12:22:51,368 [IPC Server handler 5 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c26c1b41-9621-4ea7-a0f2-632ca831791f{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 427123221923, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-30 12:22:52,177 [IPC Server handler 48 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
recon_1      | 2023-06-30 12:22:52,186 [IPC Server handler 48 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a9efabc3-aa7d-48e7-8786-4f3f9c9666fc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855], networkLocation: /default-rack, certSerialId: 431623557457, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-06-30 12:22:52,480 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 162 failover attempts. Trying to failover immediately. Current retry count: 162.
recon_1      | 2023-06-30 12:22:52,484 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 163 failover attempts. Trying to failover immediately. Current retry count: 163.
recon_1      | 2023-06-30 12:22:52,490 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 164 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 164.
recon_1      | 2023-06-30 12:22:54,463 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 30c441e1-2cd9-4c36-9389-fe8b01b88a57 to Node DB.
recon_1      | 2023-06-30 12:22:54,478 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c26c1b41-9621-4ea7-a0f2-632ca831791f to Node DB.
recon_1      | 2023-06-30 12:22:54,490 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a9efabc3-aa7d-48e7-8786-4f3f9c9666fc to Node DB.
recon_1      | 2023-06-30 12:22:54,502 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 165 failover attempts. Trying to failover immediately. Current retry count: 165.
recon_1      | 2023-06-30 12:22:54,504 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 166 failover attempts. Trying to failover immediately. Current retry count: 166.
recon_1      | 2023-06-30 12:22:54,506 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 167 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 167.
recon_1      | 2023-06-30 12:22:56,521 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 168 failover attempts. Trying to failover immediately. Current retry count: 168.
recon_1      | 2023-06-30 12:22:56,522 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 169 failover attempts. Trying to failover immediately. Current retry count: 169.
recon_1      | 2023-06-30 12:22:56,523 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 170 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 170.
recon_1      | 2023-06-30 12:22:58,525 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 171 failover attempts. Trying to failover immediately. Current retry count: 171.
recon_1      | 2023-06-30 12:22:58,532 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 172 failover attempts. Trying to failover immediately. Current retry count: 172.
recon_1      | 2023-06-30 12:22:58,541 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 173 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 173.
recon_1      | 2023-06-30 12:23:00,543 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 174 failover attempts. Trying to failover immediately. Current retry count: 174.
recon_1      | 2023-06-30 12:23:00,545 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 175 failover attempts. Trying to failover immediately. Current retry count: 175.
recon_1      | 2023-06-30 12:23:00,546 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 176 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 176.
recon_1      | 2023-06-30 12:23:02,547 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 177 failover attempts. Trying to failover immediately. Current retry count: 177.
recon_1      | 2023-06-30 12:23:02,550 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 178 failover attempts. Trying to failover immediately. Current retry count: 178.
recon_1      | 2023-06-30 12:23:02,553 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 179 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 179.
recon_1      | 2023-06-30 12:23:04,554 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 180 failover attempts. Trying to failover immediately. Current retry count: 180.
recon_1      | 2023-06-30 12:23:04,557 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 181 failover attempts. Trying to failover immediately. Current retry count: 181.
recon_1      | 2023-06-30 12:23:04,560 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 182 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 182.
recon_1      | 2023-06-30 12:23:06,561 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 183 failover attempts. Trying to failover immediately. Current retry count: 183.
recon_1      | 2023-06-30 12:23:06,563 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 184 failover attempts. Trying to failover immediately. Current retry count: 184.
recon_1      | 2023-06-30 12:23:06,564 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 185 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 185.
recon_1      | 2023-06-30 12:23:08,567 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 186 failover attempts. Trying to failover immediately. Current retry count: 186.
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-30 12:20:53,218 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-30 12:20:53,262 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-06-30 12:20:53,560 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 12:20:54,085 [main] INFO reflections.Reflections: Reflections took 429 ms to scan 3 urls, producing 132 keys and 287 values 
scm3.org_1   | 2023-06-30 12:20:54,466 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-06-30 12:20:54,466 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-30 12:20:54,598 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-30 12:20:54,600 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-30 12:20:55,184 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-30 12:20:55,184 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-30 12:20:55,366 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-30 12:20:56,287 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2023-06-30 12:20:57,414 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.updateCertSerialId(DefaultCertificateClient.java:1204)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.<init>(DefaultCertificateClient.java:150)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.SCMCertificateClient.<init>(SCMCertificateClient.java:58)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:106)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-30 12:20:57,422 [main] WARN client.SCMCertificateClient: Certificates could not be loaded.
scm3.org_1   | java.nio.file.NoSuchFileException: /data/metadata/scm/sub-ca/certs
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
scm3.org_1   | 	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
scm3.org_1   | 	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:472)
scm3.org_1   | 	at java.base/java.nio.file.Files.list(Files.java:3699)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.loadAllCertificates(DefaultCertificateClient.java:158)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertPath(DefaultCertificateClient.java:326)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.getCertificate(DefaultCertificateClient.java:302)
scm3.org_1   | 	at org.apache.hadoop.hdds.security.x509.certificate.client.DefaultCertificateClient.init(DefaultCertificateClient.java:671)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.HASecurityUtils.initializeSecurity(HASecurityUtils.java:107)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmBootstrap(StorageContainerManager.java:1202)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.bootStrap(StorageContainerManagerStarter.java:192)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.bootStrapScm(StorageContainerManagerStarter.java:135)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm3.org_1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3.org_1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm3.org_1   | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3.org_1   | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm3.org_1   | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm3.org_1   | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
scm3.org_1   | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3.org_1   | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
scm3.org_1   | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3.org_1   | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:63)
scm3.org_1   | 2023-06-30 12:20:57,437 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.19
scm2.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-06-30 12:20:31,896 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-06-30 12:20:32,166 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 12:20:32,754 [main] INFO reflections.Reflections: Reflections took 460 ms to scan 3 urls, producing 132 keys and 287 values 
scm2.org_1   | 2023-06-30 12:20:33,046 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-06-30 12:20:33,085 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-06-30 12:20:33,154 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-06-30 12:20:33,160 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-06-30 12:20:33,793 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-06-30 12:20:33,794 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-06-30 12:20:36,238 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 1
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm2.org_1   |            Start Date: Fri Jun 30 12:19:58 UTC 2023
scm2.org_1   |            Final Date: Mon Aug 07 12:19:58 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm2.org_1   |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
scm2.org_1   |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
scm2.org_1   |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
scm2.org_1   |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
scm2.org_1   |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
scm2.org_1   |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
scm2.org_1   |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
scm2.org_1   |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
scm2.org_1   |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
scm2.org_1   |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
scm2.org_1   |                        5a94528eee6bdc4274bc019b1d82a247fece378c
scm2.org_1   |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
scm2.org_1   |                        c78036d6743d01601451bd47bf51bec81c366e20
scm2.org_1   |                        347a5c23f63ab77d1a66e410a5866e0f
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0x6
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2023-06-30 12:20:36,263 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 324703466914
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm2.org_1   |            Start Date: Fri Jun 30 12:20:23 UTC 2023
scm2.org_1   |            Final Date: Mon Aug 07 12:20:23 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=58e977b4-5318-453e-8f7b-12be220c6897,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   | 2023-06-30 12:20:04,876 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO impl.FollowerState: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5141358317ns, electionTimeout:5132ms
scm1.org_1   | 2023-06-30 12:20:04,877 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: shutdown ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState
scm1.org_1   | 2023-06-30 12:20:04,878 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2023-06-30 12:20:04,881 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-30 12:20:04,881 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1
scm1.org_1   | 2023-06-30 12:20:04,883 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:04,884 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm1.org_1   | 2023-06-30 12:20:04,887 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:04,888 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-30 12:20:04,888 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: shutdown ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1
scm1.org_1   | 2023-06-30 12:20:04,888 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1.org_1   | 2023-06-30 12:20:04,888 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: change Leader from null to ce3545fb-3d5f-455d-aab8-f276e4d7a1ce at term 1 for becomeLeader, leader elected after 5437ms
scm1.org_1   | 2023-06-30 12:20:04,895 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-30 12:20:04,900 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 12:20:04,900 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 12:20:04,906 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-30 12:20:04,906 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-30 12:20:04,907 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-30 12:20:04,913 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 12:20:04,914 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-30 12:20:04,916 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl
scm1.org_1   | 2023-06-30 12:20:04,935 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2023-06-30 12:20:04,961 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:05,039 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_0
scm1.org_1   | 2023-06-30 12:20:05,839 [main] INFO server.RaftServer: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: close
scm1.org_1   | 2023-06-30 12:20:05,841 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: shutdown
scm1.org_1   | 2023-06-30 12:20:05,841 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-654B70CF4739,id=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce
scm1.org_1   | 2023-06-30 12:20:05,841 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: shutdown ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl
scm1.org_1   | 2023-06-30 12:20:05,841 [main] INFO server.GrpcService: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: shutdown server GrpcServerProtocolService now
scm1.org_1   | 2023-06-30 12:20:05,846 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO impl.PendingRequests: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2023-06-30 12:20:05,862 [main] INFO server.GrpcService: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: shutdown server GrpcServerProtocolService successfully
scm1.org_1   | 2023-06-30 12:20:05,863 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO impl.StateMachineUpdater: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2023-06-30 12:20:05,864 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO impl.StateMachineUpdater: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2023-06-30 12:20:05,864 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO impl.StateMachineUpdater: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2023-06-30 12:20:05,867 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: closes. applyIndex: 0
scm1.org_1   | 2023-06-30 12:20:06,046 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker close()
scm1.org_1   | 2023-06-30 12:20:06,047 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: Stopped
scm1.org_1   | 2023-06-30 12:20:06,048 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 12:20:06,051 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-9c10c649-f59d-4933-ad5d-654b70cf4739; layoutVersion=7; scmId=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce
scm1.org_1   | 2023-06-30 12:20:06,082 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-06-30 12:20:08,137 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.19
scm1.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-06-30 12:20:08,146 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-06-30 12:20:08,229 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 12:20:08,424 [main] INFO reflections.Reflections: Reflections took 132 ms to scan 3 urls, producing 132 keys and 287 values 
scm1.org_1   | 2023-06-30 12:20:08,556 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-06-30 12:20:08,575 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-06-30 12:20:08,597 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-06-30 12:20:08,597 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-06-30 12:20:08,787 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2023-06-30 12:20:08,787 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2023-06-30 12:20:09,594 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 1
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   |            Start Date: Fri Jun 30 12:19:58 UTC 2023
scm1.org_1   |            Final Date: Mon Aug 07 12:19:58 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
scm1.org_1   |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
scm1.org_1   |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
scm1.org_1   |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
scm1.org_1   |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
scm1.org_1   |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
scm1.org_1   |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
scm1.org_1   |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
scm1.org_1   |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
scm1.org_1   |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
scm1.org_1   |                        5a94528eee6bdc4274bc019b1d82a247fece378c
scm1.org_1   |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
scm1.org_1   |                        c78036d6743d01601451bd47bf51bec81c366e20
scm1.org_1   |                        347a5c23f63ab77d1a66e410a5866e0f
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0x6
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2023-06-30 12:20:09,599 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 299862451261
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   |            Start Date: Fri Jun 30 12:19:58 UTC 2023
scm1.org_1   |            Final Date: Mon Aug 07 12:19:58 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
scm1.org_1   |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: bceb238776cef1f96370453157455bfacebc30a6
scm1.org_1   |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
scm1.org_1   |                        92b8465e76c5427a65937f03f4c7003698371efa
scm1.org_1   |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
scm1.org_1   |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
scm1.org_1   |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
scm1.org_1   |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
scm1.org_1   |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
scm1.org_1   |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
scm1.org_1   |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
scm1.org_1   |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
scm1.org_1   |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
scm1.org_1   |                        82b7771fe8683859200f859a8629c61a
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
recon_1      | 2023-06-30 12:23:08,567 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 187 failover attempts. Trying to failover immediately. Current retry count: 187.
recon_1      | 2023-06-30 12:23:08,568 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 188 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 188.
recon_1      | 2023-06-30 12:23:10,570 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 189 failover attempts. Trying to failover immediately. Current retry count: 189.
recon_1      | 2023-06-30 12:23:10,571 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 190 failover attempts. Trying to failover immediately. Current retry count: 190.
recon_1      | 2023-06-30 12:23:10,571 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 191 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 191.
recon_1      | 2023-06-30 12:23:12,573 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 192 failover attempts. Trying to failover immediately. Current retry count: 192.
recon_1      | 2023-06-30 12:23:12,574 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 193 failover attempts. Trying to failover immediately. Current retry count: 193.
recon_1      | 2023-06-30 12:23:12,575 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 194 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 194.
recon_1      | 2023-06-30 12:23:14,577 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 195 failover attempts. Trying to failover immediately. Current retry count: 195.
recon_1      | 2023-06-30 12:23:14,578 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 196 failover attempts. Trying to failover immediately. Current retry count: 196.
recon_1      | 2023-06-30 12:23:14,579 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 197 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 197.
recon_1      | 2023-06-30 12:23:16,582 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 198 failover attempts. Trying to failover immediately. Current retry count: 198.
recon_1      | 2023-06-30 12:23:16,584 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 199 failover attempts. Trying to failover immediately. Current retry count: 199.
recon_1      | 2023-06-30 12:23:16,585 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 200 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 200.
recon_1      | 2023-06-30 12:23:22,865 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:60674 / 172.25.0.102:60674
recon_1      | 2023-06-30 12:23:22,957 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 12:23:24,234 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:53628 / 172.25.0.104:53628
recon_1      | 2023-06-30 12:23:24,318 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 12:23:25,030 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:50236 / 172.25.0.103:50236
scm2.org_1   |            Public Key: RSA Public Key [31:fa:c9:1a:6b:44:eb:75:03:45:a1:28:f9:47:0a:5a:33:0f:58:0f],[56:66:d1:a4]
scm2.org_1   |         modulus: c3ce9328a3cd41c964955a6d670dcbb283dcbbde1136293bd95c99292336b86ce9fa19fc90c163c25dccdd1cb4edfdeb2ac2cf8e391e5fde6e95735d311f8656391193c298f3d3768e70277f7bf7a1264019da6ce77ae116e452fa598240234c8b2d1e2c6faa576b0259da33067a5c7ba81fc7b61e50a6fba11fa7640a37780d78f841adcc203ecca01162ad19304ae98878f0665b4be526426ce0c01650af066470e4c61a99095ed490878a5e718e2159325e17febc8382e1ace1ff09be94b70047c30d40e305e3aac088f3eaea21c8db8bb0126bc95bae55dece1060bcdaf572b757e44061192a4f5ef58b9547d7533816170e11531f6463c8ad475780d4db
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 0dea5d8efdb736b91e8907bf60c6c084a6b5a311
scm2.org_1   |                        17f0a8d7f755f9ddfb8c117f89934fb46aa04415
scm2.org_1   |                        f7305fddb99ae97380db73e39767d6949b7a05c7
scm2.org_1   |                        0bd5eba069cb555d068c1e269f5e2819e1734fe9
scm2.org_1   |                        b14ab18c300cdb93938fb76686f90cef79cb17b6
scm2.org_1   |                        a763c6edf1a8406137ae4619872607b3fe1b6f5d
scm2.org_1   |                        d6f499b65c2ab3fb0901094f4c0c4e173f9634b3
scm2.org_1   |                        b0f150f0d5107610c4b8ed3d014066ecb13b08ab
scm2.org_1   |                        ecc5277268d3df0511fc02ae8453bf968b6931e8
scm2.org_1   |                        0adcbbd61dc44030ec8f95786948b879406d5b98
scm2.org_1   |                        52a3d100261d82549b2f450e926a532e190f8c9f
scm2.org_1   |                        e6dfb1002b9e0fd9f1303a43b6fb0ebe86257f3f
scm2.org_1   |                        e61de64dc3e2b51c24c54754c8a348ea
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2023-06-30 12:20:36,290 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm2.org_1   |          SerialNumber: 324703466914
scm2.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm2.org_1   |            Start Date: Fri Jun 30 12:20:23 UTC 2023
scm2.org_1   |            Final Date: Mon Aug 07 12:20:23 UTC 2028
scm2.org_1   |             SubjectDN: CN=scm-sub@scm2.org,OU=58e977b4-5318-453e-8f7b-12be220c6897,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm2.org_1   |            Public Key: RSA Public Key [31:fa:c9:1a:6b:44:eb:75:03:45:a1:28:f9:47:0a:5a:33:0f:58:0f],[56:66:d1:a4]
scm2.org_1   |         modulus: c3ce9328a3cd41c964955a6d670dcbb283dcbbde1136293bd95c99292336b86ce9fa19fc90c163c25dccdd1cb4edfdeb2ac2cf8e391e5fde6e95735d311f8656391193c298f3d3768e70277f7bf7a1264019da6ce77ae116e452fa598240234c8b2d1e2c6faa576b0259da33067a5c7ba81fc7b61e50a6fba11fa7640a37780d78f841adcc203ecca01162ad19304ae98878f0665b4be526426ce0c01650af066470e4c61a99095ed490878a5e718e2159325e17febc8382e1ace1ff09be94b70047c30d40e305e3aac088f3eaea21c8db8bb0126bc95bae55dece1060bcdaf572b757e44061192a4f5ef58b9547d7533816170e11531f6463c8ad475780d4db
scm2.org_1   | public exponent: 10001
scm2.org_1   | 
scm2.org_1   |   Signature Algorithm: SHA256WITHRSA
scm2.org_1   |             Signature: 0dea5d8efdb736b91e8907bf60c6c084a6b5a311
scm2.org_1   |                        17f0a8d7f755f9ddfb8c117f89934fb46aa04415
scm2.org_1   |                        f7305fddb99ae97380db73e39767d6949b7a05c7
scm2.org_1   |                        0bd5eba069cb555d068c1e269f5e2819e1734fe9
scm2.org_1   |                        b14ab18c300cdb93938fb76686f90cef79cb17b6
scm2.org_1   |                        a763c6edf1a8406137ae4619872607b3fe1b6f5d
scm2.org_1   |                        d6f499b65c2ab3fb0901094f4c0c4e173f9634b3
scm2.org_1   |                        b0f150f0d5107610c4b8ed3d014066ecb13b08ab
scm2.org_1   |                        ecc5277268d3df0511fc02ae8453bf968b6931e8
scm2.org_1   |                        0adcbbd61dc44030ec8f95786948b879406d5b98
scm2.org_1   |                        52a3d100261d82549b2f450e926a532e190f8c9f
scm2.org_1   |                        e6dfb1002b9e0fd9f1303a43b6fb0ebe86257f3f
scm2.org_1   |                        e61de64dc3e2b51c24c54754c8a348ea
scm2.org_1   |        Extensions: 
scm2.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm2.org_1   |     Tagged [7] IMPLICIT 
scm2.org_1   |         DER Octet String[4] 
scm2.org_1   |     Tagged [2] IMPLICIT 
scm2.org_1   |         DER Octet String[8] 
scm2.org_1   | 
scm2.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm2.org_1   |                        critical(true) KeyUsage: 0xbe
scm2.org_1   |  from file: /data/metadata/scm/sub-ca/certs/324703466914.crt.
scm2.org_1   | 2023-06-30 12:20:36,341 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716786703 ms and interval 86400000 ms.
scm2.org_1   | 2023-06-30 12:20:36,626 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 12:20:37,241 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-06-30 12:20:38,026 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2023-06-30 12:20:38,039 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2023-06-30 12:20:38,245 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2023-06-30 12:20:38,747 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:58e977b4-5318-453e-8f7b-12be220c6897
scm2.org_1   | 2023-06-30 12:20:38,852 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-30 12:20:38,863 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-06-30 12:20:39,000 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2023-06-30 12:20:39,030 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-30 12:20:39,036 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-30 12:20:39,036 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-06-30 12:20:39,038 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-06-30 12:20:39,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2.org_1   | 2023-06-30 12:20:39,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2023-06-30 12:20:39,042 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2023-06-30 12:20:39,047 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 12:20:39,049 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2023-06-30 12:20:39,051 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-30 12:20:39,100 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2.org_1   | 2023-06-30 12:20:39,133 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2.org_1   | 2023-06-30 12:20:39,137 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-06-30 12:20:40,419 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2.org_1   | 2023-06-30 12:20:40,426 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2.org_1   | 2023-06-30 12:20:40,427 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2.org_1   | 2023-06-30 12:20:40,429 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-30 12:20:40,430 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-30 12:20:40,452 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-30 12:20:40,477 [main] INFO server.RaftServer: 58e977b4-5318-453e-8f7b-12be220c6897: addNew group-654B70CF4739:[] returns group-654B70CF4739:java.util.concurrent.CompletableFuture@129c322f[Not completed]
scm2.org_1   | 2023-06-30 12:20:40,555 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897: new RaftServerImpl for group-654B70CF4739:[] with SCMStateMachine:uninitialized
scm2.org_1   | 2023-06-30 12:20:40,557 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2.org_1   | 2023-06-30 12:20:40,557 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2.org_1   | 2023-06-30 12:20:40,560 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2.org_1   | 2023-06-30 12:20:40,564 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-06-30 12:20:40,564 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-06-30 12:20:40,565 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2023-06-30 12:20:40,587 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2023-06-30 12:20:40,588 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-06-30 12:20:40,611 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2023-06-30 12:20:40,616 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2023-06-30 12:20:57,440 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm3.org_1   | 2023-06-30 12:21:00,018 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2023-06-30 12:21:00,097 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2023-06-30 12:21:00,098 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2023-06-30 12:21:00,107 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:fc0df56f-3802-4ab7-bbd7-f7acacab0048,clusterId:CID-9c10c649-f59d-4933-ad5d-654b70cf4739,subject:scm-sub@scm3.org
scm3.org_1   | 2023-06-30 12:21:00,956 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2023-06-30 12:21:00,981 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-9c10c649-f59d-4933-ad5d-654b70cf4739, SCMID fc0df56f-3802-4ab7-bbd7-f7acacab0048
scm3.org_1   | 2023-06-30 12:21:00,981 [main] INFO server.StorageContainerManager: Primary SCM Node ID ce3545fb-3d5f-455d-aab8-f276e4d7a1ce
scm3.org_1   | 2023-06-30 12:21:01,030 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-06-30 12:21:05,000 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/59a7b6592c874caf40eb70664d3666722cd88ed7 ; compiled by 'runner' on 2023-06-30T11:55Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.19
scm3.org_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-06-30 12:21:05,012 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-06-30 12:23:25,383 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-06-30 12:23:25,622 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=db58cf25-deb4-4e5f-a339-31d80cad3087. Trying to get from SCM.
recon_1      | 2023-06-30 12:23:26,483 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: db58cf25-deb4-4e5f-a339-31d80cad3087, Nodes: 30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:30c441e1-2cd9-4c36-9389-fe8b01b88a57, CreationTimestamp2023-06-30T12:22:51.426Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 12:23:26,947 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0. Trying to get from SCM.
recon_1      | 2023-06-30 12:23:26,964 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b411bba0-46fb-490f-9c02-42d4f8d6f6d0, Nodes: 30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:53.506Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 12:23:27,013 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 reported by 30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-30 12:23:27,592 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=394fb16c-ab50-4148-8258-9491141422f5. Trying to get from SCM.
recon_1      | 2023-06-30 12:23:27,615 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 394fb16c-ab50-4148-8258-9491141422f5, Nodes: c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:52.178Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 12:23:27,623 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=394fb16c-ab50-4148-8258-9491141422f5 reported by c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-30 12:23:28,398 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d5b5f96c-9e99-4dc5-8f79-0b2da0e47784. Trying to get from SCM.
recon_1      | 2023-06-30 12:23:28,429 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d5b5f96c-9e99-4dc5-8f79-0b2da0e47784, Nodes: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:53.198Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-06-30 12:23:28,431 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=d5b5f96c-9e99-4dc5-8f79-0b2da0e47784 reported by a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-30 12:23:28,974 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 reported by c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-30 12:23:29,459 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 reported by a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-06-30 12:23:32,244 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 reported by 30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-06-30 12:23:33,709 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 reported by c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-06-30 12:23:34,465 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 reported by a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
scm3.org_1   | 2023-06-30 12:21:05,128 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 12:21:05,338 [main] INFO reflections.Reflections: Reflections took 161 ms to scan 3 urls, producing 132 keys and 287 values 
scm3.org_1   | 2023-06-30 12:21:05,480 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-06-30 12:21:05,494 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-06-30 12:21:05,527 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-06-30 12:21:05,528 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-06-30 12:21:05,836 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-06-30 12:21:05,836 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-06-30 12:21:07,393 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 1
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm3.org_1   |            Start Date: Fri Jun 30 12:19:58 UTC 2023
scm3.org_1   |            Final Date: Mon Aug 07 12:19:58 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm3.org_1   |            Public Key: RSA Public Key [44:99:d3:95:72:f8:9a:81:34:c9:13:28:4c:68:7e:08:ee:0a:63:12],[56:66:d1:a4]
scm3.org_1   |         modulus: bf97a4ba2a5cc50879999c0563670409e1c8fc407a87b0f369b39c732a8ec9e5fe51dbd925dc03a0051b676cd47a2e4dd2f571269c1da520d032b29b26aa02c02bf7de3934462f3dc35dec1c05388912cf402d3f589e639e3bafd11101c1a08d2744e1a4c673d0e923b107b4de13753b3080723335cedc5d6207015b4d7042689f8808c362ec46bc8d3359c6bc7955f29c5bcbc7e99bdaaed8eb89ee65f8876e065d087ea601ba43990f8eae1ef0a764f8131916acc72f9190115e4ecbb20708d34b96821bda76425b13226cea2ac2fb9ca0609d767e7f9dcd334564c653be55f6aefb1675dad22893cd7a8bc2a3e566d35b21348ba7d145191390cc2a42684b
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: 84ce1867d54cca86d14858116f576b9dc65f770b
scm3.org_1   |                        1c5a2f13fbe6f99d4c8dfe1309bcea37349cfcbc
scm3.org_1   |                        23bf4ae00abb05aeec67f6f54334cd907d2a8ba2
scm3.org_1   |                        28363c1e87d0dcfbad5b507825d7737a1f34d66c
scm3.org_1   |                        31cc1bb846a720166fa21c68a8c328a18b4fd13b
scm3.org_1   |                        70d5f42abb09fd70a3fa473b28698b802c5091ad
scm3.org_1   |                        3b9437b3eca6aae9882d3b34ab9a9f7edb72e4bb
scm3.org_1   |                        2ab5c3ec2746d1345c930054a01a5c441b247a96
scm3.org_1   |                        128f2eb7aca5ddc6511897ee1fbdde443913d43d
scm3.org_1   |                        5a94528eee6bdc4274bc019b1d82a247fece378c
scm3.org_1   |                        c03e130fb8c58951977dfbb45fdafff5b9827c59
scm3.org_1   |                        c78036d6743d01601451bd47bf51bec81c366e20
scm3.org_1   |                        347a5c23f63ab77d1a66e410a5866e0f
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0x6
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2023-06-30 12:21:07,415 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 361669968044
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm3.org_1   |            Start Date: Fri Jun 30 12:21:00 UTC 2023
scm3.org_1   |            Final Date: Mon Aug 07 12:21:00 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=fc0df56f-3802-4ab7-bbd7-f7acacab0048,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm3.org_1   |            Public Key: RSA Public Key [86:fd:c3:c5:8a:ec:34:36:d1:91:77:d3:fc:97:3f:72:a6:b8:00:da],[56:66:d1:a4]
scm3.org_1   |         modulus: a665952dfbd422c93b0b767911b6d64a1edb90073a263aa60b3b6f4b64735e89c74e36f0267529e1a8fa1d19d5f7298cded6378069a0fe3a2cba0796fb81d947038b14bda9d521e90639cbed84ae1dbba99377b3cacd67ce38eec746ec1b06c92fdf7d12cb1c45947642ec01b0ab313d377734dc7694cbb8b34d07d74046182c8782dfee13b582e4137250ade02d9172d6a017ac840f13be05afd92755991e5494584e7dca723a022dd996abcd762b0960fb34c161a30ef9159d35a42d26914e4d9530813c39e865e7f5dba48211e4783d8a3dfbd14cb0793ae1ef82001ff36bb7d1162c5a9bc5cf743c32fc5abfddba39ffa4cf8a7c236f0c9c0ce25ce0d655
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: a7d3cd942c8e2a4b1f54ffec1b6b76adfb191e9c
scm3.org_1   |                        cd1cb43fe39720e3ee92bdb7f995bd578dcd7282
scm3.org_1   |                        b0e363a4bc362a1a8041c563011d9dc40415fe6d
scm3.org_1   |                        eec69a462de0520b1cf0b479f83fe81e8800c9d1
scm3.org_1   |                        5a3ca5f30997e4d5e03b134be1d5092cfcbaf5b1
scm3.org_1   |                        20934a60286ddef3b4b576d772b35c00e93ecf69
scm3.org_1   |                        a5b5c57d46c42ea06c5aa7b773a1206e2a73f7d5
scm3.org_1   |                        e79d48f25a002ba1e403a30eb6d9118d65b7285c
scm3.org_1   |                        75ca12fd8d99a5683e0efb95ab2d5e0de7e398f3
scm3.org_1   |                        a0a4e188fb253a73be8aa51d009eb341389a0c0e
scm3.org_1   |                        6c8dd1504d4ca6f4d54b42cbd1d533844380b912
scm3.org_1   |                        47dbbb77f4a41ccd626625d61db53f06456b3bf5
scm3.org_1   |                        af84b9d7c5fc0cae861c625110834646
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/361669968044.crt.
scm3.org_1   | 2023-06-30 12:21:07,434 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm3.org_1   |          SerialNumber: 361669968044
scm3.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm3.org_1   |            Start Date: Fri Jun 30 12:21:00 UTC 2023
scm3.org_1   |            Final Date: Mon Aug 07 12:21:00 UTC 2028
scm3.org_1   |             SubjectDN: CN=scm-sub@scm3.org,OU=fc0df56f-3802-4ab7-bbd7-f7acacab0048,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm3.org_1   |            Public Key: RSA Public Key [86:fd:c3:c5:8a:ec:34:36:d1:91:77:d3:fc:97:3f:72:a6:b8:00:da],[56:66:d1:a4]
scm3.org_1   |         modulus: a665952dfbd422c93b0b767911b6d64a1edb90073a263aa60b3b6f4b64735e89c74e36f0267529e1a8fa1d19d5f7298cded6378069a0fe3a2cba0796fb81d947038b14bda9d521e90639cbed84ae1dbba99377b3cacd67ce38eec746ec1b06c92fdf7d12cb1c45947642ec01b0ab313d377734dc7694cbb8b34d07d74046182c8782dfee13b582e4137250ade02d9172d6a017ac840f13be05afd92755991e5494584e7dca723a022dd996abcd762b0960fb34c161a30ef9159d35a42d26914e4d9530813c39e865e7f5dba48211e4783d8a3dfbd14cb0793ae1ef82001ff36bb7d1162c5a9bc5cf743c32fc5abfddba39ffa4cf8a7c236f0c9c0ce25ce0d655
scm3.org_1   | public exponent: 10001
scm3.org_1   | 
scm3.org_1   |   Signature Algorithm: SHA256WITHRSA
scm3.org_1   |             Signature: a7d3cd942c8e2a4b1f54ffec1b6b76adfb191e9c
scm3.org_1   |                        cd1cb43fe39720e3ee92bdb7f995bd578dcd7282
scm3.org_1   |                        b0e363a4bc362a1a8041c563011d9dc40415fe6d
scm3.org_1   |                        eec69a462de0520b1cf0b479f83fe81e8800c9d1
scm3.org_1   |                        5a3ca5f30997e4d5e03b134be1d5092cfcbaf5b1
scm3.org_1   |                        20934a60286ddef3b4b576d772b35c00e93ecf69
scm3.org_1   |                        a5b5c57d46c42ea06c5aa7b773a1206e2a73f7d5
scm3.org_1   |                        e79d48f25a002ba1e403a30eb6d9118d65b7285c
scm3.org_1   |                        75ca12fd8d99a5683e0efb95ab2d5e0de7e398f3
scm3.org_1   |                        a0a4e188fb253a73be8aa51d009eb341389a0c0e
scm3.org_1   |                        6c8dd1504d4ca6f4d54b42cbd1d533844380b912
scm3.org_1   |                        47dbbb77f4a41ccd626625d61db53f06456b3bf5
scm3.org_1   |                        af84b9d7c5fc0cae861c625110834646
scm3.org_1   |        Extensions: 
scm3.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm3.org_1   |     Tagged [7] IMPLICIT 
scm3.org_1   |         DER Octet String[4] 
scm3.org_1   |     Tagged [2] IMPLICIT 
scm3.org_1   |         DER Octet String[8] 
scm3.org_1   | 
scm3.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm3.org_1   |                        critical(true) KeyUsage: 0xbe
scm3.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm3.org_1   | 2023-06-30 12:21:07,473 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716792562 ms and interval 86400000 ms.
scm3.org_1   | 2023-06-30 12:21:07,772 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 12:21:08,270 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-06-30 12:21:08,923 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2023-06-30 12:21:08,925 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2023-06-30 12:21:09,047 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2023-06-30 12:21:09,451 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:fc0df56f-3802-4ab7-bbd7-f7acacab0048
scm3.org_1   | 2023-06-30 12:21:09,582 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-30 12:21:09,597 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-06-30 12:21:09,692 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2023-06-30 12:21:09,711 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-30 12:21:09,714 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-30 12:21:09,715 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-06-30 12:21:09,716 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-06-30 12:21:09,717 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3.org_1   | 2023-06-30 12:21:09,717 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2023-06-30 12:21:09,719 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2023-06-30 12:21:09,721 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 12:21:09,723 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2023-06-30 12:21:09,724 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-30 12:21:09,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3.org_1   | 2023-06-30 12:21:09,777 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3.org_1   | 2023-06-30 12:21:09,779 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-06-30 12:21:10,629 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3.org_1   | 2023-06-30 12:21:10,637 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3.org_1   | 2023-06-30 12:21:10,638 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3.org_1   | 2023-06-30 12:21:10,638 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-30 12:21:10,639 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-30 12:21:10,645 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-30 12:21:10,667 [main] INFO server.RaftServer: fc0df56f-3802-4ab7-bbd7-f7acacab0048: addNew group-654B70CF4739:[] returns group-654B70CF4739:java.util.concurrent.CompletableFuture@129c322f[Not completed]
scm3.org_1   | 2023-06-30 12:21:10,770 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048: new RaftServerImpl for group-654B70CF4739:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2023-06-30 12:21:10,782 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2023-06-30 12:21:10,783 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2023-06-30 12:21:10,784 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2023-06-30 12:21:10,784 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-06-30 12:21:10,785 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-06-30 12:21:10,786 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3.org_1   | 2023-06-30 12:21:10,816 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2023-06-30 12:21:10,818 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-06-30 12:21:10,838 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2023-06-30 12:21:10,841 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2023-06-30 12:20:09,604 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
scm1.org_1   |          SerialNumber: 299862451261
scm1.org_1   |              IssuerDN: CN=scm@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   |            Start Date: Fri Jun 30 12:19:58 UTC 2023
scm1.org_1   |            Final Date: Mon Aug 07 12:19:58 UTC 2028
scm1.org_1   |             SubjectDN: CN=scm-sub@scm1.org,OU=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce,O=CID-9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   |            Public Key: RSA Public Key [62:ba:46:c2:17:0a:78:a1:95:77:1b:8b:87:c5:8b:a6:f4:00:9b:e7],[56:66:d1:a4]
scm1.org_1   |         modulus: 9b0d084b11dd5931da4d1412dff1558a74b17ba34e0a4b9a606d1c969cd21aaba2bdd138f4ad94da53cf19ecf413cbc9574f720dd86fb560987666c51c7c7a191babe6291ca4499e447c33f40a96074ee104eb86bd77e8b9991a45193b0724c7b3347daa6915239bfe6ffea966a517a51acf3959c6c437301b23af70c499d9286c877e1d0128448185d09692992578b16f313ea00aa16634a9aee51f9138ad06012a5bcb01afe5df2c2353aa87a0563f7c79adcff4efc14b040982cfd87b3bf24d0e02b15bcbe9803e22bd6ce47d41f63dd8cd628fa800eb4d1f4091e6bf1b594b83e4f75fb87589d7d8ccd6f8872e85d853b3fd629885ddb6019c6bc9754699
scm1.org_1   | public exponent: 10001
scm1.org_1   | 
scm1.org_1   |   Signature Algorithm: SHA256WITHRSA
scm1.org_1   |             Signature: bceb238776cef1f96370453157455bfacebc30a6
scm1.org_1   |                        9e13db7e24ff9083bf30c83b34bd85d71a67552e
scm1.org_1   |                        92b8465e76c5427a65937f03f4c7003698371efa
scm1.org_1   |                        b3df71aa7c1328bf7a5afbd039895061388b14e9
scm1.org_1   |                        4904b2c7c92a0e8ffedfd556a8b730b9d4107946
scm1.org_1   |                        74bd484a2f7b01e19cbcbea2edee308d47079d8b
scm1.org_1   |                        7be8afe7bc3800a16afe4de0cd05e7ca41865b2d
scm1.org_1   |                        d296302b6303a2d9d6c0becb40b68f1aa8d4ab9d
scm1.org_1   |                        c561be1c3328929b7d6ea8e2e5d66163fc0453a4
scm1.org_1   |                        1a1017052c34d22e2824fc29ddc77abe37dfc718
scm1.org_1   |                        7af529ae429fe74ec01f00365702ee2769c7eb5d
scm1.org_1   |                        ca78c625e663b6ad6c1e3a7b27e8560fe19126a3
scm1.org_1   |                        82b7771fe8683859200f859a8629c61a
scm1.org_1   |        Extensions: 
scm1.org_1   |                        critical(false) 2.5.29.17 value = Sequence
scm1.org_1   |     Tagged [7] IMPLICIT 
scm1.org_1   |         DER Octet String[4] 
scm1.org_1   |     Tagged [2] IMPLICIT 
scm1.org_1   |         DER Octet String[8] 
scm1.org_1   | 
scm1.org_1   |                        critical(true) BasicConstraints: isCa(true)
scm1.org_1   |                        critical(true) KeyUsage: 0xbe
scm1.org_1   |  from file: /data/metadata/scm/sub-ca/certs/299862451261.crt.
scm1.org_1   | 2023-06-30 12:20:09,609 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor for scm/sub-ca is started with first delay 158716788394 ms and interval 86400000 ms.
scm1.org_1   | 2023-06-30 12:20:09,720 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 12:20:09,904 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-06-30 12:20:10,207 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2023-06-30 12:20:10,208 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2023-06-30 12:20:10,286 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-06-30 12:20:10,535 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce
scm1.org_1   | 2023-06-30 12:20:10,630 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 12:20:10,633 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 12:20:10,700 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-06-30 12:20:10,707 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 12:20:10,709 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 12:20:10,709 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-06-30 12:20:10,709 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-06-30 12:20:10,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-06-30 12:20:10,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-06-30 12:20:10,715 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-06-30 12:20:10,716 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:20:10,717 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-06-30 12:20:10,718 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 12:20:10,729 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 12:20:10,741 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-06-30 12:20:10,741 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-06-30 12:20:11,137 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2023-06-30 12:20:11,140 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-06-30 12:20:11,141 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-06-30 12:20:11,141 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 12:20:11,142 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 12:20:11,145 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 12:20:11,149 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: found a subdirectory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   | 2023-06-30 12:20:11,156 [main] INFO server.RaftServer: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: addNew group-654B70CF4739:[] returns group-654B70CF4739:java.util.concurrent.CompletableFuture@129c322f[Not completed]
scm1.org_1   | 2023-06-30 12:20:11,216 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: new RaftServerImpl for group-654B70CF4739:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-06-30 12:20:11,217 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-06-30 12:20:11,218 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-06-30 12:20:11,218 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-06-30 12:20:11,219 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-06-30 12:20:11,219 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-06-30 12:20:11,220 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-06-30 12:20:11,227 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-06-30 12:20:11,229 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-06-30 12:20:11,232 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-06-30 12:20:11,233 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-06-30 12:20:11,248 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-06-30 12:20:11,252 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1.org_1   | 2023-06-30 12:20:11,257 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-06-30 12:20:11,257 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-06-30 12:20:11,279 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1.org_1   | 2023-06-30 12:20:11,450 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 12:20:11,459 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-30 12:20:11,460 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-06-30 12:20:11,462 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-06-30 12:20:11,462 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-06-30 12:20:11,463 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-06-30 12:20:11,465 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2023-06-30 12:20:11,465 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2023-06-30 12:20:11,466 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2023-06-30 12:20:11,503 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm1.org_1   | 2023-06-30 12:20:11,561 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm1.org_1   | 2023-06-30 12:20:11,562 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2023-06-30 12:20:11,572 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2023-06-30 12:20:11,575 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2023-06-30 12:20:11,680 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2023-06-30 12:20:11,698 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm1.org_1   | 2023-06-30 12:20:11,701 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-30 12:20:11,711 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1.org_1   | 2023-06-30 12:20:11,733 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2023-06-30 12:20:11,734 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-06-30 12:20:11,743 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2023-06-30 12:20:11,744 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 12:20:11,747 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1.org_1   | 2023-06-30 12:20:11,748 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1.org_1   | 2023-06-30 12:20:11,757 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1.org_1   | 2023-06-30 12:20:11,758 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1.org_1   | 2023-06-30 12:20:11,793 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-30 12:20:11,793 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-06-30 12:20:11,825 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2023-06-30 12:20:11,904 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2023-06-30 12:20:11,936 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1.org_1   | 2023-06-30 12:20:11,942 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2023-06-30 12:20:11,953 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2023-06-30 12:20:11,958 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:20:11,960 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-30 12:20:12,196 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm1.org_1   | 2023-06-30 12:20:12,197 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm1.org_1   | 2023-06-30 12:20:12,220 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-30 12:20:12,223 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-06-30 12:20:12,225 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 299862451261 on primary SCM
scm1.org_1   | 2023-06-30 12:20:12,242 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2023-06-30 12:20:12,270 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 12:20:12,328 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm1.org_1   | 2023-06-30 12:20:12,331 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2023-06-30 12:20:12,370 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm1.org_1   | 2023-06-30 12:20:12,371 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm1.org_1   | 2023-06-30 12:20:13,214 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-30 12:20:40,662 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2.org_1   | 2023-06-30 12:20:40,666 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2.org_1   | 2023-06-30 12:20:40,677 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2023-06-30 12:20:40,684 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2023-06-30 12:20:40,740 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2.org_1   | 2023-06-30 12:20:41,207 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-06-30 12:20:41,221 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2023-06-30 12:20:41,225 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2.org_1   | 2023-06-30 12:20:41,225 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2.org_1   | 2023-06-30 12:20:41,227 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2.org_1   | 2023-06-30 12:20:41,228 [58e977b4-5318-453e-8f7b-12be220c6897-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2.org_1   | 2023-06-30 12:20:41,232 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2023-06-30 12:20:41,232 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2023-06-30 12:20:41,234 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2023-06-30 12:20:41,412 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm2.org_1   | 2023-06-30 12:20:41,637 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2.org_1   | 2023-06-30 12:20:41,643 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2023-06-30 12:20:41,676 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2023-06-30 12:20:41,688 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2023-06-30 12:20:41,909 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2023-06-30 12:20:42,006 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm2.org_1   | 2023-06-30 12:20:42,022 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-30 12:20:42,052 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2023-06-30 12:20:42,155 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2023-06-30 12:20:42,159 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-06-30 12:20:42,196 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2023-06-30 12:20:42,196 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2023-06-30 12:20:42,216 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2.org_1   | 2023-06-30 12:20:42,219 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2.org_1   | 2023-06-30 12:20:42,248 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2.org_1   | 2023-06-30 12:20:42,248 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2.org_1   | 2023-06-30 12:20:42,325 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-30 12:20:42,326 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-06-30 12:20:42,369 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2023-06-30 12:20:42,640 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2023-06-30 12:20:42,714 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2023-06-30 12:20:42,732 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm2.org_1   | 2023-06-30 12:20:42,750 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2023-06-30 12:20:42,771 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:20:42,779 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-30 12:20:43,591 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm2.org_1   | 2023-06-30 12:20:43,598 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm2.org_1   | 2023-06-30 12:20:43,673 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2023-06-30 12:20:43,743 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 12:20:43,851 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm2.org_1   | 2023-06-30 12:20:43,863 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2023-06-30 12:20:43,984 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm2.org_1   | 2023-06-30 12:20:43,985 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm2.org_1   | 2023-06-30 12:20:45,875 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-30 12:20:45,911 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 12:20:45,914 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm2.org_1   | 2023-06-30 12:20:45,919 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2023-06-30 12:20:46,012 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-30 12:20:46,032 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 12:20:46,035 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm2.org_1   | 2023-06-30 12:20:46,036 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2023-06-30 12:20:46,197 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-06-30 12:20:46,228 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-06-30 12:20:46,231 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm2.org_1   | 2023-06-30 12:20:46,261 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2023-06-30 12:20:46,634 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2.org_1   | 2023-06-30 12:20:46,634 [main] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        false
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                                                Value
scm2.org_1   | Threshold                                          10
scm2.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2.org_1   | Max Size to Move per Iteration                     500GB
scm2.org_1   | Max Size Entering Target per Iteration             26GB
scm2.org_1   | Max Size Leaving Source per Iteration              26GB
scm2.org_1   | 
scm2.org_1   | 2023-06-30 12:20:46,635 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2023-06-30 12:20:46,680 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-30 12:20:46,686 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2023-06-30 12:20:46,689 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739 does not exist. Creating ...
scm2.org_1   | 2023-06-30 12:20:46,713 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/in_use.lock acquired by nodename 7@scm2.org
scm2.org_1   | 2023-06-30 12:20:46,735 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739 has been successfully formatted.
scm2.org_1   | 2023-06-30 12:20:46,748 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2023-06-30 12:20:46,763 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2023-06-30 12:20:46,763 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 12:20:46,765 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2.org_1   | 2023-06-30 12:20:46,766 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2.org_1   | 2023-06-30 12:20:46,770 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-30 12:20:46,786 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2023-06-30 12:20:46,788 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2023-06-30 12:20:46,788 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 12:20:46,801 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739
scm2.org_1   | 2023-06-30 12:20:46,801 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2023-06-30 12:20:46,802 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2023-06-30 12:20:46,803 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-06-30 12:20:46,804 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2.org_1   | 2023-06-30 12:20:46,805 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2023-06-30 12:20:46,806 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2023-06-30 12:20:46,806 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2023-06-30 12:20:46,807 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2023-06-30 12:20:46,826 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2023-06-30 12:20:46,826 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-06-30 12:20:46,843 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2.org_1   | 2023-06-30 12:20:46,843 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2.org_1   | 2023-06-30 12:20:46,844 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-06-30 12:20:46,852 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-30 12:20:46,852 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-06-30 12:20:46,855 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2.org_1   | 2023-06-30 12:20:46,859 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3.org_1   | 2023-06-30 12:21:10,879 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2023-06-30 12:21:10,888 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm3.org_1   | 2023-06-30 12:21:10,895 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2023-06-30 12:21:10,897 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2023-06-30 12:21:11,001 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm3.org_1   | 2023-06-30 12:21:11,294 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-06-30 12:21:11,301 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2023-06-30 12:21:11,302 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3.org_1   | 2023-06-30 12:21:11,303 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3.org_1   | 2023-06-30 12:21:11,304 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3.org_1   | 2023-06-30 12:21:11,305 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3.org_1   | 2023-06-30 12:21:11,310 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2023-06-30 12:21:11,310 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2023-06-30 12:21:11,315 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2023-06-30 12:21:11,539 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm3.org_1   | 2023-06-30 12:21:11,645 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3.org_1   | 2023-06-30 12:21:11,647 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2023-06-30 12:21:11,659 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2023-06-30 12:21:11,667 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2023-06-30 12:21:11,867 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2023-06-30 12:21:11,895 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm3.org_1   | 2023-06-30 12:21:11,898 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-30 12:21:11,923 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2023-06-30 12:21:12,007 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2023-06-30 12:21:12,009 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-06-30 12:21:12,030 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3.org_1   | 2023-06-30 12:21:12,030 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2023-06-30 12:21:12,043 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3.org_1   | 2023-06-30 12:21:12,047 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3.org_1   | 2023-06-30 12:21:12,070 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3.org_1   | 2023-06-30 12:21:12,071 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3.org_1   | 2023-06-30 12:21:12,212 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-30 12:21:12,212 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-06-30 12:21:12,295 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2023-06-30 12:21:12,518 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2023-06-30 12:21:12,596 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2023-06-30 12:21:12,613 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm3.org_1   | 2023-06-30 12:21:12,660 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2023-06-30 12:21:12,671 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:21:12,676 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-06-30 12:21:13,317 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
scm3.org_1   | 2023-06-30 12:21:13,318 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
scm3.org_1   | 2023-06-30 12:21:13,376 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2023-06-30 12:21:13,430 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 12:21:13,506 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
scm3.org_1   | 2023-06-30 12:21:13,526 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2023-06-30 12:21:13,705 [main] INFO ha.SCMServiceManager: Registering service RootCARotationManager.
scm3.org_1   | 2023-06-30 12:21:13,708 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
scm3.org_1   | 2023-06-30 12:21:15,739 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-30 12:21:15,764 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 12:21:15,767 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm3.org_1   | 2023-06-30 12:21:15,772 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2023-06-30 12:21:15,862 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-30 12:21:15,907 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 12:21:15,909 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm3.org_1   | 2023-06-30 12:21:15,916 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2023-06-30 12:21:16,003 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-06-30 12:21:16,021 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-06-30 12:21:16,022 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm3.org_1   | 2023-06-30 12:21:16,023 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2023-06-30 12:21:16,434 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3.org_1   | 2023-06-30 12:21:16,435 [main] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        false
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                                                Value
scm3.org_1   | Threshold                                          10
scm3.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3.org_1   | Max Size to Move per Iteration                     500GB
scm3.org_1   | Max Size Entering Target per Iteration             26GB
scm3.org_1   | Max Size Leaving Source per Iteration              26GB
scm3.org_1   | 
scm3.org_1   | 2023-06-30 12:21:16,436 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2023-06-30 12:21:16,449 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-30 12:21:16,457 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2023-06-30 12:21:16,463 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739 does not exist. Creating ...
scm3.org_1   | 2023-06-30 12:21:16,483 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/in_use.lock acquired by nodename 7@scm3.org
scm3.org_1   | 2023-06-30 12:21:16,519 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739 has been successfully formatted.
scm3.org_1   | 2023-06-30 12:21:16,528 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2023-06-30 12:21:16,549 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2023-06-30 12:21:16,551 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 12:21:16,556 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3.org_1   | 2023-06-30 12:21:16,557 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3.org_1   | 2023-06-30 12:21:16,573 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-30 12:21:16,603 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2023-06-30 12:21:16,605 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2023-06-30 12:21:16,606 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 12:21:16,630 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739
scm3.org_1   | 2023-06-30 12:21:16,631 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2023-06-30 12:21:16,633 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2023-06-30 12:21:16,636 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-06-30 12:21:16,636 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3.org_1   | 2023-06-30 12:21:16,637 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2023-06-30 12:21:16,641 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2023-06-30 12:21:16,641 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | 2023-06-30 12:21:16,643 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2023-06-30 12:21:16,689 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2023-06-30 12:21:16,691 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-06-30 12:21:16,754 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3.org_1   | 2023-06-30 12:21:16,756 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3.org_1   | 2023-06-30 12:21:16,757 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3.org_1   | 2023-06-30 12:21:16,783 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-30 12:21:16,784 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-06-30 12:21:16,796 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:16,802 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: changes role from      null to FOLLOWER at term 0 for startInitializing
scm1.org_1   | 2023-06-30 12:20:13,228 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 12:20:13,228 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm1.org_1   | 2023-06-30 12:20:13,248 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2023-06-30 12:20:13,282 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-30 12:20:13,287 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 12:20:13,288 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm1.org_1   | 2023-06-30 12:20:13,288 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2023-06-30 12:20:13,332 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-06-30 12:20:13,352 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-06-30 12:20:13,352 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm1.org_1   | 2023-06-30 12:20:13,363 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2023-06-30 12:20:13,481 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1.org_1   | 2023-06-30 12:20:13,482 [main] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        false
scm1.org_1   | Container Balancer Configuration values:
scm1.org_1   | Key                                                Value
scm1.org_1   | Threshold                                          10
scm1.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1.org_1   | Max Size to Move per Iteration                     500GB
scm1.org_1   | Max Size Entering Target per Iteration             26GB
scm1.org_1   | Max Size Leaving Source per Iteration              26GB
scm1.org_1   | 
scm1.org_1   | 2023-06-30 12:20:13,482 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2023-06-30 12:20:13,489 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-30 12:20:13,492 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2023-06-30 12:20:13,498 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/in_use.lock acquired by nodename 7@scm1.org
scm1.org_1   | 2023-06-30 12:20:13,506 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce} from /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/raft-meta
scm1.org_1   | 2023-06-30 12:20:13,551 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:13,556 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-06-30 12:20:13,575 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-06-30 12:20:13,576 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:20:13,579 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-06-30 12:20:13,580 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-06-30 12:20:13,589 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 12:20:13,598 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-06-30 12:20:13,598 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-06-30 12:20:13,599 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:20:13,611 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739
scm1.org_1   | 2023-06-30 12:20:13,611 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 12:20:13,612 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 12:20:13,614 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-06-30 12:20:13,615 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-06-30 12:20:13,616 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-06-30 12:20:13,618 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-06-30 12:20:13,618 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-06-30 12:20:13,618 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-06-30 12:20:13,629 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-06-30 12:20:13,630 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:20:13,856 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 12:20:13,856 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-06-30 12:20:13,856 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-06-30 12:20:46,861 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-654B70CF4739,id=58e977b4-5318-453e-8f7b-12be220c6897
scm2.org_1   | 2023-06-30 12:20:46,864 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2.org_1   | 2023-06-30 12:20:46,864 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2023-06-30 12:20:46,865 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2023-06-30 12:20:46,866 [58e977b4-5318-453e-8f7b-12be220c6897-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2023-06-30 12:20:46,870 [main] INFO server.RaftServer: 58e977b4-5318-453e-8f7b-12be220c6897: start RPC server
scm2.org_1   | 2023-06-30 12:20:46,926 [main] INFO server.GrpcService: 58e977b4-5318-453e-8f7b-12be220c6897: GrpcService started, listening on 9894
scm2.org_1   | 2023-06-30 12:20:46,936 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-58e977b4-5318-453e-8f7b-12be220c6897: Started
scm2.org_1   | 2023-06-30 12:20:46,951 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-06-30 12:20:48,179 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: receive installSnapshot: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce->58e977b4-5318-453e-8f7b-12be220c6897#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-30 12:20:48,186 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2023-06-30 12:20:48,186 [grpc-default-executor-0] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: change Leader from null to ce3545fb-3d5f-455d-aab8-f276e4d7a1ce at term 2 for installSnapshot, leader elected after 7524ms
scm2.org_1   | 2023-06-30 12:20:48,190 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: Received notification to install snapshot at index 0
scm2.org_1   | 2023-06-30 12:20:48,191 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2.org_1   | 2023-06-30 12:20:48,590 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "ce3545fb-3d5f-455d-aab8-f276e4d7a1ce"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |     startupRole: FOLLOWER
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2023-06-30 12:20:48,596 [grpc-default-executor-0] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set configuration 1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 12:20:48,609 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: reply installSnapshot: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce<-58e977b4-5318-453e-8f7b-12be220c6897#0:OK-t0,ALREADY_INSTALLED
scm2.org_1   | 2023-06-30 12:20:48,646 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 58e977b4-5318-453e-8f7b-12be220c6897: Completed INSTALL_SNAPSHOT, lastRequest: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce->58e977b4-5318-453e-8f7b-12be220c6897#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-06-30 12:20:48,652 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 58e977b4-5318-453e-8f7b-12be220c6897: Completed INSTALL_SNAPSHOT, lastReply: null
scm2.org_1   | 2023-06-30 12:20:48,806 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO impl.RoleInfo: 58e977b4-5318-453e-8f7b-12be220c6897: start 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState
scm2.org_1   | 2023-06-30 12:20:48,809 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:20:48,809 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:20:48,812 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-06-30 12:20:48,832 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: inconsistency entries. Reply:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce<-58e977b4-5318-453e-8f7b-12be220c6897#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-06-30 12:20:48,895 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set configuration 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 12:20:48,900 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set configuration 1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 12:20:48,918 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO segmented.SegmentedRaftLogWorker: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2023-06-30 12:20:49,138 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO segmented.SegmentedRaftLogWorker: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2023-06-30 12:20:49,759 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_0
scm2.org_1   | 2023-06-30 12:20:49,782 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_0 to /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_0-0
scm2.org_1   | 2023-06-30 12:20:49,867 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_1
scm2.org_1   | 2023-06-30 12:20:49,902 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:20:49,907 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2023-06-30 12:20:49,919 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread3] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set configuration 9: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-06-30 12:20:49,921 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-06-30 12:20:49,927 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2023-06-30 12:20:49,967 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 12:20:50,112 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2023-06-30 12:20:50,174 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread1] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set configuration 11: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 12:20:50,403 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-654B70CF4739:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2.org_1   | 2023-06-30 12:20:50,403 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2023-06-30 12:20:50,532 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm2.org_1   | 2023-06-30 12:20:50,539 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm2.org_1   | 2023-06-30 12:20:50,539 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2.org_1   | 2023-06-30 12:20:50,836 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)]
scm2.org_1   | 2023-06-30 12:20:50,843 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)
scm2.org_1   | 2023-06-30 12:20:50,872 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2023-06-30 12:20:50,934 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2023-06-30 12:20:50,934 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2023-06-30 12:20:51,365 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)] to file /data/metadata/scm/keys/secret_keys.json
scm2.org_1   | 2023-06-30 12:20:51,376 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:20:51,378 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-30 12:20:51,379 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2023-06-30 12:20:51,485 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2023-06-30 12:20:51,487 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 12:20:51,488 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2023-06-30 12:20:51,702 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:20:51,804 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:20:51,913 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-30 12:20:51,916 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2023-06-30 12:20:51,920 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 12:20:51,926 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2023-06-30 12:20:52,253 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2023-06-30 12:20:52,254 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-06-30 12:20:52,255 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2023-06-30 12:20:52,275 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm2.org_1   | 2023-06-30 12:20:52,720 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 58e977b4-5318-453e-8f7b-12be220c6897
scm2.org_1   | 2023-06-30 12:20:52,781 [main] INFO server.StorageContainerManager: Persist certificate serialId 299862451261 on Scm Bootstrap Node 58e977b4-5318-453e-8f7b-12be220c6897
scm2.org_1   | 2023-06-30 12:20:52,813 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm2.org_1   | 2023-06-30 12:20:52,921 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2023-06-30 12:20:52,921 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2023-06-30 12:20:52,927 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2023-06-30 12:20:53,115 [main] INFO util.log: Logging initialized @26143ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2023-06-30 12:20:53,637 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2023-06-30 12:20:53,672 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2023-06-30 12:20:53,676 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm2.org_1   | 2023-06-30 12:20:53,677 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2023-06-30 12:20:53,677 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2023-06-30 12:20:53,685 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm2.org_1   | 2023-06-30 12:20:53,755 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm2.org_1   | 2023-06-30 12:20:53,756 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm2.org_1   | 2023-06-30 12:20:53,760 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm2.org_1   | 2023-06-30 12:20:53,799 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm2.org_1   | 2023-06-30 12:20:53,800 [main] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2023-06-30 12:20:53,804 [main] INFO server.session: node0 Scavenging every 600000ms
scm2.org_1   | 2023-06-30 12:20:53,867 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-30 12:20:53,887 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5ada5fd3{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2023-06-30 12:20:53,888 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2bb5637a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2.org_1   | 2023-06-30 12:20:53,909 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:20:53,909 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:20:54,497 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-30 12:20:54,541 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@34195ce5{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-869773747156768724/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2023-06-30 12:20:54,600 [main] INFO server.AbstractConnector: Started ServerConnector@58178c5e{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2023-06-30 12:20:54,600 [main] INFO server.Server: Started @27629ms
scm2.org_1   | 2023-06-30 12:20:54,611 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2023-06-30 12:20:54,611 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2.org_1   | 2023-06-30 12:20:54,615 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2023-06-30 12:20:58,981 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:20:58,982 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:00,789 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:21:04,016 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:04,016 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:09,114 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:09,114 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:14,254 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:14,254 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:19,317 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:19,318 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:24,367 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:24,367 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:29,495 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:29,495 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:30,871 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread3] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set configuration 15: peers:[fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 12:21:16,807 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-654B70CF4739,id=fc0df56f-3802-4ab7-bbd7-f7acacab0048
scm3.org_1   | 2023-06-30 12:21:16,814 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2023-06-30 12:21:16,816 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3.org_1   | 2023-06-30 12:21:16,818 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2023-06-30 12:21:16,821 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2023-06-30 12:21:16,841 [main] INFO server.RaftServer: fc0df56f-3802-4ab7-bbd7-f7acacab0048: start RPC server
scm3.org_1   | 2023-06-30 12:21:17,061 [main] INFO server.GrpcService: fc0df56f-3802-4ab7-bbd7-f7acacab0048: GrpcService started, listening on 9894
scm3.org_1   | 2023-06-30 12:21:17,097 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fc0df56f-3802-4ab7-bbd7-f7acacab0048: Started
scm3.org_1   | 2023-06-30 12:21:17,300 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-06-30 12:21:25,811 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: receive installSnapshot: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce->fc0df56f-3802-4ab7-bbd7-f7acacab0048#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-30 12:21:25,892 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2023-06-30 12:21:25,892 [grpc-default-executor-0] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: change Leader from null to ce3545fb-3d5f-455d-aab8-f276e4d7a1ce at term 2 for installSnapshot, leader elected after 15013ms
scm3.org_1   | 2023-06-30 12:21:25,975 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: Received notification to install snapshot at index 0
scm3.org_1   | 2023-06-30 12:21:26,017 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3.org_1   | 2023-06-30 12:21:27,525 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set new configuration index: 11
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "58e977b4-5318-453e-8f7b-12be220c6897"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "ce3545fb-3d5f-455d-aab8-f276e4d7a1ce"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2023-06-30 12:21:27,549 [grpc-default-executor-0] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 11: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:27,628 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: reply installSnapshot: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce<-fc0df56f-3802-4ab7-bbd7-f7acacab0048#0:OK-t0,ALREADY_INSTALLED
scm3.org_1   | 2023-06-30 12:21:27,737 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: fc0df56f-3802-4ab7-bbd7-f7acacab0048: Completed INSTALL_SNAPSHOT, lastRequest: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce->fc0df56f-3802-4ab7-bbd7-f7acacab0048#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-06-30 12:21:27,740 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: fc0df56f-3802-4ab7-bbd7-f7acacab0048: Completed INSTALL_SNAPSHOT, lastReply: null
scm3.org_1   | 2023-06-30 12:21:28,171 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO impl.RoleInfo: fc0df56f-3802-4ab7-bbd7-f7acacab0048: start fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState
scm3.org_1   | 2023-06-30 12:21:28,195 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:21:28,197 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:21:28,219 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-06-30 12:21:28,265 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: inconsistency entries. Reply:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce<-fc0df56f-3802-4ab7-bbd7-f7acacab0048#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-30 12:21:28,285 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-06-30 12:21:28,288 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: inconsistency entries. Reply:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce<-fc0df56f-3802-4ab7-bbd7-f7acacab0048#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-06-30 12:21:28,333 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:28,344 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:28,346 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 9: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 12:21:28,392 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 11: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:28,496 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO segmented.SegmentedRaftLogWorker: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker: Starting segment from index:0
scm3.org_1   | 2023-06-30 12:21:28,875 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread2] INFO segmented.SegmentedRaftLogWorker: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3.org_1   | 2023-06-30 12:21:29,092 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:29,096 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:29,101 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 9: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 12:21:29,103 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 11: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:30,503 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_0
scm3.org_1   | 2023-06-30 12:21:30,627 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_0 to /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_0-0
scm3.org_1   | 2023-06-30 12:21:30,768 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_1
scm3.org_1   | 2023-06-30 12:21:30,885 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:21:30,907 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2023-06-30 12:21:30,916 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-06-30 12:21:30,920 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2023-06-30 12:21:30,968 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2023-06-30 12:21:30,989 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 15: peers:[fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-06-30 12:21:30,996 [fc0df56f-3802-4ab7-bbd7-f7acacab0048-server-thread1] INFO server.RaftServer$Division: fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739: set configuration 17: peers:[fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-06-30 12:21:31,186 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 12:21:31,346 [main] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-654B70CF4739:[fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3.org_1   | 2023-06-30 12:21:31,815 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2023-06-30 12:21:31,846 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm3.org_1   | 2023-06-30 12:21:31,846 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm3.org_1   | 2023-06-30 12:21:31,846 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm3.org_1   | 2023-06-30 12:21:32,646 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)]
scm3.org_1   | 2023-06-30 12:21:32,685 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)
scm3.org_1   | 2023-06-30 12:21:33,232 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:21:33,248 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-30 12:20:13,881 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:13,882 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_0
scm1.org_1   | 2023-06-30 12:20:13,883 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-06-30 12:20:13,960 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: start as a follower, conf=0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:13,961 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2023-06-30 12:20:13,964 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState
scm1.org_1   | 2023-06-30 12:20:13,966 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-654B70CF4739,id=ce3545fb-3d5f-455d-aab8-f276e4d7a1ce
scm1.org_1   | 2023-06-30 12:20:13,968 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-06-30 12:20:13,969 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-06-30 12:20:13,969 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-06-30 12:20:13,970 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-06-30 12:20:13,973 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-30 12:20:13,973 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-30 12:20:13,974 [main] INFO server.RaftServer: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start RPC server
scm1.org_1   | 2023-06-30 12:20:14,032 [main] INFO server.GrpcService: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: GrpcService started, listening on 9894
scm1.org_1   | 2023-06-30 12:20:14,044 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: Started
scm1.org_1   | 2023-06-30 12:20:14,061 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-30 12:20:14,061 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2023-06-30 12:20:14,063 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm1.org_1   | 2023-06-30 12:20:14,064 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm1.org_1   | 2023-06-30 12:20:14,064 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm1.org_1   | 2023-06-30 12:20:14,152 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2023-06-30 12:20:14,171 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2023-06-30 12:20:14,171 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2023-06-30 12:20:14,446 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2023-06-30 12:20:14,453 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 12:20:14,460 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2023-06-30 12:20:14,478 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-30 12:20:14,489 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2023-06-30 12:20:14,490 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 12:20:14,490 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2023-06-30 12:20:14,525 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2023-06-30 12:20:14,527 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm1.org_1   | 2023-06-30 12:20:14,533 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 12:20:14,541 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm1.org_1   | 2023-06-30 12:20:14,631 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm1.org_1   | 2023-06-30 12:20:14,697 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1.org_1   | 2023-06-30 12:20:14,702 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm1.org_1   | 2023-06-30 12:20:14,704 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm1.org_1   | 2023-06-30 12:20:14,769 [main] INFO util.log: Logging initialized @8108ms to org.eclipse.jetty.util.log.Slf4jLog
scm1.org_1   | 2023-06-30 12:20:14,910 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:38535 / 172.25.0.115:38535
scm1.org_1   | 2023-06-30 12:20:14,946 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:20:15,240 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1.org_1   | 2023-06-30 12:20:15,265 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1.org_1   | 2023-06-30 12:20:15,276 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm1.org_1   | 2023-06-30 12:20:15,276 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm1.org_1   | 2023-06-30 12:20:15,276 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm1.org_1   | 2023-06-30 12:20:15,281 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | 2023-06-30 12:20:15,334 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:38535 / 172.25.0.115:38535
scm1.org_1   | org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:ce3545fb-3d5f-455d-aab8-f276e4d7a1ce is not the leader. Could not determine the leader node.
scm1.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:89)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:17361)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1.org_1   | 2023-06-30 12:20:15,387 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1.org_1   | 2023-06-30 12:20:15,390 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2023-06-30 12:20:15,394 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm1.org_1   | 2023-06-30 12:20:15,439 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2023-06-30 12:20:15,439 [main] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2023-06-30 12:20:15,441 [main] INFO server.session: node0 Scavenging every 660000ms
scm1.org_1   | 2023-06-30 12:20:15,462 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-30 12:20:15,474 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@67620d80{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2023-06-30 12:20:15,475 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@58bdcf40{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2023-06-30 12:20:15,637 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-06-30 12:20:15,649 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7cb81ae{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-17245411991827144574/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2023-06-30 12:20:15,661 [main] INFO server.AbstractConnector: Started ServerConnector@247e1f35{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2023-06-30 12:20:15,661 [main] INFO server.Server: Started @9000ms
scm1.org_1   | 2023-06-30 12:20:15,669 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2023-06-30 12:20:15,669 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2023-06-30 12:20:15,671 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2023-06-30 12:20:16,072 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:47020 / 172.25.0.117:47020
scm1.org_1   | 2023-06-30 12:20:16,089 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:20:16,723 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:35404 / 172.25.0.116:35404
scm1.org_1   | 2023-06-30 12:20:16,739 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:20:19,037 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO impl.FollowerState: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5073189871ns, electionTimeout:5062ms
scm1.org_1   | 2023-06-30 12:20:19,038 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: shutdown ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState
scm1.org_1   | 2023-06-30 12:20:19,039 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2023-06-30 12:20:19,043 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1.org_1   | 2023-06-30 12:20:19,043 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-FollowerState] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1
scm1.org_1   | 2023-06-30 12:20:19,046 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:19,047 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm1.org_1   | 2023-06-30 12:20:19,050 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:19,052 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.LeaderElection: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1.org_1   | 2023-06-30 12:20:19,052 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: shutdown ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1
scm1.org_1   | 2023-06-30 12:20:19,053 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm3.org_1   | 2023-06-30 12:21:33,537 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2023-06-30 12:21:33,822 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2023-06-30 12:21:33,822 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2023-06-30 12:21:34,104 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)] to file /data/metadata/scm/keys/secret_keys.json
scm3.org_1   | 2023-06-30 12:21:34,136 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:21:34,136 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-30 12:21:34,138 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2023-06-30 12:21:34,799 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:21:35,037 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:21:35,181 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:21:35,495 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2023-06-30 12:21:35,500 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 12:21:35,616 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2023-06-30 12:21:35,727 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-30 12:21:35,987 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3.org_1   | 2023-06-30 12:21:35,991 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-fc0df56f-3802-4ab7-bbd7-f7acacab0048: Detected pause in JVM or host machine approximately 0.191s with 0.241s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=241ms
scm3.org_1   | 2023-06-30 12:21:35,992 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 12:21:36,021 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2023-06-30 12:21:36,319 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2023-06-30 12:21:36,384 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-06-30 12:21:36,393 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2023-06-30 12:21:36,408 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2023-06-30 12:21:37,598 [main] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node fc0df56f-3802-4ab7-bbd7-f7acacab0048
scm3.org_1   | 2023-06-30 12:21:37,642 [main] INFO server.StorageContainerManager: Persist certificate serialId 299862451261 on Scm Bootstrap Node fc0df56f-3802-4ab7-bbd7-f7acacab0048
scm3.org_1   | 2023-06-30 12:21:37,731 [main] INFO security.RootCARotationManager: Monitor task for root certificate 1 is started with interval PT24H.
scm3.org_1   | 2023-06-30 12:21:38,162 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2023-06-30 12:21:38,180 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2023-06-30 12:21:38,199 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2023-06-30 12:21:38,290 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:21:38,290 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:21:38,583 [main] INFO util.log: Logging initialized @36998ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2023-06-30 12:21:41,326 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2023-06-30 12:21:41,423 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2023-06-30 12:21:41,449 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2023-06-30 12:21:41,449 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2023-06-30 12:21:41,449 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2023-06-30 12:21:41,468 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2023-06-30 12:21:42,147 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm3.org_1   | 2023-06-30 12:21:42,186 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2023-06-30 12:21:42,197 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm3.org_1   | 2023-06-30 12:21:42,925 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2023-06-30 12:21:42,932 [main] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2023-06-30 12:21:42,946 [main] INFO server.session: node0 Scavenging every 600000ms
scm3.org_1   | 2023-06-30 12:21:43,184 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-06-30 12:21:43,218 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d2aa0c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2023-06-30 12:21:43,219 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4305b87e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2023-06-30 12:21:43,309 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:21:43,311 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:21:44,327 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-06-30 12:21:30,988 [58e977b4-5318-453e-8f7b-12be220c6897-server-thread3] INFO server.RaftServer$Division: 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739: set configuration 17: peers:[fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-06-30 12:21:34,539 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:34,540 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:39,700 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:39,701 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:44,754 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:44,755 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:49,873 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:49,874 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:21:55,005 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:21:55,006 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:00,188 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:00,188 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:05,342 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:05,342 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:06,558 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:07,683 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:10,484 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:10,484 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:10,758 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:12,161 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:12,476 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:14,552 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:15,499 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:15,500 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:20,612 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:20,614 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:25,633 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:25,633 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:30,755 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:30,755 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:35,882 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:35,883 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:40,958 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-06-30 12:20:19,053 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1.org_1   | 2023-06-30 12:20:19,053 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm1.org_1   | 2023-06-30 12:20:19,059 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: change Leader from null to ce3545fb-3d5f-455d-aab8-f276e4d7a1ce at term 2 for becomeLeader, leader elected after 7805ms
scm1.org_1   | 2023-06-30 12:20:19,068 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-06-30 12:20:19,074 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 12:20:19,075 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-06-30 12:20:19,085 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-06-30 12:20:19,085 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-06-30 12:20:19,085 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-06-30 12:20:19,091 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-06-30 12:20:19,092 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-06-30 12:20:19,093 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO impl.RoleInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: start ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl
scm1.org_1   | 2023-06-30 12:20:19,098 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1.org_1   | 2023-06-30 12:20:19,103 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_0 to /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_0-0
scm1.org_1   | 2023-06-30 12:20:19,123 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/9c10c649-f59d-4933-ad5d-654b70cf4739/current/log_inprogress_1
scm1.org_1   | 2023-06-30 12:20:19,128 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderElection1] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 1: peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:19,135 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2023-06-30 12:20:19,136 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1.org_1   | 2023-06-30 12:20:19,137 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
scm1.org_1   | 2023-06-30 12:20:19,138 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:20:19,138 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137512Z, expire at: 2023-06-30T13:20:19.137512Z)
scm1.org_1   | 2023-06-30 12:20:19,138 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1.org_1   | 2023-06-30 12:20:19,139 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-06-30 12:20:19,139 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1.org_1   | 2023-06-30 12:20:19,140 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-06-30 12:20:19,149 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2023-06-30 12:20:19,264 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)]
scm1.org_1   | 2023-06-30 12:20:19,267 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)
scm1.org_1   | 2023-06-30 12:20:19,324 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 00eef41a-167d-422a-8485-740e0eefc759, creation at: 2023-06-30T12:20:19.137Z, expire at: 2023-06-30T13:20:19.137Z)] to file /data/metadata/scm/keys/secret_keys.json
scm1.org_1   | 2023-06-30 12:20:19,326 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:20:19,326 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-30 12:20:19,327 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2023-06-30 12:20:21,549 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: 86894888-4b88-49cd-b59e-c56ee77d7b03
scm1.org_1   | 2023-06-30 12:20:21,812 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:20:23,234 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:42100 / 172.25.0.117:42100
scm1.org_1   | 2023-06-30 12:20:23,264 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:20:23,266 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: 58e977b4-5318-453e-8f7b-12be220c6897
scm1.org_1   | 2023-06-30 12:20:23,493 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 12:20:23,724 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: Detected pause in JVM or host machine approximately 0.148s with 0.171s GC time.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=171ms
scm1.org_1   | 2023-06-30 12:20:26,006 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:20:43,660 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:46815 / 172.25.0.115:46815
scm1.org_1   | 2023-06-30 12:20:43,670 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:20:45,554 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:50626 / 172.25.0.116:50626
scm1.org_1   | 2023-06-30 12:20:45,620 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:20:47,299 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:37360 / 172.25.0.117:37360
scm1.org_1   | 2023-06-30 12:20:47,336 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:20:47,339 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: Submitting SetConfiguration request to Ratis server with new SCM peers list: [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-30 12:20:47,350 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: receive setConfiguration SetConfigurationRequest:client-36342F833DC7->ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 12:20:47,352 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-36342F833DC7->ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 12:20:47,379 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-30 12:20:47,381 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:20:47,381 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-30 12:20:47,388 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-30 12:20:47,389 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-30 12:20:47,390 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 12:20:47,390 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-30 12:20:47,391 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1.org_1   | 2023-06-30 12:20:47,391 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-06-30 12:20:47,392 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 12:20:47,417 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-06-30 12:20:47,431 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-GrpcLogAppender: send ce3545fb-3d5f-455d-aab8-f276e4d7a1ce->58e977b4-5318-453e-8f7b-12be220c6897#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-30 12:20:47,432 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 58e977b4-5318-453e-8f7b-12be220c6897
scm1.org_1   | 2023-06-30 12:20:48,634 [grpc-default-executor-2] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-InstallSnapshotResponseHandler: received the first reply ce3545fb-3d5f-455d-aab8-f276e4d7a1ce<-58e977b4-5318-453e-8f7b-12be220c6897#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-30 12:20:48,637 [grpc-default-executor-2] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-30 12:20:48,648 [grpc-default-executor-2] INFO leader.FollowerInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-30 12:20:48,648 [grpc-default-executor-2] INFO leader.FollowerInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-30 12:20:48,648 [grpc-default-executor-2] INFO leader.FollowerInfo: Follower ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897 acknowledged installing snapshot
scm1.org_1   | 2023-06-30 12:20:48,649 [grpc-default-executor-2] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm1.org_1   | 2023-06-30 12:20:48,865 [grpc-default-executor-0] WARN server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 12:20:48,868 [grpc-default-executor-0] INFO leader.FollowerInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->58e977b4-5318-453e-8f7b-12be220c6897: setNextIndex nextIndex: updateUnconditionally 9 -> 0
scm1.org_1   | 2023-06-30 12:20:49,899 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 9: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-30 12:20:49,991 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 11: peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:20:50,193 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 58e977b4-5318-453e-8f7b-12be220c6897.
scm1.org_1   | 2023-06-30 12:20:52,486 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:41884 / 172.25.0.117:41884
scm1.org_1   | 2023-06-30 12:20:52,519 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:20:56,138 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:35736 / 172.25.0.118:35736
scm1.org_1   | 2023-06-30 12:20:56,167 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:21:00,335 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:47228 / 172.25.0.118:47228
scm1.org_1   | 2023-06-30 12:21:00,341 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:21:00,343 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: fc0df56f-3802-4ab7-bbd7-f7acacab0048
scm1.org_1   | 2023-06-30 12:21:00,400 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-06-30 12:21:00,782 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:21:03,446 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:39054 / 172.25.0.116:39054
scm1.org_1   | 2023-06-30 12:21:03,495 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:21:21,157 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:55936 / 172.25.0.118:55936
scm1.org_1   | 2023-06-30 12:21:21,287 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:21:21,287 [IPC Server handler 15 on default port 9863] INFO ha.SCMRatisServerImpl: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: Submitting SetConfiguration request to Ratis server with new SCM peers list: [58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-06-30 12:21:21,295 [IPC Server handler 15 on default port 9863] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: receive setConfiguration SetConfigurationRequest:client-36342F833DC7->ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 12:21:21,296 [IPC Server handler 15 on default port 9863] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-36342F833DC7->ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739, cid=3, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-06-30 12:21:21,311 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-06-30 12:21:21,317 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-06-30 12:21:21,317 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-06-30 12:21:21,331 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-06-30 12:21:21,336 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-06-30 12:21:21,343 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-06-30 12:21:21,343 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1.org_1   | 2023-06-30 12:21:21,349 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1.org_1   | 2023-06-30 12:21:21,353 [IPC Server handler 15 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2023-06-30 12:22:40,959 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:45,624 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:50704 / 172.25.0.102:50704
scm2.org_1   | 2023-06-30 12:22:45,719 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 12:22:45,978 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:45,979 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:46,464 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:50312 / 172.25.0.104:50312
scm2.org_1   | 2023-06-30 12:22:46,623 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 12:22:49,367 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:50710 / 172.25.0.103:50710
scm2.org_1   | 2023-06-30 12:22:49,596 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 12:22:51,034 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:51,036 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:22:51,459 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/30c441e1-2cd9-4c36-9389-fe8b01b88a57
scm2.org_1   | 2023-06-30 12:22:51,495 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 30c441e1-2cd9-4c36-9389-fe8b01b88a57{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 428663643003, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-30 12:22:51,654 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 12:22:51,712 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-30 12:22:52,103 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c26c1b41-9621-4ea7-a0f2-632ca831791f
scm2.org_1   | 2023-06-30 12:22:52,235 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c26c1b41-9621-4ea7-a0f2-632ca831791f{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 427123221923, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-30 12:22:52,238 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-30 12:22:52,538 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 12:22:52,547 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:52,587 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:53,138 [IPC Server handler 15 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm2.org_1   | 2023-06-30 12:22:53,198 [IPC Server handler 15 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a9efabc3-aa7d-48e7-8786-4f3f9c9666fc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 431623557457, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-06-30 12:22:53,233 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 12:22:53,233 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2.org_1   | 2023-06-30 12:22:53,233 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2023-06-30 12:22:53,233 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2.org_1   | 2023-06-30 12:22:53,233 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2.org_1   | 2023-06-30 12:22:53,271 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-06-30 12:22:53,382 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:53,736 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:54,016 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:22:56,138 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:22:56,139 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:01,317 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:01,317 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:06,389 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:06,390 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:11,533 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:11,534 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:16,621 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:16,621 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:21,776 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:21,776 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:22,818 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:45302 / 172.25.0.102:45302
scm2.org_1   | 2023-06-30 12:23:22,886 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 12:23:24,158 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:44028 / 172.25.0.104:44028
scm2.org_1   | 2023-06-30 12:23:24,273 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 12:23:25,109 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:39856 / 172.25.0.103:39856
scm2.org_1   | 2023-06-30 12:23:25,347 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-06-30 12:23:25,898 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:23:25,997 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2.org_1   |   id: "db58cf25-deb4-4e5f-a339-31d80cad3087"
scm2.org_1   |   uuid128 {
scm2.org_1   |     mostSigBits: -2641133419895697825
scm2.org_1   |     leastSigBits: -6685257367859416953
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   | isLeader: false
scm2.org_1   | bytesWritten: 0
scm2.org_1   |  from dn=30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102).
scm2.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2.org_1   | 	... 3 more
scm2.org_1   | 2023-06-30 12:23:26,896 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:26,896 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:26,935 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 12:23:27,587 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2.org_1   |   id: "394fb16c-ab50-4148-8258-9491141422f5"
scm2.org_1   |   uuid128 {
scm3.org_1   | 2023-06-30 12:21:44,467 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6d3e720e{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-6307513108368694284/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2023-06-30 12:21:44,533 [main] INFO server.AbstractConnector: Started ServerConnector@6dc4b990{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2023-06-30 12:21:44,566 [main] INFO server.Server: Started @42986ms
scm3.org_1   | 2023-06-30 12:21:44,599 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2023-06-30 12:21:44,599 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2023-06-30 12:21:44,607 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2023-06-30 12:21:48,409 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:21:48,411 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:21:53,541 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:21:53,541 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:21:54,752 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-fc0df56f-3802-4ab7-bbd7-f7acacab0048: Detected pause in JVM or host machine approximately 0.123s with 0.186s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=186ms
scm3.org_1   | 2023-06-30 12:21:58,616 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:21:58,616 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:03,644 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:03,644 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:06,484 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:07,689 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:08,840 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:08,841 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:10,752 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:12,162 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:12,463 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:13,863 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:13,868 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:14,554 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:19,049 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:19,050 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:24,100 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:24,101 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:29,115 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:29,116 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:34,315 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:34,323 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:39,353 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:39,354 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:44,395 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:44,396 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:45,543 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:39164 / 172.25.0.102:39164
scm3.org_1   | 2023-06-30 12:22:45,713 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 12:22:46,349 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:32822 / 172.25.0.104:32822
scm3.org_1   | 2023-06-30 12:22:46,586 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 12:22:49,468 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:40532 / 172.25.0.103:40532
scm3.org_1   | 2023-06-30 12:22:49,508 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:49,509 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:49,647 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 12:22:51,029 [IPC Server handler 85 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/30c441e1-2cd9-4c36-9389-fe8b01b88a57
scm3.org_1   | 2023-06-30 12:22:51,278 [IPC Server handler 85 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 30c441e1-2cd9-4c36-9389-fe8b01b88a57{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 428663643003, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-30 12:22:51,452 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-30 12:22:51,508 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-30 12:22:52,111 [IPC Server handler 98 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c26c1b41-9621-4ea7-a0f2-632ca831791f
scm3.org_1   | 2023-06-30 12:22:52,129 [IPC Server handler 98 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c26c1b41-9621-4ea7-a0f2-632ca831791f{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 427123221923, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-30 12:22:52,143 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-30 12:22:52,270 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-30 12:22:52,281 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:52,469 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:53,166 [IPC Server handler 98 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm3.org_1   | 2023-06-30 12:22:53,167 [IPC Server handler 98 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a9efabc3-aa7d-48e7-8786-4f3f9c9666fc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 431623557457, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-06-30 12:22:53,176 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-30 12:22:53,184 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2023-06-30 12:22:53,184 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2023-06-30 12:22:53,184 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2023-06-30 12:22:53,184 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2023-06-30 12:22:53,216 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-06-30 12:22:53,512 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:53,749 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:54,068 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:22:54,593 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:54,601 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:22:54,594 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-fc0df56f-3802-4ab7-bbd7-f7acacab0048: Detected pause in JVM or host machine approximately 0.116s with 0.241s GC time.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=241ms
scm3.org_1   | 2023-06-30 12:22:59,697 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:22:59,698 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:04,868 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:04,868 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:09,918 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:09,918 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:14,972 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:14,973 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:20,047 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:20,048 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:22,816 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:44062 / 172.25.0.102:44062
scm3.org_1   | 2023-06-30 12:23:22,897 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 12:23:24,169 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:59616 / 172.25.0.104:59616
scm3.org_1   | 2023-06-30 12:23:24,269 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 12:23:25,159 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:37776 / 172.25.0.103:37776
scm3.org_1   | 2023-06-30 12:23:25,228 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:25,228 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:25,346 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-06-30 12:23:25,880 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:23:26,685 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3.org_1   |   id: "db58cf25-deb4-4e5f-a339-31d80cad3087"
scm3.org_1   |   uuid128 {
scm3.org_1   |     mostSigBits: -2641133419895697825
scm3.org_1   |     leastSigBits: -6685257367859416953
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   | isLeader: false
scm3.org_1   | bytesWritten: 0
scm3.org_1   |  from dn=30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102).
scm3.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3.org_1   | 	... 3 more
scm3.org_1   | 2023-06-30 12:23:26,981 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 12:23:27,642 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3.org_1   |   id: "394fb16c-ab50-4148-8258-9491141422f5"
scm3.org_1   |   uuid128 {
scm3.org_1   |     mostSigBits: 4129714463610782024
scm3.org_1   |     leastSigBits: -9054323700000349451
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   | isLeader: false
scm1.org_1   | 2023-06-30 12:21:21,354 [IPC Server handler 15 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-06-30 12:21:21,399 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-06-30 12:21:21,428 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-GrpcLogAppender: send ce3545fb-3d5f-455d-aab8-f276e4d7a1ce->fc0df56f-3802-4ab7-bbd7-f7acacab0048#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-06-30 12:21:21,429 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for fc0df56f-3802-4ab7-bbd7-f7acacab0048
scm1.org_1   | 2023-06-30 12:21:27,783 [grpc-default-executor-2] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-InstallSnapshotResponseHandler: received the first reply ce3545fb-3d5f-455d-aab8-f276e4d7a1ce<-fc0df56f-3802-4ab7-bbd7-f7acacab0048#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-06-30 12:21:27,785 [grpc-default-executor-2] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-06-30 12:21:27,785 [grpc-default-executor-2] INFO leader.FollowerInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048: matchIndex: setUnconditionally -1 -> 0
scm1.org_1   | 2023-06-30 12:21:27,786 [grpc-default-executor-2] INFO leader.FollowerInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-06-30 12:21:27,787 [grpc-default-executor-2] INFO leader.FollowerInfo: Follower ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048 acknowledged installing snapshot
scm1.org_1   | 2023-06-30 12:21:27,789 [grpc-default-executor-2] INFO server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
scm1.org_1   | 2023-06-30 12:21:28,296 [grpc-default-executor-2] WARN server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 12:21:28,301 [grpc-default-executor-2] INFO leader.FollowerInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048: setNextIndex nextIndex: updateUnconditionally 15 -> 0
scm1.org_1   | 2023-06-30 12:21:28,350 [grpc-default-executor-2] WARN server.GrpcLogAppender: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0
scm1.org_1   | 2023-06-30 12:21:28,351 [grpc-default-executor-2] INFO leader.FollowerInfo: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739->fc0df56f-3802-4ab7-bbd7-f7acacab0048: setNextIndex nextIndex: updateUnconditionally 15 -> 0
scm1.org_1   | 2023-06-30 12:21:30,868 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 15: peers:[fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-06-30 12:21:30,915 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-LeaderStateImpl] INFO server.RaftServer$Division: ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739: set configuration 17: peers:[fc0df56f-3802-4ab7-bbd7-f7acacab0048|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER, 58e977b4-5318-453e-8f7b-12be220c6897|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-06-30 12:21:31,029 [IPC Server handler 15 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: fc0df56f-3802-4ab7-bbd7-f7acacab0048.
scm1.org_1   | 2023-06-30 12:21:31,538 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:40194 / 172.25.0.116:40194
scm1.org_1   | 2023-06-30 12:21:31,826 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:21:37,121 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:33264 / 172.25.0.118:33264
scm1.org_1   | 2023-06-30 12:21:37,183 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:21:45,126 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:35131 / 172.25.0.115:35131
scm1.org_1   | 2023-06-30 12:21:45,175 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:21:57,656 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:36642 / 172.25.0.111:36642
scm1.org_1   | 2023-06-30 12:21:57,818 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:21:57,937 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:56834 / 172.25.0.113:56834
scm1.org_1   | 2023-06-30 12:21:58,025 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:21:59,623 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:47384 / 172.25.0.112:47384
scm1.org_1   | 2023-06-30 12:21:59,750 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:22:05,498 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:38162 / 172.25.0.104:38162
scm1.org_1   | 2023-06-30 12:22:05,674 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:05,682 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn aa09f3d2afc3, UUID: c26c1b41-9621-4ea7-a0f2-632ca831791f
scm1.org_1   | 2023-06-30 12:22:06,536 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:07,158 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:47282 / 172.25.0.102:47282
scm1.org_1   | 2023-06-30 12:22:07,302 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:07,305 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 56dd990c81df, UUID: 30c441e1-2cd9-4c36-9389-fe8b01b88a57
scm1.org_1   | 2023-06-30 12:22:07,687 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:08,997 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:38178 / 172.25.0.104:38178
scm1.org_1   | 2023-06-30 12:22:09,039 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-30 12:22:10,065 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:49510 / 172.25.0.103:49510
scm1.org_1   | 2023-06-30 12:22:10,263 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:10,267 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn b043fa92fe53, UUID: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm1.org_1   | 2023-06-30 12:22:10,453 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:55036 / 172.25.0.102:55036
scm1.org_1   | 2023-06-30 12:22:10,465 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-30 12:22:10,789 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:11,490 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:50184 / 172.25.0.111:50184
scm1.org_1   | 2023-06-30 12:22:11,602 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:11,653 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: 1026ed62-e8c2-4b1b-b618-b57678b6081c
scm1.org_1   | 2023-06-30 12:22:11,937 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:50866 / 172.25.0.112:50866
scm1.org_1   | 2023-06-30 12:22:11,982 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:11,988 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: da341f85-7b2f-40ca-8ed5-4a9439d740fe
scm1.org_1   | 2023-06-30 12:22:12,163 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:12,479 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:13,632 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:42546 / 172.25.0.103:42546
scm1.org_1   | 2023-06-30 12:22:13,659 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
scm1.org_1   | 2023-06-30 12:22:14,046 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:38826 / 172.25.0.113:38826
scm1.org_1   | 2023-06-30 12:22:14,080 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:14,105 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: 675a54a5-b075-4a14-81db-5e59d45ae13c
scm1.org_1   | 2023-06-30 12:22:14,543 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:34,781 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:36930 / 172.25.0.102:36930
scm1.org_1   | 2023-06-30 12:22:34,861 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:35,045 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:51118 / 172.25.0.104:51118
scm1.org_1   | 2023-06-30 12:22:35,089 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:38,527 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:51720 / 172.25.0.103:51720
scm1.org_1   | 2023-06-30 12:22:38,566 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:22:43,555 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:37146 / 172.25.0.116:37146
scm1.org_1   | 2023-06-30 12:22:43,709 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:22:45,220 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:41326 / 172.25.0.102:41326
scm1.org_1   | 2023-06-30 12:22:45,339 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 12:22:46,499 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:43916 / 172.25.0.104:43916
scm1.org_1   | 2023-06-30 12:22:46,736 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 12:22:47,608 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-ce3545fb-3d5f-455d-aab8-f276e4d7a1ce: Detected pause in JVM or host machine approximately 0.201s with 0.146s GC time.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=146ms
scm1.org_1   | 2023-06-30 12:22:49,284 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:33804 / 172.25.0.103:33804
scm1.org_1   | 2023-06-30 12:22:49,409 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 12:22:50,986 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/30c441e1-2cd9-4c36-9389-fe8b01b88a57
scm1.org_1   | 2023-06-30 12:22:51,082 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 30c441e1-2cd9-4c36-9389-fe8b01b88a57{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 428663643003, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-30 12:22:51,272 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-30 12:22:51,315 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 12:22:51,433 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=db58cf25-deb4-4e5f-a339-31d80cad3087 to datanode:30c441e1-2cd9-4c36-9389-fe8b01b88a57
scm1.org_1   | 2023-06-30 12:22:51,961 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:52,057 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: db58cf25-deb4-4e5f-a339-31d80cad3087, Nodes: 30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:51.426075Z[UTC]]
scm1.org_1   | 2023-06-30 12:22:52,090 [IPC Server handler 79 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c26c1b41-9621-4ea7-a0f2-632ca831791f
scm1.org_1   | 2023-06-30 12:22:52,102 [IPC Server handler 79 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c26c1b41-9621-4ea7-a0f2-632ca831791f{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 427123221923, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-30 12:22:52,103 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 12:22:52,123 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-30 12:22:52,178 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=394fb16c-ab50-4148-8258-9491141422f5 to datanode:c26c1b41-9621-4ea7-a0f2-632ca831791f
scm1.org_1   | 2023-06-30 12:22:52,338 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:52,341 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 394fb16c-ab50-4148-8258-9491141422f5, Nodes: c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:52.178867Z[UTC]]
scm1.org_1   | 2023-06-30 12:22:53,110 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm1.org_1   | 2023-06-30 12:22:53,166 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a9efabc3-aa7d-48e7-8786-4f3f9c9666fc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 431623557457, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-06-30 12:22:53,175 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 12:22:53,175 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2023-06-30 12:22:53,175 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2023-06-30 12:22:53,175 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2023-06-30 12:22:53,175 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2023-06-30 12:22:53,175 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-06-30 12:22:53,198 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d5b5f96c-9e99-4dc5-8f79-0b2da0e47784 to datanode:a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm2.org_1   |     mostSigBits: 4129714463610782024
scm2.org_1   |     leastSigBits: -9054323700000349451
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   | isLeader: false
scm2.org_1   | bytesWritten: 0
scm2.org_1   |  from dn=c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104).
scm2.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2.org_1   | 	... 3 more
scm2.org_1   | 2023-06-30 12:23:27,813 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:23:28,422 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2.org_1   |   id: "d5b5f96c-9e99-4dc5-8f79-0b2da0e47784"
scm2.org_1   |   uuid128 {
scm2.org_1   |     mostSigBits: -3047255327933641275
scm2.org_1   |     leastSigBits: -8108437363503106172
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   | isLeader: false
scm2.org_1   | bytesWritten: 0
scm2.org_1   |  from dn=a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103).
scm2.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2.org_1   | 	... 3 more
scm2.org_1   | 2023-06-30 12:23:28,546 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-06-30 12:23:28,969 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 12:23:29,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 12:23:32,014 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:32,015 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   | 2023-06-30 12:23:32,311 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 12:23:33,713 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | bytesWritten: 0
scm3.org_1   |  from dn=c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104).
scm3.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3.org_1   | 	... 3 more
scm3.org_1   | 2023-06-30 12:23:27,769 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:23:28,393 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3.org_1   |   id: "d5b5f96c-9e99-4dc5-8f79-0b2da0e47784"
scm3.org_1   |   uuid128 {
scm3.org_1   |     mostSigBits: -3047255327933641275
scm3.org_1   |     leastSigBits: -8108437363503106172
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   | isLeader: false
scm3.org_1   | bytesWritten: 0
scm3.org_1   |  from dn=a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103).
scm3.org_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:167)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3.org_1   | 	at com.sun.proxy.$Proxy21.updatePipelineState(Unknown Source)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3.org_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739 is not the leader ce3545fb-3d5f-455d-aab8-f276e4d7a1ce|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3.org_1   | 	... 3 more
scm3.org_1   | 2023-06-30 12:23:28,557 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-06-30 12:23:28,981 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 12:23:29,488 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 12:23:30,335 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:30,337 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:32,320 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 12:23:33,669 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 12:23:34,469 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-06-30 12:23:37,030 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2.org_1   | 2023-06-30 12:23:37,031 [58e977b4-5318-453e-8f7b-12be220c6897@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-06-30 12:22:53,322 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:53,339 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: d5b5f96c-9e99-4dc5-8f79-0b2da0e47784, Nodes: a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:53.198165Z[UTC]]
scm1.org_1   | 2023-06-30 12:22:53,506 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 to datanode:30c441e1-2cd9-4c36-9389-fe8b01b88a57
scm1.org_1   | 2023-06-30 12:22:53,569 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 to datanode:c26c1b41-9621-4ea7-a0f2-632ca831791f
scm1.org_1   | 2023-06-30 12:22:53,582 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 to datanode:a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm1.org_1   | 2023-06-30 12:22:53,684 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:53,695 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: b411bba0-46fb-490f-9c02-42d4f8d6f6d0, Nodes: 30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:53.506373Z[UTC]]
scm1.org_1   | 2023-06-30 12:22:53,799 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9977de93-017a-4e8b-97e3-2f9c7a71104c to datanode:30c441e1-2cd9-4c36-9389-fe8b01b88a57
scm1.org_1   | 2023-06-30 12:22:53,836 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9977de93-017a-4e8b-97e3-2f9c7a71104c to datanode:c26c1b41-9621-4ea7-a0f2-632ca831791f
scm1.org_1   | 2023-06-30 12:22:53,836 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9977de93-017a-4e8b-97e3-2f9c7a71104c to datanode:a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm1.org_1   | 2023-06-30 12:22:53,932 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:22:54,037 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=9977de93-017a-4e8b-97e3-2f9c7a71104c contains same datanodes as previous pipelines: PipelineID=b411bba0-46fb-490f-9c02-42d4f8d6f6d0 nodeIds: 30c441e1-2cd9-4c36-9389-fe8b01b88a57, c26c1b41-9621-4ea7-a0f2-632ca831791f, a9efabc3-aa7d-48e7-8786-4f3f9c9666fc
scm1.org_1   | 2023-06-30 12:22:54,037 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 9977de93-017a-4e8b-97e3-2f9c7a71104c, Nodes: 30c441e1-2cd9-4c36-9389-fe8b01b88a57(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)c26c1b41-9621-4ea7-a0f2-632ca831791f(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a9efabc3-aa7d-48e7-8786-4f3f9c9666fc(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-30T12:22:53.799570Z[UTC]]
scm1.org_1   | 2023-06-30 12:22:54,038 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-30 12:22:56,482 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:39610 / 172.25.0.111:39610
scm1.org_1   | 2023-06-30 12:22:56,780 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:22:57,979 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:46994 / 172.25.0.112:46994
scm1.org_1   | 2023-06-30 12:22:58,047 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:22:58,522 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:57380 / 172.25.0.113:57380
scm1.org_1   | 2023-06-30 12:22:58,753 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-06-30 12:23:22,846 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:57100 / 172.25.0.102:57100
scm1.org_1   | 2023-06-30 12:23:22,959 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 12:23:24,059 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1.org_1   | 2023-06-30 12:23:24,241 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:50280 / 172.25.0.104:50280
scm1.org_1   | 2023-06-30 12:23:24,317 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 12:23:25,181 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:33130 / 172.25.0.103:33130
scm1.org_1   | 2023-06-30 12:23:25,387 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-06-30 12:23:25,818 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:23:25,837 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=db58cf25-deb4-4e5f-a339-31d80cad3087
scm1.org_1   | 2023-06-30 12:23:25,892 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:26,137 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:36497 / 172.25.0.115:36497
scm1.org_1   | 2023-06-30 12:23:26,255 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:23:26,876 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:51108 / 172.25.0.111:51108
scm1.org_1   | 2023-06-30 12:23:26,948 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:26,983 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-30 12:23:27,693 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:23:27,766 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=394fb16c-ab50-4148-8258-9491141422f5
scm1.org_1   | 2023-06-30 12:23:27,771 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:28,125 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:51124 / 172.25.0.111:51124
scm1.org_1   | 2023-06-30 12:23:28,198 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:23:28,537 [ce3545fb-3d5f-455d-aab8-f276e4d7a1ce@group-654B70CF4739-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-06-30 12:23:28,562 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=d5b5f96c-9e99-4dc5-8f79-0b2da0e47784
scm1.org_1   | 2023-06-30 12:23:28,566 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:28,607 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:52280 / 172.25.0.113:52280
scm1.org_1   | 2023-06-30 12:23:28,692 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-30 12:23:28,766 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:56668 / 172.25.0.116:56668
scm1.org_1   | 2023-06-30 12:23:28,997 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:29,241 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-06-30 12:23:29,475 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:30,273 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:52284 / 172.25.0.113:52284
scm1.org_1   | 2023-06-30 12:23:30,329 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-06-30 12:23:32,333 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:33,689 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:34,135 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:48412 / 172.25.0.112:48412
scm1.org_1   | 2023-06-30 12:23:34,249 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
scm1.org_1   | 2023-06-30 12:23:34,495 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-06-30 12:23:35,430 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:48420 / 172.25.0.112:48420
scm1.org_1   | 2023-06-30 12:23:35,448 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm3.org_1   | 2023-06-30 12:23:34,456 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-06-30 12:23:35,440 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:35,440 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3.org_1   | 2023-06-30 12:23:40,536 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3.org_1   | 2023-06-30 12:23:40,536 [fc0df56f-3802-4ab7-bbd7-f7acacab0048@group-654B70CF4739-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
